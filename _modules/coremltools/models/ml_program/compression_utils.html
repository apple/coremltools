<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>coremltools.models.ml_program.compression_utils &mdash; coremltools API Reference  documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> coremltools API Reference
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">API Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/coremltools.converters.html">Converters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/coremltools.models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/coremltools.converters.mil.html">MIL Builder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/coremltools.converters.mil.input_types.html">MIL Input Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/coremltools.converters.mil.mil.ops.defs.html">MIL Ops</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://coremltools.readme.io/docs">Guides and examples</a></li>
<li class="toctree-l1"><a class="reference external" href="https://apple.github.io/coremltools/mlmodel/index.html">Core ML Format Specification</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/apple/coremltools">GitHub</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">coremltools API Reference</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
      <li>coremltools.models.ml_program.compression_utils</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for coremltools.models.ml_program.compression_utils</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) 2022, Apple Inc. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># Use of this source code is governed by a BSD-3-clause license that can be</span>
<span class="c1"># found in the LICENSE.txt file or at https://opensource.org/licenses/BSD-3-Clause</span>

<span class="kn">from</span> <span class="nn">coremltools.converters.mil.converter</span> <span class="kn">import</span> <span class="n">mil_convert</span> <span class="k">as</span> <span class="n">_mil_convert</span>
<span class="kn">from</span> <span class="nn">coremltools.converters.mil</span> <span class="kn">import</span> <span class="n">Operation</span> <span class="k">as</span> <span class="n">_Operation</span>
<span class="kn">from</span> <span class="nn">coremltools.converters.mil.frontend.milproto.load</span> <span class="kn">import</span> <span class="n">load</span> <span class="k">as</span> <span class="n">_milproto_to_pymil</span>
<span class="kn">from</span> <span class="nn">coremltools.converters.mil.mil.passes.compression_passes</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">WeightSparsifier</span> <span class="k">as</span> <span class="n">_WeightSparsifier</span><span class="p">,</span>
    <span class="n">WeightPalettizer</span> <span class="k">as</span> <span class="n">_WeightPalettizer</span><span class="p">,</span>
    <span class="n">WeightAffineQuantizer</span> <span class="k">as</span> <span class="n">_WeightAffineQuantizer</span><span class="p">,</span>
    <span class="n">WeightDecompressor</span> <span class="k">as</span> <span class="n">_WeightDecompressor</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">coremltools.converters.mil.mil.passes.quantization_passes</span> <span class="kn">import</span> <span class="n">AbstractQuantizationPass</span> <span class="k">as</span> <span class="n">_AbstractQuantizationPass</span>
<span class="kn">from</span> <span class="nn">coremltools</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_SPECIFICATION_VERSION_IOS_16</span><span class="p">,</span>
    <span class="n">ComputeUnit</span> <span class="k">as</span> <span class="n">_ComputeUnit</span>
<span class="p">)</span>

<span class="n">_DEFAULT_MIN_WEIGHT_SIZE_TO_COMPRESS</span> <span class="o">=</span> <span class="mi">2048</span>
<span class="n">_DEFAULT_SPECIFICATION_VERSION_FOR_COMPRESSION</span> <span class="o">=</span> <span class="n">_SPECIFICATION_VERSION_IOS_16</span>


<span class="k">def</span> <span class="nf">_default_op_selector</span><span class="p">(</span><span class="n">const_op</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">const_op</span><span class="p">,</span> <span class="n">_Operation</span><span class="p">)</span> <span class="ow">or</span> <span class="n">const_op</span><span class="o">.</span><span class="n">op_type</span> <span class="o">!=</span> <span class="s2">&quot;const&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input of the op_selector must be type of const Operation, got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">const_op</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">const_op</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="n">_DEFAULT_MIN_WEIGHT_SIZE_TO_COMPRESS</span>

<span class="k">def</span> <span class="nf">_apply_graph_pass</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">,</span> <span class="n">graph_pass</span><span class="p">):</span>
    <span class="c1"># Utility function which compresses a coreml model</span>
    <span class="c1"># convert the fully precision mlmodel into pymil program</span>
    <span class="n">model_spec</span> <span class="o">=</span> <span class="n">mlmodel</span><span class="o">.</span><span class="n">get_spec</span><span class="p">()</span>
    <span class="n">model_type</span> <span class="o">=</span> <span class="n">model_spec</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;Type&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">model_type</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;neuralNetwork&quot;</span><span class="p">,</span> <span class="s2">&quot;neuralNetworkClassifier&quot;</span><span class="p">,</span> <span class="s2">&quot;neuralNetworkRegressor&quot;</span><span class="p">,</span> <span class="s2">&quot;pipeline&quot;</span><span class="p">,</span> <span class="s2">&quot;PipelineClassifier&quot;</span><span class="p">,</span> <span class="s2">&quot;PipelineRegressor&quot;</span><span class="p">):</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;coremltools.compression_utils are meant to be used only with mlprogram typed coreml models. &quot;</span>
              <span class="s2">&quot;This model has type </span><span class="si">{}</span><span class="s2">. Please use coremltools.models.neural_network.quantization_utils.quantize_weights&quot;</span>
              <span class="s2">&quot;instead to compress the weights of the model.&quot;</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_type</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;mlProgram&quot;</span><span class="p">:</span>
        <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>    
       <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;weight compression not applicable for model type </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_type</span><span class="p">))</span>

    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">graph_pass</span><span class="p">,</span> <span class="n">_AbstractQuantizationPass</span><span class="p">),</span> <span class="s2">&quot;compression pass must be an AbstractQuantizationPass instance&quot;</span>
    
    <span class="n">program_spec</span> <span class="o">=</span> <span class="n">model_spec</span><span class="o">.</span><span class="n">mlProgram</span>
    <span class="n">model_specification_version</span> <span class="o">=</span> <span class="n">model_spec</span><span class="o">.</span><span class="n">specificationVersion</span>
    <span class="n">prog</span> <span class="o">=</span>  <span class="n">_milproto_to_pymil</span><span class="p">(</span>
        <span class="n">program_spec</span><span class="o">=</span><span class="n">program_spec</span><span class="p">,</span>
        <span class="n">specification_version</span><span class="o">=</span><span class="n">model_specification_version</span><span class="p">,</span>
        <span class="n">file_weights_dir</span><span class="o">=</span><span class="n">mlmodel</span><span class="o">.</span><span class="n">weights_dir</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">prog</span><span class="o">.</span><span class="n">skip_all_passes</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># apply compression graph pass</span>
    <span class="n">graph_pass</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">prog</span><span class="p">)</span>

    <span class="c1"># convert the pymil program back to mlmodel</span>
    <span class="n">compress_model_specification_version</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">model_specification_version</span><span class="p">,</span> <span class="n">_DEFAULT_SPECIFICATION_VERSION_FOR_COMPRESSION</span><span class="p">)</span>
    <span class="n">compressed_mlmodel</span> <span class="o">=</span> <span class="n">_mil_convert</span><span class="p">(</span>
        <span class="n">prog</span><span class="p">,</span>
        <span class="n">convert_to</span><span class="o">=</span><span class="s2">&quot;mlprogram&quot;</span><span class="p">,</span>
        <span class="n">convert_from</span><span class="o">=</span><span class="s2">&quot;milinternal&quot;</span><span class="p">,</span>
        <span class="n">specification_version</span><span class="o">=</span><span class="n">compress_model_specification_version</span><span class="p">,</span>
        <span class="n">compute_units</span><span class="o">=</span><span class="n">mlmodel</span><span class="o">.</span><span class="n">compute_unit</span><span class="p">,</span>
        <span class="n">model_description</span><span class="o">=</span><span class="n">model_spec</span><span class="o">.</span><span class="n">description</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">compressed_mlmodel</span>

<div class="viewcode-block" id="affine_quantize_weights"><a class="viewcode-back" href="../../../../source/coremltools.models.ml_program.html#coremltools.models.ml_program.compression_utils.affine_quantize_weights">[docs]</a><span class="k">def</span> <span class="nf">affine_quantize_weights</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;linear_symmetric&quot;</span><span class="p">,</span> <span class="n">op_selector</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Utility function to convert a float precision MLModel of type ``mlprogram`` that uses</span>
<span class="sd">    float-precision weights into a compressed MLModel that uses 8-bit weights. This is</span>
<span class="sd">    achieved by converting the float weight values that are stored in the ``const`` op</span>
<span class="sd">    into the ``constexpr_affine_dequantize`` op.</span>
<span class="sd">    </span>
<span class="sd">    This function uses affine quantization on the float weights, providing up to 2x</span>
<span class="sd">    savings in storage compared to float 16, or up to 4x savings compared to float 32.</span>
<span class="sd">    All computation at runtime uses float precision; the precision of the intermediate</span>
<span class="sd">    tensors and the compute precision of the ops are not altered.</span>
<span class="sd">    </span>
<span class="sd">    For each weight, this utility function converts the weight into the uint8 type using</span>
<span class="sd">    either `Linear interpolation` (``&quot;linear&quot;`` mode) or `Linear symmetric</span>
<span class="sd">    interpolation` (``&quot;linear_symmetric&quot;`` mode, the default).</span>
<span class="sd">    </span>
<span class="sd">    **Linear interpolation**</span>
<span class="sd">    </span>
<span class="sd">    Linear interpolation (``&quot;linear&quot;`` mode) maps the min/max of the float</span>
<span class="sd">    range to the range [0, 255] using a zero point (also called quantization bias, or</span>
<span class="sd">    offset) and a scale factor.</span>
<span class="sd">    </span>
<span class="sd">    ``&quot;linear&quot;`` mode uses the quantization formula ``w_r = s * (w_q - z)``, where:</span>
<span class="sd">    </span>
<span class="sd">        * ``w_r`` and  ``s`` are of type float.</span>
<span class="sd">        * ``w_r`` represents the float precision weight.</span>
<span class="sd">        * ``s`` represents the scale.</span>
<span class="sd">        * ``w_q`` and ``z`` are of type uint8.</span>
<span class="sd">        * ``w_q`` represents quantized weight.</span>
<span class="sd">        * ``z`` represents the zero point.</span>
<span class="sd">    </span>
<span class="sd">    Quantized weights are computed as follows:</span>
<span class="sd">    </span>
<span class="sd">        * ``w_q = cast_to_uint8(w_r / s + cast_to_float(z))``</span>
<span class="sd">        * Note: ``cast_to_uint8`` is the process of clipping the input to range [0, 255]</span>
<span class="sd">          followed by rounding and casting to uint8.</span>
<span class="sd">    </span>
<span class="sd">    In ``&quot;linear&quot;`` mode, ``s, z`` are computed by mapping the original float range</span>
<span class="sd">    ``[A, B]`` into the uint8 range [0, 255]. That is, you are solving the following</span>
<span class="sd">    linear equations:</span>
<span class="sd">    </span>
<span class="sd">        * ``B = s * (255 - z)``</span>
<span class="sd">        * ``A = s * (0 - z)``</span>
<span class="sd">    </span>
<span class="sd">    The equations result in the following:</span>
<span class="sd">    </span>
<span class="sd">        * ``s = (B - A) / 255``</span>
<span class="sd">        * ``z = cast_to_uint8(-255 * A / (B - A))``</span>
<span class="sd">    </span>
<span class="sd">    When the rank of weight ``w`` is 1, then ``s`` and ``z`` are both scalars. When the</span>
<span class="sd">    rank of the weight is greater than 1, then ``s`` and ``z`` are both vectors. In that</span>
<span class="sd">    case, scales are computed &quot;per channel&quot;, in which &quot;channel&quot; is the output dimension,</span>
<span class="sd">    which corresponds to the first dimension for ops such as ``conv`` and ``linear``, and</span>
<span class="sd">    the second dimension for the ``conv_transpose`` op.</span>
<span class="sd">    </span>
<span class="sd">    For ``&quot;linear&quot;`` mode, ``A = min(w_r), B = max(w_r)``.</span>
<span class="sd">    </span>
<span class="sd">    **Linear symmetric interpolation**</span>
<span class="sd">    </span>
<span class="sd">    With linear symmetric interpolation (``&quot;linear_symmetric&quot;`` mode, the default), rather than</span>
<span class="sd">    mapping the exact min/max of the float range to the quantized range,</span>
<span class="sd">    the function chooses the maximum absolute value between the min/max, which results in</span>
<span class="sd">    a zero point value of 127. The floating-point range is symmetric with respect to zero,</span>
<span class="sd">    and so is the quantized range.</span>
<span class="sd">    </span>
<span class="sd">    For ``&quot;linear_symmetric&quot;`` mode:</span>
<span class="sd">    </span>
<span class="sd">       * ``A = -R`` and ``B = R``, where ``R = max(abs(w_r))``.</span>
<span class="sd">       * This function maps to the range [0, 254].</span>
<span class="sd">       * The result is ``s=(B-A)/254`` --&gt; ``s=2R/254`` --&gt; ``s=R/127``.</span>
<span class="sd">       * Solving for ``z``: ``z = (R/2R) * 254`` --&gt; ``z=127``.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    mlmodel: MLModel</span>
<span class="sd">        Model to be quantized. This MLModel should be of type ``mlprogram``.</span>

<span class="sd">    mode: str</span>
<span class="sd">        Mode for linear quantization:</span>
<span class="sd">        </span>
<span class="sd">        * ``&quot;linear_symmetric&quot;`` (default): Input data are quantized in the range</span>
<span class="sd">          ``[-R, R]``, where ``R = max(abs(w_r))``.</span>
<span class="sd">        * ``&quot;linear&quot;``: Input data are quantized in the range</span>
<span class="sd">          ``[min(w_r), max(w_r)]``.</span>

<span class="sd">    op_selector: callable </span>
<span class="sd">        This function takes a single parameter with type ``coremltools.converters.mil.Const``;</span>
<span class="sd">        that is, a ``const`` operation. It returns a ``bool``: ``True`` to compress ``const_op``,</span>
<span class="sd">        otherwise ``False``. See the following examples:</span>
<span class="sd">        </span>
<span class="sd">        * All constants in the network are compressed:</span>
<span class="sd">          </span>
<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op):</span>
<span class="sd">                    return True</span>

<span class="sd">        * Only the constant with ``tensor.size &gt; 2048`` is compressed:</span>
<span class="sd">          </span>
<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op): </span>
<span class="sd">                    return const_op.val.val.size &gt; 2048</span>

<span class="sd">        * Compress the constant if it is the weight of a convolution layer</span>
<span class="sd">          and ``tensor.size &gt; 2048``:</span>
<span class="sd">          </span>
<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op):</span>
<span class="sd">                    return (const_op.val.val.size &gt; 2048 </span>
<span class="sd">                            and const_op.val.child_ops[0].op_type == &quot;conv&quot; </span>
<span class="sd">                            and const_op.val == const_op.val.child_ops[0].weight</span>
<span class="sd">                            )</span>

<span class="sd">        * When creating a custom ``op_selector`` function, the following attributes are helpful:</span>
<span class="sd">        </span>
<span class="sd">             * ``const_op.val.val``: The numpy array holding the value of the const.</span>
<span class="sd">             * ``const_op.val.child_ops``: A list of ops into which this constant is feeding.</span>
<span class="sd">             * ``const_op.val.child_ops[i].op_type``: The string corresponding to the op type</span>
<span class="sd">               of the i-th child op.</span>
<span class="sd">             * ``const_op.val.child_ops[i].name``: The string corresponding to the name the</span>
<span class="sd">               i-th child op.</span>

<span class="sd">        * If ``op_selector`` is not provided, it will be set to the behavior in which</span>
<span class="sd">          weights bigger than 2048 elements are compressed:</span>
<span class="sd">          </span>
<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op):</span>
<span class="sd">                    returm const_op.val.val.size &gt; 2048:</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    </span>
<span class="sd">    model: MLModel</span>
<span class="sd">        The quantized MLModel instance.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    </span>
<span class="sd">        &gt;&gt;&gt; import coremltools as ct</span>
<span class="sd">        &gt;&gt;&gt; model = ct.models.MLModel(&#39;my_model.mlpackage&#39;)</span>
<span class="sd">        &gt;&gt;&gt; compressed_model = ct.compression_utils.affine_quantize_weights(model, mode=&quot;linear_symmetric&quot;)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">op_selector</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">op_selector</span> <span class="o">=</span> <span class="n">_default_op_selector</span>
    <span class="n">affine_weight_quantizer</span> <span class="o">=</span> <span class="n">_WeightAffineQuantizer</span><span class="p">(</span><span class="n">fake_compression</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span> <span class="n">op_selector</span><span class="o">=</span><span class="n">op_selector</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_graph_pass</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">,</span> <span class="n">affine_weight_quantizer</span><span class="p">)</span></div>


<div class="viewcode-block" id="palettize_weights"><a class="viewcode-back" href="../../../../source/coremltools.models.ml_program.html#coremltools.models.ml_program.compression_utils.palettize_weights">[docs]</a><span class="k">def</span> <span class="nf">palettize_weights</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">,</span> <span class="n">nbits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;kmeans&quot;</span><span class="p">,</span> <span class="n">op_selector</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lut_function</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Utility function to convert a float precision MLModel of type ``mlprogram`` to a</span>
<span class="sd">    compressed MLModel by reducing the overall number of weights using a lookup table</span>
<span class="sd">    (LUT). A LUT contains a list float values. An `nbit` LUT has 2\ :sup:`nbits` entries.</span>
<span class="sd">    </span>
<span class="sd">    For example, a float weight vector such as ``{0.3, 0.3, 0.5, 0.5}`` can be compressed</span>
<span class="sd">    using a 1-bit LUT: ``{0.3, 0.5}``. In this case the float vector can be replaced</span>
<span class="sd">    with a 1-bit vector ``{0, 0, 1, 1}``.</span>
<span class="sd">    </span>
<span class="sd">    This function iterates over all the weights in the ``mlprogram``, discretizes its values,</span>
<span class="sd">    and constructs the LUT according to the algorithm specified in ``mode``. The float</span>
<span class="sd">    values are then converted to the `nbit` values, and the LUT is saved alongside each</span>
<span class="sd">    weight. The ``const`` ops storing weight values are replaced by</span>
<span class="sd">    ``constexpr_lut_to_dense`` ops.</span>
<span class="sd">    </span>
<span class="sd">    At runtime, the LUT and the `nbit` values are used to reconstruct the float weight</span>
<span class="sd">    values, which are then used to perform the float operaton the weight is feeding into. </span>
<span class="sd">    </span>
<span class="sd">    Consider the following example of ``&quot;uniform&quot;`` mode (a linear histogram):</span>
<span class="sd">    </span>
<span class="sd">        * ``nbits = 4``</span>
<span class="sd">        * ``mode = &quot;uniform&quot;``</span>
<span class="sd">        * ``weight = [0.11, 0.19, 0.3, 0.08, 0.0, 0.02]``</span>
<span class="sd">        </span>
<span class="sd">    The weight can be converted to a palette with indices ``[0, 1, 2, 3]`` (2 bits). The</span>
<span class="sd">    indices are a byte array.</span>
<span class="sd">    </span>
<span class="sd">    The data range ``[0.0, 0.3]`` is divided into 4 partitions linearly, which is</span>
<span class="sd">    ``[0.0, 0.1, 0.2, 0.3]``.</span>
<span class="sd">    </span>
<span class="sd">        * The LUT would be ``[0.0, 0.1, 0.2, 0.3]``.</span>
<span class="sd">     </span>
<span class="sd">        * The weight is rounded to ``[0.1, 0.2, 0.3, 0.1, 0.0, 0.0]``, and represented in</span>
<span class="sd">          the palette as indices ``[01b, 10b, 11b, 01b, 00b, 00b]``.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    mlmodel: MLModel</span>
<span class="sd">        Model to be converted by a LUT. This MLModel should be of type ``mlprogram``.</span>

<span class="sd">    nbits: int</span>
<span class="sd">        Number of bits per weight. Required for ``kmeans`` or ``uniform`` mode, but must</span>
<span class="sd">        not be set for ``unique`` or ``custom`` mode. A LUT would have</span>
<span class="sd">        2\ :sup:`nbits` entries, where `nbits` can be ``{1, 2, 4, 6, 8}``.</span>

<span class="sd">    mode: str</span>
<span class="sd">        Determine how the LUT is constructed by specifying one of the following:</span>
<span class="sd">        </span>
<span class="sd">        * ``&quot;kmeans&quot;`` (default): The LUT is generated by `k-means clustering`, a method of vector</span>
<span class="sd">          quantization that groups similar data points together to discover underlying</span>
<span class="sd">          patterns by using a fixed number (`k`) of clusters in a dataset. A cluster</span>
<span class="sd">          refers to a collection of data points aggregated together because of certain</span>
<span class="sd">          similarities. `nbits` is required.</span>

<span class="sd">        * ``&quot;uniform&quot;``: The LUT is generated by a linear histogram.</span>
<span class="sd">        </span>
<span class="sd">           - ``[v_min, v_min + scale, v_min + 2 * scale, ..., v_max]``</span>
<span class="sd">           - Where the weight is in the range ``[v_min, v_max]``, and</span>
<span class="sd">             ``scale = (v_max - v_min) / (1 &lt;&lt; nbits - 1)``.</span>
<span class="sd">           - ``nbits`` is required.</span>
<span class="sd">           </span>
<span class="sd">           A `histogram` is a representation of the distribution of a continuous variable,</span>
<span class="sd">           in which the entire range of values is divided into a series of intervals (or</span>
<span class="sd">           &quot;bins&quot;) and the representation displays how many values fall into each bin.</span>
<span class="sd">           Linear histograms have one bin at even intervals, such as one bin per integer.</span>
<span class="sd">          </span>
<span class="sd">        * ``&quot;unique&quot;``: The LUT is generated by unique values in the weights. The weights</span>
<span class="sd">          are assumed to be on a discrete lattice but stored in a float data type. This</span>
<span class="sd">          parameter identifies the weights and converts them into the palettized representation.</span>
<span class="sd">          </span>
<span class="sd">          Do not provide ``nbits`` for this mode. ``nbits`` is picked up automatically,</span>
<span class="sd">          with the smallest possible value in ``{1, 2, 4, 6, 8}`` such that the</span>
<span class="sd">          number of the unique values is ``&lt;= (1 &lt;&lt; nbits)``. If the weight has ``&gt; 256``</span>
<span class="sd">          unique values, the compression is skipped.</span>
<span class="sd">          </span>
<span class="sd">          For example:</span>
<span class="sd">          </span>
<span class="sd">          * If the weights are ``{0.1, 0.2, 0.3, 0.4}`` and ``nbits=2``, the weights are</span>
<span class="sd">            converted to ``{00b, 01b, 10b, 11b}``, and the generated LUT is</span>
<span class="sd">            ``[0.1, 0.2, 0.3, 0.4]``.</span>
<span class="sd">          * If the weights are ``{0.1, 0.2, 0.3, 0.4}`` and ``nbits=1``, nothing happens</span>
<span class="sd">            because the weights are not a 1-bit lattice.</span>
<span class="sd">          * If the weights are ``{0.1, 0.2, 0.3, 0.4, 0.5}`` and ``nbits=2``, nothing</span>
<span class="sd">            happens because the weights are not a 2-bit lattice.</span>
<span class="sd">          </span>
<span class="sd">        * ``&quot;custom&quot;``: The LUT and palettization parameters are calculated using a custom</span>
<span class="sd">          function. If this mode is selected then ``lut_function`` must be provided.</span>

<span class="sd">          Do not provide ``nbits`` for this mode. The user should customize ``nbits`` in the </span>
<span class="sd">          ``lut_function`` implementation.</span>

<span class="sd">    op_selector: callable </span>
<span class="sd">        This function takes a single parameter with type ``coremltools.converters.mil.Operation``.</span>
<span class="sd">        It returns a ``bool``: ``True`` to compress ``const_op``, otherwise ``False``.</span>
<span class="sd">        See the following examples:</span>
<span class="sd">        </span>
<span class="sd">        * All constants in the network are compressed:</span>
<span class="sd">          </span>
<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op):</span>
<span class="sd">                    return True</span>

<span class="sd">        * Only the constant with ``tensor.size &gt; 2048`` is compressed:</span>
<span class="sd">          </span>
<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op): </span>
<span class="sd">                    return const_op.val.val.size &gt; 2048</span>

<span class="sd">        * Compress the constant if it is the weight of a convolution layer</span>
<span class="sd">          and ``tensor.size &gt; 2048``:</span>
<span class="sd">          </span>
<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op):</span>
<span class="sd">                    return (const_op.val.val.size &gt; 2048 </span>
<span class="sd">                            and const_op.val.child_ops[0].op_type == &quot;conv&quot; </span>
<span class="sd">                            and const_op.val == const_op.val.child_ops[0].weight</span>
<span class="sd">                            )</span>

<span class="sd">        * When creating a custom ``op_selector`` function, the following attributes are helpful:</span>

<span class="sd">             * ``const_op.val.val``: The numpy array holding the value of the const.</span>
<span class="sd">             * ``const_op.val.child_ops``: A list of ops into which this constant is feeding.</span>
<span class="sd">             * ``const_op.val.child_ops[i].op_type``: The string corresponding to the op type</span>
<span class="sd">               of the i-th child op.</span>
<span class="sd">             * ``const_op.val.child_ops[i].name``: The string corresponding to the name the</span>
<span class="sd">               i-th child op.</span>

<span class="sd">        * If ``op_selector`` is not provided, it will be set to the behavior in which</span>
<span class="sd">          weights bigger than 2048 elements are compressed:</span>
<span class="sd">          </span>
<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op):</span>
<span class="sd">                    returm const_op.val.val.size &gt; 2048:</span>
<span class="sd">  </span>
<span class="sd">    lut_function: callable</span>
<span class="sd">        A callable function which computes the weight palettization parameters. This must</span>
<span class="sd">        be provided if the mode is set to ``&quot;custom&quot;``.</span>

<span class="sd">        weight: np.ndarray</span>
<span class="sd">            A float precision numpy array.</span>

<span class="sd">        Returns: lut: list[float]</span>
<span class="sd">            The lookup table.</span>

<span class="sd">        indices: list[int]</span>
<span class="sd">            A list of indices for each element.</span>

<span class="sd">        The following is an example that extract the ``top_k`` elements as the LUT. Given</span>
<span class="sd">        that ``weight = [0.1, 0.5, 0.3, 0.3, 0.5, 0.6, 0.7]``, the ``lut_function``</span>
<span class="sd">        produces ``lut = [0, 0.5, 0.6, 0.7], indices = [0, 1, 0, 0, 2, 3]``.</span>
<span class="sd">          </span>
<span class="sd">        .. sourcecode:: python</span>

<span class="sd">           def lut_function(weight):</span>
<span class="sd">                # In this example, we assume elements in the weights &gt;= 0</span>
<span class="sd">                weight = weight.flatten()</span>
<span class="sd">                nbits = 4</span>

<span class="sd">                # Get the LUT, from extracting top k maximum unique elements in the weight to be the LUT</span>
<span class="sd">                # Note that k = 1 &lt;&lt; nbits - 1, so we have the first element be 0</span>
<span class="sd">                unique_elements = np.unique(weight)</span>
<span class="sd">                k = (1 &lt;&lt; nbits) - 1</span>
<span class="sd">                top_k = np.partition(weight, -k)[-k:]</span>
<span class="sd">                np.sort(top_k)</span>
<span class="sd">                lut = [0.] + top_k.tolist()</span>

<span class="sd">                # Compute the indices</span>
<span class="sd">                mapping = {v: idx for idx, v in enumerate(lut)}</span>
<span class="sd">                indices = [mapping[v] if v in mapping else 0 for v in weight]</span>

<span class="sd">                return lut, indices</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    model: MLModel</span>
<span class="sd">        The palettized MLModel instance.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    </span>
<span class="sd">    .. sourcecode:: python</span>

<span class="sd">        &gt;&gt;&gt; import coremltools as ct</span>
<span class="sd">        &gt;&gt;&gt; model = ct.models.MLModel(&#39;my_model.mlpackage&#39;)</span>
<span class="sd">        &gt;&gt;&gt; compressed_model = ct.compression_utils.palettize_weights(model, mode=&quot;kmeans&quot;, nbits=4)</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">op_selector</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">op_selector</span> <span class="o">=</span> <span class="n">_default_op_selector</span>        
    <span class="n">weight_palettizer</span> <span class="o">=</span> <span class="n">_WeightPalettizer</span><span class="p">(</span><span class="n">nbits</span><span class="o">=</span><span class="n">nbits</span><span class="p">,</span> <span class="n">fake_compression</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">op_selector</span><span class="o">=</span><span class="n">op_selector</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span> <span class="n">lut_function</span><span class="o">=</span><span class="n">lut_function</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_graph_pass</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">,</span> <span class="n">weight_palettizer</span><span class="p">)</span></div>
    

<div class="viewcode-block" id="sparsify_weights"><a class="viewcode-back" href="../../../../source/coremltools.models.ml_program.html#coremltools.models.ml_program.compression_utils.sparsify_weights">[docs]</a><span class="k">def</span> <span class="nf">sparsify_weights</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;threshold_based&quot;</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">target_percentile</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">op_selector</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Utility function to convert a float precision MLModel of type ``mlprogram`` to a</span>
<span class="sd">    compressed MLModel using sparse representation. The ``const`` ops storing weight</span>
<span class="sd">    values are replaced by ``constexpr_sparse_to_dense`` ops.</span>
<span class="sd">    </span>
<span class="sd">    This function is useful if the model is trained with pruning techniques so that</span>
<span class="sd">    a lot of weights have zero values. If a large percentage of weight values are zero,</span>
<span class="sd">    a sparse representation is more efficient than a dense one (the default).</span>
<span class="sd">    </span>
<span class="sd">    The sparsified weights are stored in a bit mask. If the weight values are</span>
<span class="sd">    ``{0, 0, 0, 0, 0, 0, 0, 56.3}``, its sparse representation contains a bit mask with</span>
<span class="sd">    ones on locations where the value is non-zero: ``00000001b``. This is accompanied by</span>
<span class="sd">    non-zero data, which is a size-1 vector of value ``{56.3}``.</span>

<span class="sd">    For example, given the following:</span>
<span class="sd">    </span>
<span class="sd">        * ``weight = [0.3, 0, 0, 0.5, 0, 0]``</span>
<span class="sd">        * ``non_zero_data, bit_mask = sparsify(weight)``</span>
<span class="sd">    </span>
<span class="sd">    The indices of the non-zero elements are:</span>
<span class="sd">    </span>
<span class="sd">        * ``non_zero_data = [0.3, 0.5]``</span>
<span class="sd">        * ``bit_mask = &quot;100100&quot;``</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    mlmodel: MLModel</span>
<span class="sd">        Model to be sparsified. This MLModel should be of type ``mlprogram``.</span>

<span class="sd">    mode: str</span>
<span class="sd">        Determine the scheme to sparsify the model by specifying one of the following:</span>
<span class="sd">        </span>
<span class="sd">        * ``&quot;threshold_based&quot;`` (default): All the absolute weight values that are smaller</span>
<span class="sd">          than ``threshold`` are changed to 0, and the tensor is stored in a sparse format.</span>
<span class="sd">          For example, given the following:</span>

<span class="sd">               * ``weight = [0.3, -0.2, -0.01, 0.05]``</span>
<span class="sd">               * ``threshold = 0.03``</span>

<span class="sd">          The sparsified weight would be ``[0.3, -0.2, 0, 0.05]``.</span>

<span class="sd">        * ``&quot;percentile_based&quot;``: Sparsify the weight with a constant sparsity percentile,</span>
<span class="sd">          which is ``target_percentile``. Where </span>
<span class="sd">          ``n = floor(size_of_weight_tensor * target_percentile)``, the ``n`` lowest</span>
<span class="sd">          absolute weight values are changed to 0. For example, given the following:</span>

<span class="sd">               * ``weight = [0.3, -0.2, -0.01, 0.05]``</span>
<span class="sd">               * ``target_percentile = 0.75``</span>

<span class="sd">          The sparsified weight would be ``[0.3, 0, 0, 0]``.</span>

<span class="sd">    threshold: float</span>
<span class="sd">        Required when ``mode = &quot;prune_threshold&quot;``. The absolute threshold to sparsify the weight.</span>

<span class="sd">    target_percentile: float</span>
<span class="sd">        Required when ``mode = &quot;percentile_based&quot;``. The percentage of sparsity for</span>
<span class="sd">        compression, which needs to be in the range [0, 1]. When 0, no sparsification</span>
<span class="sd">        occurs. For 1, all weights become 0.</span>

<span class="sd">    op_selector: callable </span>
<span class="sd">        This function takes a single parameter with type ``coremltools.converters.mil.Operation``.</span>
<span class="sd">        It returns a ``bool``: ``True`` to compress ``const_op``, otherwise ``False``.</span>
<span class="sd">        See the following examples:</span>
<span class="sd">        </span>
<span class="sd">        * All constants in the network are compressed:</span>
<span class="sd">          </span>
<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op):</span>
<span class="sd">                    return True</span>

<span class="sd">        * Only the constant with ``tensor.size &gt; 2048`` is compressed:</span>
<span class="sd">          </span>
<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op): </span>
<span class="sd">                    return const_op.val.val.size &gt; 2048</span>

<span class="sd">        * Compress the constant if it is the weight of a convolution layer</span>
<span class="sd">          and ``tensor.size &gt; 2048``:</span>
<span class="sd">          </span>
<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op):</span>
<span class="sd">                    return (const_op.val.val.size &gt; 2048 </span>
<span class="sd">                            and const_op.val.child_ops[0].op_type == &quot;conv&quot; </span>
<span class="sd">                            and const_op.val == const_op.val.child_ops[0].weight</span>
<span class="sd">                            )</span>

<span class="sd">        * When creating a custom ``op_selector`` function, the following attributes are helpful:</span>
<span class="sd">        </span>
<span class="sd">             * ``const_op.val.val``: The numpy array holding the value of the const.</span>
<span class="sd">             * ``const_op.val.child_ops``: A list of ops into which this constant is feeding.</span>
<span class="sd">             * ``const_op.val.child_ops[i].op_type``: The string corresponding to the op type</span>
<span class="sd">               of the i-th child op.</span>
<span class="sd">             * ``const_op.val.child_ops[i].name``: The string corresponding to the name the</span>
<span class="sd">               i-th child op.</span>

<span class="sd">        * If ``op_selector`` is not provided, it will be set to the behavior in which</span>
<span class="sd">          weights bigger than 2048 elements are compressed:</span>
<span class="sd">          </span>
<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op):</span>
<span class="sd">                    returm const_op.val.val.size &gt; 2048:</span>
<span class="sd">  </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    model: MLModel</span>
<span class="sd">        The sparse MLModel instance.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    .. sourcecode:: python</span>

<span class="sd">        &gt;&gt;&gt; import coremltools as ct</span>
<span class="sd">        &gt;&gt;&gt; model = ct.models.MLModel(&#39;my_model.mlpackage&#39;)</span>
<span class="sd">        &gt;&gt;&gt; compressed_model = ct.compression_utils.sparsify_weights(model, mode=&quot;threshold_based&quot;, threshold=0.01)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">op_selector</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">op_selector</span> <span class="o">=</span> <span class="n">_default_op_selector</span>
    <span class="n">weight_sparsifier</span> <span class="o">=</span> <span class="n">_WeightSparsifier</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span> <span class="n">target_percentile</span><span class="o">=</span><span class="n">target_percentile</span><span class="p">,</span> <span class="n">op_selector</span><span class="o">=</span><span class="n">op_selector</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_graph_pass</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">,</span> <span class="n">weight_sparsifier</span><span class="p">)</span></div>

<span class="k">def</span> <span class="nf">decompress_weights</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Utility function to convert weights that are sparse or palettized or affine quantized, back to the float format.</span>
<span class="sd">    That is, convert any of the follwing three ops:</span>
<span class="sd">    </span>
<span class="sd">    (1) constexpr_affine_dequantize</span>
<span class="sd">    (2) constexpr_lut_to_dense</span>
<span class="sd">    (3) constexpr_sparse_to_dense</span>
<span class="sd">    </span>
<span class="sd">    to mb.const</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    mlmodel: MLModel</span>
<span class="sd">        Model which will be decompressed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    model: MLModel</span>
<span class="sd">        The MLModel with no constexpr ops included.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    .. sourcecode:: python</span>

<span class="sd">        &gt;&gt;&gt; import coremltools as ct</span>
<span class="sd">        &gt;&gt;&gt; model = ct.models.MLModel(&#39;my_compressed_model.mlpackage&#39;)</span>
<span class="sd">        &gt;&gt;&gt; decompressed_model = ct.compression_utils.decompress_weights(model)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">weight_decompressor</span> <span class="o">=</span> <span class="n">_WeightDecompressor</span><span class="p">(</span><span class="n">op_selector</span><span class="o">=</span><span class="k">lambda</span> <span class="n">op</span><span class="p">:</span> <span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_graph_pass</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">,</span> <span class="n">weight_decompressor</span><span class="p">)</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Apple Inc.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>