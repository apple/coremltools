<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>coremltools.converters.mil.mil.passes.defs.optimize_linear &mdash; coremltools API Reference 8.0b1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../_static/graphviz.css?v=fd3f3429" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../_static/sg_gallery.css?v=d2d258e8" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../_static/sg_gallery-binder.css?v=f4aeca0c" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../_static/css/norightmargin.css?v=eea1f72d" />

  
  <!--[if lt IE 9]>
    <script src="../../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../../../../../_static/documentation_options.js?v=d50bc636"></script>
        <script src="../../../../../../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../../index.html" class="icon icon-home">
            coremltools API Reference
          </a>
              <div class="version">
                8.0b1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">API Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../source/coremltools.converters.html">Converters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../source/coremltools.models.html">Model APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../source/coremltools.converters.mil.html">MIL Builder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../source/coremltools.converters.mil.input_types.html">MIL Input Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../source/coremltools.converters.mil.mil.ops.defs.html">MIL Ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../source/coremltools.converters.mil.mil.passes.defs.html">MIL Graph Passes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../source/coremltools.optimize.html">Optimizers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://apple.github.io/coremltools/docs-guides/index.html">Guide and Examples</a></li>
<li class="toctree-l1"><a class="reference external" href="https://apple.github.io/coremltools/mlmodel/index.html">Core ML Format Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../source/api-versions.html">Previous Versions</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/apple/coremltools">GitHub</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../index.html">coremltools API Reference</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">coremltools.converters.mil.mil.passes.defs.optimize_linear</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for coremltools.converters.mil.mil.passes.defs.optimize_linear</h1><div class="highlight"><pre>
<span></span><span class="c1">#  Copyright (c) 2023, Apple Inc. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1">#  Use of this source code is governed by a BSD-3-clause license that can be</span>
<span class="c1">#  found in the LICENSE.txt file or at https://opensource.org/licenses/BSD-3-Clause</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">coremltools.converters.mil.mil</span> <span class="kn">import</span> <span class="n">Block</span>
<span class="kn">from</span> <span class="nn">coremltools.converters.mil.mil</span> <span class="kn">import</span> <span class="n">Builder</span> <span class="k">as</span> <span class="n">mb</span>
<span class="kn">from</span> <span class="nn">coremltools.converters.mil.mil</span> <span class="kn">import</span> <span class="n">Operation</span><span class="p">,</span> <span class="n">Program</span><span class="p">,</span> <span class="n">Var</span>
<span class="kn">from</span> <span class="nn">coremltools.converters.mil.mil.passes.graph_pass</span> <span class="kn">import</span> <span class="n">AbstractGraphPass</span>
<span class="kn">from</span> <span class="nn">coremltools.converters.mil.mil.passes.helper</span> <span class="kn">import</span> <span class="n">block_context_manager</span>
<span class="kn">from</span> <span class="nn">coremltools.converters.mil.mil.passes.pass_registry</span> <span class="kn">import</span> <span class="n">register_pass</span>


<div class="viewcode-block" id="fuse_linear_bias">
<a class="viewcode-back" href="../../../../../../../source/coremltools.converters.mil.mil.passes.defs.html#coremltools.converters.mil.mil.passes.defs.optimize_linear.fuse_linear_bias">[docs]</a>
<span class="nd">@register_pass</span><span class="p">(</span><span class="n">namespace</span><span class="o">=</span><span class="s2">&quot;common&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">fuse_linear_bias</span><span class="p">(</span><span class="n">AbstractGraphPass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Convert ``linear + add/sub`` to a single ``linear`` by updating the weight and bias of the ``linear`` layer.</span>

<span class="sd">    .. code-block::</span>

<span class="sd">        Example 1:</span>
<span class="sd">            Original:</span>
<span class="sd">                %4 = linear(x=%1, weight=%2, bias=%3) # %2 is a rank-2 const tensor (weight)</span>
<span class="sd">                                                      # %3 is a rank-1 const tensor (bias)</span>
<span class="sd">                ...</span>
<span class="sd">                %6 = add(x=%4, y=%5) # %5 is a const tensor with same shape as %3</span>

<span class="sd">            Result:</span>
<span class="sd">                %8 = linear(x=%1, weight=%2, bias=%7) # where %7 is a new const tensor with value</span>
<span class="sd">                                                      # %7 = %3 + %6</span>

<span class="sd">        Example 2:</span>
<span class="sd">            Original:</span>
<span class="sd">                %4 = linear(x=%1, weight=%2, bias=%3) # %2 is a rank-2 const tensor (weight)</span>
<span class="sd">                                                      # %3 is a rank-1 const tensor (bias)</span>
<span class="sd">                ...</span>
<span class="sd">                %6 = sub(x=%5, y=%4) # %5 is a const tensor with a broacasable shape with %3.</span>
<span class="sd">                                       i.e. if %3 has shape (Dout), %5 could be (1, Dout).</span>

<span class="sd">            Result:</span>
<span class="sd">                %9 = linear(x=%1, weight=%7, bias=%8) # where %7 is a new const tensor with value %7 = -%2</span>
<span class="sd">                                                      # %8 = %5 - %3</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prog</span><span class="p">:</span> <span class="n">Program</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">prog</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">block_changed</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">while</span> <span class="n">block_changed</span><span class="p">:</span>
                <span class="n">block_changed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fuse_linear_bias_block</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_try_to_transform</span><span class="p">(</span><span class="n">linear_op</span><span class="p">,</span> <span class="n">add_or_sub_op</span><span class="p">,</span> <span class="n">block</span><span class="p">):</span>

        <span class="k">if</span> <span class="n">add_or_sub_op</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">val</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">add_or_sub_op</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">val</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="n">is_sub</span> <span class="o">=</span> <span class="n">add_or_sub_op</span><span class="o">.</span><span class="n">op_type</span> <span class="o">==</span> <span class="s2">&quot;sub&quot;</span>
        <span class="n">is_first_input</span> <span class="o">=</span> <span class="n">add_or_sub_op</span><span class="o">.</span><span class="n">x</span> <span class="o">==</span> <span class="n">linear_op</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Return if weight or bias are missing values</span>
        <span class="k">if</span> <span class="n">linear_op</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">val</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">linear_op</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">val</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="c1"># compute the new bias</span>
        <span class="n">linear_bias</span> <span class="o">=</span> <span class="n">linear_op</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">val</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">add_or_sub_op</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">val</span> <span class="k">if</span> <span class="n">is_first_input</span> <span class="k">else</span> <span class="n">add_or_sub_op</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">val</span>

        <span class="c1"># check if the shape is broadcasable</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">linear_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="n">Dout</span> <span class="o">=</span> <span class="n">linear_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">Dout</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="p">(</span><span class="n">Dout</span><span class="p">,))</span>

        <span class="k">if</span> <span class="n">is_sub</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">is_first_input</span><span class="p">:</span>
                <span class="n">bias</span> <span class="o">=</span> <span class="o">-</span><span class="n">bias</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">linear_bias</span> <span class="o">=</span> <span class="o">-</span><span class="n">linear_bias</span>

        <span class="n">new_bias</span> <span class="o">=</span> <span class="n">linear_bias</span> <span class="o">+</span> <span class="n">bias</span>

        <span class="c1"># compute the new weight</span>
        <span class="k">if</span> <span class="n">is_sub</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_first_input</span><span class="p">:</span>
            <span class="n">new_weight</span> <span class="o">=</span> <span class="o">-</span><span class="n">linear_op</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">val</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_weight</span> <span class="o">=</span> <span class="n">linear_op</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">val</span>

        <span class="c1"># create a new linear op with the new weight, bias value, copying rest of the attributes</span>
        <span class="n">out_name</span> <span class="o">=</span> <span class="n">add_or_sub_op</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">name</span>
        <span class="n">linear_kargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="n">new_weight</span><span class="p">,</span>
            <span class="s2">&quot;bias&quot;</span><span class="p">:</span> <span class="n">new_bias</span><span class="p">,</span>
            <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">out_name</span><span class="p">,</span>
            <span class="s2">&quot;before_op&quot;</span><span class="p">:</span> <span class="n">linear_op</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">linear_op</span><span class="o">.</span><span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">]:</span>
                <span class="k">continue</span>
            <span class="n">linear_kargs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">mb</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="o">**</span><span class="n">linear_kargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">add_or_sub_op</span><span class="o">.</span><span class="n">enclosing_block</span><span class="o">.</span><span class="n">try_replace_uses_of_var_after_op</span><span class="p">(</span>
            <span class="n">anchor_op</span><span class="o">=</span><span class="n">add_or_sub_op</span><span class="p">,</span>
            <span class="n">old_var</span><span class="o">=</span><span class="n">add_or_sub_op</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">new_var</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="n">add_or_sub_op</span><span class="o">.</span><span class="n">enclosing_block</span><span class="o">.</span><span class="n">remove_ops</span><span class="p">([</span><span class="n">linear_op</span><span class="p">,</span> <span class="n">add_or_sub_op</span><span class="p">])</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="nd">@block_context_manager</span>
    <span class="k">def</span> <span class="nf">_fuse_linear_bias_block</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">_find_candicate_op</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">op_type</span> <span class="o">!=</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">None</span>
            <span class="c1"># abort fusion if op output is also a block output</span>
            <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">enclosing_block</span><span class="o">.</span><span class="n">outputs</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">None</span>
            <span class="c1"># find add/sub op</span>
            <span class="n">child_ops</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">child_ops</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">child_ops</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">op_candidate</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">child_ops</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">op_candidate</span><span class="o">.</span><span class="n">op_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;sub&quot;</span><span class="p">]:</span>
                    <span class="k">return</span> <span class="n">op_candidate</span>

        <span class="n">fusion_occurred</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">operations</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">enclosing_block</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
                <span class="n">block_changed</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">while</span> <span class="n">block_changed</span><span class="p">:</span>
                    <span class="n">block_changed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fuse_linear_bias_block</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">blocks</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># This op can&#39;t be conv or conv_transpose</span>
                <span class="k">continue</span>

            <span class="n">add_or_sub_op</span> <span class="o">=</span> <span class="n">_find_candicate_op</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">add_or_sub_op</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_try_to_transform</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">add_or_sub_op</span><span class="p">,</span> <span class="n">block</span><span class="p">):</span>
                    <span class="n">fusion_occurred</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="n">fusion_occurred</span></div>



<div class="viewcode-block" id="fuse_matmul_weight_bias">
<a class="viewcode-back" href="../../../../../../../source/coremltools.converters.mil.mil.passes.defs.html#coremltools.converters.mil.mil.passes.defs.optimize_linear.fuse_matmul_weight_bias">[docs]</a>
<span class="nd">@register_pass</span><span class="p">(</span><span class="n">namespace</span><span class="o">=</span><span class="s2">&quot;common&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">fuse_matmul_weight_bias</span><span class="p">(</span><span class="n">AbstractGraphPass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Convert ``matmul + add/sub`` to ``linear`` whenever possible.</span>

<span class="sd">    .. code-block::</span>

<span class="sd">        Given:</span>
<span class="sd">            %3 = matmul(x=%1, y=%2)  # %1 or %2 is const and rank 2 (weight)</span>
<span class="sd">            ...</span>
<span class="sd">            %5 = add(x=%3, y=%4) # %4 is const. add(x=%4, y=%3) is equivalent</span>
<span class="sd">                                 # sub is similar.</span>

<span class="sd">        Result:</span>
<span class="sd">            # assuming %2 above is const and rank 2</span>
<span class="sd">            %5 = linear(x=%1, weight=%2, bias=%4)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prog</span><span class="p">:</span> <span class="n">Program</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">prog</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">block_changed</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">while</span> <span class="n">block_changed</span><span class="p">:</span>
                <span class="n">block_changed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fuse_matmul_weight_bias_block</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_find_candidate_op</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
        <span class="n">_CHILD_OP_TYPES</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;sub&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">op_type</span> <span class="o">!=</span> <span class="s2">&quot;matmul&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="c1"># find add</span>
        <span class="n">child_ops</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">child_ops</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">child_ops</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">add_op_candidate</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">child_ops</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">add_op_candidate</span><span class="o">.</span><span class="n">op_type</span> <span class="ow">in</span> <span class="n">_CHILD_OP_TYPES</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">add_op_candidate</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_transpose</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">before_op</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transpose the last 2 dims.</span>

<span class="sd">        - ``v``: (Var, must be a tensor).</span>
<span class="sd">        - ``before_op``: (Operation) The op right before the newly added ``transpose`` op.</span>
<span class="sd">        - ``name``: Name for the ``transpose`` op if provided.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">rank</span><span class="p">))</span>
        <span class="n">perm</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">perm</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">perm</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">mb</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">v</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="n">perm</span><span class="p">,</span> <span class="n">before_op</span><span class="o">=</span><span class="n">before_op</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">mb</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">v</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="n">perm</span><span class="p">,</span> <span class="n">before_op</span><span class="o">=</span><span class="n">before_op</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_try_to_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">matmul_op</span><span class="p">,</span> <span class="n">add_op</span><span class="p">,</span> <span class="n">block</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">matmul_op</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">val</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">matmul_op</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">val</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># This is a dynamic matmul.</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">add_op</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">val</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">add_op</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">val</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># This is a dynamic add.</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="n">x_is_weight</span> <span class="o">=</span> <span class="n">matmul_op</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">val</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">x_is_weight</span><span class="p">:</span>
            <span class="n">weight</span><span class="p">,</span> <span class="n">linear_x</span> <span class="o">=</span> <span class="n">matmul_op</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">matmul_op</span><span class="o">.</span><span class="n">y</span>
            <span class="n">transpose_weight</span> <span class="o">=</span> <span class="n">matmul_op</span><span class="o">.</span><span class="n">transpose_x</span><span class="o">.</span><span class="n">val</span>
            <span class="n">transpose_x</span> <span class="o">=</span> <span class="n">matmul_op</span><span class="o">.</span><span class="n">transpose_y</span><span class="o">.</span><span class="n">val</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">weight</span><span class="p">,</span> <span class="n">linear_x</span> <span class="o">=</span> <span class="n">matmul_op</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">matmul_op</span><span class="o">.</span><span class="n">x</span>
            <span class="n">transpose_weight</span> <span class="o">=</span> <span class="n">matmul_op</span><span class="o">.</span><span class="n">transpose_y</span><span class="o">.</span><span class="n">val</span>
            <span class="n">transpose_x</span> <span class="o">=</span> <span class="n">matmul_op</span><span class="o">.</span><span class="n">transpose_x</span><span class="o">.</span><span class="n">val</span>

        <span class="c1"># We potentially are going to transpose the weight, so if the weight itself is not removable, we skip this path</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">nonreplaceable_vars_upstream</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="n">linear_x</span><span class="o">.</span><span class="n">rank</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">weight</span><span class="o">.</span><span class="n">rank</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="c1"># We don&#39;t support these cases yet.</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="c1"># For those weights which are the input for more than one op,</span>
        <span class="c1"># we don&#39;t do the fusion.</span>
        <span class="c1"># The reason is that it might cause memory explosion by adding</span>
        <span class="c1"># those weight as a numpy array in the inner product or</span>
        <span class="c1"># the batch_mat_mul kernel.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">child_ops</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="n">d_out</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">transpose_weight</span> <span class="k">else</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">add_op</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">val</span> <span class="k">if</span> <span class="n">add_op</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">val</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">add_op</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">val</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="n">d</span> <span class="o">!=</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]):</span>
                <span class="k">return</span>  <span class="c1"># cannot transform</span>

            <span class="c1"># squeeze leading dims of size 1</span>
            <span class="n">bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">bias</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">d_out</span><span class="p">:</span>
            <span class="k">return</span>  <span class="c1"># cannot transform</span>

        <span class="k">if</span> <span class="n">add_op</span><span class="o">.</span><span class="n">op_type</span> <span class="o">==</span> <span class="s2">&quot;sub&quot;</span><span class="p">:</span>
            <span class="n">bias</span> <span class="o">=</span> <span class="o">-</span><span class="n">bias</span>
        <span class="n">out_name</span> <span class="o">=</span> <span class="n">add_op</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">name</span>

        <span class="k">if</span> <span class="n">x_is_weight</span><span class="p">:</span>
            <span class="c1"># If transpose_x == transpose_weight == False:</span>
            <span class="c1"># w*x = (x^T w^T)^T = linear(x^T, w)^T</span>
            <span class="n">x_transposed</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_transpose</span><span class="p">(</span><span class="n">linear_x</span><span class="p">,</span> <span class="n">before_op</span><span class="o">=</span><span class="n">matmul_op</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">transpose_x</span> <span class="k">else</span> <span class="n">linear_x</span>
            <span class="p">)</span>
            <span class="n">w_no_transpose</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">weight</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">transpose_weight</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transpose</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">before_op</span><span class="o">=</span><span class="n">matmul_op</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">mb</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_transposed</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">w_no_transpose</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">before_op</span><span class="o">=</span><span class="n">matmul_op</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">before_op</span><span class="o">=</span><span class="n">matmul_op</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">out_name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># If transpose_x == transpose_weight == False</span>
            <span class="c1"># x*w = x*(w^T)^T = linear(x, w^T)</span>
            <span class="n">x_no_transpose</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_transpose</span><span class="p">(</span><span class="n">linear_x</span><span class="p">,</span> <span class="n">before_op</span><span class="o">=</span><span class="n">matmul_op</span><span class="p">)</span> <span class="k">if</span> <span class="n">transpose_x</span> <span class="k">else</span> <span class="n">linear_x</span>
            <span class="p">)</span>
            <span class="n">w_transposed</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">weight</span> <span class="k">if</span> <span class="n">transpose_weight</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transpose</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">before_op</span><span class="o">=</span><span class="n">matmul_op</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">mb</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span>
                <span class="n">x</span><span class="o">=</span><span class="n">x_no_transpose</span><span class="p">,</span>
                <span class="n">weight</span><span class="o">=</span><span class="n">w_transposed</span><span class="p">,</span>
                <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
                <span class="n">before_op</span><span class="o">=</span><span class="n">matmul_op</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="n">out_name</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">add_op</span><span class="o">.</span><span class="n">enclosing_block</span><span class="o">.</span><span class="n">try_replace_uses_of_var_after_op</span><span class="p">(</span>
            <span class="n">anchor_op</span><span class="o">=</span><span class="n">add_op</span><span class="p">,</span>
            <span class="n">old_var</span><span class="o">=</span><span class="n">add_op</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">new_var</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="n">add_op</span><span class="o">.</span><span class="n">enclosing_block</span><span class="o">.</span><span class="n">remove_ops</span><span class="p">([</span><span class="n">matmul_op</span><span class="p">,</span> <span class="n">add_op</span><span class="p">])</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="nd">@block_context_manager</span>
    <span class="k">def</span> <span class="nf">_fuse_matmul_weight_bias_block</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block</span><span class="p">):</span>
        <span class="n">fusion_occurred</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">operations</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">enclosing_block</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
                <span class="n">block_changed</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">while</span> <span class="n">block_changed</span><span class="p">:</span>
                    <span class="n">block_changed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fuse_matmul_weight_bias_block</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">blocks</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># This op can&#39;t be matmul</span>
                <span class="k">continue</span>

            <span class="n">add_op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_find_candidate_op</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">add_op</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_try_to_transform</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">add_op</span><span class="p">,</span> <span class="n">block</span><span class="p">):</span>
                    <span class="n">fusion_occurred</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="n">fusion_occurred</span></div>



<div class="viewcode-block" id="fuse_transpose_matmul">
<a class="viewcode-back" href="../../../../../../../source/coremltools.converters.mil.mil.passes.defs.html#coremltools.converters.mil.mil.passes.defs.optimize_linear.fuse_transpose_matmul">[docs]</a>
<span class="nd">@register_pass</span><span class="p">(</span><span class="n">namespace</span><span class="o">=</span><span class="s2">&quot;common&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">fuse_transpose_matmul</span><span class="p">(</span><span class="n">AbstractGraphPass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fuse ``transpose + matmul`` to ``matmul`` if possible,</span>
<span class="sd">    since ``matmul`` has args ``transpose_x`` and ``transpose_y`` to transpose last 2 dims</span>

<span class="sd">    .. code-block::</span>

<span class="sd">        Positive example:</span>
<span class="sd">            Input graph:</span>
<span class="sd">                transpose(x=x, perm=(1, 0)) -|</span>
<span class="sd">                                             |-&gt; matmul(x=transposed_x, y=transposed_y)</span>
<span class="sd">                transpose(x=y, perm=(1, 0)) -|</span>

<span class="sd">            Output graph:</span>
<span class="sd">                matmul(x=x, y=y, transpose_x=True, transpose_y=True)</span>

<span class="sd">        Negative example:</span>
<span class="sd">            Input graph:</span>
<span class="sd">                transpose(x=x, perm=(1, 0, 2)) -|</span>
<span class="sd">                                                |-&gt; matmul(x=transposed_x, y=transposed_y)</span>
<span class="sd">                transpose(x=y, perm=(1, 0, 2)) -|</span>

<span class="sd">            Output graph:</span>
<span class="sd">                Same to input graph, nothing changes</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prog</span><span class="p">:</span> <span class="n">Program</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">prog</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fuse_transpose_matmul_block</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="nd">@block_context_manager</span>
    <span class="k">def</span> <span class="nf">_fuse_transpose_matmul_block</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block</span><span class="p">:</span> <span class="n">Block</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># use shallow copy to hide changes on block.operations during the loop,</span>
        <span class="c1"># since we try fusion when loop to matmul, which will not affect downstream</span>
        <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">operations</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_fuse_transpose_matmul_block</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">op_type</span> <span class="o">==</span> <span class="s2">&quot;matmul&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_try_fuse_transpose_matmul</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">block</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">is_transposed_and_fusable_to_matmul</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Var</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        1. check if x is transposed</span>
<span class="sd">        2. check if x is transposed in the last 2 dimensions,</span>
<span class="sd">           since the transpose arg in matmul only transposes the last 2 dimensions</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># x is not transposed, False</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">op</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">op_type</span> <span class="o">!=</span> <span class="s2">&quot;transpose&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="n">rank</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">rank</span>
        <span class="c1"># if transposing a rank &lt; 2 tensor, it is a noop and will be elimianted by noop_elimination</span>
        <span class="k">if</span> <span class="n">rank</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="c1"># canonicalize the input permutation to compare with last-2-dim permutation below</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">perm</span><span class="o">.</span><span class="n">val</span>
        <span class="n">perm</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">perm</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)]</span> <span class="o">+=</span> <span class="n">rank</span>
        <span class="n">perm</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span> <span class="o">-=</span> <span class="n">rank</span>

        <span class="c1"># permuting only last 2 dims should look like (0, 1, ..., -1, -2)</span>
        <span class="n">perm_only_last_2_dims</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
        <span class="n">perm_only_last_2_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">perm_only_last_2_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">perm</span> <span class="o">==</span> <span class="n">perm_only_last_2_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_try_fuse_transpose_matmul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">:</span> <span class="n">Operation</span><span class="p">,</span> <span class="n">block</span><span class="p">:</span> <span class="n">Block</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">op</span><span class="o">.</span><span class="n">op_type</span> <span class="o">==</span> <span class="s2">&quot;matmul&quot;</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">x</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">y</span>
        <span class="n">transpose_x</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">transpose_x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">op</span><span class="o">.</span><span class="n">transpose_x</span><span class="o">.</span><span class="n">val</span>
        <span class="n">transpose_y</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">transpose_y</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">op</span><span class="o">.</span><span class="n">transpose_y</span><span class="o">.</span><span class="n">val</span>

        <span class="n">is_x_transposed_and_fusable_to_matmul</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_transposed_and_fusable_to_matmul</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">is_y_transposed_and_fusable_to_matmul</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_transposed_and_fusable_to_matmul</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="c1"># if neither x nor y is transposed and fuseable with matmul, nothing we need to do</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_x_transposed_and_fusable_to_matmul</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_y_transposed_and_fusable_to_matmul</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="n">is_x_transposed_and_fusable_to_matmul</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">x</span>
            <span class="n">transpose_x</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">transpose_x</span>
        <span class="k">if</span> <span class="n">is_y_transposed_and_fusable_to_matmul</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">x</span>
            <span class="n">transpose_y</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">transpose_y</span>

        <span class="n">fused_transpose_matmul</span> <span class="o">=</span> <span class="n">mb</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
            <span class="n">transpose_x</span><span class="o">=</span><span class="n">transpose_x</span><span class="p">,</span>
            <span class="n">transpose_y</span><span class="o">=</span><span class="n">transpose_y</span><span class="p">,</span>
            <span class="n">before_op</span><span class="o">=</span><span class="n">op</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="n">op</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">block</span><span class="o">.</span><span class="n">replace_uses_of_var_after_op</span><span class="p">(</span>
            <span class="n">anchor_op</span><span class="o">=</span><span class="n">op</span><span class="p">,</span>
            <span class="n">old_var</span><span class="o">=</span><span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">new_var</span><span class="o">=</span><span class="n">fused_transpose_matmul</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">op</span><span class="o">.</span><span class="n">remove_from_block</span><span class="p">()</span></div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Apple Inc.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>