<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>coremltools.models.neural_network.quantization_utils &mdash; coremltools API Reference  documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> coremltools API Reference
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">API Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/coremltools.converters.html">Converters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/coremltools.models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/coremltools.converters.mil.html">MIL Builder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/coremltools.converters.mil.input_types.html">MIL Input Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/coremltools.converters.mil.mil.ops.defs.html">MIL Ops</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://coremltools.readme.io/docs">Guides and examples</a></li>
<li class="toctree-l1"><a class="reference external" href="https://apple.github.io/coremltools/mlmodel/index.html">Core ML Format Specification</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/apple/coremltools">GitHub</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">coremltools API Reference</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
      <li>coremltools.models.neural_network.quantization_utils</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for coremltools.models.neural_network.quantization_utils</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) 2017, Apple Inc. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># Use of this source code is governed by a BSD-3-clause license that can be</span>
<span class="c1"># found in the LICENSE.txt file or at https://opensource.org/licenses/BSD-3-Clause</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Utilities to compress Neural Network Models.</span>
<span class="sd">Only available in coremltools 2.0b1 and onwards</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">_np</span>
<span class="kn">from</span> <span class="nn">sys</span> <span class="kn">import</span> <span class="n">stdout</span> <span class="k">as</span> <span class="n">_stdout</span>
<span class="kn">from</span> <span class="nn">os</span> <span class="kn">import</span> <span class="n">listdir</span> <span class="k">as</span> <span class="n">_listdir</span>
<span class="kn">from</span> <span class="nn">.optimization_utils</span> <span class="kn">import</span> <span class="n">_optimize_nn</span>

<span class="kn">from</span> <span class="nn">coremltools.models</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_SUPPORTED_QUANTIZATION_MODES</span><span class="p">,</span>
    <span class="n">_QUANTIZATION_MODE_DEQUANTIZE</span><span class="p">,</span>
    <span class="n">_QUANTIZATION_MODE_LOOKUP_TABLE_LINEAR</span><span class="p">,</span>
    <span class="n">_QUANTIZATION_MODE_LOOKUP_TABLE_KMEANS</span><span class="p">,</span>
    <span class="n">_QUANTIZATION_MODE_CUSTOM_LOOKUP_TABLE</span><span class="p">,</span>
    <span class="n">_QUANTIZATION_MODE_LINEAR_QUANTIZATION</span><span class="p">,</span>
    <span class="n">_QUANTIZATION_MODE_LINEAR_SYMMETRIC</span><span class="p">,</span>
    <span class="n">_LUT_BASED_QUANTIZATION</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="n">_get_nn_layers</span><span class="p">,</span> <span class="n">_wp_to_fp16wp</span><span class="p">,</span> <span class="n">_get_model</span><span class="p">,</span> <span class="n">_macos_version</span>
<span class="kn">from</span> <span class="nn">..._deps</span> <span class="kn">import</span> <span class="n">_HAS_SKLEARN</span> <span class="k">as</span> <span class="n">_HAS_SKLEARN</span>
<span class="kn">from</span> <span class="nn">...</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_MINIMUM_QUANTIZED_MODEL_SPEC_VERSION</span><span class="p">,</span>
    <span class="n">_MINIMUM_FP16_SPEC_VERSION</span><span class="p">,</span>
    <span class="n">_SPECIFICATION_VERSION_IOS_14</span><span class="p">,</span>
<span class="p">)</span>


<div class="viewcode-block" id="QuantizedLayerSelector"><a class="viewcode-back" href="../../../../source/coremltools.models.neural_network.html#coremltools.models.neural_network.quantization_utils.QuantizedLayerSelector">[docs]</a><span class="k">class</span> <span class="nc">QuantizedLayerSelector</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; This is the base class to implement custom selectors to skip certain</span>
<span class="sd">    layers during quantization. To implement a custom selector, create a class</span>
<span class="sd">    that inherits this class and override `do_quantize()` method.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    .. highlight:: python</span>
<span class="sd">    .. code-block:: python</span>

<span class="sd">        class MyLayerSelector(QuantizedLayerSelector):</span>
<span class="sd">            def __init__(self):</span>
<span class="sd">                super(MyLayerSelector, self).__init__()</span>

<span class="sd">            def do_quantize(self, layer, **kwargs):</span>
<span class="sd">                ret = super(MyLayerSelector, self).do_quantize(layer)</span>
<span class="sd">                if not ret or layer.name == &#39;dense_2&#39;:</span>
<span class="sd">                    return False</span>
<span class="sd">                return True</span>

<span class="sd">        selector = MyLayerSelector()</span>
<span class="sd">        quantized_model = quantize_weights(mlmodel, 8, quantization_mode=&#39;linear&#39;, selector=selector)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quantizable_layer_types</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;convolution&quot;</span><span class="p">,</span>
            <span class="s2">&quot;innerProduct&quot;</span><span class="p">,</span>
            <span class="s2">&quot;embedding&quot;</span><span class="p">,</span>
            <span class="s2">&quot;embeddingND&quot;</span><span class="p">,</span>
            <span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span>
            <span class="s2">&quot;scale&quot;</span><span class="p">,</span>
            <span class="s2">&quot;bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;loadConstant&quot;</span><span class="p">,</span>
            <span class="s2">&quot;simpleRecurrent&quot;</span><span class="p">,</span>
            <span class="s2">&quot;gru&quot;</span><span class="p">,</span>
            <span class="s2">&quot;uniDirectionalLSTM&quot;</span><span class="p">,</span>
            <span class="s2">&quot;biDirectionalLSTM&quot;</span><span class="p">,</span>
            <span class="s2">&quot;batchedMatmul&quot;</span><span class="p">,</span>
            <span class="s2">&quot;depthwiseConv&quot;</span><span class="p">,</span>
            <span class="s2">&quot;loop&quot;</span><span class="p">,</span>
            <span class="s2">&quot;branch&quot;</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">do_quantize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">layer</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;layer&quot;</span><span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantizable_layer_types</span></div>


<div class="viewcode-block" id="AdvancedQuantizedLayerSelector"><a class="viewcode-back" href="../../../../source/coremltools.models.neural_network.html#coremltools.models.neural_network.quantization_utils.AdvancedQuantizedLayerSelector">[docs]</a><span class="k">class</span> <span class="nc">AdvancedQuantizedLayerSelector</span><span class="p">(</span><span class="n">QuantizedLayerSelector</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Quantized layer selector allowing the user to specify some types of</span>
<span class="sd">    layers to skip during quantization process and the minimum size parameters</span>
<span class="sd">    in quantized convolution layers.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    .. highlight:: python</span>
<span class="sd">    .. code-block:: python</span>

<span class="sd">        from coremltools.models.neural_network.quantization_utils import AdvancedQuantizedLayerSelector</span>
<span class="sd">        selector = AdvancedQuantizedLayerSelector(</span>
<span class="sd">                skip_layer_types=[&#39;batchnorm&#39;, &#39;bias&#39;, &#39;depthwiseConv&#39;],</span>
<span class="sd">                minimum_conv_kernel_channels=4,</span>
<span class="sd">                minimum_conv_weight_count=4096)</span>
<span class="sd">        quantized_model = quantize_weights(model, 8, selector=selector)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">skip_layer_types</span><span class="o">=</span><span class="p">[],</span>
        <span class="n">minimum_conv_kernel_channels</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">minimum_conv_weight_count</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
    <span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">AdvancedQuantizedLayerSelector</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">skip_layer_types</span> <span class="o">=</span> <span class="n">skip_layer_types</span>

        <span class="c1"># Error checking</span>
        <span class="n">invalid_skip_types</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">lt</span> <span class="ow">in</span> <span class="n">skip_layer_types</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">lt</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantizable_layer_types</span><span class="p">:</span>
                <span class="n">invalid_skip_types</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lt</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">invalid_skip_types</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">err_msg</span> <span class="o">=</span> <span class="s2">&quot;Skip quantization layer types (</span><span class="si">{}</span><span class="s2">) is not supported.</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">invalid_skip_types</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">err_msg</span> <span class="o">+=</span> <span class="s2">&quot;Supported quantization layers: (</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">quantizable_layer_types</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">err_msg</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">minimum_conv_kernel_channels</span> <span class="o">=</span> <span class="n">minimum_conv_kernel_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">minimum_conv_weight_count</span> <span class="o">=</span> <span class="n">minimum_conv_weight_count</span>

<div class="viewcode-block" id="AdvancedQuantizedLayerSelector.do_quantize"><a class="viewcode-back" href="../../../../source/coremltools.models.neural_network.html#coremltools.models.neural_network.quantization_utils.AdvancedQuantizedLayerSelector.do_quantize">[docs]</a>    <span class="k">def</span> <span class="nf">do_quantize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">weight_param</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; weight_param - should be name of the WeightParam field</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">AdvancedQuantizedLayerSelector</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">do_quantize</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">ret</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="n">layer_type</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;layer&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">layer_type</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">skip_layer_types</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;convolution&quot;</span><span class="p">:</span>
            <span class="n">oc</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">convolution</span><span class="o">.</span><span class="n">outputChannels</span>
            <span class="n">kc</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">convolution</span><span class="o">.</span><span class="n">kernelChannels</span>
            <span class="n">kh</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">convolution</span><span class="o">.</span><span class="n">kernelSize</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">kw</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">convolution</span><span class="o">.</span><span class="n">kernelSize</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">groups</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">convolution</span><span class="o">.</span><span class="n">nGroups</span>
            <span class="n">counts</span> <span class="o">=</span> <span class="n">oc</span> <span class="o">*</span> <span class="n">kc</span> <span class="o">*</span> <span class="n">kh</span> <span class="o">*</span> <span class="n">kw</span>
            <span class="n">has_bias</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">convolution</span><span class="o">.</span><span class="n">hasBias</span>

            <span class="k">if</span> <span class="n">weight_param</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">weight_param</span> <span class="o">==</span> <span class="s2">&quot;weights&quot;</span><span class="p">:</span>
                <span class="k">if</span> <span class="s2">&quot;depthwiseConv&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">skip_layer_types</span> <span class="ow">and</span> <span class="n">kc</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">groups</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">return</span> <span class="kc">False</span>

                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">kc</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">minimum_conv_kernel_channels</span>
                    <span class="ow">or</span> <span class="n">counts</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">minimum_conv_weight_count</span>
                <span class="p">):</span>
                    <span class="k">return</span> <span class="kc">False</span>

            <span class="k">elif</span> <span class="n">weight_param</span> <span class="o">==</span> <span class="s2">&quot;bias&quot;</span><span class="p">:</span>
                <span class="k">return</span> <span class="ow">not</span> <span class="s2">&quot;bias&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">skip_layer_types</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Unrecognized quantization weight field </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">weight_param</span><span class="p">)</span>
                <span class="p">)</span>

        <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;innerProduct&quot;</span> <span class="ow">or</span> <span class="s2">&quot;batchedMatmul&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">weight_param</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">weight_param</span> <span class="o">==</span> <span class="s2">&quot;weights&quot;</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">True</span>
            <span class="k">if</span> <span class="n">weight_param</span> <span class="o">==</span> <span class="s2">&quot;bias&quot;</span><span class="p">:</span>
                <span class="k">return</span> <span class="ow">not</span> <span class="s2">&quot;bias&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">skip_layer_types</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Unrecognized quantization weight field </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">weight_param</span><span class="p">)</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="kc">True</span></div></div>


<div class="viewcode-block" id="MatrixMultiplyLayerSelector"><a class="viewcode-back" href="../../../../source/coremltools.models.neural_network.html#coremltools.models.neural_network.quantization_utils.MatrixMultiplyLayerSelector">[docs]</a><span class="k">class</span> <span class="nc">MatrixMultiplyLayerSelector</span><span class="p">(</span><span class="n">QuantizedLayerSelector</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Layer selector object that allows users to select matrix multiplication layers</span>
<span class="sd">        with one of the matrices being constant, based on some criterions like total</span>
<span class="sd">        numbers of parameters/weights, number of input or output channels and/or layer</span>
<span class="sd">        names. If any of the criterion is not valid, the corresponding layer is not</span>
<span class="sd">        selected.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">minimum_weight_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">minimum_input_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">minimum_output_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">maximum_input_channels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">maximum_output_channels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">include_layers_with_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">MatrixMultiplyLayerSelector</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># weight count refers to number of parameters/weights and is equal to product of input &amp; output channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">minimum_weight_count</span> <span class="o">=</span> <span class="n">minimum_weight_count</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">minimum_input_channels</span> <span class="o">=</span> <span class="n">minimum_input_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">minimum_output_channels</span> <span class="o">=</span> <span class="n">minimum_output_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maximum_input_channels</span> <span class="o">=</span> <span class="n">maximum_input_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maximum_output_channels</span> <span class="o">=</span> <span class="n">maximum_output_channels</span>
        <span class="k">if</span> <span class="n">include_layers_with_names</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">include_layers_with_names</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">include_layers_with_names</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>
            <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span>
                <span class="p">[</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_layers_with_names</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Property &#39;include_layers_with_names&#39; must be a list/tuple of str objects&quot;</span>
            <span class="p">)</span>

<div class="viewcode-block" id="MatrixMultiplyLayerSelector.do_quantize"><a class="viewcode-back" href="../../../../source/coremltools.models.neural_network.html#coremltools.models.neural_network.quantization_utils.MatrixMultiplyLayerSelector.do_quantize">[docs]</a>    <span class="k">def</span> <span class="nf">do_quantize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">weight_param</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; weight_param - should be name of the WeightParam field</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">MatrixMultiplyLayerSelector</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">do_quantize</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">ret</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="n">layer_type</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;layer&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">layer_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;innerProduct&quot;</span><span class="p">,</span> <span class="s2">&quot;batchedMatmul&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">weight_param</span> <span class="o">==</span> <span class="s2">&quot;bias&quot;</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">True</span>
            <span class="k">elif</span> <span class="n">weight_param</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">weight_param</span> <span class="o">==</span> <span class="s2">&quot;weights&quot;</span><span class="p">:</span>

                <span class="k">if</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;innerProduct&quot;</span><span class="p">:</span>
                    <span class="n">ic</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">innerProduct</span><span class="o">.</span><span class="n">inputChannels</span>
                    <span class="n">oc</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">innerProduct</span><span class="o">.</span><span class="n">outputChannels</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">ic</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">batchedMatmul</span><span class="o">.</span><span class="n">weightMatrixFirstDimension</span>
                    <span class="n">oc</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">batchedMatmul</span><span class="o">.</span><span class="n">weightMatrixSecondDimension</span>

                <span class="n">wc</span> <span class="o">=</span> <span class="n">ic</span> <span class="o">*</span> <span class="n">oc</span>

                <span class="k">if</span> <span class="n">wc</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">minimum_weight_count</span><span class="p">:</span>
                    <span class="k">return</span> <span class="kc">False</span>
                <span class="k">if</span> <span class="n">ic</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">minimum_input_channels</span><span class="p">:</span>
                    <span class="k">return</span> <span class="kc">False</span>
                <span class="k">if</span> <span class="n">oc</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">minimum_output_channels</span><span class="p">:</span>
                    <span class="k">return</span> <span class="kc">False</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">maximum_input_channels</span> <span class="ow">and</span> <span class="n">ic</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">maximum_input_channels</span><span class="p">:</span>
                    <span class="k">return</span> <span class="kc">False</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">maximum_output_channels</span> <span class="ow">and</span> <span class="n">oc</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">maximum_output_channels</span><span class="p">:</span>
                    <span class="k">return</span> <span class="kc">False</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">include_layers_with_names</span>
                    <span class="ow">and</span> <span class="n">layer</span><span class="o">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_layers_with_names</span>
                <span class="p">):</span>
                    <span class="k">return</span> <span class="kc">False</span>

                <span class="k">return</span> <span class="kc">True</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Unrecognized quantization weight field </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">weight_param</span><span class="p">)</span>
                <span class="p">)</span>

        <span class="k">elif</span> <span class="n">layer_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;loop&quot;</span><span class="p">,</span> <span class="s2">&quot;branch&quot;</span><span class="p">]:</span>
            <span class="k">return</span> <span class="kc">True</span>

        <span class="k">return</span> <span class="kc">False</span></div></div>


<span class="k">def</span> <span class="nf">_convert_1bit_array_to_byte_array</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Convert bit array to byte array.</span>

<span class="sd">    arr: list</span>
<span class="sd">        Bits as a list where each element is an integer of 0 or 1</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    numpy.array</span>
<span class="sd">        1D numpy array of type uint8</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Padding if necessary</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">8</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="o">%</span> <span class="mi">8</span><span class="p">:</span>
        <span class="n">arr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">arr</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;uint8&quot;</span><span class="p">)</span>
    <span class="n">bit_arr</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># Iterate and combine 8-bits into a uint8</span>
    <span class="k">for</span> <span class="n">arr_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="o">/</span> <span class="mi">8</span><span class="p">)):</span>
        <span class="n">bit_arr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="p">((</span><span class="n">arr</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="mi">7</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">7</span><span class="p">))</span>
            <span class="o">|</span> <span class="p">((</span><span class="n">arr</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="mi">6</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">6</span><span class="p">))</span>
            <span class="o">|</span> <span class="p">((</span><span class="n">arr</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="mi">5</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">5</span><span class="p">))</span>
            <span class="o">|</span> <span class="p">((</span><span class="n">arr</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">3</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="mi">4</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">4</span><span class="p">))</span>
            <span class="o">|</span> <span class="p">((</span><span class="n">arr</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">4</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="mi">3</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">3</span><span class="p">))</span>
            <span class="o">|</span> <span class="p">((</span><span class="n">arr</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">5</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="mi">2</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">2</span><span class="p">))</span>
            <span class="o">|</span> <span class="p">((</span><span class="n">arr</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">6</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">1</span><span class="p">))</span>
            <span class="o">|</span> <span class="p">((</span><span class="n">arr</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">7</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">0</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="n">idx</span> <span class="o">+=</span> <span class="mi">8</span>
    <span class="k">return</span> <span class="n">_np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">bit_arr</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;uint8&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_convert_array_to_nbit_quantized_bytes</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">nbits</span><span class="p">):</span>
    <span class="n">split_arr</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">nbits</span><span class="p">)):</span>
            <span class="n">split_arr</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">arr</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">&gt;&gt;</span> <span class="n">i</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">0</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">_convert_1bit_array_to_byte_array</span><span class="p">(</span><span class="n">split_arr</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_decompose_bytes_to_bit_arr</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Unpack bytes to bits</span>

<span class="sd">    arr: list</span>
<span class="sd">        Byte Stream, as a list of uint8 values</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    bit_arr: list</span>
<span class="sd">        Decomposed bit stream as a list of 0/1s of length (len(arr) * 8)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">bit_arr</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">)):</span>
            <span class="n">bit_arr</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">arr</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">&gt;&gt;</span> <span class="n">i</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">0</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">bit_arr</span>


<span class="k">def</span> <span class="nf">_get_linear_lookup_table_and_weight</span><span class="p">(</span><span class="n">nbits</span><span class="p">,</span> <span class="n">wp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate a linear lookup table.</span>

<span class="sd">    nbits: int</span>
<span class="sd">        Number of bits to represent a quantized weight value</span>

<span class="sd">    wp: numpy.array</span>
<span class="sd">        Weight blob to be quantized</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    lookup_table: numpy.array</span>
<span class="sd">        Lookup table of shape (2^nbits, )</span>
<span class="sd">    qw: numpy.array</span>
<span class="sd">        Decomposed bit stream as a list of 0/1s of length (len(arr) * 8)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">qw</span><span class="p">,</span> <span class="n">scales</span><span class="p">,</span> <span class="n">biases</span> <span class="o">=</span> <span class="n">_quantize_channelwise_linear</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">nbits</span><span class="p">))</span>
    <span class="n">lookup_table</span> <span class="o">=</span> <span class="n">indices</span> <span class="o">*</span> <span class="n">scales</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">lookup_table</span><span class="p">,</span> <span class="n">qw</span>


<span class="k">def</span> <span class="nf">_get_kmeans_lookup_table_and_weight</span><span class="p">(</span>
    <span class="n">nbits</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;k-means++&quot;</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">rand_seed</span><span class="o">=</span><span class="mi">0</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate K-Means lookup table given a weight parameter field</span>

<span class="sd">    nbits:</span>
<span class="sd">        Number of bits for quantization</span>

<span class="sd">    w:</span>
<span class="sd">        Weight as numpy array</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    lut: numpy.array</span>
<span class="sd">        Lookup table, numpy array of shape (1 &lt;&lt; nbits, );</span>
<span class="sd">    wq: numpy.array</span>
<span class="sd">        Quantized weight of type numpy.uint8</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_HAS_SKLEARN</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;sklearn package required for k-means quantization&quot;</span><span class="p">)</span>
    <span class="n">units</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">lut_len</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">nbits</span>
    <span class="n">n_clusters</span> <span class="o">=</span> <span class="n">units</span> <span class="k">if</span> <span class="p">(</span><span class="n">units</span> <span class="o">&lt;</span> <span class="n">lut_len</span><span class="p">)</span> <span class="k">else</span> <span class="n">lut_len</span>
    <span class="n">wf</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span>
        <span class="n">n_clusters</span><span class="o">=</span><span class="n">n_clusters</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="n">n_init</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rand_seed</span>
    <span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">wf</span><span class="p">)</span>
    <span class="n">wq</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">[:</span><span class="n">units</span><span class="p">]</span>
    <span class="n">lut</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">lut_len</span><span class="p">)</span>
    <span class="n">lut</span><span class="p">[:</span><span class="n">n_clusters</span><span class="p">]</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">lut</span><span class="p">,</span> <span class="n">wq</span>


<span class="k">def</span> <span class="nf">_quantize_channelwise_linear</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Linearly quantize weight blob.</span>

<span class="sd">    weight: numpy.array</span>
<span class="sd">        Weight to be quantized.</span>

<span class="sd">    nbits: int</span>
<span class="sd">        Number of bits per weight element</span>

<span class="sd">    axis: int</span>
<span class="sd">        Axis of the weight blob to compute channel-wise quantization, can be 0 or 1</span>

<span class="sd">    symmetric: bool</span>
<span class="sd">        If true, set quantization range to be symmetrical to 0.</span>
<span class="sd">        Otherwise, set quantization range to be the minimum and maximum of</span>
<span class="sd">        weight parameters.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    quantized_weight: numpy.array</span>
<span class="sd">        quantized weight as float numpy array, with the same shape as weight</span>
<span class="sd">    scale: numpy.array</span>
<span class="sd">        per channel scale</span>
<span class="sd">    bias: numpy.array</span>
<span class="sd">        per channel bias</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># vector situation, treat as 1 channel</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

    <span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">transposed_axis_order</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">rank</span><span class="p">))</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">transposed_axis_order</span><span class="p">)</span>

    <span class="n">num_channels</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">num_channels</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># [C, L]</span>

    <span class="n">a</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">amin</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [C,]</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [C,]</span>

    <span class="k">if</span> <span class="n">symmetric</span><span class="p">:</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">_np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">_np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">r</span> <span class="o">/</span> <span class="p">((</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">nbits</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">nbits</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">scale</span>
        <span class="n">num</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">-</span> <span class="n">bias</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="n">denom</span> <span class="o">=</span> <span class="n">scale</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="n">qw</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span>
            <span class="n">num</span><span class="p">,</span> <span class="n">denom</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">_np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">num</span><span class="p">),</span> <span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">_np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">denom</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">1e-6</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">qw</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">qw</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">qb</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">nbits</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">/</span> <span class="n">qb</span>
        <span class="n">inv_scale</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span>
            <span class="mf">1.0</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">_np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">scale</span><span class="p">),</span> <span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">_np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">1e-6</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">a</span>
        <span class="n">qw</span> <span class="o">=</span> <span class="p">(</span><span class="n">weight</span> <span class="o">-</span> <span class="n">a</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">*</span> <span class="n">inv_scale</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="n">qw</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">qw</span><span class="p">)</span>

    <span class="c1"># Reshape</span>
    <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">qw</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">quantized_weight</span><span class="p">,</span> <span class="n">transposed_axis_order</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">quantized_weight</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_quantize_wp</span><span class="p">(</span><span class="n">wp</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Quantize the weight blob</span>

<span class="sd">    wp: numpy.array</span>
<span class="sd">        Weight parameters</span>
<span class="sd">    nbits: int</span>
<span class="sd">        Number of bits</span>
<span class="sd">    qm:</span>
<span class="sd">        Quantization mode</span>
<span class="sd">    lut_function: (``callable function``)</span>
<span class="sd">        Python callable representing a look-up table</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    scale: numpy.array</span>
<span class="sd">        Per-channel scale</span>
<span class="sd">    bias: numpy.array</span>
<span class="sd">        Per-channel bias</span>
<span class="sd">    lut: numpy.array</span>
<span class="sd">        Lookup table</span>
<span class="sd">    quantized_wp: numpy.array</span>
<span class="sd">        Quantized weight of same shape as wp, with dtype numpy.uint8</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">scale</span> <span class="o">=</span> <span class="n">bias</span> <span class="o">=</span> <span class="n">lut</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Linear Quantization</span>
    <span class="k">if</span> <span class="n">qm</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">_QUANTIZATION_MODE_LINEAR_QUANTIZATION</span><span class="p">,</span>
        <span class="n">_QUANTIZATION_MODE_LINEAR_SYMMETRIC</span><span class="p">,</span>
    <span class="p">]:</span>
        <span class="n">symmetric</span> <span class="o">=</span> <span class="n">qm</span> <span class="o">==</span> <span class="n">_QUANTIZATION_MODE_LINEAR_SYMMETRIC</span>
        <span class="n">qw</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="n">_quantize_channelwise_linear</span><span class="p">(</span><span class="n">wp</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">symmetric</span><span class="p">)</span>
    <span class="c1"># Lookup tables</span>
    <span class="k">elif</span> <span class="n">qm</span> <span class="o">==</span> <span class="n">_QUANTIZATION_MODE_LOOKUP_TABLE_KMEANS</span><span class="p">:</span>
        <span class="n">lut</span><span class="p">,</span> <span class="n">qw</span> <span class="o">=</span> <span class="n">_get_kmeans_lookup_table_and_weight</span><span class="p">(</span><span class="n">nbits</span><span class="p">,</span> <span class="n">wp</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">qm</span> <span class="o">==</span> <span class="n">_QUANTIZATION_MODE_CUSTOM_LOOKUP_TABLE</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;lut_function&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                <span class="s2">&quot;Custom lookup table quantization mode &quot;</span>
                <span class="s2">&quot;selected but no lookup table function passed&quot;</span>
            <span class="p">)</span>
        <span class="n">lut_function</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;lut_function&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">lut_function</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                <span class="s2">&quot;Argument for Lookup Table passed in but is &quot;</span> <span class="s2">&quot;not callable&quot;</span>
            <span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">lut</span><span class="p">,</span> <span class="n">qw</span> <span class="o">=</span> <span class="n">lut_function</span><span class="p">(</span><span class="n">nbits</span><span class="p">,</span> <span class="n">wp</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                <span class="s2">&quot;</span><span class="si">{}</span><span class="se">\n</span><span class="s2">Call to Lookup Table function failed&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">message</span><span class="p">)</span>
            <span class="p">)</span>
    <span class="k">elif</span> <span class="n">qm</span> <span class="o">==</span> <span class="n">_QUANTIZATION_MODE_LOOKUP_TABLE_LINEAR</span><span class="p">:</span>
        <span class="n">lut</span><span class="p">,</span> <span class="n">qw</span> <span class="o">=</span> <span class="n">_get_linear_lookup_table_and_weight</span><span class="p">(</span><span class="n">nbits</span><span class="p">,</span> <span class="n">wp</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;Quantization method &quot;</span><span class="si">{}</span><span class="s1">&quot; not supported&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">qm</span><span class="p">))</span>

    <span class="n">quantized_wp</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">uint8</span><span class="p">(</span><span class="n">qw</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scale</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">lut</span><span class="p">,</span> <span class="n">quantized_wp</span>


<span class="k">def</span> <span class="nf">_quantize_wp_field</span><span class="p">(</span><span class="n">wp</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Quantize WeightParam field in Neural Network Protobuf</span>

<span class="sd">    wp: MLModel.NeuralNetwork.WeightParam</span>
<span class="sd">        WeightParam field</span>
<span class="sd">    nbits: int</span>
<span class="sd">        Number of bits to be quantized</span>
<span class="sd">    qm: str</span>
<span class="sd">        Quantization mode</span>
<span class="sd">    shape: tuple</span>
<span class="sd">        Tensor shape held by wp</span>
<span class="sd">    axis: int</span>
<span class="sd">        Axis over which quantization is performed on, can be either 0 or 1</span>
<span class="sd">    lut_function: (``callable function``)</span>
<span class="sd">        Python callable representing a LUT table function</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># De-quantization</span>
    <span class="k">if</span> <span class="n">qm</span> <span class="o">==</span> <span class="n">_QUANTIZATION_MODE_DEQUANTIZE</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_dequantize_wp</span><span class="p">(</span><span class="n">wp</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>

    <span class="c1"># If the float32 field is empty do nothing and return</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">floatValue</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="c1"># Half precision (16-bit) quantization</span>
    <span class="k">if</span> <span class="n">nbits</span> <span class="o">==</span> <span class="mi">16</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_wp_to_fp16wp</span><span class="p">(</span><span class="n">wp</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">nbits</span> <span class="o">&gt;</span> <span class="mi">8</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Only 8-bit and lower quantization is supported&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">qm</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_SUPPORTED_QUANTIZATION_MODES</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Quantization mode </span><span class="si">{}</span><span class="s2"> not supported&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">qm</span><span class="p">))</span>

    <span class="c1"># axis parameter check</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
            <span class="s2">&quot;Quantization on second axis is only supported &quot;</span> <span class="s2">&quot;for rank-4 weight blob.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">axis</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
            <span class="s2">&quot;Invalid quantization axis </span><span class="si">{}</span><span class="s2"> passed in. Allowed&quot;</span>
            <span class="s2">&quot;values are 0 (first axis) and 1 (second axis)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># WeightParam size check - non-linear quantizations are applied on layer level</span>
    <span class="n">num_channels</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">qm</span>
        <span class="ow">in</span> <span class="p">[</span><span class="n">_QUANTIZATION_MODE_LINEAR_QUANTIZATION</span><span class="p">,</span> <span class="n">_QUANTIZATION_MODE_LINEAR_SYMMETRIC</span><span class="p">]</span>
        <span class="k">else</span> <span class="mi">1</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">floatValue</span><span class="p">)</span> <span class="o">%</span> <span class="n">num_channels</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
            <span class="s2">&quot;Number of quantization channels does not divide evenly into weights&quot;</span>
        <span class="p">)</span>

    <span class="n">qparams</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">quantization</span>
    <span class="n">qparams</span><span class="o">.</span><span class="n">numberOfBits</span> <span class="o">=</span> <span class="n">nbits</span>

    <span class="n">weights</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">floatValue</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">scale</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">lut</span><span class="p">,</span> <span class="n">uint8_weights</span> <span class="o">=</span> <span class="n">_quantize_wp</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">uint8_weights</span> <span class="o">=</span> <span class="n">uint8_weights</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">qm</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">_QUANTIZATION_MODE_LINEAR_QUANTIZATION</span><span class="p">,</span>
        <span class="n">_QUANTIZATION_MODE_LINEAR_SYMMETRIC</span><span class="p">,</span>
    <span class="p">]:</span>
        <span class="n">qparams</span><span class="o">.</span><span class="n">linearQuantization</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>
        <span class="n">qparams</span><span class="o">.</span><span class="n">linearQuantization</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">bias</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">qparams</span><span class="o">.</span><span class="n">lookupTableQuantization</span><span class="o">.</span><span class="n">floatValue</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">lut</span><span class="p">)</span>

    <span class="n">wp</span><span class="o">.</span><span class="n">rawValue</span> <span class="o">=</span> <span class="nb">bytes</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">nbits</span> <span class="o">==</span> <span class="mi">8</span><span class="p">:</span>
        <span class="n">wp</span><span class="o">.</span><span class="n">rawValue</span> <span class="o">+=</span> <span class="n">uint8_weights</span><span class="o">.</span><span class="n">tobytes</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">wp</span><span class="o">.</span><span class="n">rawValue</span> <span class="o">+=</span> <span class="n">_convert_array_to_nbit_quantized_bytes</span><span class="p">(</span>
            <span class="n">uint8_weights</span><span class="p">,</span> <span class="n">nbits</span>
        <span class="p">)</span><span class="o">.</span><span class="n">tobytes</span><span class="p">()</span>
    <span class="k">del</span> <span class="n">wp</span><span class="o">.</span><span class="n">floatValue</span><span class="p">[:]</span>


<span class="k">def</span> <span class="nf">_unpack_to_bytes</span><span class="p">(</span><span class="n">byte_arr</span><span class="p">,</span> <span class="n">num_weights</span><span class="p">,</span> <span class="n">nbits</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">num_weights</span> <span class="o">%</span> <span class="mi">1</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="n">num_weights</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_weights</span><span class="p">)</span>
    <span class="n">bit_arr</span> <span class="o">=</span> <span class="n">_decompose_bytes_to_bit_arr</span><span class="p">(</span><span class="n">byte_arr</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
    <span class="n">bit_arr</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">bit_arr</span><span class="p">[:</span> <span class="n">num_weights</span> <span class="o">*</span> <span class="n">nbits</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">num_weights</span><span class="p">,</span> <span class="n">nbits</span><span class="p">))</span>
    <span class="n">expo</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">_np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nbits</span><span class="p">))))</span>
    <span class="n">byte_arr</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">bit_arr</span> <span class="o">*</span> <span class="n">expo</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">byte_arr</span>


<span class="k">def</span> <span class="nf">_dequantize_linear</span><span class="p">(</span><span class="n">weight_8bit</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight_8bit</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># vector situation, treat as 1 channel</span>
        <span class="n">weight_8bit</span> <span class="o">=</span> <span class="n">weight_8bit</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">weight_8bit</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

    <span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight_8bit</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">transposed_axis_order</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">rank</span><span class="p">))</span>
        <span class="n">weight_8bit</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">weight_8bit</span><span class="p">,</span> <span class="n">transposed_axis_order</span><span class="p">)</span>

    <span class="n">num_channels</span> <span class="o">=</span> <span class="n">weight_8bit</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">broadcast_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_channels</span><span class="p">,)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">broadcast_shape</span><span class="p">)</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">broadcast_shape</span><span class="p">)</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">weight_8bit</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float&quot;</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">bias</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">transposed_axis_order</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">weight</span>


<span class="k">def</span> <span class="nf">_dequantize_lut</span><span class="p">(</span><span class="n">weight_8bit</span><span class="p">,</span> <span class="n">lut</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">lut</span><span class="p">[</span><span class="n">weight_8bit</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint8&quot;</span><span class="p">)]</span>


<span class="k">def</span> <span class="nf">_dequantize_wp</span><span class="p">(</span><span class="n">wp</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">floatValue</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="n">is_linear</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;QuantizationType&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;linearQuantization&quot;</span>
    <span class="k">if</span> <span class="n">is_linear</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">linearQuantization</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="n">wp</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">linearQuantization</span><span class="o">.</span><span class="n">bias</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                <span class="s2">&quot;Linear quantization scale and bias vectors are &quot;</span> <span class="s2">&quot;different lengths&quot;</span>
            <span class="p">)</span>

    <span class="c1"># axis parameter check</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
            <span class="s2">&quot;Dequantization on second axis is only supported &quot;</span> <span class="s2">&quot;for rank-4 weight blob.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">axis</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
            <span class="s2">&quot;Invalid quantization axis </span><span class="si">{}</span><span class="s2"> passed in. Allowed&quot;</span>
            <span class="s2">&quot;values are 0 (first axis) and 1 (second axis)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="n">nbits</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">numberOfBits</span>
    <span class="n">num_weights</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">byte_arr</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">frombuffer</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">rawValue</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">_np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>

    <span class="n">weight_8bit</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">byte_arr</span> <span class="k">if</span> <span class="n">nbits</span> <span class="o">==</span> <span class="mi">8</span> <span class="k">else</span> <span class="n">_unpack_to_bytes</span><span class="p">(</span><span class="n">byte_arr</span><span class="p">,</span> <span class="n">num_weights</span><span class="p">,</span> <span class="n">nbits</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">weight_8bit</span> <span class="o">=</span> <span class="n">weight_8bit</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">is_linear</span><span class="p">:</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">linearQuantization</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">linearQuantization</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">dequantized_weight</span> <span class="o">=</span> <span class="n">_dequantize_linear</span><span class="p">(</span><span class="n">weight_8bit</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">lut</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">lookupTableQuantization</span><span class="o">.</span><span class="n">floatValue</span><span class="p">)</span>
        <span class="n">dequantized_weight</span> <span class="o">=</span> <span class="n">_dequantize_lut</span><span class="p">(</span><span class="n">weight_8bit</span><span class="p">,</span> <span class="n">lut</span><span class="p">)</span>

    <span class="n">wp</span><span class="o">.</span><span class="n">rawValue</span> <span class="o">=</span> <span class="nb">bytes</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">Clear</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">floatValue</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">dequantized_weight</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>


<span class="k">def</span> <span class="nf">_dequantize_nn_spec</span><span class="p">(</span><span class="n">spec</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Dequantize weights in NeuralNetwork type mlmodel specifications.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_quantize_nn_spec</span><span class="p">(</span><span class="n">spec</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">_QUANTIZATION_MODE_DEQUANTIZE</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_quantize_nn_spec</span><span class="p">(</span><span class="n">nn_spec</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Quantize weights in NeuralNetwork type mlmodel specifications.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">selector</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;selector&quot;</span><span class="p">,</span> <span class="n">QuantizedLayerSelector</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">qm</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_SUPPORTED_QUANTIZATION_MODES</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Quantization mode </span><span class="si">{}</span><span class="s2"> not supported&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">qm</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">qm</span> <span class="o">!=</span> <span class="n">_QUANTIZATION_MODE_DEQUANTIZE</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">nbits</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s1">&#39;Missing argument &quot;nbits&quot;&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">nbits</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">nbits</span> <span class="o">&lt;=</span> <span class="mi">8</span> <span class="ow">or</span> <span class="n">nbits</span> <span class="o">==</span> <span class="mi">16</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                <span class="s2">&quot;Only half precision (16-bit), 1 to 8-bit &quot;</span> <span class="s2">&quot;quantization is supported&quot;</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="n">qm</span> <span class="o">==</span> <span class="n">_QUANTIZATION_MODE_LINEAR_SYMMETRIC</span> <span class="ow">and</span> <span class="n">nbits</span> <span class="o">!=</span> <span class="mi">8</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Symmetric quantization is only applicable for 8 bit&quot;</span> <span class="s2">&quot;linear&quot;</span><span class="p">)</span>

    <span class="n">layers</span> <span class="o">=</span> <span class="n">nn_spec</span><span class="o">.</span><span class="n">layers</span>

    <span class="c1"># Perform optimization step</span>
    <span class="k">if</span> <span class="n">nbits</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">nbits</span> <span class="o">&lt;</span> <span class="mi">16</span> <span class="ow">and</span> <span class="n">qm</span> <span class="o">!=</span> <span class="n">_QUANTIZATION_MODE_DEQUANTIZE</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Optimizing Neural Network before Quantization:&quot;</span><span class="p">)</span>
        <span class="n">_optimize_nn</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Finished optimizing network. Quantizing neural network..&quot;</span><span class="p">)</span>

    <span class="c1"># Quantize each layer</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
        <span class="n">layer_type</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;layer&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">selector</span><span class="o">.</span><span class="n">do_quantize</span><span class="p">(</span><span class="n">layer</span><span class="p">):</span>
            <span class="k">continue</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Quantizing layer </span><span class="si">{}</span><span class="s2"> of type </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">layer_type</span><span class="p">))</span>

        <span class="c1"># Convolution</span>
        <span class="k">if</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;convolution&quot;</span><span class="p">:</span>
            <span class="n">output_channels</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">convolution</span><span class="o">.</span><span class="n">outputChannels</span>
            <span class="n">kernel_channels</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">convolution</span><span class="o">.</span><span class="n">kernelChannels</span>
            <span class="n">kernel_height</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">convolution</span><span class="o">.</span><span class="n">kernelSize</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">kernel_width</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">convolution</span><span class="o">.</span><span class="n">kernelSize</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">groups</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">convolution</span><span class="o">.</span><span class="n">nGroups</span>
            <span class="n">counts</span> <span class="o">=</span> <span class="n">output_channels</span> <span class="o">*</span> <span class="n">kernel_channels</span> <span class="o">*</span> <span class="n">kernel_height</span> <span class="o">*</span> <span class="n">kernel_width</span>
            <span class="n">has_bias</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">convolution</span><span class="o">.</span><span class="n">hasBias</span>
            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">convolution</span><span class="o">.</span><span class="n">isDeconvolution</span><span class="p">:</span>
                <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">kernel_channels</span><span class="p">,</span>
                    <span class="nb">int</span><span class="p">(</span><span class="n">output_channels</span> <span class="o">/</span> <span class="n">groups</span><span class="p">),</span>
                    <span class="n">kernel_height</span><span class="p">,</span>
                    <span class="n">kernel_width</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">layer</span><span class="o">.</span><span class="n">convolution</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_channels</span><span class="p">,</span> <span class="n">kernel_height</span><span class="p">,</span> <span class="n">kernel_width</span><span class="p">)</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">layer</span><span class="o">.</span><span class="n">convolution</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">has_bias</span> <span class="ow">and</span> <span class="n">selector</span><span class="o">.</span><span class="n">do_quantize</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">weight_param</span><span class="o">=</span><span class="s2">&quot;bias&quot;</span><span class="p">):</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">layer</span><span class="o">.</span><span class="n">convolution</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
                    <span class="n">nbits</span><span class="p">,</span>
                    <span class="n">qm</span><span class="p">,</span>
                    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">output_channels</span><span class="p">,),</span>
                    <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>

        <span class="c1"># Batchnorm</span>
        <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;batchnorm&quot;</span><span class="p">:</span>
            <span class="n">nw</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">batchnorm</span><span class="o">.</span><span class="n">channels</span>
            <span class="n">_quantize_wp_field</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">batchnorm</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">nw</span><span class="p">,),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">_quantize_wp_field</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">batchnorm</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">nw</span><span class="p">,),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">_quantize_wp_field</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">batchnorm</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">nw</span><span class="p">,),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">_quantize_wp_field</span><span class="p">(</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">batchnorm</span><span class="o">.</span><span class="n">variance</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">nw</span><span class="p">,),</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>

        <span class="c1"># InnerProduct</span>
        <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;innerProduct&quot;</span><span class="p">:</span>
            <span class="n">output_channels</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">innerProduct</span><span class="o">.</span><span class="n">outputChannels</span>
            <span class="n">input_channels</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">innerProduct</span><span class="o">.</span><span class="n">inputChannels</span>
            <span class="n">_quantize_wp_field</span><span class="p">(</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">innerProduct</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span>
                <span class="n">nbits</span><span class="p">,</span>
                <span class="n">qm</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">output_channels</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">),</span>
                <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="n">has_bias</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">innerProduct</span><span class="o">.</span><span class="n">hasBias</span>
            <span class="k">if</span> <span class="n">has_bias</span> <span class="ow">and</span> <span class="n">selector</span><span class="o">.</span><span class="n">do_quantize</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">weight_param</span><span class="o">=</span><span class="s2">&quot;bias&quot;</span><span class="p">):</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">layer</span><span class="o">.</span><span class="n">innerProduct</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
                    <span class="n">nbits</span><span class="p">,</span>
                    <span class="n">qm</span><span class="p">,</span>
                    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">output_channels</span><span class="p">,),</span>
                    <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>

        <span class="c1"># BatchedMatmul</span>
        <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;batchedMatmul&quot;</span><span class="p">:</span>
            <span class="n">x1</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">batchedMatmul</span><span class="o">.</span><span class="n">weightMatrixFirstDimension</span>
            <span class="n">x2</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">batchedMatmul</span><span class="o">.</span><span class="n">weightMatrixSecondDimension</span>
            <span class="n">_quantize_wp_field</span><span class="p">(</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">batchedMatmul</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">x1</span><span class="p">),</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="n">has_bias</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">batchedMatmul</span><span class="o">.</span><span class="n">hasBias</span>
            <span class="k">if</span> <span class="n">has_bias</span> <span class="ow">and</span> <span class="n">selector</span><span class="o">.</span><span class="n">do_quantize</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">weight_param</span><span class="o">=</span><span class="s2">&quot;bias&quot;</span><span class="p">):</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">layer</span><span class="o">.</span><span class="n">batchedMatmul</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">x2</span><span class="p">,),</span> <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>

        <span class="c1"># Embedding layer</span>
        <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;embedding&quot;</span><span class="p">:</span>
            <span class="n">output_channels</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">outputChannels</span>
            <span class="n">input_channels</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">inputDim</span>
            <span class="n">_quantize_wp_field</span><span class="p">(</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span>
                <span class="n">nbits</span><span class="p">,</span>
                <span class="n">qm</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">output_channels</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">),</span>
                <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">hasBias</span><span class="p">:</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">layer</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">output_channels</span><span class="p">,),</span> <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>

        <span class="c1"># Embedding ND layer</span>
        <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;embeddingND&quot;</span><span class="p">:</span>
            <span class="n">output_channels</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">embeddingND</span><span class="o">.</span><span class="n">embeddingSize</span>
            <span class="n">input_channels</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">embeddingND</span><span class="o">.</span><span class="n">vocabSize</span>
            <span class="n">_quantize_wp_field</span><span class="p">(</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">embeddingND</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span>
                <span class="n">nbits</span><span class="p">,</span>
                <span class="n">qm</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">output_channels</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">),</span>
                <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">embeddingND</span><span class="o">.</span><span class="n">hasBias</span><span class="p">:</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">layer</span><span class="o">.</span><span class="n">embeddingND</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
                    <span class="n">nbits</span><span class="p">,</span>
                    <span class="n">qm</span><span class="p">,</span>
                    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">output_channels</span><span class="p">,),</span>
                    <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>

        <span class="c1"># Scale layer</span>
        <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span>
            <span class="n">nw</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">shapeScale</span><span class="p">)</span>
            <span class="n">_quantize_wp_field</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">nw</span><span class="p">,),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">hasBias</span><span class="p">:</span>
                <span class="n">nw</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">shapeBias</span><span class="p">)</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">nw</span><span class="p">,),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Bias layer</span>
        <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;bias&quot;</span><span class="p">:</span>
            <span class="n">nw</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">_quantize_wp_field</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">nw</span><span class="p">,),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># LoadConstant layer</span>
        <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;loadConstant&quot;</span><span class="p">:</span>
            <span class="n">nw</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">loadConstant</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">_quantize_wp_field</span><span class="p">(</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">loadConstant</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">nw</span><span class="p">,),</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>

        <span class="c1"># Simple Recurrent</span>
        <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;simpleRecurrent&quot;</span><span class="p">:</span>
            <span class="n">i_size</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">simpleRecurrent</span><span class="o">.</span><span class="n">inputVectorSize</span>
            <span class="n">o_size</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">simpleRecurrent</span><span class="o">.</span><span class="n">outputVectorSize</span>
            <span class="n">_quantize_wp_field</span><span class="p">(</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">simpleRecurrent</span><span class="o">.</span><span class="n">weightMatrix</span><span class="p">,</span>
                <span class="n">nbits</span><span class="p">,</span>
                <span class="n">qm</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,</span> <span class="n">i_size</span><span class="p">),</span>
                <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="n">_quantize_wp_field</span><span class="p">(</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">simpleRecurrent</span><span class="o">.</span><span class="n">recursionMatrix</span><span class="p">,</span>
                <span class="n">nbits</span><span class="p">,</span>
                <span class="n">qm</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,</span> <span class="n">o_size</span><span class="p">),</span>
                <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">simpleRecurrent</span><span class="o">.</span><span class="n">hasBiasVector</span><span class="p">:</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">layer</span><span class="o">.</span><span class="n">simpleRecurrent</span><span class="o">.</span><span class="n">biasVector</span><span class="p">,</span>
                    <span class="n">nbits</span><span class="p">,</span>
                    <span class="n">qm</span><span class="p">,</span>
                    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,),</span>
                    <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>

        <span class="c1"># GRU</span>
        <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;gru&quot;</span><span class="p">:</span>
            <span class="n">i_size</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">gru</span><span class="o">.</span><span class="n">inputVectorSize</span>
            <span class="n">o_size</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">gru</span><span class="o">.</span><span class="n">outputVectorSize</span>
            <span class="c1"># Weight Matrix</span>
            <span class="n">_quantize_wp_field</span><span class="p">(</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">gru</span><span class="o">.</span><span class="n">updateGateWeightMatrix</span><span class="p">,</span>
                <span class="n">nbits</span><span class="p">,</span>
                <span class="n">qm</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,</span> <span class="n">i_size</span><span class="p">),</span>
                <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="n">_quantize_wp_field</span><span class="p">(</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">gru</span><span class="o">.</span><span class="n">resetGateWeightMatrix</span><span class="p">,</span>
                <span class="n">nbits</span><span class="p">,</span>
                <span class="n">qm</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,</span> <span class="n">i_size</span><span class="p">),</span>
                <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="n">_quantize_wp_field</span><span class="p">(</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">gru</span><span class="o">.</span><span class="n">outputGateWeightMatrix</span><span class="p">,</span>
                <span class="n">nbits</span><span class="p">,</span>
                <span class="n">qm</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,</span> <span class="n">i_size</span><span class="p">),</span>
                <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="c1"># Recursion Weights</span>
            <span class="n">_quantize_wp_field</span><span class="p">(</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">gru</span><span class="o">.</span><span class="n">updateGateRecursionMatrix</span><span class="p">,</span>
                <span class="n">nbits</span><span class="p">,</span>
                <span class="n">qm</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,</span> <span class="n">o_size</span><span class="p">),</span>
                <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="n">_quantize_wp_field</span><span class="p">(</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">gru</span><span class="o">.</span><span class="n">resetGateRecursionMatrix</span><span class="p">,</span>
                <span class="n">nbits</span><span class="p">,</span>
                <span class="n">qm</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,</span> <span class="n">o_size</span><span class="p">),</span>
                <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="n">_quantize_wp_field</span><span class="p">(</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">gru</span><span class="o">.</span><span class="n">outputGateRecursionMatrix</span><span class="p">,</span>
                <span class="n">nbits</span><span class="p">,</span>
                <span class="n">qm</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,</span> <span class="n">o_size</span><span class="p">),</span>
                <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="c1"># Bias</span>
            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">gru</span><span class="o">.</span><span class="n">hasBiasVectors</span><span class="p">:</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">layer</span><span class="o">.</span><span class="n">gru</span><span class="o">.</span><span class="n">updateGateBiasVector</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,),</span> <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">layer</span><span class="o">.</span><span class="n">gru</span><span class="o">.</span><span class="n">resetGateBiasVector</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,),</span> <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">layer</span><span class="o">.</span><span class="n">gru</span><span class="o">.</span><span class="n">outputGateBiasVector</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,),</span> <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>

        <span class="c1"># LSTM Layers</span>
        <span class="k">elif</span> <span class="n">layer_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;uniDirectionalLSTM&quot;</span><span class="p">,</span> <span class="s2">&quot;biDirectionalLSTM&quot;</span><span class="p">]:</span>

            <span class="k">def</span> <span class="nf">_lstmwp_to_fp16_lstmwp</span><span class="p">(</span>
                <span class="n">lstm_wp</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">i_size</span><span class="p">,</span> <span class="n">o_size</span><span class="p">,</span> <span class="n">has_peephole</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">):</span>
                <span class="k">assert</span> <span class="n">lstm_wp</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">lstm_wp</span><span class="o">.</span><span class="n">inputGateWeightMatrix</span><span class="p">,</span>
                    <span class="n">nbits</span><span class="p">,</span>
                    <span class="n">qm</span><span class="p">,</span>
                    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,</span> <span class="n">i_size</span><span class="p">),</span>
                    <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">lstm_wp</span><span class="o">.</span><span class="n">forgetGateWeightMatrix</span><span class="p">,</span>
                    <span class="n">nbits</span><span class="p">,</span>
                    <span class="n">qm</span><span class="p">,</span>
                    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,</span> <span class="n">i_size</span><span class="p">),</span>
                    <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">lstm_wp</span><span class="o">.</span><span class="n">blockInputWeightMatrix</span><span class="p">,</span>
                    <span class="n">nbits</span><span class="p">,</span>
                    <span class="n">qm</span><span class="p">,</span>
                    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,</span> <span class="n">i_size</span><span class="p">),</span>
                    <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">lstm_wp</span><span class="o">.</span><span class="n">outputGateWeightMatrix</span><span class="p">,</span>
                    <span class="n">nbits</span><span class="p">,</span>
                    <span class="n">qm</span><span class="p">,</span>
                    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,</span> <span class="n">i_size</span><span class="p">),</span>
                    <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>

                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">lstm_wp</span><span class="o">.</span><span class="n">inputGateRecursionMatrix</span><span class="p">,</span>
                    <span class="n">nbits</span><span class="p">,</span>
                    <span class="n">qm</span><span class="p">,</span>
                    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,</span> <span class="n">o_size</span><span class="p">),</span>
                    <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">lstm_wp</span><span class="o">.</span><span class="n">forgetGateRecursionMatrix</span><span class="p">,</span>
                    <span class="n">nbits</span><span class="p">,</span>
                    <span class="n">qm</span><span class="p">,</span>
                    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,</span> <span class="n">o_size</span><span class="p">),</span>
                    <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">lstm_wp</span><span class="o">.</span><span class="n">blockInputRecursionMatrix</span><span class="p">,</span>
                    <span class="n">nbits</span><span class="p">,</span>
                    <span class="n">qm</span><span class="p">,</span>
                    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,</span> <span class="n">o_size</span><span class="p">),</span>
                    <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">lstm_wp</span><span class="o">.</span><span class="n">outputGateRecursionMatrix</span><span class="p">,</span>
                    <span class="n">nbits</span><span class="p">,</span>
                    <span class="n">qm</span><span class="p">,</span>
                    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,</span> <span class="n">o_size</span><span class="p">),</span>
                    <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>

                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">lstm_wp</span><span class="o">.</span><span class="n">inputGateBiasVector</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,),</span> <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">lstm_wp</span><span class="o">.</span><span class="n">forgetGateBiasVector</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,),</span> <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">lstm_wp</span><span class="o">.</span><span class="n">blockInputBiasVector</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,),</span> <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>
                <span class="n">_quantize_wp_field</span><span class="p">(</span>
                    <span class="n">lstm_wp</span><span class="o">.</span><span class="n">outputGateBiasVector</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,),</span> <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>

                <span class="k">if</span> <span class="n">has_peephole</span><span class="p">:</span>
                    <span class="n">_quantize_wp_field</span><span class="p">(</span>
                        <span class="n">lstm_wp</span><span class="o">.</span><span class="n">inputGatePeepholeVector</span><span class="p">,</span>
                        <span class="n">nbits</span><span class="p">,</span>
                        <span class="n">qm</span><span class="p">,</span>
                        <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,),</span>
                        <span class="o">**</span><span class="n">kwargs</span>
                    <span class="p">)</span>
                    <span class="n">_quantize_wp_field</span><span class="p">(</span>
                        <span class="n">lstm_wp</span><span class="o">.</span><span class="n">forgetGatePeepholeVector</span><span class="p">,</span>
                        <span class="n">nbits</span><span class="p">,</span>
                        <span class="n">qm</span><span class="p">,</span>
                        <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,),</span>
                        <span class="o">**</span><span class="n">kwargs</span>
                    <span class="p">)</span>
                    <span class="n">_quantize_wp_field</span><span class="p">(</span>
                        <span class="n">lstm_wp</span><span class="o">.</span><span class="n">outputGatePeepholeVector</span><span class="p">,</span>
                        <span class="n">nbits</span><span class="p">,</span>
                        <span class="n">qm</span><span class="p">,</span>
                        <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">o_size</span><span class="p">,),</span>
                        <span class="o">**</span><span class="n">kwargs</span>
                    <span class="p">)</span>

            <span class="k">if</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;uniDirectionalLSTM&quot;</span><span class="p">:</span>
                <span class="n">_lstmwp_to_fp16_lstmwp</span><span class="p">(</span>
                    <span class="n">lstm_wp</span><span class="o">=</span><span class="n">layer</span><span class="o">.</span><span class="n">uniDirectionalLSTM</span><span class="o">.</span><span class="n">weightParams</span><span class="p">,</span>
                    <span class="n">nbits</span><span class="o">=</span><span class="n">nbits</span><span class="p">,</span>
                    <span class="n">qm</span><span class="o">=</span><span class="n">qm</span><span class="p">,</span>
                    <span class="n">i_size</span><span class="o">=</span><span class="n">layer</span><span class="o">.</span><span class="n">uniDirectionalLSTM</span><span class="o">.</span><span class="n">inputVectorSize</span><span class="p">,</span>
                    <span class="n">o_size</span><span class="o">=</span><span class="n">layer</span><span class="o">.</span><span class="n">uniDirectionalLSTM</span><span class="o">.</span><span class="n">outputVectorSize</span><span class="p">,</span>
                    <span class="n">has_peephole</span><span class="o">=</span><span class="n">layer</span><span class="o">.</span><span class="n">uniDirectionalLSTM</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">hasPeepholeVectors</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;biDirectionalLSTM&quot;</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">lstm_wp</span> <span class="ow">in</span> <span class="n">layer</span><span class="o">.</span><span class="n">biDirectionalLSTM</span><span class="o">.</span><span class="n">weightParams</span><span class="p">:</span>
                    <span class="n">_lstmwp_to_fp16_lstmwp</span><span class="p">(</span>
                        <span class="n">lstm_wp</span><span class="o">=</span><span class="n">lstm_wp</span><span class="p">,</span>
                        <span class="n">nbits</span><span class="o">=</span><span class="n">nbits</span><span class="p">,</span>
                        <span class="n">qm</span><span class="o">=</span><span class="n">qm</span><span class="p">,</span>
                        <span class="n">i_size</span><span class="o">=</span><span class="n">layer</span><span class="o">.</span><span class="n">biDirectionalLSTM</span><span class="o">.</span><span class="n">inputVectorSize</span><span class="p">,</span>
                        <span class="n">o_size</span><span class="o">=</span><span class="n">layer</span><span class="o">.</span><span class="n">biDirectionalLSTM</span><span class="o">.</span><span class="n">outputVectorSize</span><span class="p">,</span>
                        <span class="n">has_peephole</span><span class="o">=</span><span class="n">layer</span><span class="o">.</span><span class="n">biDirectionalLSTM</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">hasPeepholeVectors</span><span class="p">,</span>
                    <span class="p">)</span>

        <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;custom&quot;</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="s2">&quot;Skipping custom layer </span><span class="si">{}</span><span class="s2">. Weights for this layer need to&quot;</span>
                <span class="s2">&quot;be converted manually&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;branch&quot;</span><span class="p">:</span>
            <span class="n">_quantize_nn_spec</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">branch</span><span class="o">.</span><span class="n">ifBranch</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">_quantize_nn_spec</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">branch</span><span class="o">.</span><span class="n">elseBranch</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;loop&quot;</span><span class="p">:</span>
            <span class="n">_quantize_nn_spec</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">loop</span><span class="o">.</span><span class="n">conditionNetwork</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">_quantize_nn_spec</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">loop</span><span class="o">.</span><span class="n">bodyNetwork</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qm</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Unknown layer &quot;</span> <span class="o">+</span> <span class="n">layer_type</span> <span class="o">+</span> <span class="s2">&quot; to be quantized&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_quantize_spec_weights</span><span class="p">(</span><span class="n">spec</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">quantization_mode</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">nn_model_types</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;neuralNetwork&quot;</span><span class="p">,</span>
        <span class="s2">&quot;neuralNetworkClassifier&quot;</span><span class="p">,</span>
        <span class="s2">&quot;neuralNetworkRegressor&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="n">model_type</span> <span class="o">=</span> <span class="n">spec</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;Type&quot;</span><span class="p">)</span>

    <span class="c1"># Neural network models</span>
    <span class="k">if</span> <span class="n">model_type</span> <span class="ow">in</span> <span class="n">nn_model_types</span><span class="p">:</span>
        <span class="c1"># Bump up to appropriate spec version if required</span>
        <span class="k">if</span> <span class="n">nbits</span> <span class="o">==</span> <span class="mi">16</span><span class="p">:</span>
            <span class="n">spec</span><span class="o">.</span><span class="n">specificationVersion</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
                <span class="n">_MINIMUM_FP16_SPEC_VERSION</span><span class="p">,</span> <span class="n">spec</span><span class="o">.</span><span class="n">specificationVersion</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">spec</span><span class="o">.</span><span class="n">specificationVersion</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
                <span class="n">_MINIMUM_QUANTIZED_MODEL_SPEC_VERSION</span><span class="p">,</span> <span class="n">spec</span><span class="o">.</span><span class="n">specificationVersion</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">spec</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;Type&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;neuralNetwork&quot;</span><span class="p">:</span>
            <span class="n">_quantize_nn_spec</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">neuralNetwork</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">quantization_mode</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">spec</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;Type&quot;</span><span class="p">)</span> <span class="ow">in</span> <span class="s2">&quot;neuralNetworkClassifier&quot;</span><span class="p">:</span>
            <span class="n">_quantize_nn_spec</span><span class="p">(</span>
                <span class="n">spec</span><span class="o">.</span><span class="n">neuralNetworkClassifier</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">quantization_mode</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="n">spec</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;Type&quot;</span><span class="p">)</span> <span class="ow">in</span> <span class="s2">&quot;neuralNetworkRegressor&quot;</span><span class="p">:</span>
            <span class="n">_quantize_nn_spec</span><span class="p">(</span>
                <span class="n">spec</span><span class="o">.</span><span class="n">neuralNetworkRegressor</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">quantization_mode</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>

    <span class="c1"># Recursively convert all pipeline models</span>
    <span class="k">elif</span> <span class="n">spec</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;Type&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;pipeline&quot;</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">model_spec</span> <span class="ow">in</span> <span class="n">spec</span><span class="o">.</span><span class="n">pipeline</span><span class="o">.</span><span class="n">models</span><span class="p">:</span>
            <span class="n">_quantize_spec_weights</span><span class="p">(</span><span class="n">model_spec</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">quantization_mode</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">spec</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;Type&quot;</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;pipelineClassifier&quot;</span><span class="p">,</span> <span class="s2">&quot;pipelineRegressor&quot;</span><span class="p">]:</span>
        <span class="n">_quantize_spec_weights</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">quantization_mode</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">spec</span>


<span class="k">def</span> <span class="nf">_load_and_resize_image</span><span class="p">(</span><span class="n">image_path</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">img</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">Image</span><span class="o">.</span><span class="n">ANTIALIAS</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TopKMetrics</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topk</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_topk</span> <span class="o">=</span> <span class="n">topk</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_correct_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_total_count</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">add_metric</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output1</span><span class="p">,</span> <span class="n">output2</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_total_count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_topk</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">output1</span> <span class="o">==</span> <span class="n">output2</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_correct_count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_topk</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">output1</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_topk</span><span class="p">)</span>
            <span class="n">out1_topk</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">output1</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">output1</span><span class="o">.</span><span class="n">get</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)[:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_topk</span><span class="p">]</span>
            <span class="n">out2_topk</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">output2</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">output2</span><span class="o">.</span><span class="n">get</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)[:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_topk</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">out1_topk</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">out2_topk</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_correct_count</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">display_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">pcorrect</span> <span class="o">=</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_correct_count</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_total_count</span><span class="p">))</span> <span class="o">*</span> <span class="mi">100</span>
        <span class="n">pcorrect</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">pcorrect</span><span class="p">,</span> <span class="n">decimals</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_topk</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Top 1 Agreement: </span><span class="si">{}</span><span class="s2">%</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pcorrect</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Top </span><span class="si">{}</span><span class="s2"> Agreement: </span><span class="si">{}</span><span class="s2">%</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_topk</span><span class="p">,</span> <span class="n">pcorrect</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">NoiseMetrics</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_snr</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_psnr</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_compute_snr</span><span class="p">(</span><span class="n">arr1</span><span class="p">,</span> <span class="n">arr2</span><span class="p">):</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">arr1</span> <span class="o">-</span> <span class="n">arr2</span>
        <span class="n">noise_var</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">noise</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-7</span>
        <span class="n">signal_energy</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">arr2</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr2</span><span class="p">)</span>
        <span class="n">max_signal_energy</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">arr2</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">snr</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">_np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">signal_energy</span> <span class="o">/</span> <span class="n">noise_var</span><span class="p">)</span>
        <span class="n">psnr</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">_np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">max_signal_energy</span> <span class="o">/</span> <span class="n">noise_var</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">snr</span><span class="p">,</span> <span class="n">psnr</span>

    <span class="k">def</span> <span class="nf">add_metric</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output1</span><span class="p">,</span> <span class="n">output2</span><span class="p">):</span>
        <span class="kn">import</span> <span class="nn">PIL</span>

        <span class="c1"># Output is Image</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output1</span><span class="p">,</span> <span class="n">PIL</span><span class="o">.</span><span class="n">Image</span><span class="o">.</span><span class="n">Image</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">output1</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;RGBA&quot;</span><span class="p">:</span>
                <span class="n">output1</span> <span class="o">=</span> <span class="n">output1</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&quot;RGB&quot;</span><span class="p">)</span>
                <span class="n">output2</span> <span class="o">=</span> <span class="n">output2</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&quot;RGB&quot;</span><span class="p">)</span>
            <span class="n">arr1</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">output1</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">arr2</span> <span class="o">=</span> <span class="n">_np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">output2</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">snr</span><span class="p">,</span> <span class="n">psnr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_snr</span><span class="p">(</span><span class="n">arr1</span><span class="p">,</span> <span class="n">arr2</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_snr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">snr</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_psnr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">psnr</span><span class="p">)</span>

        <span class="c1"># Output is multiArray</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">arr1</span> <span class="o">=</span> <span class="n">output1</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">arr2</span> <span class="o">=</span> <span class="n">output2</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">snr</span><span class="p">,</span> <span class="n">psnr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_snr</span><span class="p">(</span><span class="n">arr1</span><span class="p">,</span> <span class="n">arr2</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_snr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">snr</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_psnr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">psnr</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">display_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SNR:  </span><span class="si">{}</span><span class="s2"> +/- </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">_np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_snr</span><span class="p">),</span> <span class="n">_np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_snr</span><span class="p">)))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PSNR: </span><span class="si">{}</span><span class="s2"> +/- </span><span class="si">{}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">_np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_psnr</span><span class="p">),</span> <span class="n">_np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_psnr</span><span class="p">)))</span>


<div class="viewcode-block" id="OutputMetric"><a class="viewcode-back" href="../../../../source/coremltools.models.neural_network.html#coremltools.models.neural_network.quantization_utils.OutputMetric">[docs]</a><span class="k">class</span> <span class="nc">OutputMetric</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Utility class to calculate and hold metrics between</span>
<span class="sd">    two model outputs</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="nb">type</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;stringType&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">TopKMetrics</span><span class="p">(</span><span class="n">topk</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">elif</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;dictionaryType&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">TopKMetrics</span><span class="p">(</span><span class="n">topk</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>

        <span class="k">elif</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;imageType&quot;</span> <span class="ow">or</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;multiArrayType&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">NoiseMetrics</span><span class="p">())</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                <span class="sd">&quot;&quot;&quot;Unable to determine which metric to</span>
<span class="sd">            compute for output: {}&quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">name</span>
                <span class="p">)</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">add_metric</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output1</span><span class="p">,</span> <span class="n">output2</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span><span class="p">:</span>
            <span class="n">metric</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">output1</span><span class="p">,</span> <span class="n">output2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">display_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span><span class="p">:</span>
            <span class="n">metric</span><span class="o">.</span><span class="n">display_metrics</span><span class="p">()</span></div>


<div class="viewcode-block" id="ModelMetrics"><a class="viewcode-back" href="../../../../source/coremltools.models.neural_network.html#coremltools.models.neural_network.quantization_utils.ModelMetrics">[docs]</a><span class="k">class</span> <span class="nc">ModelMetrics</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A utility class to hold evaluation metrics</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spec</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_metrics</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">spec</span><span class="o">.</span><span class="n">description</span><span class="o">.</span><span class="n">output</span><span class="p">:</span>
            <span class="n">output_type</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">type</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;Type&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_metrics</span><span class="p">[</span><span class="n">output</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">OutputMetric</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">output_type</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model1_output</span><span class="p">,</span> <span class="n">model2_output</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model1_output</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_metrics</span><span class="p">[</span><span class="n">output</span><span class="p">]</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span>
                <span class="n">model1_output</span><span class="p">[</span><span class="n">output</span><span class="p">],</span> <span class="n">model2_output</span><span class="p">[</span><span class="n">output</span><span class="p">]</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">display_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_metrics</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output </span><span class="si">{}</span><span class="s2">:&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">metric</span><span class="p">))</span>
            <span class="n">dash</span> <span class="o">=</span> <span class="s2">&quot;----------&quot;</span>
            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">metric</span><span class="p">)):</span>
                <span class="n">dash</span> <span class="o">+=</span> <span class="s2">&quot;-&quot;</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">dash</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_metrics</span><span class="p">[</span><span class="n">metric</span><span class="p">]</span><span class="o">.</span><span class="n">display_metrics</span><span class="p">()</span></div>


<span class="k">def</span> <span class="nf">_characterize_qmodel_perf_with_data_dir</span><span class="p">(</span><span class="n">fpmodel</span><span class="p">,</span> <span class="n">qspec</span><span class="p">,</span> <span class="n">data_dir</span><span class="p">):</span>
    <span class="n">supported_image_exts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;jpg&quot;</span><span class="p">,</span> <span class="s2">&quot;bmp&quot;</span><span class="p">,</span> <span class="s2">&quot;png&quot;</span><span class="p">,</span> <span class="s2">&quot;jpeg&quot;</span><span class="p">]</span>
    <span class="n">test_image_paths</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">/</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">fn</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">_listdir</span><span class="p">(</span><span class="n">data_dir</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">fn</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="n">ext</span><span class="p">)</span> <span class="k">for</span> <span class="n">ext</span> <span class="ow">in</span> <span class="n">supported_image_exts</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">test_image_paths</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
            <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> contains no supported image files. &quot;</span>
            <span class="s2">&quot;Supported file types include jpg, bmp, png and jpeg.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">data_dir</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="n">qmodel</span> <span class="o">=</span> <span class="n">_get_model</span><span class="p">(</span><span class="n">qspec</span><span class="p">)</span>
    <span class="n">model_metrics</span> <span class="o">=</span> <span class="n">ModelMetrics</span><span class="p">(</span><span class="n">qspec</span><span class="p">)</span>

    <span class="n">input_name</span> <span class="o">=</span> <span class="n">qspec</span><span class="o">.</span><span class="n">description</span><span class="o">.</span><span class="n">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">name</span>
    <span class="n">input_size</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">qspec</span><span class="o">.</span><span class="n">description</span><span class="o">.</span><span class="n">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">type</span><span class="o">.</span><span class="n">imageType</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
        <span class="n">qspec</span><span class="o">.</span><span class="n">description</span><span class="o">.</span><span class="n">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">type</span><span class="o">.</span><span class="n">imageType</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Analyzing </span><span class="si">{}</span><span class="s2"> images&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_image_paths</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Running Analysis this may take a while ...&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">analyzed</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">tried</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">test_image_paths</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="nb">input</span> <span class="o">=</span> <span class="p">{</span><span class="n">input_name</span><span class="p">:</span> <span class="n">_load_and_resize_image</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)}</span>
            <span class="n">fp_pred</span> <span class="o">=</span> <span class="n">fpmodel</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">useCPUOnly</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">q_pred</span> <span class="o">=</span> <span class="n">qmodel</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">useCPUOnly</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">analyzed</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">model_metrics</span><span class="o">.</span><span class="n">add_metrics</span><span class="p">(</span><span class="n">fp_pred</span><span class="p">,</span> <span class="n">q_pred</span><span class="p">)</span>

        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
            <span class="k">continue</span>

        <span class="c1"># Update Progress</span>
        <span class="n">tried</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">tried</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">_stdout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\r</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">_stdout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;Analyzed </span><span class="si">{}</span><span class="s2">/</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tried</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_image_paths</span><span class="p">)))</span>
            <span class="n">_stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">model_metrics</span><span class="o">.</span><span class="n">display_metrics</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_characterize_quantized_model_perf</span><span class="p">(</span><span class="n">fpmodel</span><span class="p">,</span> <span class="n">qspec</span><span class="p">,</span> <span class="n">sample_data</span><span class="p">):</span>
    <span class="n">qmodel</span> <span class="o">=</span> <span class="n">_get_model</span><span class="p">(</span><span class="n">qspec</span><span class="p">)</span>
    <span class="n">model_metrics</span> <span class="o">=</span> <span class="n">ModelMetrics</span><span class="p">(</span><span class="n">qspec</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Analyzing </span><span class="si">{}</span><span class="s2"> samples&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sample_data</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Running Analysis this may take a while ...&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">analyzed</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">tried</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">sample_data</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">fp_pred</span> <span class="o">=</span> <span class="n">fpmodel</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">useCPUOnly</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">q_pred</span> <span class="o">=</span> <span class="n">qmodel</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">useCPUOnly</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">analyzed</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">model_metrics</span><span class="o">.</span><span class="n">add_metrics</span><span class="p">(</span><span class="n">fp_pred</span><span class="p">,</span> <span class="n">q_pred</span><span class="p">)</span>

        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
            <span class="k">continue</span>

        <span class="c1"># Update Progress</span>
        <span class="n">tried</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">tried</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">_stdout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\r</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">_stdout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;Analyzed </span><span class="si">{}</span><span class="s2">/</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tried</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample_data</span><span class="p">)))</span>
            <span class="n">_stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">model_metrics</span><span class="o">.</span><span class="n">display_metrics</span><span class="p">()</span>


<div class="viewcode-block" id="compare_models"><a class="viewcode-back" href="../../../../source/coremltools.models.neural_network.html#coremltools.models.neural_network.quantization_utils.compare_models">[docs]</a><span class="k">def</span> <span class="nf">compare_models</span><span class="p">(</span><span class="n">full_precision_model</span><span class="p">,</span> <span class="n">quantized_model</span><span class="p">,</span> <span class="n">sample_data</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Utility function to compare the performance of a full precision vs quantized model</span>

<span class="sd">    full_precision_model: MLModel</span>
<span class="sd">        The full precision model with float32 weights</span>

<span class="sd">    quantized_model: MLModel</span>
<span class="sd">        Quantized version of the model with quantized weights</span>

<span class="sd">    sample_data: str | [dict]</span>
<span class="sd">        Data used to characterize performance of the quantized model in</span>
<span class="sd">        comparison to the full precision model. Either a list of sample input</span>
<span class="sd">        dictionaries or an absolute path to a directory containing images.</span>
<span class="sd">        Path to a directory containing images is only valid for models with</span>
<span class="sd">        one image input. For all other models a list of sample inputs must be</span>
<span class="sd">        provided.</span>

<span class="sd">    :return:</span>
<span class="sd">        None. Performance metrics are printed out</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">emessage</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Invalid sample data provided. Only a list of dictionaries</span>
<span class="s2">    containing sample data or path to a folder containing images is</span>
<span class="s2">    supported&quot;&quot;&quot;</span>

    <span class="n">spec</span> <span class="o">=</span> <span class="n">full_precision_model</span><span class="o">.</span><span class="n">get_spec</span><span class="p">()</span>
    <span class="n">num_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">description</span><span class="o">.</span><span class="n">input</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sample_data</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">input_type</span> <span class="o">=</span> <span class="n">spec</span><span class="o">.</span><span class="n">description</span><span class="o">.</span><span class="n">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">type</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;Type&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">num_inputs</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">input_type</span> <span class="o">!=</span> <span class="s2">&quot;imageType&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                <span class="sd">&quot;&quot;&quot;Unable to analyze quantized models. Sample data</span>
<span class="sd">            was a path to a directory which is only supported with models with</span>
<span class="sd">            one image type input. Please try passing in a list of sample inputs</span>
<span class="sd">            as sample data.</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="p">)</span>
        <span class="n">_characterize_qmodel_perf_with_data_dir</span><span class="p">(</span>
            <span class="n">full_precision_model</span><span class="p">,</span> <span class="n">quantized_model</span><span class="o">.</span><span class="n">get_spec</span><span class="p">(),</span> <span class="n">sample_data</span>
        <span class="p">)</span>

    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sample_data</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">dict</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">sample_data</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="n">emessage</span><span class="p">)</span>
        <span class="n">_characterize_quantized_model_perf</span><span class="p">(</span>
            <span class="n">full_precision_model</span><span class="p">,</span> <span class="n">quantized_model</span><span class="o">.</span><span class="n">get_spec</span><span class="p">(),</span> <span class="n">sample_data</span>
        <span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="n">emessage</span><span class="p">)</span></div>


<div class="viewcode-block" id="activate_int8_int8_matrix_multiplications"><a class="viewcode-back" href="../../../../source/coremltools.models.neural_network.html#coremltools.models.neural_network.quantization_utils.activate_int8_int8_matrix_multiplications">[docs]</a><span class="k">def</span> <span class="nf">activate_int8_int8_matrix_multiplications</span><span class="p">(</span><span class="n">spec</span><span class="p">,</span> <span class="n">selector</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Utility function that takes in either a full precision (float) spec or</span>
<span class="sd">    an nbit quantized spec to selectively enable int8 activation + weight quantization</span>
<span class="sd">    of matrix multiplication operations where the second matrix represents a constant weight.</span>

<span class="sd">    spec: MLModel.get_spec()</span>
<span class="sd">        Currently conversion for only neural network models is supported.</span>
<span class="sd">        If a pipeline model is passed in then all embedded neural network models embedded within</span>
<span class="sd">        will be modified.</span>

<span class="sd">    selector: (optional) MatrixMultiplyLayerSelector</span>
<span class="sd">        A MatrixMultiplyLayerSelector object that enables int8 activation + weight quantization</span>
<span class="sd">        only on those layers for which the user-specified criterion on the minimum/maximum number</span>
<span class="sd">        of size/channels in constant weight parameters is met.</span>
<span class="sd">        It can also be derived to provide custom selection.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Recursively convert all pipeline models</span>
    <span class="k">if</span> <span class="n">spec</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;Type&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;pipeline&quot;</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">model_spec</span> <span class="ow">in</span> <span class="n">spec</span><span class="o">.</span><span class="n">pipeline</span><span class="o">.</span><span class="n">models</span><span class="p">:</span>
            <span class="n">activate_int8_int8_matrix_multiplications</span><span class="p">(</span><span class="n">model_spec</span><span class="p">,</span> <span class="n">selector</span><span class="o">=</span><span class="n">selector</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">spec</span>

    <span class="k">elif</span> <span class="n">spec</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;Type&quot;</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;pipelineClassifier&quot;</span><span class="p">,</span> <span class="s2">&quot;pipelineRegressor&quot;</span><span class="p">]:</span>
        <span class="n">activate_int8_int8_matrix_multiplications</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">selector</span><span class="o">=</span><span class="n">selector</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">spec</span>

    <span class="c1"># Neural network models</span>
    <span class="k">elif</span> <span class="n">spec</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;Type&quot;</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="s2">&quot;neuralNetwork&quot;</span><span class="p">,</span>
        <span class="s2">&quot;neuralNetworkClassifier&quot;</span><span class="p">,</span>
        <span class="s2">&quot;neuralNetworkRegressor&quot;</span><span class="p">,</span>
    <span class="p">]:</span>

        <span class="k">if</span> <span class="n">selector</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">selector</span> <span class="o">=</span> <span class="n">MatrixMultiplyLayerSelector</span><span class="p">()</span>

        <span class="c1"># Dequantize all the selected matrix multiplication layers</span>
        <span class="n">spec</span> <span class="o">=</span> <span class="n">_quantize_spec_weights</span><span class="p">(</span>
            <span class="n">spec</span><span class="p">,</span>
            <span class="n">nbits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">quantization_mode</span><span class="o">=</span><span class="n">_QUANTIZATION_MODE_DEQUANTIZE</span><span class="p">,</span>
            <span class="n">selector</span><span class="o">=</span><span class="n">selector</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">def</span> <span class="nf">_quantized_weight_and_scale</span><span class="p">(</span><span class="n">W</span><span class="p">):</span>
            <span class="n">W_max</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">_np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">_np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">W</span><span class="p">)),</span> <span class="n">_np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">_np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">W</span><span class="p">)))</span>
            <span class="n">W_normalized</span> <span class="o">=</span> <span class="n">W</span> <span class="o">/</span> <span class="n">W_max</span>  <span class="c1"># [-1,1]</span>
            <span class="n">W_quantized_int8</span> <span class="o">=</span> <span class="mf">127.0</span> <span class="o">*</span> <span class="n">W_normalized</span>  <span class="c1"># [-127, 127]</span>
            <span class="n">W_quantized_int8</span> <span class="o">=</span> <span class="n">W_quantized_int8</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">_np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
            <span class="n">quant_scale</span> <span class="o">=</span> <span class="n">W_max</span> <span class="o">/</span> <span class="mf">127.0</span>
            <span class="k">return</span> <span class="n">W_quantized_int8</span><span class="p">,</span> <span class="n">quant_scale</span>

        <span class="k">if</span> <span class="n">spec</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;Type&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;neuralNetwork&quot;</span><span class="p">:</span>
            <span class="n">nn_spec</span> <span class="o">=</span> <span class="n">spec</span><span class="o">.</span><span class="n">neuralNetwork</span>

        <span class="k">elif</span> <span class="n">spec</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;Type&quot;</span><span class="p">)</span> <span class="ow">in</span> <span class="s2">&quot;neuralNetworkClassifier&quot;</span><span class="p">:</span>
            <span class="n">nn_spec</span> <span class="o">=</span> <span class="n">spec</span><span class="o">.</span><span class="n">neuralNetworkClassifier</span>

        <span class="k">elif</span> <span class="n">spec</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;Type&quot;</span><span class="p">)</span> <span class="ow">in</span> <span class="s2">&quot;neuralNetworkRegressor&quot;</span><span class="p">:</span>
            <span class="n">nn_spec</span> <span class="o">=</span> <span class="n">spec</span><span class="o">.</span><span class="n">neuralNetworkRegressor</span>

        <span class="k">def</span> <span class="nf">_process_nn_layers</span><span class="p">(</span><span class="n">nn_spec</span><span class="p">):</span>
            <span class="n">layers</span> <span class="o">=</span> <span class="n">nn_spec</span><span class="o">.</span><span class="n">layers</span>

            <span class="c1"># Replacing each matrix multiplication</span>
            <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
                <span class="n">layer_type</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;layer&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">selector</span><span class="o">.</span><span class="n">do_quantize</span><span class="p">(</span><span class="n">layer</span><span class="p">):</span>
                    <span class="k">continue</span>

                <span class="k">if</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;branch&quot;</span><span class="p">:</span>
                    <span class="n">_process_nn_layers</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">branch</span><span class="o">.</span><span class="n">ifBranch</span><span class="p">)</span>
                    <span class="n">_process_nn_layers</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">branch</span><span class="o">.</span><span class="n">elseBranch</span><span class="p">)</span>

                <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;loop&quot;</span><span class="p">:</span>
                    <span class="n">_process_nn_layers</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">loop</span><span class="o">.</span><span class="n">conditionNetwork</span><span class="p">)</span>
                    <span class="n">_process_nn_layers</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">loop</span><span class="o">.</span><span class="n">bodyNetwork</span><span class="p">)</span>

                <span class="k">elif</span> <span class="n">layer_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;innerProduct&quot;</span><span class="p">,</span> <span class="s2">&quot;batchedMatmul&quot;</span><span class="p">]:</span>
                    <span class="c1"># Bump up to appropriate spec version if at least one replacement occurs</span>
                    <span class="n">spec</span><span class="o">.</span><span class="n">specificationVersion</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
                        <span class="n">_SPECIFICATION_VERSION_IOS_14</span><span class="p">,</span> <span class="n">spec</span><span class="o">.</span><span class="n">specificationVersion</span><span class="p">,</span>
                    <span class="p">)</span>

                    <span class="c1"># InnerProduct</span>
                    <span class="k">if</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;innerProduct&quot;</span><span class="p">:</span>
                        <span class="n">matmul_layer</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">innerProduct</span>

                    <span class="c1"># BatchedMatmul</span>
                    <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s2">&quot;batchedMatmul&quot;</span><span class="p">:</span>
                        <span class="n">matmul_layer</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">batchedMatmul</span>

                    <span class="n">wp</span> <span class="o">=</span> <span class="n">matmul_layer</span><span class="o">.</span><span class="n">weights</span>

                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">floatValue</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">qw</span><span class="p">,</span> <span class="n">qs</span> <span class="o">=</span> <span class="n">_quantized_weight_and_scale</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">floatValue</span><span class="p">)</span>

                    <span class="nb">print</span><span class="p">(</span>
                        <span class="s2">&quot;Modifying layer </span><span class="si">{}</span><span class="s2"> with size of weights </span><span class="si">{}</span><span class="s2">, to use Int8 * Int8 matrix multiplication&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">qw</span><span class="o">.</span><span class="n">size</span>
                        <span class="p">)</span>
                    <span class="p">)</span>

                    <span class="n">matmul_layer</span><span class="o">.</span><span class="n">int8DynamicQuantize</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">wp</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">numberOfBits</span> <span class="o">=</span> <span class="mi">8</span>
                    <span class="n">wp</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">linearQuantization</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="p">[</span><span class="n">qs</span><span class="p">]))</span>
                    <span class="n">wp</span><span class="o">.</span><span class="n">int8RawValue</span> <span class="o">=</span> <span class="nb">bytes</span><span class="p">()</span>
                    <span class="n">wp</span><span class="o">.</span><span class="n">int8RawValue</span> <span class="o">+=</span> <span class="n">qw</span><span class="o">.</span><span class="n">tobytes</span><span class="p">()</span>
                    <span class="k">del</span> <span class="n">wp</span><span class="o">.</span><span class="n">floatValue</span><span class="p">[:]</span>

        <span class="n">_process_nn_layers</span><span class="p">(</span><span class="n">nn_spec</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">spec</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Model Type </span><span class="si">{}</span><span class="s2"> not supported.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;Type&quot;</span><span class="p">)))</span></div>


<div class="viewcode-block" id="quantize_weights"><a class="viewcode-back" href="../../../../source/coremltools.models.neural_network.html#coremltools.models.neural_network.quantization_utils.quantize_weights">[docs]</a><span class="k">def</span> <span class="nf">quantize_weights</span><span class="p">(</span>
    <span class="n">full_precision_model</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">quantization_mode</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="n">sample_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Utility function to convert a full precision (float) MLModel to a</span>
<span class="sd">    nbit quantized MLModel (float16).</span>

<span class="sd">    full_precision_model: MLModel</span>
<span class="sd">        Model which will be converted to half precision. Currently conversion</span>
<span class="sd">        for only neural network models is supported. If a pipeline model is</span>
<span class="sd">        passed in then all embedded neural network models embedded within</span>
<span class="sd">        will be converted.</span>

<span class="sd">    nbits: int</span>
<span class="sd">        Number of bits per quantized weight. Only 16-bit float point and</span>
<span class="sd">            1-8 bit is supported</span>

<span class="sd">    quantization_mode: str</span>
<span class="sd">        One of the following:</span>

<span class="sd">        &quot;linear&quot;:</span>
<span class="sd">            Linear quantization with scale and bias assuming the range of weight</span>
<span class="sd">            values is [A, B], where A = min(weight), B = max(weight)</span>
<span class="sd">        &quot;linear_lut&quot;:</span>
<span class="sd">            Simple linear quantization represented as a lookup table</span>
<span class="sd">        &quot;kmeans_lut&quot;:</span>
<span class="sd">            LUT based quantization, where LUT is generated by K-Means clustering</span>
<span class="sd">        &quot;custom_lut&quot;:</span>
<span class="sd">            LUT quantization where LUT and quantized weight params are</span>
<span class="sd">            calculated using a custom function. If this mode is selected then</span>
<span class="sd">            a custom function must be passed in kwargs with key lut_function.</span>
<span class="sd">            The function must have input params (nbits, wp) where nbits is the</span>
<span class="sd">            number of quantization bits and wp is the list of weights for a</span>
<span class="sd">            given layer. The function should return two parameters (lut, qw)</span>
<span class="sd">            where lut is an array of length (2^n bits)containing LUT values and</span>
<span class="sd">            qw is the list of quantized weight parameters. See</span>
<span class="sd">            ``_get_linear_lookup_table_and_weight`` for a sample implementation.</span>
<span class="sd">        &quot;linear_symmetric&quot;:</span>
<span class="sd">            Linear quantization with scale and bias assuming the range of weight</span>
<span class="sd">            values is [-A, A], where A = max(abs(weight)).</span>

<span class="sd">    sample_data: str | [dict]</span>
<span class="sd">        Data used to characterize performance of the quantized model in</span>
<span class="sd">        comparison to the full precision model. Either a list of sample input</span>
<span class="sd">        dictionaries or an absolute path to a directory containing images.</span>
<span class="sd">        Path to a directory containing images is only valid for models with</span>
<span class="sd">        one image input. For all other models a list of sample inputs must be</span>
<span class="sd">        provided.</span>

<span class="sd">    kwargs: keyword arguments</span>
<span class="sd">            *lut_function* : (``callable function``)</span>
<span class="sd">                A callable function provided when quantization mode is set to</span>
<span class="sd">                ``_QUANTIZATION_MODE_CUSTOM_LOOKUP_TABLE``. See ``quantization_mode``</span>
<span class="sd">                for more details.</span>
<span class="sd">            *selector*: QuantizedLayerSelector</span>
<span class="sd">                A QuanatizedLayerSelector object that can be derived to provide</span>
<span class="sd">                custom quantization selection.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    model: MLModel</span>
<span class="sd">        The quantized MLModel instance if running on macOS 10.14 or later,</span>
<span class="sd">        otherwise the quantized model specification is returned</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    .. sourcecode:: python</span>

<span class="sd">        &gt;&gt;&gt; import coremltools</span>
<span class="sd">        &gt;&gt;&gt; from coremltools.models.neural_network import quantization_utils</span>
<span class="sd">        &gt;&gt;&gt; model = coremltools.models.MLModel(&#39;my_model.mlmodel&#39;)</span>
<span class="sd">        &gt;&gt;&gt; quantized_model = quantization_utils.quantize_weights(model, 8, &quot;linear&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">qmode_mapping</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;linear&quot;</span><span class="p">:</span> <span class="n">_QUANTIZATION_MODE_LINEAR_QUANTIZATION</span><span class="p">,</span>
        <span class="s2">&quot;kmeans&quot;</span><span class="p">:</span> <span class="n">_QUANTIZATION_MODE_LOOKUP_TABLE_KMEANS</span><span class="p">,</span>
        <span class="s2">&quot;kmeans_lut&quot;</span><span class="p">:</span> <span class="n">_QUANTIZATION_MODE_LOOKUP_TABLE_KMEANS</span><span class="p">,</span>
        <span class="s2">&quot;linear_lut&quot;</span><span class="p">:</span> <span class="n">_QUANTIZATION_MODE_LOOKUP_TABLE_LINEAR</span><span class="p">,</span>
        <span class="s2">&quot;custom_lut&quot;</span><span class="p">:</span> <span class="n">_QUANTIZATION_MODE_CUSTOM_LOOKUP_TABLE</span><span class="p">,</span>
        <span class="s2">&quot;dequantization&quot;</span><span class="p">:</span> <span class="n">_QUANTIZATION_MODE_DEQUANTIZE</span><span class="p">,</span>
        <span class="s2">&quot;linear_symmetric&quot;</span><span class="p">:</span> <span class="n">_QUANTIZATION_MODE_LINEAR_SYMMETRIC</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">qmode</span> <span class="o">=</span> <span class="n">qmode_mapping</span><span class="p">[</span><span class="n">quantization_mode</span><span class="p">]</span>
    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
        <span class="c1"># kmeans is deprecated. Instead kmeans_lut is used. No need to show it.</span>
        <span class="k">del</span> <span class="n">qmode_mapping</span><span class="p">[</span><span class="s2">&quot;kmeans&quot;</span><span class="p">]</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
            <span class="s2">&quot;Invalid quantization mode. Quantization mode must be &quot;</span>
            <span class="s2">&quot;one of </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">qmode_mapping</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Quantizing using </span><span class="si">{}</span><span class="s2"> quantization&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">quantization_mode</span><span class="p">))</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="n">full_precision_model</span><span class="o">.</span><span class="n">get_spec</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">nbits</span> <span class="o">==</span> <span class="mi">16</span> <span class="ow">and</span> <span class="n">spec</span><span class="o">.</span><span class="n">isUpdatable</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;updatable models cannot get quantized to FP16.&quot;</span><span class="p">)</span>
    <span class="n">qspec</span> <span class="o">=</span> <span class="n">_quantize_spec_weights</span><span class="p">(</span><span class="n">spec</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">qmode</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">_macos_version</span><span class="p">()</span> <span class="o">&lt;</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">14</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="s2">&quot;WARNING - Unable to return a quantized MLModel instance since &quot;</span>
            <span class="s2">&quot;OS is not macOS 10.14 or later. Returning a quantized model &quot;</span>
            <span class="s2">&quot;specification instead.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">qspec</span>

    <span class="n">quantized_model</span> <span class="o">=</span> <span class="n">_get_model</span><span class="p">(</span><span class="n">qspec</span><span class="p">,</span> <span class="n">compute_units</span><span class="o">=</span><span class="n">full_precision_model</span><span class="o">.</span><span class="n">compute_unit</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">sample_data</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">quantized_model</span>

    <span class="n">compare_models</span><span class="p">(</span><span class="n">full_precision_model</span><span class="p">,</span> <span class="n">quantized_model</span><span class="p">,</span> <span class="n">sample_data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">quantized_model</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Apple Inc.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>