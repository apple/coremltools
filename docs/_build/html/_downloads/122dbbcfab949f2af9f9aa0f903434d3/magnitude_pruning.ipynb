{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Magnitude Pruning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, you learn how to train a simple convolutional neural network on\n[MNIST](http://yann.lecun.com/exdb/mnist/) using :py:class:`~.pruning.MagnitudePruner`.\n\nLearn more about other pruners and schedulers in the coremltools \n[Training-Time Pruning Documentation](https://coremltools.readme.io/v7.0/docs/data-dependent-pruning).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Network and Dataset Definition\nFirst define your network, which consists of a single convolution layer\nfollowed by a dense (linear) layer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef mnist_net(num_classes=10):\n    return nn.Sequential(\n        OrderedDict(\n            [('conv', nn.Conv2d(1, 12, 3, padding='same')),\n             ('relu', nn.ReLU()),\n             ('pool', nn.MaxPool2d(2, stride=2, padding=0)),\n             ('flatten', nn.Flatten()),\n             ('dense', nn.Linear(2352, num_classes)),\n             ('softmax', nn.LogSoftmax())]\n        )\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the [MNIST dataset provided by PyTorch](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#mnist)\nfor training. Apply a very simple transformation to the input\nimages to normalize them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nfrom torchvision import datasets, transforms\n\n\ndef mnist_dataset(data_dir=\"~/.mnist_pruning_data\"):\n    transform = transforms.Compose(\n        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n    )\n    data_path = os.path.expanduser(f\"{data_dir}/mnist\")\n    if not os.path.exists(data_path):\n        os.makedirs(data_path)\n    train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n    test = datasets.MNIST(data_path, train=False, transform=transform)\n    return train, test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, initialize the model and the dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = mnist_net()\n\nbatch_size = 128\ntrain_dataset, test_dataset = mnist_dataset()\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the Model Without Pruning\nTrain the model without any pruning applied.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), eps=1e-07)\naccuracy_unpruned = 0.0\nnum_epochs = 4\n\n\ndef train_step(model, optimizer, train_loader, data, target, batch_idx, epoch):\n    optimizer.zero_grad()\n    output = model(data)\n    loss = F.nll_loss(output, target)\n    loss.backward()\n    optimizer.step()\n    if batch_idx % 100 == 0:\n        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n            epoch, batch_idx * len(data), len(train_loader.dataset),\n            100. * batch_idx / len(train_loader), loss.item()))\n\n\ndef eval_model(model, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n        test_loss /= len(test_loader.dataset)\n        accuracy = 100. * correct / len(test_loader.dataset)\n\n        print(\n            \"\\nTest set: Average loss: {:.4f}, Accuracy: {:.1f}%\\n\".format(\n                test_loss, accuracy\n            )\n        )\n    return accuracy\n\n\nfor epoch in range(num_epochs):\n    # train one epoch\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        train_step(model, optimizer, train_loader, data, target, batch_idx, epoch)\n\n    # evaluate\n    accuracy_unpruned = eval_model(model, test_loader)\n\n\nprint(\"Accuracy of unpruned network: {:.1f}%\\n\".format(accuracy_unpruned))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installing the Pruner in the Model\nInstall :py:class:`~.pruning.MagnitudePruner` in the trained model.\n\nFirst, construct a :py:class:`~.pruning.pruning_scheduler.PruningScheduler` class,\nwhich specifies how the sparsity of your pruned layers should evolve over the course of the training.\nFor this tutorial, use a :py:class:`~.pruning.PolynomialDecayScheduler`,\nwhich is introduced in the paper [\"To prune or not to prune\"](https://arxiv.org/pdf/1710.01878.pdf).\n\nBegin pruning from step ``0`` and prune every ``100`` steps for two epochs. As you\nstep through this pruning scheduler, the sparsity of pruned modules will increase\ngradually from the initial value to the target value.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from coremltools.optimize.torch.pruning import PolynomialDecayScheduler\n\nscheduler = PolynomialDecayScheduler(update_steps=list(range(0, 900, 100)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, create an instance of the :py:class:`~.pruning.MagnitudePrunerConfig` class\nto specify how you want different submodules to be pruned.\nSet the target sparsity of the convolution layer\nto ``70 %`` and the dense layer to ``80 %``. The point of this is to demonstrate that\ndifferent layers can be targeted at different sparsity levels. In practice, the sparsity\nlevel of a layer is a hyperparameter, which needs to be tuned for your requirements and\nthe amenability of the layer to sparsification.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from coremltools.optimize.torch.pruning import (\n    MagnitudePruner,\n    MagnitudePrunerConfig,\n    ModuleMagnitudePrunerConfig,\n)\n\nconv_config = ModuleMagnitudePrunerConfig(target_sparsity=0.7)\nlinear_config = ModuleMagnitudePrunerConfig(target_sparsity=0.8)\n\nconfig = MagnitudePrunerConfig().set_module_type(torch.nn.Conv2d, conv_config)\nconfig = config.set_module_type(torch.nn.Linear, linear_config)\n\npruner = MagnitudePruner(model, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, call :py:meth:`~.pruning.MagnitudePruner.prepare` to insert pruning\n``forward pre hooks`` on the modules configured previously.\nThese forward pre hooks are called before a call to the forward\nmethod of the module. They multiply the parameter with a pruning mask, which\nis a tensor of the same shape as the parameter, in which each element has a value of\neither ``1`` or ``0``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pruner.prepare(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-Tuning the Pruned Model\nThe next step is to fine tune the model with pruning applied. In order to prune the model,\ncall the :py:meth:`~.pruning.MagnitudePruner.step` method on the pruner\nafter every call to ``optimizer.step()`` to step through the pruning schedule.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), eps=1e-07)\naccuracy_pruned = 0.0\nnum_epochs = 2\n\nfor epoch in range(num_epochs):\n    # train one epoch\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        train_step(model, optimizer, train_loader, data, target, batch_idx, epoch)\n        pruner.step()\n\n    # evaluate\n    accuracy_pruned = eval_model(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The evaluation shows that you can train a pruned network without losing\naccuracy with the final model. In practice, for more complex models,\nyou have a trade-off between the sparsity and the validation accuracy\nthat can be achieved for the model. Finding the right sweet spot on this\ntrade-off curve depends on the model and task.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy of pruned network: {:.1f}%\\n\".format(accuracy_pruned))\nprint(\"Accuracy of unpruned network: {:.1f}%\\n\".format(accuracy_unpruned))\n\nnp.testing.assert_allclose(accuracy_pruned, accuracy_unpruned, atol=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Finalizing the Model for Export\n\nThe example shows that you can prune the model with a few code changes to your\nexisting PyTorch training code. Now you can deploy this model on a device.\n\nTo finalize the model for export, call :py:meth:`~.pruning.MagnitudePruner.finalize`\non the pruner. This removes all the forward pre-hooks you had attached on the submodules.\nIt also freezes the state of the pruner and multiplies the pruning mask with the corresponding\nweight matrix.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.eval()\npruner.finalize(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exporting the Model for On-Device Execution\n\nIn order to deploy the model, convert it to a Core ML model.\n\nFollow the same steps in Core ML Tools for exporting a regular PyTorch model\n(for details, see [Converting from PyTorch](https://coremltools.readme.io/docs/pytorch-conversion)).\nThe parameter ``ct.PassPipeline.DEFAULT_PRUNING`` signals to the converter that\nthe model being converted is a pruned model, and allows the model weights to be represented as\nsparse matrices, which have a smaller memory footprint than dense matrices.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import coremltools as ct\n\nexample_input = torch.rand(1, 1, 28, 28)\ntraced_model = torch.jit.trace(model, example_input)\n\ncoreml_model = ct.convert(\n    traced_model,\n    inputs=[ct.TensorType(shape=example_input.shape)],\n    pass_pipeline=ct.PassPipeline.DEFAULT_PRUNING,\n    minimum_deployment_target=ct.target.iOS16,\n)\n\ncoreml_model.save(\"~/.mnist_pruning_data/pruned_model.mlpackage\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}