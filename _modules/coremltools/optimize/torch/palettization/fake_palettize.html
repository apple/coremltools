<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>coremltools.optimize.torch.palettization.fake_palettize &mdash; coremltools API Reference 7.0 documentation</title>
      <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/sg_gallery-rendered-html.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/css/norightmargin.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../../" id="documentation_options" src="../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../_static/jquery.js"></script>
        <script src="../../../../../_static/underscore.js"></script>
        <script src="../../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../../../_static/doctools.js"></script>
        <script src="../../../../../_static/sphinx_highlight.js"></script>
    <script src="../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../index.html" class="icon icon-home">
            coremltools API Reference
          </a>
              <div class="version">
                7.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">API Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../source/coremltools.converters.html">Converters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../source/coremltools.models.html">Model APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../source/coremltools.converters.mil.html">MIL Builder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../source/coremltools.converters.mil.input_types.html">MIL Input Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../source/coremltools.converters.mil.mil.ops.defs.html">MIL Ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../source/coremltools.converters.mil.mil.passes.defs.html">MIL Graph Passes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../source/coremltools.optimize.html">Optimizers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://apple.github.io/coremltools/docs-guides/index.html">Guide and Examples</a></li>
<li class="toctree-l1"><a class="reference external" href="https://apple.github.io/coremltools/mlmodel/index.html">Core ML Format Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../source/api-versions.html">Previous Versions</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/apple/coremltools">GitHub</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">coremltools API Reference</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">coremltools.optimize.torch.palettization.fake_palettize</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for coremltools.optimize.torch.palettization.fake_palettize</h1><div class="highlight"><pre>
<span></span><span class="c1">#  Copyright (c) 2023, Apple Inc. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1">#  Use of this source code is governed by a BSD-3-clause license that can be</span>
<span class="c1">#  found in the LICENSE.txt file or at https://opensource.org/licenses/BSD-3-Clause</span>

<span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">gc</span>

<span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">_torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">_F</span>
<span class="kn">from</span> <span class="nn">torch.ao.quantization.observer</span> <span class="kn">import</span> <span class="n">ObserverBase</span> <span class="k">as</span> <span class="n">_ObserverBase</span>
<span class="kn">from</span> <span class="nn">torch.quantization</span> <span class="kn">import</span> <span class="n">FakeQuantize</span> <span class="k">as</span> <span class="n">_FakeQuantize</span>

<span class="kn">from</span> <span class="nn">._efficient_kmeans</span> <span class="kn">import</span> <span class="n">_EfficientKMeans</span>
<span class="kn">from</span> <span class="nn">._fake_palettizer_tensor_hook</span> <span class="kn">import</span> <span class="n">_FakePalettizationTensorHook</span>
<span class="kn">from</span> <span class="nn">._partitioner</span> <span class="kn">import</span> <span class="n">_Partitioner</span>
<span class="kn">from</span> <span class="nn">.palettization_config</span> <span class="kn">import</span> <span class="n">DEFAULT_PALETTIZATION_ADVANCED_OPTIONS</span>


<div class="viewcode-block" id="FakePalettize"><a class="viewcode-back" href="../../../../../source/coremltools.optimize.torch.palettization.html#coremltools.optimize.torch.palettization.FakePalettize">[docs]</a><span class="k">class</span> <span class="nc">FakePalettize</span><span class="p">(</span><span class="n">_FakeQuantize</span><span class="p">,</span> <span class="n">_Partitioner</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A class that implements palettization algorithm described in</span>
<span class="sd">    `DKM: Differentiable K-Means Clustering Layer for Neural Network Compression</span>
<span class="sd">    &lt;https://arxiv.org/abs/2108.12659&gt;`_. It clusters the weights</span>
<span class="sd">    using a differentiable version of ``k-means``, allowing the look-up-table (LUT)</span>
<span class="sd">    and indices of palettized weights to be learnt using a gradient-based optimization</span>
<span class="sd">    algorithm such as SGD.</span>

<span class="sd">    Extends :py:class:`torch.quantization.FakeQuantize` to add support for</span>
<span class="sd">    palettization.</span>

<span class="sd">    Example:</span>
<span class="sd">            .. code-block:: python</span>

<span class="sd">                from collections import OrderedDict</span>
<span class="sd">                import torch</span>
<span class="sd">                import torch.nn as nn</span>
<span class="sd">                import coremltools.optimize.torch.palettization as palett</span>

<span class="sd">                model = nn.Sequential(</span>
<span class="sd">                    OrderedDict(</span>
<span class="sd">                        [</span>
<span class="sd">                            (&quot;linear1&quot;, nn.Linear(4, 5)),</span>
<span class="sd">                            (&quot;sigmoid1&quot;, nn.Sigmoid()),</span>
<span class="sd">                            (&quot;linear2&quot;, nn.Linear(5, 4)),</span>
<span class="sd">                            (&quot;sigmoid2&quot;, nn.Sigmoid),</span>
<span class="sd">                        ]</span>
<span class="sd">                    )</span>
<span class="sd">                )</span>

<span class="sd">                fq_activation = nn.Identity</span>
<span class="sd">                fq_weight = palett.FakePalettize.with_args(</span>
<span class="sd">                    observer=torch.quantization.MovingAveragePerChannelMinMaxObserver.with_args(</span>
<span class="sd">                        quant_min=-128, quant_max=127, dtype=torch.qint8</span>
<span class="sd">                    ),</span>
<span class="sd">                    n_bits=2,</span>
<span class="sd">                    cluster_dim=1,</span>
<span class="sd">                )</span>
<span class="sd">                model.linear2.qconfig = torch.quantization.QConfig(</span>
<span class="sd">                    activation=fq_activation, weight=fq_weight</span>
<span class="sd">                )</span>

<span class="sd">                palettized_model = palett.prepare_palettizer(model)</span>

<span class="sd">                train_model(palettized_model)</span>

<span class="sd">                palettized_converted_model = palett.finalize(palettized_model)</span>


<span class="sd">    Args:</span>
<span class="sd">        observer (:obj:`torch.ao.quantization.observer.ObserverBase`): Observer for quantizing the ``LUT``.</span>
<span class="sd">        n_bits (:obj:`int`): Number of palettization bits. There would be :math:`2^{n\_bits}` unique weights in the ``LUT``.</span>
<span class="sd">        cluster_dim (:obj:`int`): Dimensionality of centroids to use for clustering.</span>
<span class="sd">        quant_min (:obj:`int`): The minimum allowable quantized value.</span>
<span class="sd">        quant_max (:obj:`int`): The maximum allowable quantized value.</span>
<span class="sd">        cluster_dtype (:obj:`str`): String that decides whether to quantize the ``LUT`` or not. The following are the ``str``</span>
<span class="sd">            LUT quantization combinations: (``u8``, ``uint8``), (``i8``, ``int8``), and (``f16``, ``float16``).</span>
<span class="sd">        advanced_options (:obj:`dict`): Advanced options to configure the palettization algorithm.</span>
<span class="sd">        observer_kwargs (optional): Arguments for the observer module.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Allowed keys for ``advanced_options`` are the parameters listed as ``optional`` in</span>
<span class="sd">        :py:class:`ModuleDKMPalettizerConfig`, besides the ones already covered by other parameters in this class.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">fake_palett_enabled</span><span class="p">:</span> <span class="n">_torch</span><span class="o">.</span><span class="n">Tensor</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">observer</span><span class="p">:</span> <span class="n">_ObserverBase</span><span class="p">,</span>
        <span class="n">n_bits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">cluster_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">128</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">127</span><span class="p">,</span>
        <span class="n">cluster_dtype</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;f32&quot;</span><span class="p">,</span>
        <span class="n">advanced_options</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{},</span>
        <span class="o">**</span><span class="n">observer_kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">partition_size</span> <span class="o">=</span> <span class="n">advanced_options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;partition_size&quot;</span><span class="p">,</span> <span class="n">DEFAULT_PALETTIZATION_ADVANCED_OPTIONS</span><span class="p">[</span><span class="s2">&quot;partition_size&quot;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">cluster_permute</span> <span class="o">=</span> <span class="n">advanced_options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;cluster_permute&quot;</span><span class="p">,</span> <span class="n">DEFAULT_PALETTIZATION_ADVANCED_OPTIONS</span><span class="p">[</span><span class="s2">&quot;cluster_permute&quot;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">palett_max_mem</span> <span class="o">=</span> <span class="n">advanced_options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;palett_max_mem&quot;</span><span class="p">,</span> <span class="n">DEFAULT_PALETTIZATION_ADVANCED_OPTIONS</span><span class="p">[</span><span class="s2">&quot;palett_max_mem&quot;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">kmeans_max_iter</span> <span class="o">=</span> <span class="n">advanced_options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;kmeans_max_iter&quot;</span><span class="p">,</span> <span class="n">DEFAULT_PALETTIZATION_ADVANCED_OPTIONS</span><span class="p">[</span><span class="s2">&quot;kmeans_max_iter&quot;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">prune_threshold</span> <span class="o">=</span> <span class="n">advanced_options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;prune_threshold&quot;</span><span class="p">,</span> <span class="n">DEFAULT_PALETTIZATION_ADVANCED_OPTIONS</span><span class="p">[</span><span class="s2">&quot;prune_threshold&quot;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">kmeans_init</span> <span class="o">=</span> <span class="n">advanced_options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;kmeans_init&quot;</span><span class="p">,</span> <span class="n">DEFAULT_PALETTIZATION_ADVANCED_OPTIONS</span><span class="p">[</span><span class="s2">&quot;kmeans_init&quot;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">kmeans_opt1d_threshold</span> <span class="o">=</span> <span class="n">advanced_options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;kmeans_opt1d_threshold&quot;</span><span class="p">,</span>
            <span class="n">DEFAULT_PALETTIZATION_ADVANCED_OPTIONS</span><span class="p">[</span><span class="s2">&quot;kmeans_opt1d_threshold&quot;</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">enforce_zero</span> <span class="o">=</span> <span class="n">advanced_options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;enforce_zero&quot;</span><span class="p">,</span> <span class="n">DEFAULT_PALETTIZATION_ADVANCED_OPTIONS</span><span class="p">[</span><span class="s2">&quot;enforce_zero&quot;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">palett_mode</span> <span class="o">=</span> <span class="n">advanced_options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;palett_mode&quot;</span><span class="p">,</span> <span class="n">DEFAULT_PALETTIZATION_ADVANCED_OPTIONS</span><span class="p">[</span><span class="s2">&quot;palett_mode&quot;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">palett_cluster_tol</span> <span class="o">=</span> <span class="n">advanced_options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;palett_cluster_tol&quot;</span><span class="p">,</span> <span class="n">DEFAULT_PALETTIZATION_ADVANCED_OPTIONS</span><span class="p">[</span><span class="s2">&quot;palett_cluster_tol&quot;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">palett_tau</span> <span class="o">=</span> <span class="n">advanced_options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;palett_tau&quot;</span><span class="p">,</span> <span class="n">DEFAULT_PALETTIZATION_ADVANCED_OPTIONS</span><span class="p">[</span><span class="s2">&quot;palett_tau&quot;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">palett_epsilon</span> <span class="o">=</span> <span class="n">advanced_options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;palett_epsilon&quot;</span><span class="p">,</span> <span class="n">DEFAULT_PALETTIZATION_ADVANCED_OPTIONS</span><span class="p">[</span><span class="s2">&quot;palett_epsilon&quot;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">palett_lambda</span> <span class="o">=</span> <span class="n">advanced_options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;palett_lambda&quot;</span><span class="p">,</span> <span class="n">DEFAULT_PALETTIZATION_ADVANCED_OPTIONS</span><span class="p">[</span><span class="s2">&quot;palett_lambda&quot;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">add_extra_centroid</span> <span class="o">=</span> <span class="n">advanced_options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;add_extra_centroid&quot;</span><span class="p">,</span> <span class="n">DEFAULT_PALETTIZATION_ADVANCED_OPTIONS</span><span class="p">[</span><span class="s2">&quot;add_extra_centroid&quot;</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_target_module_level_sparsity</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="n">_FakeQuantize</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observer</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">,</span> <span class="o">**</span><span class="n">observer_kwargs</span><span class="p">)</span>
        <span class="n">_Partitioner</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">n_bits</span><span class="p">,</span>
            <span class="n">enforce_zero</span><span class="p">,</span>
            <span class="n">partition_size</span><span class="p">,</span>
            <span class="n">cluster_dim</span><span class="p">,</span>
            <span class="n">cluster_permute</span><span class="p">,</span>
            <span class="n">palett_tau</span><span class="p">,</span>
            <span class="n">kmeans_init</span><span class="p">,</span>
            <span class="n">prune_threshold</span><span class="p">,</span>
            <span class="n">kmeans_opt1d_threshold</span><span class="p">,</span>
            <span class="n">add_extra_centroid</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">cluster_dtype</span> <span class="o">=</span> <span class="n">cluster_dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_extra_centroid</span> <span class="o">=</span> <span class="n">add_extra_centroid</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">need_to_quantize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster_dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;i8&quot;</span><span class="p">,</span> <span class="s2">&quot;u8&quot;</span><span class="p">,</span> <span class="s2">&quot;f16&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">autograd_graph</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">_torch</span><span class="o">.</span><span class="n">autograd</span><span class="p">,</span> <span class="s2">&quot;graph&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">palett_max_mem</span> <span class="o">&lt;</span> <span class="mf">1.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">palett_max_mem</span> <span class="o">=</span> <span class="n">palett_max_mem</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">palett_cluster_tol</span> <span class="o">=</span> <span class="n">palett_cluster_tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kmeans_max_iter</span> <span class="o">=</span> <span class="n">kmeans_max_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">palett_mode</span> <span class="o">=</span> <span class="n">palett_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">palett_tau</span> <span class="o">=</span> <span class="n">palett_tau</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">palett_epsilon</span> <span class="o">=</span> <span class="n">palett_epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">palett_lambda</span> <span class="o">=</span> <span class="n">palett_lambda</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_bits</span> <span class="o">=</span> <span class="n">n_bits</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cluster_dim</span> <span class="o">=</span> <span class="n">cluster_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kmeans_init</span> <span class="o">=</span> <span class="n">kmeans_init</span>
        <span class="c1"># Temporary create placeholder buffers that will get replaced with proper centroids on the first forward,</span>
        <span class="c1"># or when we reload a checkpoint. Having placeholder values is useful to maintain the structure of the state</span>
        <span class="c1"># dict constant.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;centroids&quot;</span><span class="p">,</span> <span class="n">_torch</span><span class="o">.</span><span class="n">rand</span><span class="p">([</span><span class="mi">1</span><span class="p">]))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;labels&quot;</span><span class="p">,</span> <span class="n">_torch</span><span class="o">.</span><span class="n">rand</span><span class="p">([</span><span class="mi">1</span><span class="p">]))</span>
        <span class="c1"># During init, we would want the fake_palett_enabled flag to be False, i.e. to be at a state of 0. Also, we</span>
        <span class="c1"># would have set the fake_quant_enabled and observer_enabled to be 0 as well so that palettizer does nothing</span>
        <span class="c1"># until the first milestone.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;fake_palett_enabled&quot;</span><span class="p">,</span> <span class="n">_torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">_torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">disable_fake_quant</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">disable_observer</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffers_are_placeholders</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">enable_fake_palett</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enabled</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fake_palett_enabled</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">enabled</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">disable_fake_palett</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enable_fake_palett</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">diff_palettize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">_torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method called to run the differentiable k-means operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">use_cpu_if_cuda_available</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">_torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">t</span> <span class="o">=</span> <span class="n">_torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">total_memory</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">_torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">use_cpu_if_cuda_available</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">/</span> <span class="n">t</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">palett_max_mem</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">autograd_graph</span>
            <span class="k">if</span> <span class="n">use_cpu_if_cuda_available</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">_FakePalettizationTensorHook</span><span class="o">.</span><span class="n">gc_trigger</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">_FakePalettizationTensorHook</span><span class="o">.</span><span class="n">gc_trigger</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="n">_FakePalettizationTensorHook</span><span class="o">.</span><span class="n">gc_trigger</span><span class="p">:</span>
            <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>

        <span class="n">auto_grad_graph_on_cpu</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">_torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">save_on_cpu</span><span class="p">(</span><span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">use_cpu_if_cuda_available</span>
            <span class="k">else</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">nullcontext</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">partition</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">partitions</span><span class="p">):</span>

            <span class="n">current_partition_clone</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">partition</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">:</span> <span class="n">partition</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="n">cX</span><span class="p">,</span> <span class="n">pad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">current_partition_clone</span><span class="p">)</span>

            <span class="k">with</span> <span class="n">_torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">palett_table</span> <span class="o">=</span> <span class="n">_torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">palett_table</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_clusters</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">palett_cluster_tol</span><span class="p">:</span>
                    <span class="c1"># We use n_init as 3 so as to not spend a lot of time running this operation</span>
                    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">_EfficientKMeans</span><span class="p">(</span>
                        <span class="n">n_clusters</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_clusters</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                        <span class="n">init</span><span class="o">=</span><span class="s2">&quot;kmeans++&quot;</span><span class="p">,</span>
                        <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                        <span class="n">n_init</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                        <span class="n">max_iter</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">kmeans</span><span class="o">.</span><span class="n">kmeans_pp</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">cX</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>

            <span class="n">centroids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

            <span class="k">assert</span> <span class="ow">not</span> <span class="n">centroids</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="n">last_inertia</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kmeans_max_iter</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">autograd_graph</span><span class="p">:</span>
                    <span class="n">tensor_hook</span> <span class="o">=</span> <span class="n">_FakePalettizationTensorHook</span><span class="p">(</span>
                        <span class="p">[</span><span class="n">_torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">cX</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">centroids</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]])],</span>
                        <span class="n">use_cpu_if_cuda_available</span><span class="p">,</span>
                        <span class="sa">f</span><span class="s2">&quot;FakePalettizationTensorHook.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">j</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">palett_tau</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">auto_grad_graph_hook_init</span> <span class="o">=</span> <span class="n">_torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">saved_tensors_hooks</span><span class="p">(</span>
                        <span class="n">tensor_hook</span><span class="o">.</span><span class="n">init_pack</span><span class="p">,</span> <span class="n">tensor_hook</span><span class="o">.</span><span class="n">init_unpack</span>
                    <span class="p">)</span>
                    <span class="n">auto_grad_graph_hook_reuse</span> <span class="o">=</span> <span class="n">_torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">saved_tensors_hooks</span><span class="p">(</span>
                        <span class="n">tensor_hook</span><span class="o">.</span><span class="n">reuse_pack</span><span class="p">,</span> <span class="n">tensor_hook</span><span class="o">.</span><span class="n">reuse_unpack</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">auto_grad_graph_hook_init</span> <span class="o">=</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">nullcontext</span><span class="p">()</span>
                    <span class="n">auto_grad_graph_hook_reuse</span> <span class="o">=</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">nullcontext</span><span class="p">()</span>

                <span class="k">with</span> <span class="n">auto_grad_graph_hook_init</span><span class="p">:</span>
                    <span class="n">x_c_dist</span> <span class="o">=</span> <span class="n">_EfficientKMeans</span><span class="o">.</span><span class="n">x_c_dist</span><span class="p">(</span><span class="n">cX</span><span class="p">,</span> <span class="n">centroids</span><span class="p">)</span>
                    <span class="n">min_error</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x_c_dist</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

                <span class="k">with</span> <span class="n">auto_grad_graph_hook_reuse</span><span class="p">:</span>
                    <span class="k">if</span> <span class="s2">&quot;dkm&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">palett_mode</span><span class="p">:</span>
                        <span class="n">attention</span> <span class="o">=</span> <span class="n">_F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="o">-</span><span class="n">x_c_dist</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">palett_tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="s2">&quot;gsm&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">palett_mode</span><span class="p">:</span>
                        <span class="n">attention</span> <span class="o">=</span> <span class="n">_F</span><span class="o">.</span><span class="n">gumbel_softmax</span><span class="p">(</span><span class="o">-</span><span class="n">x_c_dist</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">palett_tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="s2">&quot;hard&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">palett_mode</span><span class="p">:</span>
                        <span class="n">col_idx</span> <span class="o">=</span> <span class="n">x_c_dist</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">indices</span>
                        <span class="n">row_idx</span> <span class="o">=</span> <span class="n">_torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">col_idx</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">_torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
                            <span class="n">cX</span><span class="o">.</span><span class="n">device</span>
                        <span class="p">)</span>
                        <span class="n">attention</span> <span class="o">=</span> <span class="n">_torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span>
                            <span class="n">_torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">row_idx</span><span class="p">,</span> <span class="n">col_idx</span><span class="p">]),</span>
                            <span class="n">_torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">row_idx</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cX</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
                            <span class="n">x_c_dist</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="n">x_c_dist</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                            <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>

                <span class="k">assert</span> <span class="n">attention</span><span class="o">.</span><span class="n">requires_grad</span>
                <span class="n">attention_sum</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">attention_sum</span><span class="p">[</span><span class="n">attention_sum</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1e-6</span>

                <span class="k">with</span> <span class="n">auto_grad_graph_hook_reuse</span><span class="p">:</span>
                    <span class="n">centroids</span> <span class="o">=</span> <span class="n">_torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">cX</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">attention</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> <span class="o">/</span> <span class="n">attention_sum</span>

                <span class="k">with</span> <span class="n">auto_grad_graph_on_cpu</span><span class="p">:</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">need_to_quantize</span><span class="p">:</span>
                        <span class="n">centroids</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">centroids</span><span class="p">)</span>

                    <span class="k">assert</span> <span class="n">centroids</span><span class="o">.</span><span class="n">requires_grad</span>

                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prune_threshold</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">centroids</span> <span class="o">=</span> <span class="n">_torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Hardshrink</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prune_threshold</span><span class="o">.</span><span class="n">item</span><span class="p">())(</span><span class="n">centroids</span><span class="p">)</span>

                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enforce_zero</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
                        <span class="n">zero_point</span> <span class="o">=</span> <span class="p">(</span>
                            <span class="n">_torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">centroids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">centroids</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                        <span class="p">)</span>
                        <span class="n">zero_idx</span> <span class="o">=</span> <span class="n">_torch</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">_torch</span><span class="o">.</span><span class="n">cdist</span><span class="p">(</span><span class="n">centroids</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">))</span>
                        <span class="n">centroids</span><span class="p">[</span><span class="n">zero_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">zero_point</span>

                <span class="n">cur_inertia</span> <span class="o">=</span> <span class="n">min_error</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

                <span class="k">if</span> <span class="n">last_inertia</span> <span class="ow">and</span> <span class="nb">abs</span><span class="p">(</span><span class="n">last_inertia</span> <span class="o">-</span> <span class="n">cur_inertia</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">palett_epsilon</span><span class="p">:</span>
                    <span class="k">break</span>

                <span class="n">last_inertia</span> <span class="o">=</span> <span class="n">cur_inertia</span>

            <span class="k">with</span> <span class="n">auto_grad_graph_hook_reuse</span><span class="p">:</span>
                <span class="n">weights</span><span class="p">[</span><span class="n">partition</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">:</span> <span class="n">partition</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">deflatten</span><span class="p">(</span>
                    <span class="n">_torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">centroids</span><span class="p">),</span> <span class="n">current_partition_clone</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">pad</span>
                <span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">palett_lambda</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">palett_lambda</span><span class="p">)</span> <span class="o">*</span> <span class="n">centroids</span>
                <span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">data</span>

        <span class="k">return</span> <span class="n">weights</span>

    <span class="k">def</span> <span class="nf">palettize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">_torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method is run during inference time by the forward method of the ``FakePalettize`` class. It calculates the</span>
<span class="sd">        weight from the ``LUT`` and ``indices`` across all partitions and returns them.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">partition</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">partitions</span><span class="p">):</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">current_weight_partition</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">partition</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">:</span> <span class="n">partition</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">pad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">current_weight_partition</span><span class="p">)</span>

                <span class="n">weights</span><span class="p">[</span><span class="n">partition</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">:</span> <span class="n">partition</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">deflatten</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">labels</span><span class="o">.</span><span class="n">long</span><span class="p">()],</span> <span class="n">current_weight_partition</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">pad</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="n">weights</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">_torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">partition_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">forwarded_weights</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fake_palett_enabled</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">_torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                    <span class="n">quant_centroids</span><span class="p">,</span> <span class="n">quant_labels</span> <span class="o">=</span> <span class="n">forwarded_weights</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">return_inverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">centroids</span> <span class="o">=</span> <span class="n">_torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">quant_centroids</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster_dim</span><span class="p">)])</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">_torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">quant_labels</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">forwarded_weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fake_palett_enabled</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">partitions</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">init_partitions</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">centroids</span> <span class="o">=</span> <span class="n">_torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">centroids_init</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">_torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels_init</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">buffers_are_placeholders</span> <span class="o">=</span> <span class="kc">False</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                    <span class="n">forwarded_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">diff_palettize</span><span class="p">(</span><span class="n">forwarded_weights</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">forwarded_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">palettize</span><span class="p">(</span><span class="n">forwarded_weights</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">forwarded_weights</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster_dtype</span> <span class="o">==</span> <span class="s2">&quot;f16&quot;</span><span class="p">:</span>
            <span class="n">forwarded_weights</span> <span class="o">=</span> <span class="n">forwarded_weights</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster_dtype</span> <span class="o">==</span> <span class="s2">&quot;b16&quot;</span><span class="p">:</span>
            <span class="n">forwarded_weights</span> <span class="o">=</span> <span class="n">forwarded_weights</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">forwarded_weights</span>

    <span class="k">def</span> <span class="nf">_load_from_state_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">local_metadata</span><span class="p">,</span> <span class="n">strict</span><span class="p">,</span> <span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">,</span> <span class="n">error_msgs</span>
    <span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">cluster_dtype</span> <span class="o">=</span> <span class="n">local_metadata</span><span class="p">[</span><span class="s2">&quot;cluster_dtype&quot;</span><span class="p">]</span>
        <span class="n">state_dict_buffers_are_placeholders</span> <span class="o">=</span> <span class="n">local_metadata</span><span class="p">[</span><span class="s2">&quot;buffers_are_placeholders&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffers_are_placeholders</span> <span class="ow">and</span> <span class="n">state_dict_buffers_are_placeholders</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Trying to reload an uninitialized state dict onto an initialized module: </span><span class="si">{</span><span class="n">prefix</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffers_are_placeholders</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">state_dict_buffers_are_placeholders</span><span class="p">:</span>
            <span class="c1"># We only change the size of the placeholders if we intend to reload a proper checkpoint</span>
            <span class="c1"># onto an uninitialized module. In the other cases, we expect the state dict and the module to be compatible.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">centroids</span> <span class="o">=</span> <span class="n">_torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="n">state_dict</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;centroids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">centroids</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">_torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="n">state_dict</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;labels&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fake_palett_enabled</span> <span class="o">=</span> <span class="n">_torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="n">state_dict</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;fake_palett_enabled&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">buffers_are_placeholders</span> <span class="o">=</span> <span class="n">state_dict_buffers_are_placeholders</span>

        <span class="n">_Partitioner</span><span class="o">.</span><span class="n">_load_from_state_dict_</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;palett.&quot;</span><span class="p">,</span>
            <span class="n">local_metadata</span><span class="p">,</span>
            <span class="n">strict</span><span class="p">,</span>
            <span class="n">missing_keys</span><span class="p">,</span>
            <span class="n">unexpected_keys</span><span class="p">,</span>
            <span class="n">error_msgs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">need_to_quantize</span><span class="p">:</span>
            <span class="c1"># We will go through FakeQuantize._load_from_state_dict and then nn.Module._load_from_state_dict</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_load_from_state_dict</span><span class="p">(</span>
                <span class="n">state_dict</span><span class="p">,</span>
                <span class="n">prefix</span><span class="p">,</span>
                <span class="n">local_metadata</span><span class="p">,</span>
                <span class="n">strict</span><span class="p">,</span>
                <span class="n">missing_keys</span><span class="p">,</span>
                <span class="n">unexpected_keys</span><span class="p">,</span>
                <span class="n">error_msgs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Jump FakeQuantize and go to nn.Module directly</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">_FakeQuantize</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_load_from_state_dict</span><span class="p">(</span>
                <span class="n">state_dict</span><span class="p">,</span>
                <span class="n">prefix</span><span class="p">,</span>
                <span class="n">local_metadata</span><span class="p">,</span>
                <span class="n">strict</span><span class="p">,</span>
                <span class="n">missing_keys</span><span class="p">,</span>
                <span class="n">unexpected_keys</span><span class="p">,</span>
                <span class="n">error_msgs</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_save_to_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">keep_vars</span><span class="p">):</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">need_to_quantize</span><span class="p">:</span>
            <span class="c1"># Use normal inheritance, go through FakeQuantize._save_to_state_dict</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_save_to_state_dict</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">keep_vars</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">centroids</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">centroids</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Skip FakeQuantize._save_to_state_dict and go directly to nn.Module._save_to_state_dict</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">_FakeQuantize</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_save_to_state_dict</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">keep_vars</span><span class="p">)</span>

        <span class="c1"># State dicts can only contain tensors (for DDP), so store infos in the metatadata dict (in particular str)</span>
        <span class="n">destination</span><span class="o">.</span><span class="n">_metadata</span><span class="p">[</span><span class="n">prefix</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]][</span><span class="s2">&quot;cluster_dtype&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster_dtype</span>
        <span class="n">destination</span><span class="o">.</span><span class="n">_metadata</span><span class="p">[</span><span class="n">prefix</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]][</span>
            <span class="s2">&quot;buffers_are_placeholders&quot;</span>
        <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffers_are_placeholders</span>
        <span class="n">_Partitioner</span><span class="o">.</span><span class="n">_save_to_state_dict_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;palett.&quot;</span><span class="p">,</span> <span class="n">keep_vars</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">rep</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">centroids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_clusters</span><span class="p">:</span>
            <span class="n">rep</span> <span class="o">+=</span> <span class="s2">&quot; ===&gt; centroids: uninitialised buffer, &quot;</span>
            <span class="n">rep</span> <span class="o">+=</span> <span class="s2">&quot;labels: uninitialised buffer, &quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">rep</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot; ===&gt; centroids: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">centroids</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="n">rep</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;labels: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="si">}</span><span class="s2">, &quot;</span>
        <span class="n">rep</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;cluster_dtype: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cluster_dtype</span><span class="si">}</span><span class="s2">, &quot;</span>
        <span class="n">rep</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;n_bits: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">n_bits</span><span class="si">}</span><span class="s2">, &quot;</span>
        <span class="n">rep</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;cluster_dim: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cluster_dim</span><span class="si">}</span><span class="s2">, &quot;</span>
        <span class="n">rep</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;palett_tau: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">palett_tau</span><span class="si">}</span><span class="s2">, &quot;</span>
        <span class="n">rep</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;palett_mode: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">palett_mode</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">return</span> <span class="n">rep</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Apple Inc.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>