<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>coremltools.models.ml_program.compression_utils &mdash; coremltools API Reference  documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/norightmargin.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/sphinx_highlight.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            coremltools API Reference
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">API Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/coremltools.converters.html">Converters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/coremltools.models.html">Model APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/coremltools.converters.mil.html">MIL Builder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/coremltools.converters.mil.input_types.html">MIL Input Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/coremltools.converters.mil.mil.ops.defs.html">MIL Ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/coremltools.converters.mil.mil.passes.defs.html">MIL Graph Passes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://coremltools.readme.io/docs">Guides and examples</a></li>
<li class="toctree-l1"><a class="reference external" href="https://apple.github.io/coremltools/mlmodel/index.html">Core ML Format Specification</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/apple/coremltools">GitHub</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">coremltools API Reference</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">coremltools.models.ml_program.compression_utils</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for coremltools.models.ml_program.compression_utils</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) 2022, Apple Inc. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># Use of this source code is governed by a BSD-3-clause license that can be</span>
<span class="c1"># found in the LICENSE.txt file or at https://opensource.org/licenses/BSD-3-Clause</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">_np</span>

<span class="kn">from</span> <span class="nn">coremltools</span> <span class="kn">import</span> <span class="n">_SPECIFICATION_VERSION_IOS_16</span>
<span class="kn">from</span> <span class="nn">coremltools.converters.mil</span> <span class="kn">import</span> <span class="n">Operation</span> <span class="k">as</span> <span class="n">_Operation</span>
<span class="kn">from</span> <span class="nn">coremltools.converters.mil.converter</span> <span class="kn">import</span> <span class="n">mil_convert</span> <span class="k">as</span> <span class="n">_mil_convert</span>
<span class="kn">from</span> <span class="nn">coremltools.converters.mil.frontend.milproto.load</span> <span class="kn">import</span> <span class="n">load</span> <span class="k">as</span> <span class="n">_milproto_to_pymil</span>
<span class="kn">from</span> <span class="nn">coremltools.converters.mil.mil.passes.defs.quantization</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AbstractQuantizationPass</span> <span class="k">as</span> <span class="n">_AbstractQuantizationPass</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">coremltools.converters.mil.mil.passes.defs.quantization</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">WeightAffineQuantizer</span> <span class="k">as</span> <span class="n">_WeightAffineQuantizer</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">coremltools.converters.mil.mil.passes.defs.quantization</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">WeightDecompressor</span> <span class="k">as</span> <span class="n">_WeightDecompressor</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">coremltools.converters.mil.mil.passes.defs.quantization</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">WeightPalettizer</span> <span class="k">as</span> <span class="n">_WeightPalettizer</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">coremltools.converters.mil.mil.passes.defs.quantization</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">WeightSparsifier</span> <span class="k">as</span> <span class="n">_WeightSparsifier</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">_DEFAULT_MIN_WEIGHT_SIZE_TO_COMPRESS</span> <span class="o">=</span> <span class="mi">2048</span>
<span class="n">_DEFAULT_SPECIFICATION_VERSION_FOR_COMPRESSION</span> <span class="o">=</span> <span class="n">_SPECIFICATION_VERSION_IOS_16</span>


<span class="k">def</span> <span class="nf">_default_op_selector</span><span class="p">(</span><span class="n">const_op</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">const_op</span><span class="p">,</span> <span class="n">_Operation</span><span class="p">)</span> <span class="ow">or</span> <span class="n">const_op</span><span class="o">.</span><span class="n">op_type</span> <span class="o">!=</span> <span class="s2">&quot;const&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input of the op_selector must be type of const Operation, got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">const_op</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">const_op</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="n">_DEFAULT_MIN_WEIGHT_SIZE_TO_COMPRESS</span>

<span class="k">def</span> <span class="nf">_apply_graph_pass</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">,</span> <span class="n">graph_pass</span><span class="p">):</span>
    <span class="c1"># Utility function which compresses a coreml model</span>
    <span class="c1"># convert the fully precision mlmodel into pymil program</span>
    <span class="n">model_spec</span> <span class="o">=</span> <span class="n">mlmodel</span><span class="o">.</span><span class="n">get_spec</span><span class="p">()</span>
    <span class="n">model_type</span> <span class="o">=</span> <span class="n">model_spec</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;Type&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">model_type</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;neuralNetwork&quot;</span><span class="p">,</span> <span class="s2">&quot;neuralNetworkClassifier&quot;</span><span class="p">,</span> <span class="s2">&quot;neuralNetworkRegressor&quot;</span><span class="p">,</span> <span class="s2">&quot;pipeline&quot;</span><span class="p">,</span> <span class="s2">&quot;PipelineClassifier&quot;</span><span class="p">,</span> <span class="s2">&quot;PipelineRegressor&quot;</span><span class="p">):</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;coremltools.compression_utils are meant to be used only with mlprogram typed coreml models. &quot;</span>
              <span class="s2">&quot;This model has type </span><span class="si">{}</span><span class="s2">. Please use coremltools.models.neural_network.quantization_utils.quantize_weights&quot;</span>
              <span class="s2">&quot;instead to compress the weights of the model.&quot;</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_type</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;mlProgram&quot;</span><span class="p">:</span>
        <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>
       <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;weight compression not applicable for model type </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_type</span><span class="p">))</span>

    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">graph_pass</span><span class="p">,</span> <span class="n">_AbstractQuantizationPass</span><span class="p">),</span> <span class="s2">&quot;compression pass must be an AbstractQuantizationPass instance&quot;</span>
    <span class="n">specification_version</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">model_spec</span><span class="o">.</span><span class="n">specificationVersion</span><span class="p">,</span> <span class="n">_DEFAULT_SPECIFICATION_VERSION_FOR_COMPRESSION</span><span class="p">)</span>
    <span class="n">prog</span> <span class="o">=</span> <span class="n">_milproto_to_pymil</span><span class="p">(</span>
        <span class="n">model_spec</span><span class="o">=</span><span class="n">model_spec</span><span class="p">,</span>
        <span class="n">specification_version</span><span class="o">=</span><span class="n">specification_version</span><span class="p">,</span>
        <span class="n">file_weights_dir</span><span class="o">=</span><span class="n">mlmodel</span><span class="o">.</span><span class="n">weights_dir</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># apply compression graph pass</span>
    <span class="n">graph_pass</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">prog</span><span class="p">)</span>

    <span class="c1"># convert the pymil program back to mlmodel</span>
    <span class="n">compressed_mlmodel</span> <span class="o">=</span> <span class="n">_mil_convert</span><span class="p">(</span>
        <span class="n">prog</span><span class="p">,</span>
        <span class="n">convert_to</span><span class="o">=</span><span class="s2">&quot;mlprogram&quot;</span><span class="p">,</span>
        <span class="n">convert_from</span><span class="o">=</span><span class="s2">&quot;milinternal&quot;</span><span class="p">,</span>
        <span class="n">specification_version</span><span class="o">=</span><span class="n">specification_version</span><span class="p">,</span>
        <span class="n">compute_units</span><span class="o">=</span><span class="n">mlmodel</span><span class="o">.</span><span class="n">compute_unit</span><span class="p">,</span>
        <span class="n">model_description</span><span class="o">=</span><span class="n">model_spec</span><span class="o">.</span><span class="n">description</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">compressed_mlmodel</span>

<div class="viewcode-block" id="affine_quantize_weights"><a class="viewcode-back" href="../../../../source/coremltools.models.ml_program.html#coremltools.models.ml_program.compression_utils.affine_quantize_weights">[docs]</a><span class="k">def</span> <span class="nf">affine_quantize_weights</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;linear_symmetric&quot;</span><span class="p">,</span> <span class="n">op_selector</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">_np</span><span class="o">.</span><span class="n">int8</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Utility function to convert a float precision MLModel of type ``mlprogram`` that uses</span>
<span class="sd">    float-precision weights into a compressed MLModel that uses 8-bit weights. This is</span>
<span class="sd">    achieved by converting the float weight values that are stored in the ``const`` op</span>
<span class="sd">    into the ``constexpr_affine_dequantize`` op.</span>

<span class="sd">    This function uses affine quantization on the float weights, providing up to 2x</span>
<span class="sd">    savings in storage compared to float 16, or up to 4x savings compared to float 32.</span>
<span class="sd">    All computation at runtime uses float precision; the precision of the intermediate</span>
<span class="sd">    tensors and the compute precision of the ops are not altered.</span>

<span class="sd">    For each weight, this utility function converts the weight into the int8 or uint8 type using</span>
<span class="sd">    either `Linear interpolation` (``&quot;linear&quot;`` mode) or `Linear symmetric</span>
<span class="sd">    interpolation` (``&quot;linear_symmetric&quot;`` mode, the default).</span>

<span class="sd">    **Linear interpolation**</span>

<span class="sd">    Linear interpolation (``&quot;linear&quot;`` mode) maps the min/max of the float</span>
<span class="sd">    range to the 8-bit integer range ``[low, high]`` using a zero point (also called quantization bias, or</span>
<span class="sd">    offset) and a scale factor. For the int8 quantization, ``[low, high] = [-128, 127]``, while uint8</span>
<span class="sd">    quantization uses range ``[0, 255]``.</span>

<span class="sd">    ``&quot;linear&quot;`` mode uses the quantization formula:</span>

<span class="sd">    .. math::</span>
<span class="sd">       w_r = s * (w_q - z)</span>

<span class="sd">    Where:</span>

<span class="sd">        * :math:`w_r` and  :math:`s` are of type float.</span>
<span class="sd">        * :math:`w_r`` represents the float precision weight.</span>
<span class="sd">        * :math:`s` represents the scale.</span>
<span class="sd">        * :math:`w_q` and :math:`z` are of type 8-bit integer.</span>
<span class="sd">        * :math:`w_q` represents quantized weight.</span>
<span class="sd">        * :math:`z` represents the zero point.</span>

<span class="sd">    Quantized weights are computed as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">       w_q = cast\_to\_8\_bit\_integer(w_r / s + cast\_to\_float(z))</span>

<span class="sd">    Note: :math:`cast\_to\_8\_bit\_integer` is the process of clipping the input to range ``[low, high]`` followed by rounding and casting to 8-bit integer.</span>
<span class="sd">    </span>
<span class="sd">    In ``&quot;linear&quot;`` mode, ``s, z`` are computed by mapping the original float range</span>
<span class="sd">    ``[A, B]`` into the 8-bit integer range ``[-128, 127]`` or ``[0, 255]``. That is, you are solving the</span>
<span class="sd">    following linear equations:</span>

<span class="sd">        * ``B = s * (high - z)``</span>
<span class="sd">        * ``A = s * (low - z)``</span>

<span class="sd">    The equations result in the following:</span>

<span class="sd">        * ``s = (B - A) / (high - low)``</span>
<span class="sd">        * ``z = cast_to_8_bit_integer((low * B - high * A) / (B - A))``</span>

<span class="sd">    When the rank of weight ``w`` is 1, then ``s`` and ``z`` are both scalars. When the</span>
<span class="sd">    rank of the weight is greater than 1, then ``s`` and ``z`` are both vectors. In that</span>
<span class="sd">    case, scales are computed per `channel`, in which `channel` is the output dimension,</span>
<span class="sd">    which corresponds to the first dimension for ops such as ``conv`` and ``linear``, and</span>
<span class="sd">    the second dimension for the ``conv_transpose`` op.</span>
<span class="sd">    </span>
<span class="sd">    For ``&quot;linear&quot;`` mode, :math:`A = min(w_r)`, :math:`B = max(w_r)`.</span>
<span class="sd">    </span>
<span class="sd">    **Linear symmetric interpolation**</span>

<span class="sd">    With linear symmetric interpolation (``&quot;linear_symmetric&quot;`` mode, the default), rather than</span>
<span class="sd">    mapping the exact min/max of the float range to the quantized range,</span>

<span class="sd">    the function chooses the maximum absolute value between the min/max, which results in a </span>
<span class="sd">    floating-point range that is symmetric with respect to zero. This also makes the resulting zero </span>
<span class="sd">    point ``0`` for int8 weight and ``127`` for uint8 weight.</span>
<span class="sd">    </span>
<span class="sd">    For ``&quot;linear_symmetric&quot;`` mode:</span>
<span class="sd">    </span>
<span class="sd">       * :math:`A = -R` and :math:`B = R`, where :math:`R = max(abs(w_r))`.</span>
<span class="sd">       * This function maps to the range of ``[-127, 127]`` for int8 weight and ``[0, 254]`` for uint8 weight.</span>
<span class="sd">       * The result is ``s=(B-A)/254`` -&gt; ``s=2R/254`` -&gt; ``s=R/127``.</span>
<span class="sd">       * Solving for ``z``: </span>
<span class="sd">            * int8:  ``z = (-127 * R + 127 * R)/2R`` -&gt; ``z=0``.</span>
<span class="sd">            * uint8: ``z = (0 * R + 254 * R)/2R`` -&gt; ``z=127``.      </span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    mlmodel: MLModel</span>
<span class="sd">        Model to be quantized. This MLModel should be of type ``mlprogram``.</span>

<span class="sd">    mode: str</span>
<span class="sd">        Mode for linear quantization:</span>

<span class="sd">        * ``&quot;linear_symmetric&quot;`` (default): Input data are quantized in the range</span>
<span class="sd">          ``[-R, R]``, where :math:`R = max(abs(w_r))`.</span>
<span class="sd">        * ``&quot;linear&quot;``: Input data are quantized in the range</span>
<span class="sd">          :math:`[min(w_r), max(w_r)]`.</span>

<span class="sd">    op_selector: callable</span>
<span class="sd">        This function takes a single parameter with type ``coremltools.converters.mil.Const``;</span>
<span class="sd">        that is, a ``const`` operation. It returns a ``bool``: ``True`` to compress ``const_op``,</span>
<span class="sd">        otherwise ``False``. See the following examples:</span>

<span class="sd">        * All constants in the network are compressed:</span>

<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op):</span>
<span class="sd">                  return True</span>

<span class="sd">        * Only the constant with ``tensor.size &gt; 2048`` is compressed:</span>

<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op):</span>
<span class="sd">                  return const_op.val.val.size &gt; 2048</span>

<span class="sd">        * Compress the constant if it is the weight of a convolution layer</span>
<span class="sd">          and ``tensor.size &gt; 2048``:</span>

<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op):</span>
<span class="sd">                  return (</span>
<span class="sd">                      const_op.val.val.size &gt; 2048</span>
<span class="sd">                      and const_op.val.child_ops[0].op_type == &quot;conv&quot;</span>
<span class="sd">                      and const_op.val == const_op.val.child_ops[0].weight</span>
<span class="sd">                  )</span>

<span class="sd">        * When creating a custom ``op_selector`` function, the following attributes are helpful:</span>

<span class="sd">             * ``const_op.val.val``: The numpy array holding the value of the const.</span>
<span class="sd">             * ``const_op.val.child_ops``: A list of ops into which this constant is feeding.</span>
<span class="sd">             * ``const_op.val.child_ops[i].op_type``: The string corresponding to the op type</span>
<span class="sd">               of the i-th child op.</span>
<span class="sd">             * ``const_op.val.child_ops[i].name``: The string corresponding to the name the</span>
<span class="sd">               i-th child op.</span>

<span class="sd">        * If ``op_selector`` is not provided, it will be set to the behavior in which</span>
<span class="sd">          weights bigger than 2048 elements are compressed:</span>

<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op):</span>
<span class="sd">                  return const_op.val.val.size &gt; 2048</span>

<span class="sd">    dtype: np.generic or mil.type type</span>
<span class="sd">        Determines the quantizaed data type (int8/uint8).</span>

<span class="sd">        * The allowed values are:</span>
<span class="sd">            * ``np.int8`` (the default)</span>
<span class="sd">            * ``np.uint8``</span>
<span class="sd">            * ``coremltools.converters.mil.mil.types.int8``</span>
<span class="sd">            * ``coremltools.converters.mil.mil.types.uint8``</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>

<span class="sd">    model: MLModel</span>
<span class="sd">        The quantized MLModel instance.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>

<span class="sd">        import coremltools as ct</span>
<span class="sd">        model = ct.models.MLModel(&#39;my_model.mlpackage&#39;)</span>
<span class="sd">        compressed_model = ct.compression_utils.affine_quantize_weights(model, mode=&quot;linear_symmetric&quot;)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">op_selector</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">op_selector</span> <span class="o">=</span> <span class="n">_default_op_selector</span>
    <span class="n">affine_weight_quantizer</span> <span class="o">=</span> <span class="n">_WeightAffineQuantizer</span><span class="p">(</span><span class="n">fake_compression</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span> <span class="n">op_selector</span><span class="o">=</span><span class="n">op_selector</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_graph_pass</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">,</span> <span class="n">affine_weight_quantizer</span><span class="p">)</span></div>


<div class="viewcode-block" id="palettize_weights"><a class="viewcode-back" href="../../../../source/coremltools.models.ml_program.html#coremltools.models.ml_program.compression_utils.palettize_weights">[docs]</a><span class="k">def</span> <span class="nf">palettize_weights</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">,</span> <span class="n">nbits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;kmeans&quot;</span><span class="p">,</span> <span class="n">op_selector</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lut_function</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Utility function to convert a float precision MLModel of type ``mlprogram`` to a</span>
<span class="sd">    compressed MLModel by reducing the overall number of weights using a lookup table</span>
<span class="sd">    (LUT). A LUT contains a list of float values. An `nbit` LUT has 2\ :sup:`nbits` entries.</span>

<span class="sd">    For example, a float weight vector such as ``{0.3, 0.3, 0.5, 0.5}`` can be compressed</span>
<span class="sd">    using a 1-bit LUT: ``{0.3, 0.5}``. In this case the float vector can be replaced</span>
<span class="sd">    with a 1-bit vector ``{0, 0, 1, 1}``.</span>

<span class="sd">    This function iterates over all the weights in the ``mlprogram``, discretizes its values,</span>
<span class="sd">    and constructs the LUT according to the algorithm specified in ``mode``. The float</span>
<span class="sd">    values are then converted to the `nbit` values, and the LUT is saved alongside each</span>
<span class="sd">    weight. The ``const`` ops storing weight values are replaced by</span>
<span class="sd">    ``constexpr_lut_to_dense`` ops.</span>

<span class="sd">    At runtime, the LUT and the `nbit` values are used to reconstruct the float weight</span>
<span class="sd">    values, which are then used to perform the float operaton the weight is feeding into.</span>

<span class="sd">    Consider the following example of ``&quot;uniform&quot;`` mode (a linear histogram):</span>

<span class="sd">        * ``nbits = 4``</span>
<span class="sd">        * ``mode = &quot;uniform&quot;``</span>
<span class="sd">        * ``weight = [0.11, 0.19, 0.3, 0.08, 0.0, 0.02]``</span>

<span class="sd">    The weight can be converted to a palette with indices ``[0, 1, 2, 3]`` (2 bits). The</span>
<span class="sd">    indices are a byte array.</span>

<span class="sd">    The data range ``[0.0, 0.3]`` is divided into 4 partitions linearly, which is</span>
<span class="sd">    ``[0.0, 0.1, 0.2, 0.3]``.</span>

<span class="sd">        * The LUT would be ``[0.0, 0.1, 0.2, 0.3]``.</span>

<span class="sd">        * The weight is rounded to ``[0.1, 0.2, 0.3, 0.1, 0.0, 0.0]``, and represented in</span>
<span class="sd">          the palette as indices ``[01b, 10b, 11b, 01b, 00b, 00b]``.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    mlmodel: MLModel</span>
<span class="sd">        Model to be converted by a LUT. This MLModel should be of type ``mlprogram``.</span>

<span class="sd">    nbits: int</span>
<span class="sd">        Number of bits per weight. Required for ``kmeans`` or ``uniform`` mode, but must</span>
<span class="sd">        not be set for ``unique`` or ``custom`` mode. A LUT would have</span>
<span class="sd">        2\ :sup:`nbits` entries, where `nbits` can be ``{1, 2, 4, 6, 8}``.</span>

<span class="sd">    mode: str</span>
<span class="sd">        Determine how the LUT is constructed by specifying one of the following:</span>

<span class="sd">        * ``&quot;kmeans&quot;`` (default): The LUT is generated by `k-means clustering`, a method of vector</span>
<span class="sd">          quantization that groups similar data points together to discover underlying</span>
<span class="sd">          patterns by using a fixed number (`k`) of clusters in a dataset. A cluster</span>
<span class="sd">          refers to a collection of data points aggregated together because of certain</span>
<span class="sd">          similarities. `nbits` is required.</span>

<span class="sd">        * ``&quot;uniform&quot;``: The LUT is generated by a linear histogram.</span>

<span class="sd">           - ``[v_min, v_min + scale, v_min + 2 * scale, ..., v_max]``</span>
<span class="sd">           - Where the weight is in the range ``[v_min, v_max]``, and</span>
<span class="sd">             ``scale = (v_max - v_min) / (1 &lt;&lt; nbits - 1)``.</span>
<span class="sd">           - ``nbits`` is required.</span>

<span class="sd">           A `histogram` is a representation of the distribution of a continuous variable,</span>
<span class="sd">           in which the entire range of values is divided into a series of intervals (or</span>
<span class="sd">           `bins`) and the representation displays how many values fall into each bin.</span>
<span class="sd">           Linear histograms have one bin at even intervals, such as one bin per integer.</span>

<span class="sd">        * ``&quot;unique&quot;``: The LUT is generated by unique values in the weights. The weights</span>
<span class="sd">          are assumed to be on a discrete lattice but stored in a float data type. This</span>
<span class="sd">          parameter identifies the weights and converts them into the palettized representation.</span>

<span class="sd">          Do not provide ``nbits`` for this mode. ``nbits`` is picked up automatically,</span>
<span class="sd">          with the smallest possible value in ``{1, 2, 4, 6, 8}`` such that the</span>
<span class="sd">          number of the unique values is ``&lt;= (1 &lt;&lt; nbits)``. If the weight has ``&gt; 256``</span>
<span class="sd">          unique values, the compression is skipped.</span>

<span class="sd">          For example:</span>

<span class="sd">          * If the weights are ``{0.1, 0.2, 0.3, 0.4}`` and ``nbits=2``, the weights are</span>
<span class="sd">            converted to ``{00b, 01b, 10b, 11b}``, and the generated LUT is</span>
<span class="sd">            ``[0.1, 0.2, 0.3, 0.4]``.</span>
<span class="sd">          * If the weights are ``{0.1, 0.2, 0.3, 0.4}`` and ``nbits=1``, nothing happens</span>
<span class="sd">            because the weights are not a 1-bit lattice.</span>
<span class="sd">          * If the weights are ``{0.1, 0.2, 0.3, 0.4, 0.5}`` and ``nbits=2``, nothing</span>
<span class="sd">            happens because the weights are not a 2-bit lattice.</span>

<span class="sd">        * ``&quot;custom&quot;``: The LUT and palettization parameters are calculated using a custom</span>
<span class="sd">          function. If this mode is selected then ``lut_function`` must be provided.</span>

<span class="sd">          Do not provide ``nbits`` for this mode. The user should customize ``nbits`` in the</span>
<span class="sd">          ``lut_function`` implementation.</span>

<span class="sd">    op_selector: callable</span>
<span class="sd">        This function takes a single parameter with type ``coremltools.converters.mil.Operation``.</span>
<span class="sd">        It returns a ``bool``: ``True`` to compress ``const_op``, otherwise ``False``.</span>
<span class="sd">        See the following examples:</span>

<span class="sd">        * All constants in the network are compressed:</span>

<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op):</span>
<span class="sd">                  return True</span>

<span class="sd">        * Only the constant with ``tensor.size &gt; 2048`` is compressed:</span>

<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op):</span>
<span class="sd">                  return const_op.val.val.size &gt; 2048</span>

<span class="sd">        * Compress the constant if it is the weight of a convolution layer</span>
<span class="sd">          and ``tensor.size &gt; 2048``:</span>

<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op):</span>
<span class="sd">                  return (</span>
<span class="sd">                      const_op.val.val.size &gt; 2048</span>
<span class="sd">                      and const_op.val.child_ops[0].op_type == &quot;conv&quot;</span>
<span class="sd">                      and const_op.val == const_op.val.child_ops[0].weight</span>
<span class="sd">                  )</span>

<span class="sd">        * When creating a custom ``op_selector`` function, the following attributes are helpful:</span>

<span class="sd">             * ``const_op.val.val``: The numpy array holding the value of the const.</span>
<span class="sd">             * ``const_op.val.child_ops``: A list of ops into which this constant is feeding.</span>
<span class="sd">             * ``const_op.val.child_ops[i].op_type``: The string corresponding to the op type</span>
<span class="sd">               of the i-th child op.</span>
<span class="sd">             * ``const_op.val.child_ops[i].name``: The string corresponding to the name the</span>
<span class="sd">               i-th child op.</span>

<span class="sd">        * If ``op_selector`` is not provided, it will be set to the behavior in which</span>
<span class="sd">          weights bigger than 2048 elements are compressed:</span>

<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op):</span>
<span class="sd">                  return const_op.val.val.size &gt; 2048</span>

<span class="sd">    lut_function: callable</span>
<span class="sd">        A callable function which computes the weight palettization parameters. This must</span>
<span class="sd">        be provided if the mode is set to ``&quot;custom&quot;``.</span>

<span class="sd">        weight: np.ndarray</span>
<span class="sd">            A float precision numpy array.</span>

<span class="sd">        Returns: lut: list[float]</span>
<span class="sd">            The lookup table.</span>

<span class="sd">        indices: list[int]</span>
<span class="sd">            A list of indices for each element.</span>

<span class="sd">        The following is an example that extract the ``top_k`` elements as the LUT. Given</span>
<span class="sd">        that ``weight = [0.1, 0.5, 0.3, 0.3, 0.5, 0.6, 0.7]``, the ``lut_function``</span>
<span class="sd">        produces ``lut = [0, 0.5, 0.6, 0.7], indices = [0, 1, 0, 0, 2, 3]``.</span>

<span class="sd">        .. sourcecode:: python</span>

<span class="sd">           def lut_function(weight):</span>
<span class="sd">               # In this example, we assume elements in the weights &gt;= 0</span>
<span class="sd">               weight = weight.flatten()</span>
<span class="sd">               nbits = 4</span>

<span class="sd">               # Get the LUT, from extracting top k maximum unique elements in the weight to be the LUT</span>
<span class="sd">               # Note that k = 1 &lt;&lt; nbits - 1, so we have the first element be 0</span>
<span class="sd">               unique_elements = np.unique(weight)</span>
<span class="sd">               k = (1 &lt;&lt; nbits) - 1</span>
<span class="sd">               top_k = np.partition(weight, -k)[-k:]</span>
<span class="sd">               np.sort(top_k)</span>
<span class="sd">               lut = [0.0] + top_k.tolist()</span>

<span class="sd">               # Compute the indices</span>
<span class="sd">               mapping = {v: idx for idx, v in enumerate(lut)}</span>
<span class="sd">               indices = [mapping[v] if v in mapping else 0 for v in weight]</span>

<span class="sd">               return lut, indices</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    model: MLModel</span>
<span class="sd">        The palettized MLModel instance.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>

<span class="sd">    .. sourcecode:: python</span>

<span class="sd">        import coremltools as ct</span>

<span class="sd">        model = ct.models.MLModel(&quot;my_model.mlpackage&quot;)</span>
<span class="sd">        compressed_model = ct.compression_utils.palettize_weights(model, mode=&quot;kmeans&quot;, nbits=4)</span>


<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">op_selector</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">op_selector</span> <span class="o">=</span> <span class="n">_default_op_selector</span>
    <span class="n">weight_palettizer</span> <span class="o">=</span> <span class="n">_WeightPalettizer</span><span class="p">(</span><span class="n">nbits</span><span class="o">=</span><span class="n">nbits</span><span class="p">,</span> <span class="n">fake_compression</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">op_selector</span><span class="o">=</span><span class="n">op_selector</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span> <span class="n">lut_function</span><span class="o">=</span><span class="n">lut_function</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_graph_pass</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">,</span> <span class="n">weight_palettizer</span><span class="p">)</span></div>


<div class="viewcode-block" id="sparsify_weights"><a class="viewcode-back" href="../../../../source/coremltools.models.ml_program.html#coremltools.models.ml_program.compression_utils.sparsify_weights">[docs]</a><span class="k">def</span> <span class="nf">sparsify_weights</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;threshold_based&quot;</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">target_percentile</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">op_selector</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Utility function to convert a float precision MLModel of type ``mlprogram`` to a</span>
<span class="sd">    compressed MLModel using sparse representation. The ``const`` ops storing weight</span>
<span class="sd">    values are replaced by ``constexpr_sparse_to_dense`` ops.</span>

<span class="sd">    This function is useful if the model is trained with pruning techniques so that</span>
<span class="sd">    a lot of weights have zero values. If a large percentage of weight values are zero,</span>
<span class="sd">    a sparse representation is more efficient than a dense one (the default).</span>

<span class="sd">    The sparsified weights are stored in a bit mask. If the weight values are</span>
<span class="sd">    ``{0, 0, 0, 0, 0, 0, 0, 56.3}``, its sparse representation contains a bit mask with</span>
<span class="sd">    ones on locations where the value is non-zero: ``00000001b``. This is accompanied by</span>
<span class="sd">    non-zero data, which is a size-1 vector of value ``{56.3}``.</span>

<span class="sd">    For example, given the following:</span>

<span class="sd">        * ``weight = [0.3, 0, 0, 0.5, 0, 0]``</span>
<span class="sd">        * ``non_zero_data, bit_mask = sparsify(weight)``</span>

<span class="sd">    The indices of the non-zero elements are:</span>

<span class="sd">        * ``non_zero_data = [0.3, 0.5]``</span>
<span class="sd">        * ``bit_mask = &quot;100100&quot;``</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    mlmodel: MLModel</span>
<span class="sd">        Model to be sparsified. This MLModel should be of type ``mlprogram``.</span>

<span class="sd">    mode: str</span>
<span class="sd">        Determine the scheme to sparsify the model by specifying one of the following:</span>

<span class="sd">        * ``&quot;threshold_based&quot;`` (default): All the absolute weight values that are smaller</span>
<span class="sd">          than ``threshold`` are changed to 0, and the tensor is stored in a sparse format.</span>
<span class="sd">          For example, given the following:</span>

<span class="sd">               * ``weight = [0.3, -0.2, -0.01, 0.05]``</span>
<span class="sd">               * ``threshold = 0.03``</span>

<span class="sd">          The sparsified weight would be ``[0.3, -0.2, 0, 0.05]``.</span>

<span class="sd">        * ``&quot;percentile_based&quot;``: Sparsify the weight with a constant sparsity percentile,</span>
<span class="sd">          which is ``target_percentile``. Where</span>
<span class="sd">          ``n = floor(size_of_weight_tensor * target_percentile)``, the ``n`` lowest</span>
<span class="sd">          absolute weight values are changed to 0. For example, given the following:</span>

<span class="sd">               * ``weight = [0.3, -0.2, -0.01, 0.05]``</span>
<span class="sd">               * ``target_percentile = 0.75``</span>

<span class="sd">          The sparsified weight would be ``[0.3, 0, 0, 0]``.</span>

<span class="sd">    threshold: float</span>
<span class="sd">        Required when ``mode = &quot;prune_threshold&quot;``. The absolute threshold to sparsify the weight.</span>

<span class="sd">    target_percentile: float</span>
<span class="sd">        Required when ``mode = &quot;percentile_based&quot;``. The percentage of sparsity for</span>
<span class="sd">        compression, which needs to be in the range [0, 1]. When 0, no sparsification</span>
<span class="sd">        occurs. For 1, all weights become 0.</span>

<span class="sd">    op_selector: callable</span>
<span class="sd">        This function takes a single parameter with type ``coremltools.converters.mil.Operation``.</span>
<span class="sd">        It returns a ``bool``: ``True`` to compress ``const_op``, otherwise ``False``.</span>
<span class="sd">        See the following examples:</span>

<span class="sd">        * All constants in the network are compressed:</span>

<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op):</span>
<span class="sd">                  return True</span>

<span class="sd">        * Only the constant with ``tensor.size &gt; 2048`` is compressed:</span>

<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op):</span>
<span class="sd">                  return const_op.val.val.size &gt; 2048</span>

<span class="sd">        * Compress the constant if it is the weight of a convolution layer</span>
<span class="sd">          and ``tensor.size &gt; 2048``:</span>

<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op):</span>
<span class="sd">                  return (</span>
<span class="sd">                      const_op.val.val.size &gt; 2048</span>
<span class="sd">                      and const_op.val.child_ops[0].op_type == &quot;conv&quot;</span>
<span class="sd">                      and const_op.val == const_op.val.child_ops[0].weight</span>
<span class="sd">                  )</span>

<span class="sd">        * When creating a custom ``op_selector`` function, the following attributes are helpful:</span>

<span class="sd">             * ``const_op.val.val``: The numpy array holding the value of the const.</span>
<span class="sd">             * ``const_op.val.child_ops``: A list of ops into which this constant is feeding.</span>
<span class="sd">             * ``const_op.val.child_ops[i].op_type``: The string corresponding to the op type</span>
<span class="sd">               of the i-th child op.</span>
<span class="sd">             * ``const_op.val.child_ops[i].name``: The string corresponding to the name the</span>
<span class="sd">               i-th child op.</span>

<span class="sd">        * If ``op_selector`` is not provided, it will be set to the behavior in which</span>
<span class="sd">          weights bigger than 2048 elements are compressed:</span>

<span class="sd">          .. sourcecode:: python</span>

<span class="sd">              def op_selector(const_op):</span>
<span class="sd">                  return const_op.val.val.size &gt; 2048</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    model: MLModel</span>
<span class="sd">        The sparse MLModel instance.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    .. sourcecode:: python</span>

<span class="sd">        import coremltools as ct</span>

<span class="sd">        model = ct.models.MLModel(&quot;my_model.mlpackage&quot;)</span>
<span class="sd">        compressed_model = ct.compression_utils.sparsify_weights(</span>
<span class="sd">            model, mode=&quot;threshold_based&quot;, threshold=0.01</span>
<span class="sd">        )</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">op_selector</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">op_selector</span> <span class="o">=</span> <span class="n">_default_op_selector</span>
    <span class="n">weight_sparsifier</span> <span class="o">=</span> <span class="n">_WeightSparsifier</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span> <span class="n">target_percentile</span><span class="o">=</span><span class="n">target_percentile</span><span class="p">,</span> <span class="n">op_selector</span><span class="o">=</span><span class="n">op_selector</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_graph_pass</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">,</span> <span class="n">weight_sparsifier</span><span class="p">)</span></div>

<span class="k">def</span> <span class="nf">decompress_weights</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Utility function to convert weights that are sparse or palettized or affine quantized, back to the float format.</span>
<span class="sd">    That is, convert any of the follwing three ops:</span>

<span class="sd">    (1) constexpr_affine_dequantize</span>
<span class="sd">    (2) constexpr_lut_to_dense</span>
<span class="sd">    (3) constexpr_sparse_to_dense</span>

<span class="sd">    to mb.const</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    mlmodel: MLModel</span>
<span class="sd">        Model which will be decompressed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    model: MLModel</span>
<span class="sd">        The MLModel with no constexpr ops included.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    .. sourcecode:: python</span>

<span class="sd">        import coremltools as ct</span>

<span class="sd">        model = ct.models.MLModel(&quot;my_compressed_model.mlpackage&quot;)</span>
<span class="sd">        decompressed_model = ct.compression_utils.decompress_weights(model)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">weight_decompressor</span> <span class="o">=</span> <span class="n">_WeightDecompressor</span><span class="p">(</span><span class="n">op_selector</span><span class="o">=</span><span class="k">lambda</span> <span class="n">op</span><span class="p">:</span> <span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_graph_pass</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">,</span> <span class="n">weight_decompressor</span><span class="p">)</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Apple Inc.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>