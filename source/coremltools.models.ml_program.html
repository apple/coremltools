<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>affine_quantize_weights &mdash; coremltools API Reference  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="neural_network.builder" href="coremltools.models.neural_network.html" />
    <link rel="prev" title="Models" href="coremltools.models.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> coremltools API Reference
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">API Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="coremltools.converters.html">Converters</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="coremltools.models.html">Models</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="coremltools.models.html#module-coremltools.models.model">MLModel</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="coremltools.models.html#compression-utils">compression_utils</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">affine_quantize_weights</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-0">palettize_weights</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-1">sparsify_weights</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="coremltools.models.html#module-coremltools.models.array_feature_extractor">array_feature_extractor</a></li>
<li class="toctree-l2"><a class="reference internal" href="coremltools.models.html#module-coremltools.models.feature_vectorizer">feature_vectorizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="coremltools.models.html#module-coremltools.models.nearest_neighbors.builder">nearest_neighbors</a></li>
<li class="toctree-l2"><a class="reference internal" href="coremltools.models.html#neural-network">neural_network</a></li>
<li class="toctree-l2"><a class="reference internal" href="coremltools.models.html#module-coremltools.models.pipeline">pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="coremltools.models.html#module-coremltools.models.tree_ensemble">tree_ensemble</a></li>
<li class="toctree-l2"><a class="reference internal" href="coremltools.models.html#module-coremltools.models.utils">utils</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="coremltools.converters.mil.html">MIL Builder</a></li>
<li class="toctree-l1"><a class="reference internal" href="coremltools.converters.mil.input_types.html">MIL Input Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="coremltools.converters.mil.mil.ops.defs.html">MIL Ops</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://coremltools.readme.io/docs">Guides and examples</a></li>
<li class="toctree-l1"><a class="reference external" href="https://apple.github.io/coremltools/mlmodel/index.html">Core ML Format Specification</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/apple/coremltools">GitHub</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">coremltools API Reference</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="coremltools.models.html">Models</a> &raquo;</li>
      <li>affine_quantize_weights</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/source/coremltools.models.ml_program.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-coremltools.models.ml_program.compression_utils">
<span id="affine-quantize-weights"></span><h1>affine_quantize_weights<a class="headerlink" href="#module-coremltools.models.ml_program.compression_utils" title="Permalink to this heading"></a></h1>
<dl class="py function">
<dt class="sig sig-object py" id="coremltools.models.ml_program.compression_utils.affine_quantize_weights">
<span class="sig-prename descclassname"><span class="pre">coremltools.models.ml_program.compression_utils.</span></span><span class="sig-name descname"><span class="pre">affine_quantize_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mlmodel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear_symmetric'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">op_selector</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/coremltools/models/ml_program/compression_utils.html#affine_quantize_weights"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#coremltools.models.ml_program.compression_utils.affine_quantize_weights" title="Permalink to this definition"></a></dt>
<dd><p>Utility function to convert a float precision MLModel of type <code class="docutils literal notranslate"><span class="pre">mlprogram</span></code> that uses
float-precision weights into a compressed MLModel that uses 8-bit weights. This is
achieved by converting the float weight values that are stored in the <code class="docutils literal notranslate"><span class="pre">const</span></code> op
into the <code class="docutils literal notranslate"><span class="pre">constexpr_affine_dequantize</span></code> op.</p>
<p>This function uses affine quantization on the float weights, providing up to 2x
savings in storage compared to float 16, or up to 4x savings compared to float 32.
All computation at runtime uses float precision; the precision of the intermediate
tensors and the compute precision of the ops are not altered.</p>
<p>For each weight, this utility function converts the weight into the uint8 type using
either <cite>Linear interpolation</cite> (<code class="docutils literal notranslate"><span class="pre">&quot;linear&quot;</span></code> mode) or <cite>Linear symmetric
interpolation</cite> (<code class="docutils literal notranslate"><span class="pre">&quot;linear_symmetric&quot;</span></code> mode, the default).</p>
<p><strong>Linear interpolation</strong></p>
<p>Linear interpolation (<code class="docutils literal notranslate"><span class="pre">&quot;linear&quot;</span></code> mode) maps the min/max of the float
range to the range [0, 255] using a zero point (also called quantization bias, or
offset) and a scale factor.</p>
<p><code class="docutils literal notranslate"><span class="pre">&quot;linear&quot;</span></code> mode uses the quantization formula <code class="docutils literal notranslate"><span class="pre">w_r</span> <span class="pre">=</span> <span class="pre">s</span> <span class="pre">*</span> <span class="pre">(w_q</span> <span class="pre">-</span> <span class="pre">z)</span></code>, where:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">w_r</span></code> and  <code class="docutils literal notranslate"><span class="pre">s</span></code> are of type float.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">w_r</span></code> represents the float precision weight.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">s</span></code> represents the scale.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">w_q</span></code> and <code class="docutils literal notranslate"><span class="pre">z</span></code> are of type uint8.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">w_q</span></code> represents quantized weight.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">z</span></code> represents the zero point.</p></li>
</ul>
</div></blockquote>
<p>Quantized weights are computed as follows:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">w_q</span> <span class="pre">=</span> <span class="pre">cast_to_uint8(w_r</span> <span class="pre">/</span> <span class="pre">s</span> <span class="pre">+</span> <span class="pre">cast_to_float(z))</span></code></p></li>
<li><p>Note: <code class="docutils literal notranslate"><span class="pre">cast_to_uint8</span></code> is the process of clipping the input to range [0, 255]
followed by rounding and casting to uint8.</p></li>
</ul>
</div></blockquote>
<p>In <code class="docutils literal notranslate"><span class="pre">&quot;linear&quot;</span></code> mode, <code class="docutils literal notranslate"><span class="pre">s,</span> <span class="pre">z</span></code> are computed by mapping the original float range
<code class="docutils literal notranslate"><span class="pre">[A,</span> <span class="pre">B]</span></code> into the uint8 range [0, 255]. That is, you are solving the following
linear equations:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">=</span> <span class="pre">s</span> <span class="pre">*</span> <span class="pre">(255</span> <span class="pre">-</span> <span class="pre">z)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">=</span> <span class="pre">s</span> <span class="pre">*</span> <span class="pre">(0</span> <span class="pre">-</span> <span class="pre">z)</span></code></p></li>
</ul>
</div></blockquote>
<p>The equations result in the following:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">s</span> <span class="pre">=</span> <span class="pre">(B</span> <span class="pre">-</span> <span class="pre">A)</span> <span class="pre">/</span> <span class="pre">255</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">=</span> <span class="pre">cast_to_uint8(-255</span> <span class="pre">*</span> <span class="pre">A</span> <span class="pre">/</span> <span class="pre">(B</span> <span class="pre">-</span> <span class="pre">A))</span></code></p></li>
</ul>
</div></blockquote>
<p>When the rank of weight <code class="docutils literal notranslate"><span class="pre">w</span></code> is 1, then <code class="docutils literal notranslate"><span class="pre">s</span></code> and <code class="docutils literal notranslate"><span class="pre">z</span></code> are both scalars. When the
rank of the weight is greater than 1, then <code class="docutils literal notranslate"><span class="pre">s</span></code> and <code class="docutils literal notranslate"><span class="pre">z</span></code> are both vectors. In that
case, scales are computed “per channel”, in which “channel” is the output dimension,
which corresponds to the first dimension for ops such as <code class="docutils literal notranslate"><span class="pre">conv</span></code> and <code class="docutils literal notranslate"><span class="pre">linear</span></code>, and
the second dimension for the <code class="docutils literal notranslate"><span class="pre">conv_transpose</span></code> op.</p>
<p>For <code class="docutils literal notranslate"><span class="pre">&quot;linear&quot;</span></code> mode, <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">=</span> <span class="pre">min(w_r),</span> <span class="pre">B</span> <span class="pre">=</span> <span class="pre">max(w_r)</span></code>.</p>
<p><strong>Linear symmetric interpolation</strong></p>
<p>With linear symmetric interpolation (<code class="docutils literal notranslate"><span class="pre">&quot;linear_symmetric&quot;</span></code> mode, the default), rather than
mapping the exact min/max of the float range to the quantized range,
the function chooses the maximum absolute value between the min/max, which results in
a zero point value of 127. The floating-point range is symmetric with respect to zero,
and so is the quantized range.</p>
<p>For <code class="docutils literal notranslate"><span class="pre">&quot;linear_symmetric&quot;</span></code> mode:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">=</span> <span class="pre">-R</span></code> and <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">=</span> <span class="pre">R</span></code>, where <code class="docutils literal notranslate"><span class="pre">R</span> <span class="pre">=</span> <span class="pre">max(abs(w_r))</span></code>.</p></li>
<li><p>This function maps to the range [0, 254].</p></li>
<li><p>The result is <code class="docutils literal notranslate"><span class="pre">s=(B-A)/254</span></code> –&gt; <code class="docutils literal notranslate"><span class="pre">s=2R/254</span></code> –&gt; <code class="docutils literal notranslate"><span class="pre">s=R/127</span></code>.</p></li>
<li><p>Solving for <code class="docutils literal notranslate"><span class="pre">z</span></code>: <code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">=</span> <span class="pre">(R/2R)</span> <span class="pre">*</span> <span class="pre">254</span></code> –&gt; <code class="docutils literal notranslate"><span class="pre">z=127</span></code>.</p></li>
</ul>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>mlmodel: MLModel</strong></dt><dd><p>Model to be quantized. This MLModel should be of type <code class="docutils literal notranslate"><span class="pre">mlprogram</span></code>.</p>
</dd>
<dt><strong>mode: str</strong></dt><dd><p>Mode for linear quantization:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;linear_symmetric&quot;</span></code> (default): Input data are quantized in the range
<code class="docutils literal notranslate"><span class="pre">[-R,</span> <span class="pre">R]</span></code>, where <code class="docutils literal notranslate"><span class="pre">R</span> <span class="pre">=</span> <span class="pre">max(abs(w_r))</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;linear&quot;</span></code>: Input data are quantized in the range
<code class="docutils literal notranslate"><span class="pre">[min(w_r),</span> <span class="pre">max(w_r)]</span></code>.</p></li>
</ul>
</dd>
<dt><strong>op_selector: callable</strong></dt><dd><p>This function takes a single parameter with type <code class="docutils literal notranslate"><span class="pre">coremltools.converters.mil.Const</span></code>;
that is, a <code class="docutils literal notranslate"><span class="pre">const</span></code> operation. It returns a <code class="docutils literal notranslate"><span class="pre">bool</span></code>: <code class="docutils literal notranslate"><span class="pre">True</span></code> to compress <code class="docutils literal notranslate"><span class="pre">const_op</span></code>,
otherwise <code class="docutils literal notranslate"><span class="pre">False</span></code>. See the following examples:</p>
<ul>
<li><p>All constants in the network are compressed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">op_selector</span><span class="p">(</span><span class="n">const_op</span><span class="p">):</span>
      <span class="k">return</span> <span class="kc">True</span>
</pre></div>
</div>
</li>
<li><p>Only the constant with <code class="docutils literal notranslate"><span class="pre">tensor.size</span> <span class="pre">&gt;</span> <span class="pre">2048</span></code> is compressed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">op_selector</span><span class="p">(</span><span class="n">const_op</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">const_op</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="mi">2048</span>
</pre></div>
</div>
</li>
<li><p>Compress the constant if it is the weight of a convolution layer
and <code class="docutils literal notranslate"><span class="pre">tensor.size</span> <span class="pre">&gt;</span> <span class="pre">2048</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">op_selector</span><span class="p">(</span><span class="n">const_op</span><span class="p">):</span>
      <span class="k">return</span> <span class="p">(</span><span class="n">const_op</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="mi">2048</span>
              <span class="ow">and</span> <span class="n">const_op</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">child_ops</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">op_type</span> <span class="o">==</span> <span class="s2">&quot;conv&quot;</span>
              <span class="ow">and</span> <span class="n">const_op</span><span class="o">.</span><span class="n">val</span> <span class="o">==</span> <span class="n">const_op</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">child_ops</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
              <span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>When creating a custom <code class="docutils literal notranslate"><span class="pre">op_selector</span></code> function, the following attributes are helpful:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">const_op.val.val</span></code>: The numpy array holding the value of the const.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">const_op.val.child_ops</span></code>: A list of ops into which this constant is feeding.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">const_op.val.child_ops[i].op_type</span></code>: The string corresponding to the op type
of the i-th child op.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">const_op.val.child_ops[i].name</span></code>: The string corresponding to the name the
i-th child op.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">op_selector</span></code> is not provided, it will be set to the behavior in which
weights bigger than 2048 elements are compressed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">op_selector</span><span class="p">(</span><span class="n">const_op</span><span class="p">):</span>
      <span class="n">returm</span> <span class="n">const_op</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="mi">2048</span><span class="p">:</span>
</pre></div>
</div>
</li>
</ul>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>model: MLModel</dt><dd><p>The quantized MLModel instance.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">coremltools</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">MLModel</span><span class="p">(</span><span class="s1">&#39;my_model.mlpackage&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">compressed_model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">compression_utils</span><span class="o">.</span><span class="n">affine_quantize_weights</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;linear_symmetric&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="module-0">
<span id="palettize-weights"></span><h1>palettize_weights<a class="headerlink" href="#module-0" title="Permalink to this heading"></a></h1>
<dl class="py function">
<dt class="sig sig-object py" id="coremltools.models.ml_program.compression_utils.palettize_weights">
<span class="sig-prename descclassname"><span class="pre">coremltools.models.ml_program.compression_utils.</span></span><span class="sig-name descname"><span class="pre">palettize_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mlmodel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nbits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'kmeans'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">op_selector</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lut_function</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/coremltools/models/ml_program/compression_utils.html#palettize_weights"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#coremltools.models.ml_program.compression_utils.palettize_weights" title="Permalink to this definition"></a></dt>
<dd><p>Utility function to convert a float precision MLModel of type <code class="docutils literal notranslate"><span class="pre">mlprogram</span></code> to a
compressed MLModel by reducing the overall number of weights using a lookup table
(LUT). A LUT contains a list float values. An <cite>nbit</cite> LUT has 2<sup>nbits</sup> entries.</p>
<p>For example, a float weight vector such as <code class="docutils literal notranslate"><span class="pre">{0.3,</span> <span class="pre">0.3,</span> <span class="pre">0.5,</span> <span class="pre">0.5}</span></code> can be compressed
using a 1-bit LUT: <code class="docutils literal notranslate"><span class="pre">{0.3,</span> <span class="pre">0.5}</span></code>. In this case the float vector can be replaced
with a 1-bit vector <code class="docutils literal notranslate"><span class="pre">{0,</span> <span class="pre">0,</span> <span class="pre">1,</span> <span class="pre">1}</span></code>.</p>
<p>This function iterates over all the weights in the <code class="docutils literal notranslate"><span class="pre">mlprogram</span></code>, discretizes its values,
and constructs the LUT according to the algorithm specified in <code class="docutils literal notranslate"><span class="pre">mode</span></code>. The float
values are then converted to the <cite>nbit</cite> values, and the LUT is saved alongside each
weight. The <code class="docutils literal notranslate"><span class="pre">const</span></code> ops storing weight values are replaced by
<code class="docutils literal notranslate"><span class="pre">constexpr_lut_to_dense</span></code> ops.</p>
<p>At runtime, the LUT and the <cite>nbit</cite> values are used to reconstruct the float weight
values, which are then used to perform the float operaton the weight is feeding into.</p>
<p>Consider the following example of <code class="docutils literal notranslate"><span class="pre">&quot;uniform&quot;</span></code> mode (a linear histogram):</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">nbits</span> <span class="pre">=</span> <span class="pre">4</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mode</span> <span class="pre">=</span> <span class="pre">&quot;uniform&quot;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight</span> <span class="pre">=</span> <span class="pre">[0.11,</span> <span class="pre">0.19,</span> <span class="pre">0.3,</span> <span class="pre">0.08,</span> <span class="pre">0.0,</span> <span class="pre">0.02]</span></code></p></li>
</ul>
</div></blockquote>
<p>The weight can be converted to a palette with indices <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">3]</span></code> (2 bits). The
indices are a byte array.</p>
<p>The data range <code class="docutils literal notranslate"><span class="pre">[0.0,</span> <span class="pre">0.3]</span></code> is divided into 4 partitions linearly, which is
<code class="docutils literal notranslate"><span class="pre">[0.0,</span> <span class="pre">0.1,</span> <span class="pre">0.2,</span> <span class="pre">0.3]</span></code>.</p>
<blockquote>
<div><ul class="simple">
<li><p>The LUT would be <code class="docutils literal notranslate"><span class="pre">[0.0,</span> <span class="pre">0.1,</span> <span class="pre">0.2,</span> <span class="pre">0.3]</span></code>.</p></li>
<li><p>The weight is rounded to <code class="docutils literal notranslate"><span class="pre">[0.1,</span> <span class="pre">0.2,</span> <span class="pre">0.3,</span> <span class="pre">0.1,</span> <span class="pre">0.0,</span> <span class="pre">0.0]</span></code>, and represented in
the palette as indices <code class="docutils literal notranslate"><span class="pre">[01b,</span> <span class="pre">10b,</span> <span class="pre">11b,</span> <span class="pre">01b,</span> <span class="pre">00b,</span> <span class="pre">00b]</span></code>.</p></li>
</ul>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>mlmodel: MLModel</strong></dt><dd><p>Model to be converted by a LUT. This MLModel should be of type <code class="docutils literal notranslate"><span class="pre">mlprogram</span></code>.</p>
</dd>
<dt><strong>nbits: int</strong></dt><dd><p>Number of bits per weight. Required for <code class="docutils literal notranslate"><span class="pre">kmeans</span></code> or <code class="docutils literal notranslate"><span class="pre">uniform</span></code> mode, but must
not be set for <code class="docutils literal notranslate"><span class="pre">unique</span></code> or <code class="docutils literal notranslate"><span class="pre">custom</span></code> mode. A LUT would have
2<sup>nbits</sup> entries, where <cite>nbits</cite> can be <code class="docutils literal notranslate"><span class="pre">{1,</span> <span class="pre">2,</span> <span class="pre">4,</span> <span class="pre">6,</span> <span class="pre">8}</span></code>.</p>
</dd>
<dt><strong>mode: str</strong></dt><dd><p>Determine how the LUT is constructed by specifying one of the following:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;kmeans&quot;</span></code> (default): The LUT is generated by <cite>k-means clustering</cite>, a method of vector
quantization that groups similar data points together to discover underlying
patterns by using a fixed number (<cite>k</cite>) of clusters in a dataset. A cluster
refers to a collection of data points aggregated together because of certain
similarities. <cite>nbits</cite> is required.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;uniform&quot;</span></code>: The LUT is generated by a linear histogram.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">[v_min,</span> <span class="pre">v_min</span> <span class="pre">+</span> <span class="pre">scale,</span> <span class="pre">v_min</span> <span class="pre">+</span> <span class="pre">2</span> <span class="pre">*</span> <span class="pre">scale,</span> <span class="pre">...,</span> <span class="pre">v_max]</span></code></p></li>
<li><p>Where the weight is in the range <code class="docutils literal notranslate"><span class="pre">[v_min,</span> <span class="pre">v_max]</span></code>, and
<code class="docutils literal notranslate"><span class="pre">scale</span> <span class="pre">=</span> <span class="pre">(v_max</span> <span class="pre">-</span> <span class="pre">v_min)</span> <span class="pre">/</span> <span class="pre">(1</span> <span class="pre">&lt;&lt;</span> <span class="pre">nbits</span> <span class="pre">-</span> <span class="pre">1)</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nbits</span></code> is required.</p></li>
</ul>
<p>A <cite>histogram</cite> is a representation of the distribution of a continuous variable,
in which the entire range of values is divided into a series of intervals (or
“bins”) and the representation displays how many values fall into each bin.
Linear histograms have one bin at even intervals, such as one bin per integer.</p>
</div></blockquote>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;unique&quot;</span></code>: The LUT is generated by unique values in the weights. The weights
are assumed to be on a discrete lattice but stored in a float data type. This
parameter identifies the weights and converts them into the palettized representation.</p>
<p>Do not provide <code class="docutils literal notranslate"><span class="pre">nbits</span></code> for this mode. <code class="docutils literal notranslate"><span class="pre">nbits</span></code> is picked up automatically,
with the smallest possible value in <code class="docutils literal notranslate"><span class="pre">{1,</span> <span class="pre">2,</span> <span class="pre">4,</span> <span class="pre">6,</span> <span class="pre">8}</span></code> such that the
number of the unique values is <code class="docutils literal notranslate"><span class="pre">&lt;=</span> <span class="pre">(1</span> <span class="pre">&lt;&lt;</span> <span class="pre">nbits)</span></code>. If the weight has <code class="docutils literal notranslate"><span class="pre">&gt;</span> <span class="pre">256</span></code>
unique values, the compression is skipped.</p>
<p>For example:</p>
<ul class="simple">
<li><p>If the weights are <code class="docutils literal notranslate"><span class="pre">{0.1,</span> <span class="pre">0.2,</span> <span class="pre">0.3,</span> <span class="pre">0.4}</span></code> and <code class="docutils literal notranslate"><span class="pre">nbits=2</span></code>, the weights are
converted to <code class="docutils literal notranslate"><span class="pre">{00b,</span> <span class="pre">01b,</span> <span class="pre">10b,</span> <span class="pre">11b}</span></code>, and the generated LUT is
<code class="docutils literal notranslate"><span class="pre">[0.1,</span> <span class="pre">0.2,</span> <span class="pre">0.3,</span> <span class="pre">0.4]</span></code>.</p></li>
<li><p>If the weights are <code class="docutils literal notranslate"><span class="pre">{0.1,</span> <span class="pre">0.2,</span> <span class="pre">0.3,</span> <span class="pre">0.4}</span></code> and <code class="docutils literal notranslate"><span class="pre">nbits=1</span></code>, nothing happens
because the weights are not a 1-bit lattice.</p></li>
<li><p>If the weights are <code class="docutils literal notranslate"><span class="pre">{0.1,</span> <span class="pre">0.2,</span> <span class="pre">0.3,</span> <span class="pre">0.4,</span> <span class="pre">0.5}</span></code> and <code class="docutils literal notranslate"><span class="pre">nbits=2</span></code>, nothing
happens because the weights are not a 2-bit lattice.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;custom&quot;</span></code>: The LUT and palettization parameters are calculated using a custom
function. If this mode is selected then <code class="docutils literal notranslate"><span class="pre">lut_function</span></code> must be provided.</p>
<p>Do not provide <code class="docutils literal notranslate"><span class="pre">nbits</span></code> for this mode. The user should customize <code class="docutils literal notranslate"><span class="pre">nbits</span></code> in the
<code class="docutils literal notranslate"><span class="pre">lut_function</span></code> implementation.</p>
</li>
</ul>
</dd>
<dt><strong>op_selector: callable</strong></dt><dd><p>This function takes a single parameter with type <code class="docutils literal notranslate"><span class="pre">coremltools.converters.mil.Operation</span></code>.
It returns a <code class="docutils literal notranslate"><span class="pre">bool</span></code>: <code class="docutils literal notranslate"><span class="pre">True</span></code> to compress <code class="docutils literal notranslate"><span class="pre">const_op</span></code>, otherwise <code class="docutils literal notranslate"><span class="pre">False</span></code>.
See the following examples:</p>
<ul>
<li><p>All constants in the network are compressed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">op_selector</span><span class="p">(</span><span class="n">const_op</span><span class="p">):</span>
      <span class="k">return</span> <span class="kc">True</span>
</pre></div>
</div>
</li>
<li><p>Only the constant with <code class="docutils literal notranslate"><span class="pre">tensor.size</span> <span class="pre">&gt;</span> <span class="pre">2048</span></code> is compressed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">op_selector</span><span class="p">(</span><span class="n">const_op</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">const_op</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="mi">2048</span>
</pre></div>
</div>
</li>
<li><p>Compress the constant if it is the weight of a convolution layer
and <code class="docutils literal notranslate"><span class="pre">tensor.size</span> <span class="pre">&gt;</span> <span class="pre">2048</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">op_selector</span><span class="p">(</span><span class="n">const_op</span><span class="p">):</span>
      <span class="k">return</span> <span class="p">(</span><span class="n">const_op</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="mi">2048</span>
              <span class="ow">and</span> <span class="n">const_op</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">child_ops</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">op_type</span> <span class="o">==</span> <span class="s2">&quot;conv&quot;</span>
              <span class="ow">and</span> <span class="n">const_op</span><span class="o">.</span><span class="n">val</span> <span class="o">==</span> <span class="n">const_op</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">child_ops</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
              <span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>When creating a custom <code class="docutils literal notranslate"><span class="pre">op_selector</span></code> function, the following attributes are helpful:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">const_op.val.val</span></code>: The numpy array holding the value of the const.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">const_op.val.child_ops</span></code>: A list of ops into which this constant is feeding.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">const_op.val.child_ops[i].op_type</span></code>: The string corresponding to the op type
of the i-th child op.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">const_op.val.child_ops[i].name</span></code>: The string corresponding to the name the
i-th child op.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">op_selector</span></code> is not provided, it will be set to the behavior in which
weights bigger than 2048 elements are compressed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">op_selector</span><span class="p">(</span><span class="n">const_op</span><span class="p">):</span>
      <span class="n">returm</span> <span class="n">const_op</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="mi">2048</span><span class="p">:</span>
</pre></div>
</div>
</li>
</ul>
</dd>
<dt><strong>lut_function: callable</strong></dt><dd><p>A callable function which computes the weight palettization parameters. This must
be provided if the mode is set to <code class="docutils literal notranslate"><span class="pre">&quot;custom&quot;</span></code>.</p>
<dl class="simple">
<dt>weight: np.ndarray</dt><dd><p>A float precision numpy array.</p>
</dd>
<dt>Returns: lut: list[float]</dt><dd><p>The lookup table.</p>
</dd>
<dt>indices: list[int]</dt><dd><p>A list of indices for each element.</p>
</dd>
</dl>
<p>The following is an example that extract the <code class="docutils literal notranslate"><span class="pre">top_k</span></code> elements as the LUT. Given
that <code class="docutils literal notranslate"><span class="pre">weight</span> <span class="pre">=</span> <span class="pre">[0.1,</span> <span class="pre">0.5,</span> <span class="pre">0.3,</span> <span class="pre">0.3,</span> <span class="pre">0.5,</span> <span class="pre">0.6,</span> <span class="pre">0.7]</span></code>, the <code class="docutils literal notranslate"><span class="pre">lut_function</span></code>
produces <code class="docutils literal notranslate"><span class="pre">lut</span> <span class="pre">=</span> <span class="pre">[0,</span> <span class="pre">0.5,</span> <span class="pre">0.6,</span> <span class="pre">0.7],</span> <span class="pre">indices</span> <span class="pre">=</span> <span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">2,</span> <span class="pre">3]</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">lut_function</span><span class="p">(</span><span class="n">weight</span><span class="p">):</span>
     <span class="c1"># In this example, we assume elements in the weights &gt;= 0</span>
     <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
     <span class="n">nbits</span> <span class="o">=</span> <span class="mi">4</span>

     <span class="c1"># Get the LUT, from extracting top k maximum unique elements in the weight to be the LUT</span>
     <span class="c1"># Note that k = 1 &lt;&lt; nbits - 1, so we have the first element be 0</span>
     <span class="n">unique_elements</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
     <span class="n">k</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">nbits</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
     <span class="n">top_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">partition</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="o">-</span><span class="n">k</span><span class="p">)[</span><span class="o">-</span><span class="n">k</span><span class="p">:]</span>
     <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">top_k</span><span class="p">)</span>
     <span class="n">lut</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.</span><span class="p">]</span> <span class="o">+</span> <span class="n">top_k</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

     <span class="c1"># Compute the indices</span>
     <span class="n">mapping</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lut</span><span class="p">)}</span>
     <span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">mapping</span><span class="p">[</span><span class="n">v</span><span class="p">]</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">mapping</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">weight</span><span class="p">]</span>

     <span class="k">return</span> <span class="n">lut</span><span class="p">,</span> <span class="n">indices</span>
</pre></div>
</div>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>model: MLModel</dt><dd><p>The palettized MLModel instance.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">coremltools</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">MLModel</span><span class="p">(</span><span class="s1">&#39;my_model.mlpackage&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">compressed_model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">compression_utils</span><span class="o">.</span><span class="n">palettize_weights</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;kmeans&quot;</span><span class="p">,</span> <span class="n">nbits</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="module-1">
<span id="sparsify-weights"></span><h1>sparsify_weights<a class="headerlink" href="#module-1" title="Permalink to this heading"></a></h1>
<dl class="py function">
<dt class="sig sig-object py" id="coremltools.models.ml_program.compression_utils.sparsify_weights">
<span class="sig-prename descclassname"><span class="pre">coremltools.models.ml_program.compression_utils.</span></span><span class="sig-name descname"><span class="pre">sparsify_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mlmodel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'threshold_based'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_percentile</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">op_selector</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/coremltools/models/ml_program/compression_utils.html#sparsify_weights"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#coremltools.models.ml_program.compression_utils.sparsify_weights" title="Permalink to this definition"></a></dt>
<dd><p>Utility function to convert a float precision MLModel of type <code class="docutils literal notranslate"><span class="pre">mlprogram</span></code> to a
compressed MLModel using sparse representation. The <code class="docutils literal notranslate"><span class="pre">const</span></code> ops storing weight
values are replaced by <code class="docutils literal notranslate"><span class="pre">constexpr_sparse_to_dense</span></code> ops.</p>
<p>This function is useful if the model is trained with pruning techniques so that
a lot of weights have zero values. If a large percentage of weight values are zero,
a sparse representation is more efficient than a dense one (the default).</p>
<p>The sparsified weights are stored in a bit mask. If the weight values are
<code class="docutils literal notranslate"><span class="pre">{0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">56.3}</span></code>, its sparse representation contains a bit mask with
ones on locations where the value is non-zero: <code class="docutils literal notranslate"><span class="pre">00000001b</span></code>. This is accompanied by
non-zero data, which is a size-1 vector of value <code class="docutils literal notranslate"><span class="pre">{56.3}</span></code>.</p>
<p>For example, given the following:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">weight</span> <span class="pre">=</span> <span class="pre">[0.3,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0.5,</span> <span class="pre">0,</span> <span class="pre">0]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">non_zero_data,</span> <span class="pre">bit_mask</span> <span class="pre">=</span> <span class="pre">sparsify(weight)</span></code></p></li>
</ul>
</div></blockquote>
<p>The indices of the non-zero elements are:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">non_zero_data</span> <span class="pre">=</span> <span class="pre">[0.3,</span> <span class="pre">0.5]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bit_mask</span> <span class="pre">=</span> <span class="pre">&quot;100100&quot;</span></code></p></li>
</ul>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>mlmodel: MLModel</strong></dt><dd><p>Model to be sparsified. This MLModel should be of type <code class="docutils literal notranslate"><span class="pre">mlprogram</span></code>.</p>
</dd>
<dt><strong>mode: str</strong></dt><dd><p>Determine the scheme to sparsify the model by specifying one of the following:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;threshold_based&quot;</span></code> (default): All the absolute weight values that are smaller
than <code class="docutils literal notranslate"><span class="pre">threshold</span></code> are changed to 0, and the tensor is stored in a sparse format.
For example, given the following:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">weight</span> <span class="pre">=</span> <span class="pre">[0.3,</span> <span class="pre">-0.2,</span> <span class="pre">-0.01,</span> <span class="pre">0.05]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">threshold</span> <span class="pre">=</span> <span class="pre">0.03</span></code></p></li>
</ul>
</div></blockquote>
<p>The sparsified weight would be <code class="docutils literal notranslate"><span class="pre">[0.3,</span> <span class="pre">-0.2,</span> <span class="pre">0,</span> <span class="pre">0.05]</span></code>.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;percentile_based&quot;</span></code>: Sparsify the weight with a constant sparsity percentile,
which is <code class="docutils literal notranslate"><span class="pre">target_percentile</span></code>. Where
<code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">=</span> <span class="pre">floor(size_of_weight_tensor</span> <span class="pre">*</span> <span class="pre">target_percentile)</span></code>, the <code class="docutils literal notranslate"><span class="pre">n</span></code> lowest
absolute weight values are changed to 0. For example, given the following:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">weight</span> <span class="pre">=</span> <span class="pre">[0.3,</span> <span class="pre">-0.2,</span> <span class="pre">-0.01,</span> <span class="pre">0.05]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">target_percentile</span> <span class="pre">=</span> <span class="pre">0.75</span></code></p></li>
</ul>
</div></blockquote>
<p>The sparsified weight would be <code class="docutils literal notranslate"><span class="pre">[0.3,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0]</span></code>.</p>
</li>
</ul>
</dd>
<dt><strong>threshold: float</strong></dt><dd><p>Required when <code class="docutils literal notranslate"><span class="pre">mode</span> <span class="pre">=</span> <span class="pre">&quot;prune_threshold&quot;</span></code>. The absolute threshold to sparsify the weight.</p>
</dd>
<dt><strong>target_percentile: float</strong></dt><dd><p>Required when <code class="docutils literal notranslate"><span class="pre">mode</span> <span class="pre">=</span> <span class="pre">&quot;percentile_based&quot;</span></code>. The percentage of sparsity for
compression, which needs to be in the range [0, 1]. When 0, no sparsification
occurs. For 1, all weights become 0.</p>
</dd>
<dt><strong>op_selector: callable</strong></dt><dd><p>This function takes a single parameter with type <code class="docutils literal notranslate"><span class="pre">coremltools.converters.mil.Operation</span></code>.
It returns a <code class="docutils literal notranslate"><span class="pre">bool</span></code>: <code class="docutils literal notranslate"><span class="pre">True</span></code> to compress <code class="docutils literal notranslate"><span class="pre">const_op</span></code>, otherwise <code class="docutils literal notranslate"><span class="pre">False</span></code>.
See the following examples:</p>
<ul>
<li><p>All constants in the network are compressed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">op_selector</span><span class="p">(</span><span class="n">const_op</span><span class="p">):</span>
      <span class="k">return</span> <span class="kc">True</span>
</pre></div>
</div>
</li>
<li><p>Only the constant with <code class="docutils literal notranslate"><span class="pre">tensor.size</span> <span class="pre">&gt;</span> <span class="pre">2048</span></code> is compressed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">op_selector</span><span class="p">(</span><span class="n">const_op</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">const_op</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="mi">2048</span>
</pre></div>
</div>
</li>
<li><p>Compress the constant if it is the weight of a convolution layer
and <code class="docutils literal notranslate"><span class="pre">tensor.size</span> <span class="pre">&gt;</span> <span class="pre">2048</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">op_selector</span><span class="p">(</span><span class="n">const_op</span><span class="p">):</span>
      <span class="k">return</span> <span class="p">(</span><span class="n">const_op</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="mi">2048</span>
              <span class="ow">and</span> <span class="n">const_op</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">child_ops</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">op_type</span> <span class="o">==</span> <span class="s2">&quot;conv&quot;</span>
              <span class="ow">and</span> <span class="n">const_op</span><span class="o">.</span><span class="n">val</span> <span class="o">==</span> <span class="n">const_op</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">child_ops</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
              <span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>When creating a custom <code class="docutils literal notranslate"><span class="pre">op_selector</span></code> function, the following attributes are helpful:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">const_op.val.val</span></code>: The numpy array holding the value of the const.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">const_op.val.child_ops</span></code>: A list of ops into which this constant is feeding.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">const_op.val.child_ops[i].op_type</span></code>: The string corresponding to the op type
of the i-th child op.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">const_op.val.child_ops[i].name</span></code>: The string corresponding to the name the
i-th child op.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">op_selector</span></code> is not provided, it will be set to the behavior in which
weights bigger than 2048 elements are compressed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">op_selector</span><span class="p">(</span><span class="n">const_op</span><span class="p">):</span>
      <span class="n">returm</span> <span class="n">const_op</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="mi">2048</span><span class="p">:</span>
</pre></div>
</div>
</li>
</ul>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>model: MLModel</dt><dd><p>The sparse MLModel instance.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">coremltools</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">MLModel</span><span class="p">(</span><span class="s1">&#39;my_model.mlpackage&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">compressed_model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">compression_utils</span><span class="o">.</span><span class="n">sparsify_weights</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;threshold_based&quot;</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="coremltools.models.html" class="btn btn-neutral float-left" title="Models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="coremltools.models.neural_network.html" class="btn btn-neutral float-right" title="neural_network.builder" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Apple Inc.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>