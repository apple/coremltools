
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Combining Compression Types &#8212; Guide to Core ML Tools</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/imgstyle.css?v=27a1495e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=c1ce5b23"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'source/opt-joint-compression';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Conversion" href="opt-conversion.html" />
    <link rel="prev" title="API Overview" href="opt-pruning-api.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Guide to Core ML Tools</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://apple.github.io/coremltools/index.html">coremltools API Reference</a></li>
<li class="toctree-l1"><a class="reference external" href="https://apple.github.io/coremltools/mlmodel/index.html">Core ML Model Format</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="overview-coremltools.html">What Is Core ML Tools?</a></li>
<li class="toctree-l1"><a class="reference internal" href="installing-coremltools.html">Installing Core ML Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="introductory-quickstart.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="new-features.html">New Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="faqs.html">Core ML Tools FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="coremltools-examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="how-to-contribute.html">Contributing</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unified Conversion</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="unified-conversion-api.html">Core ML Tools API Overview</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="convert-learning-models.html">Converting Deep Learning Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="target-conversion-formats.html">Source and Conversion Formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="load-and-convert-model.html">Load and Convert Model Workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-to-ml-program.html">Convert Models to ML Programs</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-to-neural-network.html">Convert Models to Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="comparing-ml-programs-and-neural-networks.html">Comparing ML Programs and Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="typed-execution.html">Typed Execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="typed-execution-example.html">Typed Execution Workflow Example</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="convert-tensorflow.html">Converting from TensorFlow</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="tensorflow-1-workflow.html">TensorFlow 1 Workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-a-tensorflow-1-image-classifier.html">Converting a TensorFlow 1 Image Classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-a-tensorflow-1-deepspeech-model.html">Converting a TensorFlow 1 DeepSpeech Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensorflow-2.html">TensorFlow 2 Workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-tensorflow-2-bert-transformer-models.html">Converting TensorFlow 2 BERT Transformer Models</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="convert-pytorch.html">Converting from PyTorch</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="convert-pytorch-workflow.html">PyTorch Conversion Workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="model-tracing.html">Model Tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="model-exporting.html">Model Exporting</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-a-torchvision-model-from-pytorch.html">Converting a torchvision Model from PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-a-pytorch-segmentation-model.html">Converting a PyTorch Segmentation Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-openelm.html">Converting an Open Efficient Language Model</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="conversion-options.html">Conversion Options</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="new-conversion-options.html">New Conversion Options</a></li>
<li class="toctree-l2"><a class="reference internal" href="model-input-and-output-types.html">Model Input and Output Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-inputs.html">Image Input and Output</a></li>
<li class="toctree-l2"><a class="reference internal" href="stateful-models.html">Stateful Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="classifiers.html">Classifiers</a></li>
<li class="toctree-l2"><a class="reference internal" href="flexible-inputs.html">Flexible Input Shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="composite-operators.html">Composite Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="custom-operators.html">Custom Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="graph-passes-intro.html">Graph Passes</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="model-intermediate-language.html">Model Intermediate Language</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Optimization</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="opt-overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="opt-whats-new.html">What’s New</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="opt-overview-examples.html">Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="opt-resnet.html">Optimizing ResNet50 Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-opt1_3.html">Optimizing OPT Model</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="opt-workflow.html">Optimization Workflow</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="opt-palettization.html">Palettization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="opt-palettization-overview.html">Palettization Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-palettization-perf.html">Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-palettization-algos.html">Palettization Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-palettization-api.html">API Overview</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="opt-quantization.html">Linear Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="opt-quantization-overview.html">Quantization Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-quantization-perf.html">Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-quantization-algos.html">Quantization Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-quantization-api.html">API Overview</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="opt-pruning.html">Pruning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="opt-pruning-overview.html">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-pruning-perf.html">Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-pruning-algos.html">Pruning Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-pruning-api.html">API Overview</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Combining Compression Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="opt-conversion.html">Conversion</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization-neural-network.html">Compressing Neural Network Weights</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Other Converters</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="libsvm-conversion.html">LibSVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="sci-kit-learn-conversion.html">Scikit-learn</a></li>
<li class="toctree-l1"><a class="reference internal" href="xgboost-conversion.html">XGBoost</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">MLModel</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mlmodel.html">MLModel Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="multifunction-models.html">Multifunction Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="xcode-model-preview-types.html">Xcode Model Preview Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlmodel-utilities.html">MLModel Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="model-prediction.html">Model Prediction</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="updatable-model-examples.html">Updatable Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="updatable-neural-network-classifier-on-mnist-dataset.html">Neural Network Classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="updatable-tiny-drawing-classifier-pipeline-model.html">Pipeline Classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="updatable-nearest-neighbor-classifier.html">Nearest Neighbor Classifier</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/apple/coremltools" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/source/opt-joint-compression.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Combining Compression Types</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-compression-types-on-an-mlpackage">Combining compression types on an mlpackage</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-palettization-and-quantization">Joint palettization and quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-sparsity-and-quantization">Joint sparsity and quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-sparsity-and-palettization">Joint sparsity and palettization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-compression-types-on-a-torch-model">Combining compression types on a Torch model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Joint palettization and quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Joint sparsity and quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Joint sparsity and palettization</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="combining-compression-types">
<h1>Combining Compression Types<a class="headerlink" href="#combining-compression-types" title="Link to this heading">#</a></h1>
<p>In previous sections on <a class="reference internal" href="opt-palettization.html"><span class="doc std std-doc">palettization</span></a>,
<a class="reference internal" href="opt-quantization.html"><span class="doc std std-doc">quantization</span></a>, and <a class="reference internal" href="opt-pruning.html"><span class="doc std std-doc">sparsity</span></a>,
we considered how to apply the various compression
techniques to the weights and activations of the model independently.
In this section, we describe how these techniques can be combined,
which may be beneficial to get even more disk savings and latency improvements.</p>
<p>We first start by looking at how to take an uncompressed <code class="docutils literal notranslate"><span class="pre">mlpackage</span></code>
and get a joint compressed model by using the <code class="docutils literal notranslate"><span class="pre">ct.optimize.coreml.*</span></code> APIs.
As discussed in previous sections, this approach may or
may not yield a highly accurate model. In some cases, however,
this is the best way to get the model in the desired format to test
out the expected disk size savings and performance (latency, runtime memory etc).
Once a model has the desired performance characteristics,
a better accuracy model can be generated by applying the various data
based optimization methods available in <code class="docutils literal notranslate"><span class="pre">ct.optimize.torch.*</span></code>.
This last topic is discussed via a few API code snippets in the section below.</p>
<section id="combining-compression-types-on-an-mlpackage">
<h2>Combining compression types on an mlpackage<a class="headerlink" href="#combining-compression-types-on-an-mlpackage" title="Link to this heading">#</a></h2>
<section id="joint-palettization-and-quantization">
<h3>Joint palettization and quantization<a class="headerlink" href="#joint-palettization-and-quantization" title="Link to this heading">#</a></h3>
<p>This means using a lookup table (LUT) whose values are of the
dtype INT8/UINT8 instead of Float16 which is the default.
This can help speed up inference when combined with INT8 activations.
For instance, you could take a A16W16 model, quantize the activations to get A8W16 model, and then quantize the
weights to a 4-bit LUT with INT8 dtype to yield an A8W4 model,
where “W4” refers to a palettized weights with a LUT that has 2^4 entries, and
each entry has a dtype of INT8.
When such a model is run on the Neural Engine (on newer SoCs &gt;= A17pro, M4),
it will utilize the faster int8-int8 compute path.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">coremltools.optimize.coreml</span> <span class="kn">import</span> <span class="p">(</span>
   <span class="n">OptimizationConfig</span><span class="p">,</span>
   <span class="n">OpPalettizerConfig</span><span class="p">,</span>
   <span class="n">OpLinearQuantizerConfig</span><span class="p">,</span>
   <span class="n">palettize_weights</span><span class="p">,</span>
   <span class="n">linear_quantize_weights</span><span class="p">,</span>
 <span class="p">)</span> 
     
<span class="c1"># mlmodel: an uncompressed mlpackage, loaded into memory </span>
                                                                          
<span class="c1"># first palettize the model</span>
<span class="c1"># this will produce an LUT with Float values</span>
<span class="n">op_config</span> <span class="o">=</span> <span class="n">OpPalettizerConfig</span><span class="p">(</span><span class="n">nbits</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">OptimizationConfig</span><span class="p">(</span><span class="n">global_config</span><span class="o">=</span><span class="n">op_config</span><span class="p">)</span>
<span class="n">mlmodel_palettized</span> <span class="o">=</span> <span class="n">palettize_weights</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

<span class="c1"># now apply weight quantization on the model, </span>
<span class="c1"># with &quot;joint_compression&quot; set to True. </span>
<span class="c1"># this will result in quantizing the LUT to 8 bits. </span>
<span class="c1"># (granularity must be set to &quot;per-tensor&quot; for this scenario) </span>
<span class="n">op_config</span> <span class="o">=</span> <span class="n">OpLinearQuantizerConfig</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;linear_symmetric&quot;</span><span class="p">,</span>  
                                    <span class="n">granularity</span><span class="o">=</span><span class="s2">&quot;per_tensor&quot;</span><span class="p">)</span>
<span class="n">linear_weight_quantize_config</span> <span class="o">=</span> <span class="n">OptimizationConfig</span><span class="p">(</span><span class="n">global_config</span><span class="o">=</span><span class="n">op_config</span><span class="p">)</span>

<span class="n">mlmodel_palettized_with_8bit_lut</span> <span class="o">=</span> <span class="n">linear_quantize_weights</span><span class="p">(</span><span class="n">mlmodel_palettized</span><span class="p">,</span> 
                                                           <span class="n">linear_weight_quantize_config</span><span class="p">,</span> 
                                                           <span class="n">joint_compression</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="joint-sparsity-and-quantization">
<h3>Joint sparsity and quantization<a class="headerlink" href="#joint-sparsity-and-quantization" title="Link to this heading">#</a></h3>
<p>This means quantizing the non-zero values in the sparse weight tensor to INT8/UINT8 values.
This could improve inference speed and disk savings.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">coremltools.optimize.coreml</span> <span class="kn">import</span> <span class="p">(</span>
   <span class="n">OptimizationConfig</span><span class="p">,</span>
   <span class="n">OpMagnitudePrunerConfig</span><span class="p">,</span>
   <span class="n">OpLinearQuantizerConfig</span><span class="p">,</span>
   <span class="n">prune_weights</span><span class="p">,</span>
   <span class="n">linear_quantize_weights</span><span class="p">,</span>
 <span class="p">)</span>
 
<span class="c1"># first prune the model</span>
<span class="n">op_config</span> <span class="o">=</span> <span class="n">OpMagnitudePrunerConfig</span><span class="p">(</span><span class="n">target_sparsity</span><span class="o">=</span><span class="mf">0.80</span><span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">OptimizationConfig</span><span class="p">(</span><span class="n">global_config</span><span class="o">=</span><span class="n">op_config</span><span class="p">)</span>
<span class="n">mlmodel_pruned</span> <span class="o">=</span> <span class="n">prune_weights</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

<span class="c1"># now apply weight quantization on the model,</span>
<span class="c1"># with &quot;joint_compression&quot; set to True. </span>
<span class="c1"># this will result in quantizing the non-zero values to 8 bits. </span>
<span class="n">linear_weight_quantize_config</span> <span class="o">=</span> <span class="n">OptimizationConfig</span><span class="p">(</span>
    <span class="n">global_config</span><span class="o">=</span><span class="n">OpLinearQuantizerConfig</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;linear_symmetric&quot;</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">mlmodel_pruned_quantized</span> <span class="o">=</span> <span class="n">linear_quantize_weights</span><span class="p">(</span><span class="n">mlmodel_pruned</span><span class="p">,</span> 
                                                   <span class="n">linear_weight_quantize_config</span><span class="p">,</span> 
                                                   <span class="n">joint_compression</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="joint-sparsity-and-palettization">
<h3>Joint sparsity and palettization<a class="headerlink" href="#joint-sparsity-and-palettization" title="Link to this heading">#</a></h3>
<p>This means representing the non-zero values in a sparse weight tensor with
discrete values pointing to a lookup table (i.e. palettized).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">coremltools.optimize.coreml</span> <span class="kn">import</span> <span class="p">(</span>
   <span class="n">OptimizationConfig</span><span class="p">,</span>
   <span class="n">OpMagnitudePrunerConfig</span><span class="p">,</span>
   <span class="n">OpPalettizerConfig</span><span class="p">,</span>
   <span class="n">prune_weights</span><span class="p">,</span>
   <span class="n">palettize_weights</span><span class="p">,</span>
 <span class="p">)</span>
 
<span class="c1"># first prune the model</span>
<span class="n">op_config</span> <span class="o">=</span> <span class="n">OpMagnitudePrunerConfig</span><span class="p">(</span><span class="n">target_sparsity</span><span class="o">=</span><span class="mf">0.80</span><span class="p">)</span>
<span class="n">pruning_config</span> <span class="o">=</span> <span class="n">OptimizationConfig</span><span class="p">(</span><span class="n">global_config</span><span class="o">=</span><span class="n">op_config</span><span class="p">)</span>
<span class="n">mlmodel_pruned</span> <span class="o">=</span> <span class="n">prune_weights</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">pruning_config</span><span class="p">)</span>

<span class="c1"># now apply weight palettization on the model, </span>
<span class="c1"># with &quot;joint_compression&quot; set to True. </span>
<span class="c1"># this will result in palettizing the non-zero values. </span>
<span class="n">palettization_config</span> <span class="o">=</span> <span class="n">OptimizationConfig</span><span class="p">(</span><span class="n">global_config</span><span class="o">=</span><span class="n">OpPalettizerConfig</span><span class="p">(</span><span class="n">nbits</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>
<span class="n">mlmodel_pruned_palettized</span> <span class="o">=</span> <span class="n">palettize_weights</span><span class="p">(</span><span class="n">mlmodel_pruned</span><span class="p">,</span> 
                                              <span class="n">palettization_config</span><span class="p">,</span> 
                                              <span class="n">joint_compression</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

</pre></div>
</div>
</section>
</section>
<section id="combining-compression-types-on-a-torch-model">
<h2>Combining compression types on a Torch model<a class="headerlink" href="#combining-compression-types-on-a-torch-model" title="Link to this heading">#</a></h2>
<section id="id1">
<h3>Joint palettization and quantization<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>This means using a lookup table (LUT) whose values
are of the dtype INT8/UINT8 instead of the default Float16.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torchvision</span> 
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">coremltools</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">from</span> <span class="nn">coremltools.optimize.torch.palettization</span> <span class="kn">import</span> <span class="n">PostTrainingPalettizerConfig</span><span class="p">,</span>\
                                                     <span class="n">PostTrainingPalettizer</span>

<span class="c1"># load a torch model</span>
<span class="c1"># e.g. resnet50 </span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s2">&quot;IMAGENET1K_V2&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># specify &quot;lut_dtype&quot; as torch.int8</span>
<span class="c1"># when not specified, it defaults to None and FP16 LUT is constructed </span>
<span class="n">config_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;global_config&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;n_bits&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;lut_dtype&quot;</span> <span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">}}</span>
<span class="n">palettizer_config</span> <span class="o">=</span> <span class="n">PostTrainingPalettizerConfig</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">config_dict</span><span class="p">)</span>

<span class="n">compressor</span> <span class="o">=</span> <span class="n">PostTrainingPalettizer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">palettizer_config</span><span class="p">)</span>
<span class="n">compressed_model</span> <span class="o">=</span> <span class="n">compressor</span><span class="o">.</span><span class="n">compress</span><span class="p">()</span>   

<span class="c1"># convert the compressed model</span>
<span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">compressed_model</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>

<span class="n">mlmodel</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">traced_model</span><span class="p">,</span>
                     <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">ct</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">))],</span>
                     <span class="n">minimum_deployment_target</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">macOS15</span><span class="p">,</span>
                     <span class="p">)</span>
<span class="n">mlmodel</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;model_4bit_palettized_with_8bit_quantized_lut.mlpackage&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id2">
<h3>Joint sparsity and quantization<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>One way to combine sparsity and quantization is to
first prune a torch model using the <code class="docutils literal notranslate"><span class="pre">MagnitudePruner</span></code> class,
export it as an <code class="docutils literal notranslate"><span class="pre">mlpackage</span></code> and then apply weight quantization (A16W8) on
the <code class="docutils literal notranslate"><span class="pre">mlpackage</span></code>, as shown in the section above.</p>
<p>However, if we want to apply pruning and weight-only quantization (A16W8)
on the torch model at training time, it can be done in the way explained below.</p>
<p>Note that if <code class="docutils literal notranslate"><span class="pre">&quot;activation_dtype&quot;</span></code> argument in
<code class="docutils literal notranslate"><span class="pre">ModuleLinearQuantizerConfig</span></code> is set to its default value of <code class="docutils literal notranslate"><span class="pre">torch.qint8</span></code>,
then activations will also be quantized to get an A8W8 model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torchvision</span> 
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">coremltools</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">from</span> <span class="nn">coremltools.optimize.torch.quantization</span> <span class="kn">import</span> <span class="n">ModuleLinearQuantizerConfig</span><span class="p">,</span> \
                                                    <span class="n">LinearQuantizerConfig</span><span class="p">,</span> \
                                                    <span class="n">LinearQuantizer</span>

<span class="kn">from</span> <span class="nn">coremltools.optimize.torch.pruning</span> <span class="kn">import</span> <span class="n">ModuleMagnitudePrunerConfig</span><span class="p">,</span> \
                                               <span class="n">MagnitudePrunerConfig</span><span class="p">,</span> \
                                               <span class="n">MagnitudePruner</span> 
                                               

<span class="c1"># Initialize model and optimizer</span>
<span class="c1"># e.g. Resnet50</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s2">&quot;IMAGENET1K_V2&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Prepare model for joint quantization and pruning</span>
<span class="n">quant_config</span> <span class="o">=</span> <span class="n">LinearQuantizerConfig</span><span class="p">(</span>
    <span class="n">global_config</span><span class="o">=</span><span class="n">ModuleLinearQuantizerConfig</span><span class="p">(</span>
        <span class="n">quantization_scheme</span><span class="o">=</span><span class="s2">&quot;symmetric&quot;</span><span class="p">,</span>
        <span class="n">activation_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">prune_config</span> <span class="o">=</span> <span class="n">MagnitudePrunerConfig</span><span class="p">(</span>
    <span class="n">global_config</span><span class="o">=</span><span class="n">ModuleMagnitudePrunerConfig</span><span class="p">(</span>
         <span class="n">target_sparsity</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>       
    <span class="p">)</span>
<span class="p">)</span>


<span class="c1"># The quantizer config needs to be applied before the pruner config</span>

<span class="n">quantizer</span> <span class="o">=</span> <span class="n">LinearQuantizer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">quant_config</span><span class="p">)</span>
<span class="n">quant_model</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">example_inputs</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">])</span>

<span class="n">pruner</span> <span class="o">=</span> <span class="n">MagnitudePruner</span><span class="p">(</span><span class="n">quant_model</span><span class="p">,</span> <span class="n">prune_config</span><span class="p">)</span>
<span class="c1">#  in-place is required to ensure quantizer and pruner are </span>
<span class="c1"># operating on the same model </span>
<span class="n">pruned_quant_model</span> <span class="o">=</span> <span class="n">pruner</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="n">n_classes</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="c1"># run a couple of training iterations with random data</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="c1"># Dummy data</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>  <span class="c1"># Batch of samples</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,))</span>  <span class="c1"># Target labels </span>
    
    <span class="c1"># Forward pass</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">pruned_quant_model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    
    <span class="c1"># Backward and optimize</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> 
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">quantizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">pruner</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>         
    

<span class="c1"># finalize the model for export</span>
<span class="c1"># we first finalize the quantizer followed by the pruner </span>
<span class="n">quant_finalized_model</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">finalize</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">finalized_model</span> <span class="o">=</span> <span class="n">pruner</span><span class="o">.</span><span class="n">finalize</span><span class="p">(</span><span class="n">quant_finalized_model</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">finalized_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>                                                                                                                                                                                          


<span class="c1"># trace and export to mlpackage</span>
<span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">finalized_model</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>
<span class="n">mlmodel</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">traced_model</span><span class="p">,</span>
                     <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">ct</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">))],</span>
                     <span class="n">minimum_deployment_target</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">macOS15</span><span class="p">,</span>
                     <span class="p">)</span>
<span class="n">mlmodel</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;model_torch_pruned_and_quantized.mlpackage&quot;</span><span class="p">)</span>  
</pre></div>
</div>
</section>
<section id="id3">
<h3>Joint sparsity and palettization<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>Here we apply magnitude pruning to the torch model, followed by data-free palettization.</p>
<p>Note: to apply training time pruning and
palettization (e.g. <a class="reference internal" href="opt-palettization-algos.html#differentiable-k-means"><span class="std std-ref">DKM</span></a>), follow
the same pattern as the section above, replacing the <code class="docutils literal notranslate"><span class="pre">LinearQuantizer</span></code> with <code class="docutils literal notranslate"><span class="pre">DKMPalettizer</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torchvision</span> 
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">coremltools</span> <span class="k">as</span> <span class="nn">ct</span>


<span class="kn">from</span> <span class="nn">coremltools.optimize.torch.pruning</span> <span class="kn">import</span> <span class="n">ModuleMagnitudePrunerConfig</span><span class="p">,</span> \
                                               <span class="n">MagnitudePrunerConfig</span><span class="p">,</span> \
                                               <span class="n">MagnitudePruner</span> 
                                               
<span class="kn">from</span> <span class="nn">coremltools.optimize.torch.palettization</span> <span class="kn">import</span> <span class="n">PostTrainingPalettizer</span><span class="p">,</span> \
                                                 <span class="n">PostTrainingPalettizerConfig</span><span class="p">,</span> \
                                                 <span class="n">ModulePostTrainingPalettizerConfig</span>                                               


<span class="c1"># Apply pruning </span>

<span class="c1"># Initialize model and optimizer</span>
<span class="c1"># e.g. Resnet50</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s2">&quot;IMAGENET1K_V2&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Prepare model for pruning</span>
<span class="n">prune_config</span> <span class="o">=</span> <span class="n">MagnitudePrunerConfig</span><span class="p">(</span>
    <span class="n">global_config</span><span class="o">=</span><span class="n">ModuleMagnitudePrunerConfig</span><span class="p">(</span>
         <span class="n">target_sparsity</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>       
    <span class="p">)</span>
<span class="p">)</span>

<span class="n">pruner</span> <span class="o">=</span> <span class="n">MagnitudePruner</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">prune_config</span><span class="p">)</span>
<span class="n">pruned_model</span> <span class="o">=</span> <span class="n">pruner</span><span class="o">.</span><span class="n">prepare</span><span class="p">()</span>

<span class="c1"># run a couple of training iterations with random data</span>
<span class="n">n_classes</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>  <span class="c1"># Batch of samples</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,))</span>  <span class="c1"># Target labels </span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">pruned_model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> 
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">pruner</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># finalize model </span>
<span class="n">pruned_model</span> <span class="o">=</span> <span class="n">pruner</span><span class="o">.</span><span class="n">finalize</span><span class="p">(</span><span class="n">pruned_model</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>



<span class="c1"># Apply palettization </span>
<span class="n">palettization_config</span> <span class="o">=</span> <span class="n">PostTrainingPalettizerConfig</span><span class="p">(</span>
    <span class="n">global_config</span><span class="o">=</span><span class="n">ModulePostTrainingPalettizerConfig</span><span class="p">(</span>
         <span class="n">n_bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>       
    <span class="p">)</span>
<span class="p">)</span>

<span class="n">palettizer</span> <span class="o">=</span> <span class="n">PostTrainingPalettizer</span><span class="p">(</span><span class="n">pruned_model</span><span class="p">,</span> <span class="n">palettization_config</span><span class="p">)</span>
<span class="n">joint_compressed_model</span> <span class="o">=</span> <span class="n">palettizer</span><span class="o">.</span><span class="n">compress</span><span class="p">()</span>

<span class="c1"># convert the compressed model</span>
<span class="n">joint_compressed_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">joint_compressed_model</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>

<span class="n">mlmodel</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">traced_model</span><span class="p">,</span>
                     <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">ct</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">))],</span>
                     <span class="n">minimum_deployment_target</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">macOS15</span><span class="p">,</span>
                     <span class="p">)</span>
<span class="n">mlmodel</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;model_torch_pruned_and_palettized.mlpackage&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="opt-pruning-api.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">API Overview</p>
      </div>
    </a>
    <a class="right-next"
       href="opt-conversion.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Conversion</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-compression-types-on-an-mlpackage">Combining compression types on an mlpackage</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-palettization-and-quantization">Joint palettization and quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-sparsity-and-quantization">Joint sparsity and quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-sparsity-and-palettization">Joint sparsity and palettization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-compression-types-on-a-torch-model">Combining compression types on a Torch model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Joint palettization and quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Joint sparsity and quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Joint sparsity and palettization</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Apple
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023, Apple Inc.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>