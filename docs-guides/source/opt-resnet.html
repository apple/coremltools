
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Optimizing ResNet50 Model &#8212; Guide to Core ML Tools</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/imgstyle.css?v=27a1495e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=c1ce5b23"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'source/opt-resnet';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Optimizing OPT Model" href="opt-opt1_3.html" />
    <link rel="prev" title="Examples" href="opt-overview-examples.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Guide to Core ML Tools</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://apple.github.io/coremltools/index.html">coremltools API Reference</a></li>
<li class="toctree-l1"><a class="reference external" href="https://apple.github.io/coremltools/mlmodel/index.html">Core ML Model Format</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="overview-coremltools.html">What Is Core ML Tools?</a></li>
<li class="toctree-l1"><a class="reference internal" href="installing-coremltools.html">Installing Core ML Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="introductory-quickstart.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="new-features.html">New Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="faqs.html">Core ML Tools FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="coremltools-examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="how-to-contribute.html">Contributing</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unified Conversion</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="unified-conversion-api.html">Core ML Tools API Overview</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="convert-learning-models.html">Converting Deep Learning Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="target-conversion-formats.html">Source and Conversion Formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="load-and-convert-model.html">Load and Convert Model Workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-to-ml-program.html">Convert Models to ML Programs</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-to-neural-network.html">Convert Models to Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="comparing-ml-programs-and-neural-networks.html">Comparing ML Programs and Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="typed-execution.html">Typed Execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="typed-execution-example.html">Typed Execution Workflow Example</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="convert-tensorflow.html">Converting from TensorFlow</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="tensorflow-1-workflow.html">TensorFlow 1 Workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-a-tensorflow-1-image-classifier.html">Converting a TensorFlow 1 Image Classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-a-tensorflow-1-deepspeech-model.html">Converting a TensorFlow 1 DeepSpeech Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensorflow-2.html">TensorFlow 2 Workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-tensorflow-2-bert-transformer-models.html">Converting TensorFlow 2 BERT Transformer Models</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="convert-pytorch.html">Converting from PyTorch</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="convert-pytorch-workflow.html">PyTorch Conversion Workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="model-tracing.html">Model Tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="model-scripting.html">Model Scripting</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-nlp-model.html">Converting a Natural Language Processing Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-a-torchvision-model-from-pytorch.html">Converting a torchvision Model from PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytorch-conversion-examples.html">Converting a PyTorch Segmentation Model</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="conversion-options.html">Conversion Options</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="new-conversion-options.html">New Conversion Options</a></li>
<li class="toctree-l2"><a class="reference internal" href="model-input-and-output-types.html">Model Input and Output Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-inputs.html">Image Input and Output</a></li>
<li class="toctree-l2"><a class="reference internal" href="stateful-models.html">Stateful Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="classifiers.html">Classifiers</a></li>
<li class="toctree-l2"><a class="reference internal" href="flexible-inputs.html">Flexible Input Shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="composite-operators.html">Composite Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="custom-operators.html">Custom Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="graph-passes-intro.html">Graph Passes</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="model-intermediate-language.html">Model Intermediate Language</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Optimization</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="opt-overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="opt-whats-new.html">What’s New</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="opt-overview-examples.html">Examples</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Optimizing ResNet50 Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-opt1_3.html">Optimizing OPT Model</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="opt-workflow.html">Optimization Workflow</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="opt-palettization.html">Palettization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="opt-palettization-overview.html">Palettization Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-palettization-perf.html">Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-palettization-algos.html">Palettization Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-palettization-api.html">API Overview</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="opt-quantization.html">Linear Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="opt-quantization-overview.html">Quantization Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-quantization-perf.html">Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-quantization-algos.html">Quantization Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-quantization-api.html">API Overview</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="opt-pruning.html">Pruning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="opt-pruning-overview.html">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-pruning-perf.html">Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-pruning-algos.html">Pruning Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-pruning-api.html">API Overview</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="opt-joint-compression.html">Combining Compression Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="opt-conversion.html">Conversion</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization-neural-network.html">Compressing Neural Network Weights</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Other Converters</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="libsvm-conversion.html">LibSVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="sci-kit-learn-conversion.html">Scikit-learn</a></li>
<li class="toctree-l1"><a class="reference internal" href="xgboost-conversion.html">XGBoost</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">MLModel</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mlmodel.html">MLModel Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="multifunction-models.html">Multifunction Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="xcode-model-preview-types.html">Xcode Model Preview Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlmodel-utilities.html">MLModel Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="model-prediction.html">Model Prediction</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="updatable-model-examples.html">Updatable Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="updatable-neural-network-classifier-on-mnist-dataset.html">Neural Network Classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="updatable-tiny-drawing-classifier-pipeline-model.html">Pipeline Classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="updatable-nearest-neighbor-classifier.html">Nearest Neighbor Classifier</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/apple/coremltools" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/source/opt-resnet.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Optimizing ResNet50 Model</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-1-minimizing-model-size">Scenario 1 : Minimizing model size</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#palettization-using-data-free-compression">Palettization using data free compression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#palettization-using-fine-tuning">Palettization using fine tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-2-minimizing-latency">Scenario 2: Minimizing latency</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latency-reduction-with-pruning">Latency reduction with pruning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latency-reduction-with-activation-quantization">Latency reduction with activation quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Summary</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="optimizing-resnet50-model">
<h1>Optimizing ResNet50 Model<a class="headerlink" href="#optimizing-resnet50-model" title="Link to this heading">#</a></h1>
<p>In this article we will experiment with various ways to compress a convolutional neural network (CNN) for meeting different performance objectives
while staying within a specified accuracy loss budget.  In particular, we will consider two scenarios -
one with the goal to reduce the model size, the other with the goal to reduce runtime latency.</p>
<p>For this exercise, we will use the pretrained <a class="reference external" href="https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet50.html#torchvision.models.resnet50">ResNet50</a> model from torchvision.
Baseline ResNet50 model has a top 1% accuracy of <code class="docutils literal notranslate"><span class="pre">76.13%</span></code>, mlpackage size of <code class="docutils literal notranslate"><span class="pre">49MB</span></code> (<code class="docutils literal notranslate"><span class="pre">float16</span></code> precision) and latency of <code class="docutils literal notranslate"><span class="pre">~1.63ms</span></code><a class="footnote-reference brackets" href="#id2" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>.</p>
<section id="scenario-1-minimizing-model-size">
<h2>Scenario 1 : Minimizing model size<a class="headerlink" href="#scenario-1-minimizing-model-size" title="Link to this heading">#</a></h2>
<p>In this scenario, our goal would be to minimize the disk size of the model, while trying to retain as much accuracy as possible of the <code class="docutils literal notranslate"><span class="pre">float16</span></code> model (within 5%).</p>
<section id="palettization-using-data-free-compression">
<h3>Palettization using data free compression<a class="headerlink" href="#palettization-using-data-free-compression" title="Link to this heading">#</a></h3>
<p>Let’s start with the quickest workflow, which is to apply data free compression. We will take the model and apply palettization with different bit precisions and see how the accuracy behaves.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">coremltools.optimize.torch.palettization</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">PostTrainingPalettizer</span><span class="p">,</span> 
    <span class="n">PostTrainingPalettizerConfig</span>
<span class="p">)</span>

<span class="n">config_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;global_config&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;n_bits&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">}}</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">PostTrainingPalettizerConfig</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">config_dict</span><span class="p">)</span>
<span class="n">palettizer</span> <span class="o">=</span> <span class="n">PostTrainingPalettizer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

<span class="c1"># Compress model</span>
<span class="n">palettized_model</span> <span class="o">=</span> <span class="n">palettizer</span><span class="o">.</span><span class="n">compress</span><span class="p">()</span>
</pre></div>
</div>
<p>In the above code snippet, we apply data free compression directly to the PyTorch model.
However, you can also use <a class="reference external" href="https://apple.github.io/coremltools/source/coremltools.optimize.coreml.post_training_quantization.html#coremltools.optimize.coreml.palettize_weights">ct.optimize.coreml.palettize_weights</a> if working with a Core ML model.</p>
<p>We apply <code class="docutils literal notranslate"><span class="pre">n_bits</span></code> equal to 8, 6, 4 &amp; 3 bits to get accuracies of <code class="docutils literal notranslate"><span class="pre">76.09%</span></code>, <code class="docutils literal notranslate"><span class="pre">75.55%</span></code>, <code class="docutils literal notranslate"><span class="pre">66.81%</span></code> and <code class="docutils literal notranslate"><span class="pre">23.09%</span></code> respectively.
We see that there is marginal loss of accuracy with 8 and 6 bit palettized models, whereas there is a big drop with 4 bits, and the model becomes unusable for 3 bits.</p>
<p>Let’s try to regain the accuracy loss with 4 bit palletization by applying the <code class="docutils literal notranslate"><span class="pre">per_grouped_channel</span></code> palettization, which increases the number of LUTs (look up tables) per weight tensor.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">coremltools.optimize.torch.palettization</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">PostTrainingPalettizer</span><span class="p">,</span> 
    <span class="n">PostTrainingPalettizerConfig</span>
<span class="p">)</span>

<span class="n">config_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;global_config&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;n_bits&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
        <span class="s2">&quot;granularity&quot;</span><span class="p">:</span> <span class="s2">&quot;per_grouped_channel&quot;</span><span class="p">,</span>
        <span class="s2">&quot;group_size&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">}</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">PostTrainingPalettizerConfig</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">config_dict</span><span class="p">)</span>
<span class="n">palettizer</span> <span class="o">=</span> <span class="n">PostTrainingPalettizer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

<span class="c1"># Compress model</span>
<span class="n">palettized_model</span> <span class="o">=</span> <span class="n">palettizer</span><span class="o">.</span><span class="n">compress</span><span class="p">()</span>
</pre></div>
</div>
<p>We try <code class="docutils literal notranslate"><span class="pre">group_size</span></code> equal to 16, 8 &amp; 4 and see accuracy improve to <code class="docutils literal notranslate"><span class="pre">69.29%</span></code>, <code class="docutils literal notranslate"><span class="pre">72.26%</span></code> and <code class="docutils literal notranslate"><span class="pre">73.05%</span></code> respectively.</p>
<p>We summarize our results below:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Config</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>Model Size</p></th>
<th class="head"><p>Latency</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>6-bit (per tensor)</p></td>
<td><p>75.55%</p></td>
<td><p>18.6 MB</p></td>
<td><p>1.25ms</p></td>
</tr>
<tr class="row-odd"><td><p>4-bit (per-tensor)</p></td>
<td><p>66.81%</p></td>
<td><p>12.5 MB</p></td>
<td><p>1.12ms</p></td>
</tr>
<tr class="row-even"><td><p>4-bit (group_size=16)</p></td>
<td><p>69.29%</p></td>
<td><p>15.5 MB</p></td>
<td><p>1.38ms</p></td>
</tr>
<tr class="row-odd"><td><p>4 bit (group_size=8)</p></td>
<td><p>72.26%</p></td>
<td><p>12.6 MB</p></td>
<td><p>1.34ms</p></td>
</tr>
<tr class="row-even"><td><p>4 bit (group_size=4)</p></td>
<td><p>73.05%</p></td>
<td><p>12.7 MB</p></td>
<td><p>1.71ms</p></td>
</tr>
</tbody>
</table>
</div>
<p>Note that while higher granularity achieved with grouped channel palettization helps improve accuracy, we may lose runtime performance.<br />
For this model 4-bit palettization with group_size=8 configuration is a good sweet spot for good accuracy and runtime performance for minimal model size.</p>
</section>
<section id="palettization-using-fine-tuning">
<h3>Palettization using fine tuning<a class="headerlink" href="#palettization-using-fine-tuning" title="Link to this heading">#</a></h3>
<p>For this particular model, we do not see any benefits of using calibration data based compression w.r.to accuracy.
So we move on to training time compression workflow, where we will fine tune the model as we compress it. We can do so by using the <a class="reference internal" href="opt-palettization-algos.html#differentiable-k-means"><span class="std std-ref">DKM algorithm</span></a>, as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">coremltools.optimize.torch.palettization</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">DKMPalettizer</span><span class="p">,</span>
    <span class="n">DKMPalettizerConfig</span><span class="p">,</span>
    <span class="n">ModuleDKMPalettizerConfig</span>
<span class="p">)</span>

<span class="n">global_config</span> <span class="o">=</span> <span class="n">ModuleDKMPalettizerConfig</span><span class="p">(</span><span class="n">n_bits</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">DKMPalettizerConfig</span><span class="p">()</span><span class="o">.</span><span class="n">set_global</span><span class="p">(</span><span class="n">global_config</span><span class="p">)</span>

<span class="n">palettizer</span> <span class="o">=</span> <span class="n">DKMPalettizer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

<span class="n">palettizer</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">train_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
        <span class="n">palettizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">palettized_model</span> <span class="o">=</span> <span class="n">palettizer</span><span class="o">.</span><span class="n">finalize</span><span class="p">()</span>
</pre></div>
</div>
<p>With training time compression we can go up to 1-bit palettized model while still being within our accuracy budget.
2-bit palettized model has accuracy of <code class="docutils literal notranslate"><span class="pre">75.51%</span></code> and size of <code class="docutils literal notranslate"><span class="pre">6.3</span> <span class="pre">MB</span></code>, while the 1-bit palettized model has accuracy
of <code class="docutils literal notranslate"><span class="pre">71.22%</span></code> and size of <code class="docutils literal notranslate"><span class="pre">3.4</span> <span class="pre">MB</span></code>, giving over 15x model size reduction over baseline.</p>
</section>
<section id="summary">
<h3>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h3>
<p>Below we summarize results from above experiments as well as note the order of time taken to apply each of these compression workflows.
Post training algorithms are easier to set up and take less time, while training time techniques can provide better
accuracy to compression trade-off. Note also that while the goal here was to reduce the model size, compressing the model also helps reduce latency (results may vary based on model).</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Optimization API</p></th>
<th class="head"><p>Best config</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>Model Size</p></th>
<th class="head"><p>Latency</p></th>
<th class="head"><p>Time to compress</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Baseline</p></td>
<td><p>-</p></td>
<td><p>76.13%</p></td>
<td><p>48.8 MB</p></td>
<td><p>1.63ms</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p>PostTrainingPalettizer</p></td>
<td><p>4-bit (group_size=8)</p></td>
<td><p>72.26%</p></td>
<td><p>13.1 MB</p></td>
<td><p>1.34ms</p></td>
<td><p>O(minutes)</p></td>
</tr>
<tr class="row-even"><td><p>DKMPalettizer</p></td>
<td><p>1-bit (per-tensor)</p></td>
<td><p>71.22%</p></td>
<td><p>3.4 MB</p></td>
<td><p>1.14ms</p></td>
<td><p>O(hours) (300 epochs)</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="scenario-2-minimizing-latency">
<h2>Scenario 2: Minimizing latency<a class="headerlink" href="#scenario-2-minimizing-latency" title="Link to this heading">#</a></h2>
<p>Next, we will try to minimize latency of our model with less than 5% accuracy loss. Let’s say our latency target is &lt; 1ms.</p>
<section id="latency-reduction-with-pruning">
<h3>Latency reduction with pruning<a class="headerlink" href="#latency-reduction-with-pruning" title="Link to this heading">#</a></h3>
<p>We start out by pruning the model using data free compression, to see which sparsity configuration gives us the desired latency.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">coremltools.optimize.torch.pruning</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">MagnitudePruner</span><span class="p">,</span>
    <span class="n">MagnitudePrunerConfig</span><span class="p">,</span>
    <span class="n">ModuleMagnitudePrunerConfig</span>
<span class="p">)</span>

<span class="n">global_config</span> <span class="o">=</span> <span class="n">ModuleMagnitudePrunerConfig</span><span class="p">(</span><span class="n">initial_sparsity</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">target_sparsity</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">MagnitudePrunerConfig</span><span class="p">()</span><span class="o">.</span><span class="n">set_global</span><span class="p">(</span><span class="n">global_config</span><span class="p">)</span>

<span class="n">pruner</span> <span class="o">=</span> <span class="n">MagnitudePruner</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
<span class="n">pruner</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Skip training </span>

<span class="n">pruned_model</span> <span class="o">=</span> <span class="n">pruner</span><span class="o">.</span><span class="n">finalize</span><span class="p">()</span>
</pre></div>
</div>
<p>In the above code snippet, we apply data free compression directly to the PyTorch model.
However, you can also use <a class="reference external" href="https://apple.github.io/coremltools/source/coremltools.optimize.coreml.post_training_quantization.html#coremltools.optimize.coreml.prune_weights">ct.optimize.coreml.prune_weights</a> if working with a Core ML model.</p>
<p>Trying with 50%, 75% and 90% <code class="docutils literal notranslate"><span class="pre">target_sparsity</span></code> we get <code class="docutils literal notranslate"><span class="pre">1.23ms</span></code>, <code class="docutils literal notranslate"><span class="pre">1.05ms</span></code> and <code class="docutils literal notranslate"><span class="pre">0.86ms</span></code> latency respectively.</p>
<p>Since we meet our latency goals with <code class="docutils literal notranslate"><span class="pre">90%</span></code> sparsity, we will now fine-tune the pretrained PyTorch model to see if we can get good enough accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">coremltools.optimize.torch.pruning</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">MagnitudePruner</span><span class="p">,</span>
    <span class="n">MagnitudePrunerConfig</span><span class="p">,</span>
    <span class="n">ModuleMagnitudePrunerConfig</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">coremltools.optimize.torch.pruning.pruning_scheduler</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">PolynomialDecayScheduler</span>
<span class="p">)</span>

<span class="c1"># Setup scheduler for applying sparsity during fine-tuning</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">PolynomialDecayScheduler</span><span class="p">(</span><span class="n">update_steps</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">25000</span><span class="p">,</span> <span class="mi">62500</span><span class="p">,</span> <span class="mi">100</span><span class="p">)))</span>
<span class="n">global_config</span> <span class="o">=</span> <span class="n">ModuleMagnitudePrunerConfig</span><span class="p">(</span><span class="n">target_sparsity</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">scheduler</span><span class="o">=</span><span class="n">scheduler</span><span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">MagnitudePrunerConfig</span><span class="p">()</span><span class="o">.</span><span class="n">set_global</span><span class="p">(</span><span class="n">global_config</span><span class="p">)</span>

<span class="n">pruner</span> <span class="o">=</span> <span class="n">MagnitudePruner</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

<span class="n">pruner</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Train the model</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">train_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
        <span class="n">pruner</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">pruned_model</span> <span class="o">=</span> <span class="n">pruner</span><span class="o">.</span><span class="n">finalize</span><span class="p">()</span>
</pre></div>
</div>
<p>With fine-tuning, we observe an accuracy of <code class="docutils literal notranslate"><span class="pre">74.6%</span></code> for <code class="docutils literal notranslate"><span class="pre">90%</span></code> sparse ResNet50 model. This is quite good for this model, the results will vary based on the model.</p>
</section>
<section id="latency-reduction-with-activation-quantization">
<h3>Latency reduction with activation quantization<a class="headerlink" href="#latency-reduction-with-activation-quantization" title="Link to this heading">#</a></h3>
<p>In this section, we explore activation quantization on ResNet50 model, where we quantize both
model weights and activations to 8-bit (W8A8).
This can give latency gains by leveraging int8-int8 compute that is available on the Neural Engine (NE) on newer chips (A17 pro, M4).</p>
<p>You can apply activation quantization with calibration data using
<a class="reference external" href="https://apple.github.io/coremltools/source/coremltools.optimize.torch.quantization.html#coremltools.optimize.torch.quantization.LinearQuantizer"><code class="docutils literal notranslate"><span class="pre">LinearQuantizer</span></code></a>
API on the PyTorch model.
We use the calibration data to measure statistics of activations and weights without actually
simulating quantization during model’s forward pass, and without needing to perform a backward pass.
Learn more about this workflow in the <a class="reference internal" href="opt-quantization-api.html#id3"><span class="std std-ref">API Overview</span></a>
section.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">coremltools.optimize.torch.quantization</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">LinearQuantizer</span><span class="p">,</span>
    <span class="n">LinearQuantizerConfig</span><span class="p">,</span>
    <span class="n">ModuleLinearQuantizerConfig</span>
<span class="p">)</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">LinearQuantizerConfig</span><span class="p">(</span>
    <span class="n">global_config</span><span class="o">=</span><span class="n">ModuleLinearQuantizerConfig</span><span class="p">(</span>
        <span class="n">quantization_scheme</span><span class="o">=</span><span class="s2">&quot;symmetric&quot;</span><span class="p">,</span>
        <span class="n">milestones</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="n">quantizer</span> <span class="o">=</span> <span class="n">LinearQuantizer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

<span class="n">quantizer</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">example_inputs</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Only step through quantizer once to enable statistics collection (milestone 0),</span>
<span class="c1"># and turn batch norm to inference mode (milestone 3) </span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># Do a forward pass through the model with calibration data</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">finalize</span><span class="p">()</span>
</pre></div>
</div>
<p>With 128 calibration samples, this gives us an accuracy of <code class="docutils literal notranslate"><span class="pre">76.1%</span></code> and a latency of <code class="docutils literal notranslate"><span class="pre">1.07ms</span></code>.</p>
<p>Note that if you have a Core ML model, you can use the <a class="reference internal" href="opt-quantization-api.html#quantizing-weights-and-activations"><span class="std std-ref"><code class="docutils literal notranslate"><span class="pre">linear_quantize_activations</span></code></span></a>
method to quantize the activations.</p>
</section>
<section id="id3">
<h3>Summary<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>Below we summarize results from above experiments as well as note the order of time taken to apply each of these compression workflows.
With calibration data based activation quantization workflow we are able to achieve good speedup for minimal loss in accuracy, and it is much quicker to set up and apply.
On the other hand, by pruning the model with fine-tuning we are able to get even better speedup and a much smaller model for a slightly higher loss in accuracy, but with a more extensive set up.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Optimization API</p></th>
<th class="head"><p>Best config</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>Model Size</p></th>
<th class="head"><p>Latency</p></th>
<th class="head"><p>Time to compress</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Baseline</p></td>
<td><p>-</p></td>
<td><p>76.13%</p></td>
<td><p>48.8 MB</p></td>
<td><p>1.63ms</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p>MagnitudePruner</p></td>
<td><p>90% sparsity</p></td>
<td><p>74.60%</p></td>
<td><p>8.6 MB</p></td>
<td><p>0.86ms</p></td>
<td><p>O(hours) (200 epochs)</p></td>
</tr>
<tr class="row-even"><td><p>LinearQuantizer</p></td>
<td><p>W8A8 symmetric per-channel</p></td>
<td><p>76.09%</p></td>
<td><p>25.8 MB</p></td>
<td><p>1.07ms</p></td>
<td><p>O(minutes)</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
</section>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id2" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>All latency numbers reported in this article have been measured on iPhone 15 Pro with iOS18 seed build. The latency numbers are sensitive to the device state, and may vary depending on the device state and build versions.</p>
</aside>
</aside>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="opt-overview-examples.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Examples</p>
      </div>
    </a>
    <a class="right-next"
       href="opt-opt1_3.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Optimizing OPT Model</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-1-minimizing-model-size">Scenario 1 : Minimizing model size</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#palettization-using-data-free-compression">Palettization using data free compression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#palettization-using-fine-tuning">Palettization using fine tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-2-minimizing-latency">Scenario 2: Minimizing latency</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latency-reduction-with-pruning">Latency reduction with pruning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latency-reduction-with-activation-quantization">Latency reduction with activation quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Summary</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Apple
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023, Apple Inc.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>