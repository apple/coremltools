<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Converting a TensorFlow 1 DeepSpeech Model &mdash; Core ML Tools Guide  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/imgstyle.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
        <script src="../../_static/clipboard.min.js"></script>
        <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="TensorFlow 2 Workflow" href="tensorflow-2.html" />
    <link rel="prev" title="Converting a TensorFlow 1 Image Classifier" href="convert-a-tensorflow-1-image-classifier.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> Core ML Tools Guide
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../overview/overview-coremltools.html">What Is Core ML Tools?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../overview/new-features.html">New Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../overview/faqs.html">Core ML Tools FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../overview/coremltools-examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../overview/migration.html">Migration Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../overview/contributing.html">Contributing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Essentials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../essentials/installing-coremltools.html">Installing Core ML Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../essentials/introductory-quickstart.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../essentials/unified-conversion-api.html">Core ML Tools API Overview</a></li>
<li class="toctree-l1"><a class="reference external" href="https://apple.github.io/coremltools/mlmodel/index.html">Core ML Model Format</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/apple/coremltools">API GitHub</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Unified Conversion</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../convert-learning-models/convert-learning-models.html">Converting Deep Learning Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ml-programs/ml-programs.html">ML Programs</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="convert-tensorflow.html">Converting from TensorFlow</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="tensorflow-1-workflow.html">TensorFlow 1 Workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-a-tensorflow-1-image-classifier.html">Converting a TensorFlow 1 Image Classifier</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Converting a TensorFlow 1 DeepSpeech Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#set-up-the-model">Set Up the Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#convert-the-model-and-preprocess-an-audio-file">Convert the Model and Preprocess an Audio File</a></li>
<li class="toctree-l3"><a class="reference internal" href="#feed-the-input-into-the-model">Feed the Input Into the Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#use-a-dynamic-tensorflow-model">Use a Dynamic TensorFlow Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#convert-a-dynamic-model-to-a-static-one">Convert a Dynamic Model to a Static One</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="tensorflow-2.html">TensorFlow 2 Workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-tensorflow-2-bert-transformer-models.html">Converting TensorFlow 2 BERT Transformer Models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../convert-pytorch/convert-pytorch.html">Converting from PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../conversion-options/conversion-options.html">Conversion Options</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model-intermediate-language.html">Model Intermediate Language</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../optimization/optimizing-models/optimizing-models.html">Optimizing Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../optimization/api-overview/api-overview.html">Optimize API Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../optimization/pruning/pruning.html">Pruning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../optimization/palettization/palettization.html">Palettization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../optimization/quantization-aware-training/quantization-aware-training.html">Linear 8-Bit Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../optimization/quantization-neural-network.html">Compressing Neural Network Weights</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Converters</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../other-converters/libsvm-conversion.html">LibSVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../other-converters/sci-kit-learn-conversion.html">Scikit-learn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../other-converters/xgboost-conversion.html">XGBoost</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MLModel</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../mlmodel/mlmodel.html">MLModel Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mlmodel/xcode-model-preview-types.html">Xcode Model Preview Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mlmodel/mlmodel-utilities.html">MLModel Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mlmodel/model-prediction.html">Model Prediction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mlmodel/updatable-model-examples/updatable-model-examples.html">Updatable Models</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Core ML Tools Guide</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="convert-tensorflow.html">Converting from TensorFlow</a> &raquo;</li>
      <li>Converting a TensorFlow 1 DeepSpeech Model</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/unified/convert-tensorflow/convert-a-tensorflow-1-deepspeech-model.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="converting-a-tensorflow-1-deepspeech-model">
<h1>Converting a TensorFlow 1 DeepSpeech Model<a class="headerlink" href="#converting-a-tensorflow-1-deepspeech-model" title="Permalink to this heading"></a></h1>
<p>The following example explores the automatic handling of flexible shapes and other related capabilities of the Core ML Tools converter. It uses an <a class="reference external" href="https://en.wikipedia.org/wiki/Speech_recognition#End-to-end_automatic_speech_recognition">automatic speech recognition</a> (ASR) task in which the input is a speech audio file and the output is the text transcription of it.</p>
<p>The ASR system for this example consists of three stages: preprocessing, post-processing, and a neural network model between them that does most of the heavy lifting. The preprocessing and post-processing stages employ standard techniques which can be easily implemented. The focus of this example is on converting the neural network model.</p>
<div class="admonition-how-asr-works admonition">
<p class="admonition-title">How ASR Works</p>
<p>Preprocessing involves extracting the <a class="reference external" href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum">Mel-frequency cepstral coefficients</a> (MFCCs) from the raw audio file. The MFCCs are fed into the neural network model, which returns a character-level time series of probability distributions. Those are then postprocessed by a CTC decoder to produce the final transcription.</p>
<p>The example uses a pre-trained TensorFlow model called <a class="reference external" href="https://github.com/mozilla/DeepSpeech/blob/master/DeepSpeech.py">DeepSpeech</a> that uses <a class="reference external" href="https://en.wikipedia.org/wiki/Long_short-term_memory">long short-term memory</a> (LSTM) and a few dense layers stacked on top of each other — an architecture common for <code class="docutils literal notranslate"><span class="pre">seq2seq</span></code> models.</p>
</div>
<section id="set-up-the-model">
<h2>Set Up the Model<a class="headerlink" href="#set-up-the-model" title="Permalink to this heading"></a></h2>
<p>To run this example on your system, follow these steps:</p>
<ol class="arabic">
<li><p>Download the following assets:</p>
<ul class="simple">
<li><p>Processing and inspection utilities (<a class="reference external" href="https://docs-assets.developer.apple.com/coremltools/deepspeech/demo_utils.py">demo_utils.py</a>)</p></li>
<li><p>Sample audio file (<a class="reference external" href="https://docs-assets.developer.apple.com/coremltools/deepspeech/audio_sample_16bit_mono_16khz.wav">audio_sample_16bit_mono_16khz.wav</a>)</p></li>
<li><p>Alphabet configuration file (<a class="reference external" href="https://github.com/mozilla/DeepSpeech/blob/master/data/alphabet.txt">alphabet.txt</a>)</p></li>
<li><p>Language model scorer (<a class="reference external" href="https://github.com/mozilla/DeepSpeech/blob/master/data/lm/kenlm.scorer">kenlm.scorer</a>)</p></li>
<li><p>Pre-trained weights (<a class="reference external" href="https://github.com/mozilla/DeepSpeech/releases/download/v0.7.1/deepspeech-0.7.1-checkpoint.tar.gz">deepspeech-0.7.1-checkpoint</a>)</p></li>
<li><p>Script to export TensorFlow 1 model (<a class="reference external" href="https://github.com/mozilla/DeepSpeech/blob/master/DeepSpeech.py">DeepSpeech.py</a>)</p></li>
</ul>
</li>
<li><p>Install the <code class="docutils literal notranslate"><span class="pre">deepspeech</span></code> package using <code class="docutils literal notranslate"><span class="pre">pip</span></code>:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>deepspeech
</pre></div>
</div>
</li>
<li><p>Run the following script downloaded from the <a class="reference external" href="https://github.com/mozilla/DeepSpeech">DeepSpeech repository</a> to export the TensorFlow 1 model:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>DeepSpeech.py<span class="w"> </span>--export_dir<span class="w"> </span>/tmp<span class="w"> </span>--checkpoint_dir<span class="w"> </span>./deepspeech-0.7.1-checkpoint<span class="w"> </span>--alphabet_config_path<span class="o">=</span>alphabet.txt<span class="w"> </span>--scorer_path<span class="o">=</span>kenlm.scorer<span class="w"> </span>&gt;/dev/null<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span>
</pre></div>
</div>
</li>
<li><p>After the model is exported, inspect the outputs of the TensorFlow graph:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tf_model</span> <span class="o">=</span> <span class="s2">&quot;/tmp/output_graph.pb&quot;</span>

<span class="kn">from</span> <span class="nn">demo_utils</span> <span class="kn">import</span> <span class="n">inspect_tf_outputs</span>

<span class="n">inspect_tf_outputs</span><span class="p">(</span><span class="n">tf_model</span><span class="p">)</span>
</pre></div>
</div>
<p>The TensorFlow graph outputs are <code class="docutils literal notranslate"><span class="pre">'mfccs'</span></code>, <code class="docutils literal notranslate"><span class="pre">'logits'</span></code>, <code class="docutils literal notranslate"><span class="pre">'new_state_c'</span></code>, and <code class="docutils literal notranslate"><span class="pre">'new_state_h'</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">'mfccs'</span></code> output represents the output of the preprocessing stage. This means that the exported TensorFlow graph contains not just the DeepSpeech model, but also the preprocessing subgraph.</p>
</li>
<li><p>Strip off this preprocessing component by providing the remaining three output names to the unified converter function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">,</span> <span class="s2">&quot;new_state_c&quot;</span><span class="p">,</span> <span class="s2">&quot;new_state_h&quot;</span><span class="p">]</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="convert-the-model-and-preprocess-an-audio-file">
<h2>Convert the Model and Preprocess an Audio File<a class="headerlink" href="#convert-the-model-and-preprocess-an-audio-file" title="Permalink to this heading"></a></h2>
<p>Preprocessing and post-processing functions have already been constructed using code in the <a class="reference external" href="https://github.com/mozilla/DeepSpeech">DeepSpeech repository</a>. To convert the model and preprocess an audio file, follow these steps:</p>
<ol class="arabic">
<li><p>Convert the model to a Core ML neural network model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">coremltools</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">mlmodel</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">tf_model</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>After the model is converted, load and preprocess an audio file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">audiofile</span> <span class="o">=</span> <span class="s2">&quot;./audio_sample_16bit_mono_16khz.wav&quot;</span>

<span class="kn">from</span> <span class="nn">demo_utils</span> <span class="kn">import</span> <span class="n">preprocessing</span><span class="p">,</span> <span class="n">postprocessing</span>

<span class="n">mfccs</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="p">(</span><span class="n">audiofile</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">mfccs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
<p>Preprocessing transforms the audio file into a tensor object of shape <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">636,</span> <span class="pre">19,</span> <span class="pre">26)</span></code>. The shape of the tensor can be viewed as one audio file, preprocessed into 636 sequences, each of width 19, and containing 26 coefficients. The number of sequences changes with the length of the audio. In this 12-second audio file there are 636 sequences.</p>
</section>
<section id="feed-the-input-into-the-model">
<h2>Feed the Input Into the Model<a class="headerlink" href="#feed-the-input-into-the-model" title="Permalink to this heading"></a></h2>
<p>Inspect the input shapes that the Core ML model expects:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">demo_utils</span> <span class="kn">import</span> <span class="n">inspect_inputs</span>

<span class="n">inspect_inputs</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">,</span> <span class="n">tf_model</span><span class="p">)</span>
</pre></div>
</div>
<p>The model input with the name <code class="docutils literal notranslate"><span class="pre">input_node</span></code> has the shape <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">16,</span> <span class="pre">19,</span> <span class="pre">26)</span></code> which matches the shape of the preprocessed tensor in all the dimensions except for the sequence dimension. Since the converted Core ML model can process only 16 sequences at a time, create a loop to break the input features into chunks and feed each segment into the model one-by-one:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">start</span> <span class="o">=</span> <span class="mi">0</span> 
<span class="n">step</span> <span class="o">=</span> <span class="mi">16</span>

<span class="n">max_time_steps</span> <span class="o">=</span> <span class="n">mfccs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="n">logits_sequence</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">input_dict</span> <span class="o">=</span> <span class="p">{}</span>

<span class="n">input_dict</span><span class="p">[</span><span class="s2">&quot;input_lengths&quot;</span><span class="p">]</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">step</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">input_dict</span><span class="p">[</span><span class="s2">&quot;previous_state_c&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2048</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="c1"># Initializing cell state </span>
<span class="n">input_dict</span><span class="p">[</span><span class="s2">&quot;previous_state_h&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2048</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="c1"># Initializing hidden state </span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Transcription: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">while</span> <span class="p">(</span><span class="n">start</span> <span class="o">+</span> <span class="n">step</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">max_time_steps</span><span class="p">:</span>
    <span class="n">input_dict</span><span class="p">[</span><span class="s2">&quot;input_node&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mfccs</span><span class="p">[:,</span> <span class="n">start</span><span class="p">:(</span><span class="n">start</span> <span class="o">+</span> <span class="n">step</span><span class="p">),</span> <span class="p">:,</span> <span class="p">:]</span>
    
    <span class="c1"># Evaluation</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">mlmodel</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_dict</span><span class="p">)</span>
    
    
    <span class="n">start</span> <span class="o">+=</span> <span class="n">step</span>
    <span class="n">logits_sequence</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">])</span>

    
    <span class="c1"># Updating states</span>
    <span class="n">input_dict</span><span class="p">[</span><span class="s2">&quot;previous_state_c&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="s2">&quot;new_state_c&quot;</span><span class="p">]</span>
    <span class="n">input_dict</span><span class="p">[</span><span class="s2">&quot;previous_state_h&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="s2">&quot;new_state_h&quot;</span><span class="p">]</span>
    
    
    <span class="c1"># Decoding</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">logits_sequence</span><span class="p">)</span>
    <span class="n">transcription</span> <span class="o">=</span> <span class="n">postprocessing</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">transcription</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\r</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>The above code breaks the preprocessed feature into size-16 slices, and runs a prediction on each slice, along with state management, inside a loop. After running the above code, the transcription matches the contents of the audio file.</p>
</section>
<section id="use-a-dynamic-tensorflow-model">
<h2>Use a Dynamic TensorFlow Model<a class="headerlink" href="#use-a-dynamic-tensorflow-model" title="Permalink to this heading"></a></h2>
<p>It is also possible to run the prediction on the entire preprocessed feature in just one go using a dynamic TensorFlow model. Follow these steps:</p>
<ol class="arabic">
<li><p>Rerun the same script from the DeepSpeech repository to obtain a dynamic graph. Provide an additional flag <code class="docutils literal notranslate"><span class="pre">n_steps</span></code> which corresponds to the sequence length and has a default value of 16. Setting it to -1 means that the sequence length can take any positive value:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>!python<span class="w"> </span>DeepSpeech.py<span class="w"> </span>--n_steps<span class="w"> </span>-1<span class="w"> </span>--export_dir<span class="w"> </span>/tmp<span class="w"> </span>--checkpoint_dir<span class="w"> </span>./deepspeech-0.7.1-checkpoint<span class="w"> </span>--alphabet_config_path<span class="o">=</span>alphabet.txt<span class="w"> </span>--scorer_path<span class="o">=</span>kenlm.scorer<span class="w"> </span>&gt;/dev/null<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span>
</pre></div>
</div>
</li>
<li><p>Convert the newly exported dynamic TensorFlow model to a Core ML neural network model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mlmodel</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">tf_model</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>After the model is converted, inspect how this new model is different from the previous static one:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inspect_inputs</span><span class="p">(</span><span class="n">mlmodel</span><span class="p">,</span><span class="n">tf_model</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
<p>The shape of input <code class="docutils literal notranslate"><span class="pre">input_node</span></code> is now <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">None,</span> <span class="pre">19,</span> <span class="pre">26)</span></code>, which mean that this CoreML model can work on inputs of arbitrary-sequence length.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The dynamic Core ML model offers dynamic operations, such as “get shape” and “dynamic reshape”, which are not available in the previous static model. The Core ML Tools converter offers the same simplicity with dynamic models as it does with static models.</p>
</div>
<ol class="arabic simple" start="4">
<li><p>Validate the transcription accuracy on the same audio file:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">input_dict</span> <span class="o">=</span> <span class="p">{}</span>

<span class="n">input_dict</span><span class="p">[</span><span class="s2">&quot;input_node&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mfccs</span>

<span class="n">input_dict</span><span class="p">[</span><span class="s2">&quot;input_lengths&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">mfccs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">input_dict</span><span class="p">[</span><span class="s2">&quot;previous_state_c&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2048</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="c1"># Initializing cell state </span>

<span class="n">input_dict</span><span class="p">[</span><span class="s2">&quot;previous_state_h&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2048</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="c1"># Initializing hidden state</span>
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p>With the dynamic model you don’t need to create a loop. You can feed the entire input feature directly into the model:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">mlmodel</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_dict</span><span class="p">)[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span>

<span class="n">transcription</span> <span class="o">=</span> <span class="n">postprocessing</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">transcription</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<p>The result is the same transcription with the dynamic Core ML model as with the static model.</p>
</section>
<section id="convert-a-dynamic-model-to-a-static-one">
<h2>Convert a Dynamic Model to a Static One<a class="headerlink" href="#convert-a-dynamic-model-to-a-static-one" title="Permalink to this heading"></a></h2>
<p>So far you worked with two variants of the DeepSpeech model:</p>
<ul class="simple">
<li><p>Static TF graph: The converter produced a Core ML neural network model with inputs of fixed shape.</p></li>
<li><p>Dynamic model: The converter produced a Core ML neural network model that can accept inputs of any sequence length.</p></li>
</ul>
<p>The converter handles both cases transparently without needing to make a change to the conversion call.</p>
<p>It is also possible with the Core ML Tools converter to start with a dynamic TF graph and obtain a static Core ML model. Provide the type description object containing the name and shape of the input to the conversion API:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;input_node&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="mi">19</span><span class="p">,</span><span class="mi">26</span><span class="p">))</span>

<span class="n">mlmodel</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">tf_model</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="nb">input</span><span class="p">])</span>
</pre></div>
</div>
<p>Under the hood, the type and value inference propagates this shape information to remove all the unnecessary dynamic operations.</p>
<p>Static models are likely to be more performant while the dynamic ones are more flexible.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="convert-a-tensorflow-1-image-classifier.html" class="btn btn-neutral float-left" title="Converting a TensorFlow 1 Image Classifier" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="tensorflow-2.html" class="btn btn-neutral float-right" title="TensorFlow 2 Workflow" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Apple Inc.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>