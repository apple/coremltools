// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: NeuralNetwork.proto

#ifndef GOOGLE_PROTOBUF_INCLUDED_NeuralNetwork_2eproto
#define GOOGLE_PROTOBUF_INCLUDED_NeuralNetwork_2eproto

#include <limits>
#include <string>

#include <google/protobuf/port_def.inc>
#if PROTOBUF_VERSION < 3019000
#error This file was generated by a newer version of protoc which is
#error incompatible with your Protocol Buffer headers. Please update
#error your headers.
#endif
#if 3019000 < PROTOBUF_MIN_PROTOC_VERSION
#error This file was generated by an older version of protoc which is
#error incompatible with your Protocol Buffer headers. Please
#error regenerate this file with a newer version of protoc.
#endif

#include <google/protobuf/port_undef.inc>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/arena.h>
#include <google/protobuf/arenastring.h>
#include <google/protobuf/generated_message_table_driven.h>
#include <google/protobuf/generated_message_util.h>
#include <google/protobuf/metadata_lite.h>
#include <google/protobuf/message_lite.h>
#include <google/protobuf/repeated_field.h>  // IWYU pragma: export
#include <google/protobuf/extension_set.h>  // IWYU pragma: export
#include <google/protobuf/map.h>  // IWYU pragma: export
#include <google/protobuf/map_entry_lite.h>
#include <google/protobuf/map_field_lite.h>
#include <google/protobuf/generated_enum_util.h>
#include "DataStructures.pb.h"
#include "Parameters.pb.h"
// @@protoc_insertion_point(includes)
#include <google/protobuf/port_def.inc>
#define PROTOBUF_INTERNAL_EXPORT_NeuralNetwork_2eproto
PROTOBUF_NAMESPACE_OPEN
namespace internal {
class AnyMetadata;
}  // namespace internal
PROTOBUF_NAMESPACE_CLOSE

// Internal implementation detail -- do not use these members.
struct TableStruct_NeuralNetwork_2eproto {
  static const ::PROTOBUF_NAMESPACE_ID::internal::ParseTableField entries[]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::AuxiliaryParseTableField aux[]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::ParseTable schema[204]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::FieldMetadata field_metadata[];
  static const ::PROTOBUF_NAMESPACE_ID::internal::SerializationTable serialization_table[];
  static const uint32_t offsets[];
};
namespace CoreML {
namespace Specification {
class AcosLayerParams;
struct AcosLayerParamsDefaultTypeInternal;
extern AcosLayerParamsDefaultTypeInternal _AcosLayerParams_default_instance_;
class AcoshLayerParams;
struct AcoshLayerParamsDefaultTypeInternal;
extern AcoshLayerParamsDefaultTypeInternal _AcoshLayerParams_default_instance_;
class ActivationELU;
struct ActivationELUDefaultTypeInternal;
extern ActivationELUDefaultTypeInternal _ActivationELU_default_instance_;
class ActivationLeakyReLU;
struct ActivationLeakyReLUDefaultTypeInternal;
extern ActivationLeakyReLUDefaultTypeInternal _ActivationLeakyReLU_default_instance_;
class ActivationLinear;
struct ActivationLinearDefaultTypeInternal;
extern ActivationLinearDefaultTypeInternal _ActivationLinear_default_instance_;
class ActivationPReLU;
struct ActivationPReLUDefaultTypeInternal;
extern ActivationPReLUDefaultTypeInternal _ActivationPReLU_default_instance_;
class ActivationParametricSoftplus;
struct ActivationParametricSoftplusDefaultTypeInternal;
extern ActivationParametricSoftplusDefaultTypeInternal _ActivationParametricSoftplus_default_instance_;
class ActivationParams;
struct ActivationParamsDefaultTypeInternal;
extern ActivationParamsDefaultTypeInternal _ActivationParams_default_instance_;
class ActivationReLU;
struct ActivationReLUDefaultTypeInternal;
extern ActivationReLUDefaultTypeInternal _ActivationReLU_default_instance_;
class ActivationScaledTanh;
struct ActivationScaledTanhDefaultTypeInternal;
extern ActivationScaledTanhDefaultTypeInternal _ActivationScaledTanh_default_instance_;
class ActivationSigmoid;
struct ActivationSigmoidDefaultTypeInternal;
extern ActivationSigmoidDefaultTypeInternal _ActivationSigmoid_default_instance_;
class ActivationSigmoidHard;
struct ActivationSigmoidHardDefaultTypeInternal;
extern ActivationSigmoidHardDefaultTypeInternal _ActivationSigmoidHard_default_instance_;
class ActivationSoftplus;
struct ActivationSoftplusDefaultTypeInternal;
extern ActivationSoftplusDefaultTypeInternal _ActivationSoftplus_default_instance_;
class ActivationSoftsign;
struct ActivationSoftsignDefaultTypeInternal;
extern ActivationSoftsignDefaultTypeInternal _ActivationSoftsign_default_instance_;
class ActivationTanh;
struct ActivationTanhDefaultTypeInternal;
extern ActivationTanhDefaultTypeInternal _ActivationTanh_default_instance_;
class ActivationThresholdedReLU;
struct ActivationThresholdedReLUDefaultTypeInternal;
extern ActivationThresholdedReLUDefaultTypeInternal _ActivationThresholdedReLU_default_instance_;
class AdamOptimizer;
struct AdamOptimizerDefaultTypeInternal;
extern AdamOptimizerDefaultTypeInternal _AdamOptimizer_default_instance_;
class AddBroadcastableLayerParams;
struct AddBroadcastableLayerParamsDefaultTypeInternal;
extern AddBroadcastableLayerParamsDefaultTypeInternal _AddBroadcastableLayerParams_default_instance_;
class AddLayerParams;
struct AddLayerParamsDefaultTypeInternal;
extern AddLayerParamsDefaultTypeInternal _AddLayerParams_default_instance_;
class ArgMaxLayerParams;
struct ArgMaxLayerParamsDefaultTypeInternal;
extern ArgMaxLayerParamsDefaultTypeInternal _ArgMaxLayerParams_default_instance_;
class ArgMinLayerParams;
struct ArgMinLayerParamsDefaultTypeInternal;
extern ArgMinLayerParamsDefaultTypeInternal _ArgMinLayerParams_default_instance_;
class ArgSortLayerParams;
struct ArgSortLayerParamsDefaultTypeInternal;
extern ArgSortLayerParamsDefaultTypeInternal _ArgSortLayerParams_default_instance_;
class AsinLayerParams;
struct AsinLayerParamsDefaultTypeInternal;
extern AsinLayerParamsDefaultTypeInternal _AsinLayerParams_default_instance_;
class AsinhLayerParams;
struct AsinhLayerParamsDefaultTypeInternal;
extern AsinhLayerParamsDefaultTypeInternal _AsinhLayerParams_default_instance_;
class AtanLayerParams;
struct AtanLayerParamsDefaultTypeInternal;
extern AtanLayerParamsDefaultTypeInternal _AtanLayerParams_default_instance_;
class AtanhLayerParams;
struct AtanhLayerParamsDefaultTypeInternal;
extern AtanhLayerParamsDefaultTypeInternal _AtanhLayerParams_default_instance_;
class AverageLayerParams;
struct AverageLayerParamsDefaultTypeInternal;
extern AverageLayerParamsDefaultTypeInternal _AverageLayerParams_default_instance_;
class BatchedMatMulLayerParams;
struct BatchedMatMulLayerParamsDefaultTypeInternal;
extern BatchedMatMulLayerParamsDefaultTypeInternal _BatchedMatMulLayerParams_default_instance_;
class BatchnormLayerParams;
struct BatchnormLayerParamsDefaultTypeInternal;
extern BatchnormLayerParamsDefaultTypeInternal _BatchnormLayerParams_default_instance_;
class BiDirectionalLSTMLayerParams;
struct BiDirectionalLSTMLayerParamsDefaultTypeInternal;
extern BiDirectionalLSTMLayerParamsDefaultTypeInternal _BiDirectionalLSTMLayerParams_default_instance_;
class BiasLayerParams;
struct BiasLayerParamsDefaultTypeInternal;
extern BiasLayerParamsDefaultTypeInternal _BiasLayerParams_default_instance_;
class BorderAmounts;
struct BorderAmountsDefaultTypeInternal;
extern BorderAmountsDefaultTypeInternal _BorderAmounts_default_instance_;
class BorderAmounts_EdgeSizes;
struct BorderAmounts_EdgeSizesDefaultTypeInternal;
extern BorderAmounts_EdgeSizesDefaultTypeInternal _BorderAmounts_EdgeSizes_default_instance_;
class BoxCoordinatesMode;
struct BoxCoordinatesModeDefaultTypeInternal;
extern BoxCoordinatesModeDefaultTypeInternal _BoxCoordinatesMode_default_instance_;
class BranchLayerParams;
struct BranchLayerParamsDefaultTypeInternal;
extern BranchLayerParamsDefaultTypeInternal _BranchLayerParams_default_instance_;
class BroadcastToDynamicLayerParams;
struct BroadcastToDynamicLayerParamsDefaultTypeInternal;
extern BroadcastToDynamicLayerParamsDefaultTypeInternal _BroadcastToDynamicLayerParams_default_instance_;
class BroadcastToLikeLayerParams;
struct BroadcastToLikeLayerParamsDefaultTypeInternal;
extern BroadcastToLikeLayerParamsDefaultTypeInternal _BroadcastToLikeLayerParams_default_instance_;
class BroadcastToStaticLayerParams;
struct BroadcastToStaticLayerParamsDefaultTypeInternal;
extern BroadcastToStaticLayerParamsDefaultTypeInternal _BroadcastToStaticLayerParams_default_instance_;
class CategoricalCrossEntropyLossLayer;
struct CategoricalCrossEntropyLossLayerDefaultTypeInternal;
extern CategoricalCrossEntropyLossLayerDefaultTypeInternal _CategoricalCrossEntropyLossLayer_default_instance_;
class CategoricalDistributionLayerParams;
struct CategoricalDistributionLayerParamsDefaultTypeInternal;
extern CategoricalDistributionLayerParamsDefaultTypeInternal _CategoricalDistributionLayerParams_default_instance_;
class CeilLayerParams;
struct CeilLayerParamsDefaultTypeInternal;
extern CeilLayerParamsDefaultTypeInternal _CeilLayerParams_default_instance_;
class ClampedReLULayerParams;
struct ClampedReLULayerParamsDefaultTypeInternal;
extern ClampedReLULayerParamsDefaultTypeInternal _ClampedReLULayerParams_default_instance_;
class ClipLayerParams;
struct ClipLayerParamsDefaultTypeInternal;
extern ClipLayerParamsDefaultTypeInternal _ClipLayerParams_default_instance_;
class ConcatLayerParams;
struct ConcatLayerParamsDefaultTypeInternal;
extern ConcatLayerParamsDefaultTypeInternal _ConcatLayerParams_default_instance_;
class ConcatNDLayerParams;
struct ConcatNDLayerParamsDefaultTypeInternal;
extern ConcatNDLayerParamsDefaultTypeInternal _ConcatNDLayerParams_default_instance_;
class ConstantPaddingLayerParams;
struct ConstantPaddingLayerParamsDefaultTypeInternal;
extern ConstantPaddingLayerParamsDefaultTypeInternal _ConstantPaddingLayerParams_default_instance_;
class Convolution3DLayerParams;
struct Convolution3DLayerParamsDefaultTypeInternal;
extern Convolution3DLayerParamsDefaultTypeInternal _Convolution3DLayerParams_default_instance_;
class ConvolutionLayerParams;
struct ConvolutionLayerParamsDefaultTypeInternal;
extern ConvolutionLayerParamsDefaultTypeInternal _ConvolutionLayerParams_default_instance_;
class CopyLayerParams;
struct CopyLayerParamsDefaultTypeInternal;
extern CopyLayerParamsDefaultTypeInternal _CopyLayerParams_default_instance_;
class CosLayerParams;
struct CosLayerParamsDefaultTypeInternal;
extern CosLayerParamsDefaultTypeInternal _CosLayerParams_default_instance_;
class CoshLayerParams;
struct CoshLayerParamsDefaultTypeInternal;
extern CoshLayerParamsDefaultTypeInternal _CoshLayerParams_default_instance_;
class CropLayerParams;
struct CropLayerParamsDefaultTypeInternal;
extern CropLayerParamsDefaultTypeInternal _CropLayerParams_default_instance_;
class CropResizeLayerParams;
struct CropResizeLayerParamsDefaultTypeInternal;
extern CropResizeLayerParamsDefaultTypeInternal _CropResizeLayerParams_default_instance_;
class CumSumLayerParams;
struct CumSumLayerParamsDefaultTypeInternal;
extern CumSumLayerParamsDefaultTypeInternal _CumSumLayerParams_default_instance_;
class CustomLayerParams;
struct CustomLayerParamsDefaultTypeInternal;
extern CustomLayerParamsDefaultTypeInternal _CustomLayerParams_default_instance_;
class CustomLayerParams_CustomLayerParamValue;
struct CustomLayerParams_CustomLayerParamValueDefaultTypeInternal;
extern CustomLayerParams_CustomLayerParamValueDefaultTypeInternal _CustomLayerParams_CustomLayerParamValue_default_instance_;
class CustomLayerParams_ParametersEntry_DoNotUse;
struct CustomLayerParams_ParametersEntry_DoNotUseDefaultTypeInternal;
extern CustomLayerParams_ParametersEntry_DoNotUseDefaultTypeInternal _CustomLayerParams_ParametersEntry_DoNotUse_default_instance_;
class DivideBroadcastableLayerParams;
struct DivideBroadcastableLayerParamsDefaultTypeInternal;
extern DivideBroadcastableLayerParamsDefaultTypeInternal _DivideBroadcastableLayerParams_default_instance_;
class DotProductLayerParams;
struct DotProductLayerParamsDefaultTypeInternal;
extern DotProductLayerParamsDefaultTypeInternal _DotProductLayerParams_default_instance_;
class EmbeddingLayerParams;
struct EmbeddingLayerParamsDefaultTypeInternal;
extern EmbeddingLayerParamsDefaultTypeInternal _EmbeddingLayerParams_default_instance_;
class EmbeddingNDLayerParams;
struct EmbeddingNDLayerParamsDefaultTypeInternal;
extern EmbeddingNDLayerParamsDefaultTypeInternal _EmbeddingNDLayerParams_default_instance_;
class EqualLayerParams;
struct EqualLayerParamsDefaultTypeInternal;
extern EqualLayerParamsDefaultTypeInternal _EqualLayerParams_default_instance_;
class ErfLayerParams;
struct ErfLayerParamsDefaultTypeInternal;
extern ErfLayerParamsDefaultTypeInternal _ErfLayerParams_default_instance_;
class Exp2LayerParams;
struct Exp2LayerParamsDefaultTypeInternal;
extern Exp2LayerParamsDefaultTypeInternal _Exp2LayerParams_default_instance_;
class ExpandDimsLayerParams;
struct ExpandDimsLayerParamsDefaultTypeInternal;
extern ExpandDimsLayerParamsDefaultTypeInternal _ExpandDimsLayerParams_default_instance_;
class FillDynamicLayerParams;
struct FillDynamicLayerParamsDefaultTypeInternal;
extern FillDynamicLayerParamsDefaultTypeInternal _FillDynamicLayerParams_default_instance_;
class FillLikeLayerParams;
struct FillLikeLayerParamsDefaultTypeInternal;
extern FillLikeLayerParamsDefaultTypeInternal _FillLikeLayerParams_default_instance_;
class FillStaticLayerParams;
struct FillStaticLayerParamsDefaultTypeInternal;
extern FillStaticLayerParamsDefaultTypeInternal _FillStaticLayerParams_default_instance_;
class FlattenLayerParams;
struct FlattenLayerParamsDefaultTypeInternal;
extern FlattenLayerParamsDefaultTypeInternal _FlattenLayerParams_default_instance_;
class FlattenTo2DLayerParams;
struct FlattenTo2DLayerParamsDefaultTypeInternal;
extern FlattenTo2DLayerParamsDefaultTypeInternal _FlattenTo2DLayerParams_default_instance_;
class FloorDivBroadcastableLayerParams;
struct FloorDivBroadcastableLayerParamsDefaultTypeInternal;
extern FloorDivBroadcastableLayerParamsDefaultTypeInternal _FloorDivBroadcastableLayerParams_default_instance_;
class FloorLayerParams;
struct FloorLayerParamsDefaultTypeInternal;
extern FloorLayerParamsDefaultTypeInternal _FloorLayerParams_default_instance_;
class GRULayerParams;
struct GRULayerParamsDefaultTypeInternal;
extern GRULayerParamsDefaultTypeInternal _GRULayerParams_default_instance_;
class GatherAlongAxisLayerParams;
struct GatherAlongAxisLayerParamsDefaultTypeInternal;
extern GatherAlongAxisLayerParamsDefaultTypeInternal _GatherAlongAxisLayerParams_default_instance_;
class GatherLayerParams;
struct GatherLayerParamsDefaultTypeInternal;
extern GatherLayerParamsDefaultTypeInternal _GatherLayerParams_default_instance_;
class GatherNDLayerParams;
struct GatherNDLayerParamsDefaultTypeInternal;
extern GatherNDLayerParamsDefaultTypeInternal _GatherNDLayerParams_default_instance_;
class GeluLayerParams;
struct GeluLayerParamsDefaultTypeInternal;
extern GeluLayerParamsDefaultTypeInternal _GeluLayerParams_default_instance_;
class GetShapeLayerParams;
struct GetShapeLayerParamsDefaultTypeInternal;
extern GetShapeLayerParamsDefaultTypeInternal _GetShapeLayerParams_default_instance_;
class GlobalPooling3DLayerParams;
struct GlobalPooling3DLayerParamsDefaultTypeInternal;
extern GlobalPooling3DLayerParamsDefaultTypeInternal _GlobalPooling3DLayerParams_default_instance_;
class GreaterEqualLayerParams;
struct GreaterEqualLayerParamsDefaultTypeInternal;
extern GreaterEqualLayerParamsDefaultTypeInternal _GreaterEqualLayerParams_default_instance_;
class GreaterThanLayerParams;
struct GreaterThanLayerParamsDefaultTypeInternal;
extern GreaterThanLayerParamsDefaultTypeInternal _GreaterThanLayerParams_default_instance_;
class InnerProductLayerParams;
struct InnerProductLayerParamsDefaultTypeInternal;
extern InnerProductLayerParamsDefaultTypeInternal _InnerProductLayerParams_default_instance_;
class L2NormalizeLayerParams;
struct L2NormalizeLayerParamsDefaultTypeInternal;
extern L2NormalizeLayerParamsDefaultTypeInternal _L2NormalizeLayerParams_default_instance_;
class LRNLayerParams;
struct LRNLayerParamsDefaultTypeInternal;
extern LRNLayerParamsDefaultTypeInternal _LRNLayerParams_default_instance_;
class LSTMParams;
struct LSTMParamsDefaultTypeInternal;
extern LSTMParamsDefaultTypeInternal _LSTMParams_default_instance_;
class LSTMWeightParams;
struct LSTMWeightParamsDefaultTypeInternal;
extern LSTMWeightParamsDefaultTypeInternal _LSTMWeightParams_default_instance_;
class LayerNormalizationLayerParams;
struct LayerNormalizationLayerParamsDefaultTypeInternal;
extern LayerNormalizationLayerParamsDefaultTypeInternal _LayerNormalizationLayerParams_default_instance_;
class LessEqualLayerParams;
struct LessEqualLayerParamsDefaultTypeInternal;
extern LessEqualLayerParamsDefaultTypeInternal _LessEqualLayerParams_default_instance_;
class LessThanLayerParams;
struct LessThanLayerParamsDefaultTypeInternal;
extern LessThanLayerParamsDefaultTypeInternal _LessThanLayerParams_default_instance_;
class LinearQuantizationParams;
struct LinearQuantizationParamsDefaultTypeInternal;
extern LinearQuantizationParamsDefaultTypeInternal _LinearQuantizationParams_default_instance_;
class LoadConstantLayerParams;
struct LoadConstantLayerParamsDefaultTypeInternal;
extern LoadConstantLayerParamsDefaultTypeInternal _LoadConstantLayerParams_default_instance_;
class LoadConstantNDLayerParams;
struct LoadConstantNDLayerParamsDefaultTypeInternal;
extern LoadConstantNDLayerParamsDefaultTypeInternal _LoadConstantNDLayerParams_default_instance_;
class LogicalAndLayerParams;
struct LogicalAndLayerParamsDefaultTypeInternal;
extern LogicalAndLayerParamsDefaultTypeInternal _LogicalAndLayerParams_default_instance_;
class LogicalNotLayerParams;
struct LogicalNotLayerParamsDefaultTypeInternal;
extern LogicalNotLayerParamsDefaultTypeInternal _LogicalNotLayerParams_default_instance_;
class LogicalOrLayerParams;
struct LogicalOrLayerParamsDefaultTypeInternal;
extern LogicalOrLayerParamsDefaultTypeInternal _LogicalOrLayerParams_default_instance_;
class LogicalXorLayerParams;
struct LogicalXorLayerParamsDefaultTypeInternal;
extern LogicalXorLayerParamsDefaultTypeInternal _LogicalXorLayerParams_default_instance_;
class LookUpTableQuantizationParams;
struct LookUpTableQuantizationParamsDefaultTypeInternal;
extern LookUpTableQuantizationParamsDefaultTypeInternal _LookUpTableQuantizationParams_default_instance_;
class LoopBreakLayerParams;
struct LoopBreakLayerParamsDefaultTypeInternal;
extern LoopBreakLayerParamsDefaultTypeInternal _LoopBreakLayerParams_default_instance_;
class LoopContinueLayerParams;
struct LoopContinueLayerParamsDefaultTypeInternal;
extern LoopContinueLayerParamsDefaultTypeInternal _LoopContinueLayerParams_default_instance_;
class LoopLayerParams;
struct LoopLayerParamsDefaultTypeInternal;
extern LoopLayerParamsDefaultTypeInternal _LoopLayerParams_default_instance_;
class LossLayer;
struct LossLayerDefaultTypeInternal;
extern LossLayerDefaultTypeInternal _LossLayer_default_instance_;
class LowerTriangularLayerParams;
struct LowerTriangularLayerParamsDefaultTypeInternal;
extern LowerTriangularLayerParamsDefaultTypeInternal _LowerTriangularLayerParams_default_instance_;
class MatrixBandPartLayerParams;
struct MatrixBandPartLayerParamsDefaultTypeInternal;
extern MatrixBandPartLayerParamsDefaultTypeInternal _MatrixBandPartLayerParams_default_instance_;
class MaxBroadcastableLayerParams;
struct MaxBroadcastableLayerParamsDefaultTypeInternal;
extern MaxBroadcastableLayerParamsDefaultTypeInternal _MaxBroadcastableLayerParams_default_instance_;
class MaxLayerParams;
struct MaxLayerParamsDefaultTypeInternal;
extern MaxLayerParamsDefaultTypeInternal _MaxLayerParams_default_instance_;
class MeanSquaredErrorLossLayer;
struct MeanSquaredErrorLossLayerDefaultTypeInternal;
extern MeanSquaredErrorLossLayerDefaultTypeInternal _MeanSquaredErrorLossLayer_default_instance_;
class MeanVarianceNormalizeLayerParams;
struct MeanVarianceNormalizeLayerParamsDefaultTypeInternal;
extern MeanVarianceNormalizeLayerParamsDefaultTypeInternal _MeanVarianceNormalizeLayerParams_default_instance_;
class MinBroadcastableLayerParams;
struct MinBroadcastableLayerParamsDefaultTypeInternal;
extern MinBroadcastableLayerParamsDefaultTypeInternal _MinBroadcastableLayerParams_default_instance_;
class MinLayerParams;
struct MinLayerParamsDefaultTypeInternal;
extern MinLayerParamsDefaultTypeInternal _MinLayerParams_default_instance_;
class ModBroadcastableLayerParams;
struct ModBroadcastableLayerParamsDefaultTypeInternal;
extern ModBroadcastableLayerParamsDefaultTypeInternal _ModBroadcastableLayerParams_default_instance_;
class MultiplyBroadcastableLayerParams;
struct MultiplyBroadcastableLayerParamsDefaultTypeInternal;
extern MultiplyBroadcastableLayerParamsDefaultTypeInternal _MultiplyBroadcastableLayerParams_default_instance_;
class MultiplyLayerParams;
struct MultiplyLayerParamsDefaultTypeInternal;
extern MultiplyLayerParamsDefaultTypeInternal _MultiplyLayerParams_default_instance_;
class NetworkUpdateParameters;
struct NetworkUpdateParametersDefaultTypeInternal;
extern NetworkUpdateParametersDefaultTypeInternal _NetworkUpdateParameters_default_instance_;
class NeuralNetwork;
struct NeuralNetworkDefaultTypeInternal;
extern NeuralNetworkDefaultTypeInternal _NeuralNetwork_default_instance_;
class NeuralNetworkClassifier;
struct NeuralNetworkClassifierDefaultTypeInternal;
extern NeuralNetworkClassifierDefaultTypeInternal _NeuralNetworkClassifier_default_instance_;
class NeuralNetworkImageScaler;
struct NeuralNetworkImageScalerDefaultTypeInternal;
extern NeuralNetworkImageScalerDefaultTypeInternal _NeuralNetworkImageScaler_default_instance_;
class NeuralNetworkLayer;
struct NeuralNetworkLayerDefaultTypeInternal;
extern NeuralNetworkLayerDefaultTypeInternal _NeuralNetworkLayer_default_instance_;
class NeuralNetworkMeanImage;
struct NeuralNetworkMeanImageDefaultTypeInternal;
extern NeuralNetworkMeanImageDefaultTypeInternal _NeuralNetworkMeanImage_default_instance_;
class NeuralNetworkPreprocessing;
struct NeuralNetworkPreprocessingDefaultTypeInternal;
extern NeuralNetworkPreprocessingDefaultTypeInternal _NeuralNetworkPreprocessing_default_instance_;
class NeuralNetworkRegressor;
struct NeuralNetworkRegressorDefaultTypeInternal;
extern NeuralNetworkRegressorDefaultTypeInternal _NeuralNetworkRegressor_default_instance_;
class NonMaximumSuppressionLayerParams;
struct NonMaximumSuppressionLayerParamsDefaultTypeInternal;
extern NonMaximumSuppressionLayerParamsDefaultTypeInternal _NonMaximumSuppressionLayerParams_default_instance_;
class NotEqualLayerParams;
struct NotEqualLayerParamsDefaultTypeInternal;
extern NotEqualLayerParamsDefaultTypeInternal _NotEqualLayerParams_default_instance_;
class OneHotLayerParams;
struct OneHotLayerParamsDefaultTypeInternal;
extern OneHotLayerParamsDefaultTypeInternal _OneHotLayerParams_default_instance_;
class Optimizer;
struct OptimizerDefaultTypeInternal;
extern OptimizerDefaultTypeInternal _Optimizer_default_instance_;
class PaddingLayerParams;
struct PaddingLayerParamsDefaultTypeInternal;
extern PaddingLayerParamsDefaultTypeInternal _PaddingLayerParams_default_instance_;
class PaddingLayerParams_PaddingConstant;
struct PaddingLayerParams_PaddingConstantDefaultTypeInternal;
extern PaddingLayerParams_PaddingConstantDefaultTypeInternal _PaddingLayerParams_PaddingConstant_default_instance_;
class PaddingLayerParams_PaddingReflection;
struct PaddingLayerParams_PaddingReflectionDefaultTypeInternal;
extern PaddingLayerParams_PaddingReflectionDefaultTypeInternal _PaddingLayerParams_PaddingReflection_default_instance_;
class PaddingLayerParams_PaddingReplication;
struct PaddingLayerParams_PaddingReplicationDefaultTypeInternal;
extern PaddingLayerParams_PaddingReplicationDefaultTypeInternal _PaddingLayerParams_PaddingReplication_default_instance_;
class PermuteLayerParams;
struct PermuteLayerParamsDefaultTypeInternal;
extern PermuteLayerParamsDefaultTypeInternal _PermuteLayerParams_default_instance_;
class Pooling3DLayerParams;
struct Pooling3DLayerParamsDefaultTypeInternal;
extern Pooling3DLayerParamsDefaultTypeInternal _Pooling3DLayerParams_default_instance_;
class PoolingLayerParams;
struct PoolingLayerParamsDefaultTypeInternal;
extern PoolingLayerParamsDefaultTypeInternal _PoolingLayerParams_default_instance_;
class PoolingLayerParams_ValidCompletePadding;
struct PoolingLayerParams_ValidCompletePaddingDefaultTypeInternal;
extern PoolingLayerParams_ValidCompletePaddingDefaultTypeInternal _PoolingLayerParams_ValidCompletePadding_default_instance_;
class PowBroadcastableLayerParams;
struct PowBroadcastableLayerParamsDefaultTypeInternal;
extern PowBroadcastableLayerParamsDefaultTypeInternal _PowBroadcastableLayerParams_default_instance_;
class QuantizationParams;
struct QuantizationParamsDefaultTypeInternal;
extern QuantizationParamsDefaultTypeInternal _QuantizationParams_default_instance_;
class RandomBernoulliDynamicLayerParams;
struct RandomBernoulliDynamicLayerParamsDefaultTypeInternal;
extern RandomBernoulliDynamicLayerParamsDefaultTypeInternal _RandomBernoulliDynamicLayerParams_default_instance_;
class RandomBernoulliLikeLayerParams;
struct RandomBernoulliLikeLayerParamsDefaultTypeInternal;
extern RandomBernoulliLikeLayerParamsDefaultTypeInternal _RandomBernoulliLikeLayerParams_default_instance_;
class RandomBernoulliStaticLayerParams;
struct RandomBernoulliStaticLayerParamsDefaultTypeInternal;
extern RandomBernoulliStaticLayerParamsDefaultTypeInternal _RandomBernoulliStaticLayerParams_default_instance_;
class RandomNormalDynamicLayerParams;
struct RandomNormalDynamicLayerParamsDefaultTypeInternal;
extern RandomNormalDynamicLayerParamsDefaultTypeInternal _RandomNormalDynamicLayerParams_default_instance_;
class RandomNormalLikeLayerParams;
struct RandomNormalLikeLayerParamsDefaultTypeInternal;
extern RandomNormalLikeLayerParamsDefaultTypeInternal _RandomNormalLikeLayerParams_default_instance_;
class RandomNormalStaticLayerParams;
struct RandomNormalStaticLayerParamsDefaultTypeInternal;
extern RandomNormalStaticLayerParamsDefaultTypeInternal _RandomNormalStaticLayerParams_default_instance_;
class RandomUniformDynamicLayerParams;
struct RandomUniformDynamicLayerParamsDefaultTypeInternal;
extern RandomUniformDynamicLayerParamsDefaultTypeInternal _RandomUniformDynamicLayerParams_default_instance_;
class RandomUniformLikeLayerParams;
struct RandomUniformLikeLayerParamsDefaultTypeInternal;
extern RandomUniformLikeLayerParamsDefaultTypeInternal _RandomUniformLikeLayerParams_default_instance_;
class RandomUniformStaticLayerParams;
struct RandomUniformStaticLayerParamsDefaultTypeInternal;
extern RandomUniformStaticLayerParamsDefaultTypeInternal _RandomUniformStaticLayerParams_default_instance_;
class RangeDynamicLayerParams;
struct RangeDynamicLayerParamsDefaultTypeInternal;
extern RangeDynamicLayerParamsDefaultTypeInternal _RangeDynamicLayerParams_default_instance_;
class RangeStaticLayerParams;
struct RangeStaticLayerParamsDefaultTypeInternal;
extern RangeStaticLayerParamsDefaultTypeInternal _RangeStaticLayerParams_default_instance_;
class RankPreservingReshapeLayerParams;
struct RankPreservingReshapeLayerParamsDefaultTypeInternal;
extern RankPreservingReshapeLayerParamsDefaultTypeInternal _RankPreservingReshapeLayerParams_default_instance_;
class ReduceL1LayerParams;
struct ReduceL1LayerParamsDefaultTypeInternal;
extern ReduceL1LayerParamsDefaultTypeInternal _ReduceL1LayerParams_default_instance_;
class ReduceL2LayerParams;
struct ReduceL2LayerParamsDefaultTypeInternal;
extern ReduceL2LayerParamsDefaultTypeInternal _ReduceL2LayerParams_default_instance_;
class ReduceLayerParams;
struct ReduceLayerParamsDefaultTypeInternal;
extern ReduceLayerParamsDefaultTypeInternal _ReduceLayerParams_default_instance_;
class ReduceLogSumExpLayerParams;
struct ReduceLogSumExpLayerParamsDefaultTypeInternal;
extern ReduceLogSumExpLayerParamsDefaultTypeInternal _ReduceLogSumExpLayerParams_default_instance_;
class ReduceLogSumLayerParams;
struct ReduceLogSumLayerParamsDefaultTypeInternal;
extern ReduceLogSumLayerParamsDefaultTypeInternal _ReduceLogSumLayerParams_default_instance_;
class ReduceMaxLayerParams;
struct ReduceMaxLayerParamsDefaultTypeInternal;
extern ReduceMaxLayerParamsDefaultTypeInternal _ReduceMaxLayerParams_default_instance_;
class ReduceMeanLayerParams;
struct ReduceMeanLayerParamsDefaultTypeInternal;
extern ReduceMeanLayerParamsDefaultTypeInternal _ReduceMeanLayerParams_default_instance_;
class ReduceMinLayerParams;
struct ReduceMinLayerParamsDefaultTypeInternal;
extern ReduceMinLayerParamsDefaultTypeInternal _ReduceMinLayerParams_default_instance_;
class ReduceProdLayerParams;
struct ReduceProdLayerParamsDefaultTypeInternal;
extern ReduceProdLayerParamsDefaultTypeInternal _ReduceProdLayerParams_default_instance_;
class ReduceSumLayerParams;
struct ReduceSumLayerParamsDefaultTypeInternal;
extern ReduceSumLayerParamsDefaultTypeInternal _ReduceSumLayerParams_default_instance_;
class ReduceSumSquareLayerParams;
struct ReduceSumSquareLayerParamsDefaultTypeInternal;
extern ReduceSumSquareLayerParamsDefaultTypeInternal _ReduceSumSquareLayerParams_default_instance_;
class ReorganizeDataLayerParams;
struct ReorganizeDataLayerParamsDefaultTypeInternal;
extern ReorganizeDataLayerParamsDefaultTypeInternal _ReorganizeDataLayerParams_default_instance_;
class ReshapeDynamicLayerParams;
struct ReshapeDynamicLayerParamsDefaultTypeInternal;
extern ReshapeDynamicLayerParamsDefaultTypeInternal _ReshapeDynamicLayerParams_default_instance_;
class ReshapeLayerParams;
struct ReshapeLayerParamsDefaultTypeInternal;
extern ReshapeLayerParamsDefaultTypeInternal _ReshapeLayerParams_default_instance_;
class ReshapeLikeLayerParams;
struct ReshapeLikeLayerParamsDefaultTypeInternal;
extern ReshapeLikeLayerParamsDefaultTypeInternal _ReshapeLikeLayerParams_default_instance_;
class ReshapeStaticLayerParams;
struct ReshapeStaticLayerParamsDefaultTypeInternal;
extern ReshapeStaticLayerParamsDefaultTypeInternal _ReshapeStaticLayerParams_default_instance_;
class ResizeBilinearLayerParams;
struct ResizeBilinearLayerParamsDefaultTypeInternal;
extern ResizeBilinearLayerParamsDefaultTypeInternal _ResizeBilinearLayerParams_default_instance_;
class ReverseLayerParams;
struct ReverseLayerParamsDefaultTypeInternal;
extern ReverseLayerParamsDefaultTypeInternal _ReverseLayerParams_default_instance_;
class ReverseSeqLayerParams;
struct ReverseSeqLayerParamsDefaultTypeInternal;
extern ReverseSeqLayerParamsDefaultTypeInternal _ReverseSeqLayerParams_default_instance_;
class RoundLayerParams;
struct RoundLayerParamsDefaultTypeInternal;
extern RoundLayerParamsDefaultTypeInternal _RoundLayerParams_default_instance_;
class SGDOptimizer;
struct SGDOptimizerDefaultTypeInternal;
extern SGDOptimizerDefaultTypeInternal _SGDOptimizer_default_instance_;
class SamePadding;
struct SamePaddingDefaultTypeInternal;
extern SamePaddingDefaultTypeInternal _SamePadding_default_instance_;
class SamplingMode;
struct SamplingModeDefaultTypeInternal;
extern SamplingModeDefaultTypeInternal _SamplingMode_default_instance_;
class ScaleLayerParams;
struct ScaleLayerParamsDefaultTypeInternal;
extern ScaleLayerParamsDefaultTypeInternal _ScaleLayerParams_default_instance_;
class ScatterAlongAxisLayerParams;
struct ScatterAlongAxisLayerParamsDefaultTypeInternal;
extern ScatterAlongAxisLayerParamsDefaultTypeInternal _ScatterAlongAxisLayerParams_default_instance_;
class ScatterLayerParams;
struct ScatterLayerParamsDefaultTypeInternal;
extern ScatterLayerParamsDefaultTypeInternal _ScatterLayerParams_default_instance_;
class ScatterNDLayerParams;
struct ScatterNDLayerParamsDefaultTypeInternal;
extern ScatterNDLayerParamsDefaultTypeInternal _ScatterNDLayerParams_default_instance_;
class SequenceRepeatLayerParams;
struct SequenceRepeatLayerParamsDefaultTypeInternal;
extern SequenceRepeatLayerParamsDefaultTypeInternal _SequenceRepeatLayerParams_default_instance_;
class SignLayerParams;
struct SignLayerParamsDefaultTypeInternal;
extern SignLayerParamsDefaultTypeInternal _SignLayerParams_default_instance_;
class SimpleRecurrentLayerParams;
struct SimpleRecurrentLayerParamsDefaultTypeInternal;
extern SimpleRecurrentLayerParamsDefaultTypeInternal _SimpleRecurrentLayerParams_default_instance_;
class SinLayerParams;
struct SinLayerParamsDefaultTypeInternal;
extern SinLayerParamsDefaultTypeInternal _SinLayerParams_default_instance_;
class SinhLayerParams;
struct SinhLayerParamsDefaultTypeInternal;
extern SinhLayerParamsDefaultTypeInternal _SinhLayerParams_default_instance_;
class SliceBySizeLayerParams;
struct SliceBySizeLayerParamsDefaultTypeInternal;
extern SliceBySizeLayerParamsDefaultTypeInternal _SliceBySizeLayerParams_default_instance_;
class SliceDynamicLayerParams;
struct SliceDynamicLayerParamsDefaultTypeInternal;
extern SliceDynamicLayerParamsDefaultTypeInternal _SliceDynamicLayerParams_default_instance_;
class SliceLayerParams;
struct SliceLayerParamsDefaultTypeInternal;
extern SliceLayerParamsDefaultTypeInternal _SliceLayerParams_default_instance_;
class SliceStaticLayerParams;
struct SliceStaticLayerParamsDefaultTypeInternal;
extern SliceStaticLayerParamsDefaultTypeInternal _SliceStaticLayerParams_default_instance_;
class SlidingWindowsLayerParams;
struct SlidingWindowsLayerParamsDefaultTypeInternal;
extern SlidingWindowsLayerParamsDefaultTypeInternal _SlidingWindowsLayerParams_default_instance_;
class SoftmaxLayerParams;
struct SoftmaxLayerParamsDefaultTypeInternal;
extern SoftmaxLayerParamsDefaultTypeInternal _SoftmaxLayerParams_default_instance_;
class SoftmaxNDLayerParams;
struct SoftmaxNDLayerParamsDefaultTypeInternal;
extern SoftmaxNDLayerParamsDefaultTypeInternal _SoftmaxNDLayerParams_default_instance_;
class SplitLayerParams;
struct SplitLayerParamsDefaultTypeInternal;
extern SplitLayerParamsDefaultTypeInternal _SplitLayerParams_default_instance_;
class SplitNDLayerParams;
struct SplitNDLayerParamsDefaultTypeInternal;
extern SplitNDLayerParamsDefaultTypeInternal _SplitNDLayerParams_default_instance_;
class SqueezeLayerParams;
struct SqueezeLayerParamsDefaultTypeInternal;
extern SqueezeLayerParamsDefaultTypeInternal _SqueezeLayerParams_default_instance_;
class StackLayerParams;
struct StackLayerParamsDefaultTypeInternal;
extern StackLayerParamsDefaultTypeInternal _StackLayerParams_default_instance_;
class SubtractBroadcastableLayerParams;
struct SubtractBroadcastableLayerParamsDefaultTypeInternal;
extern SubtractBroadcastableLayerParamsDefaultTypeInternal _SubtractBroadcastableLayerParams_default_instance_;
class TanLayerParams;
struct TanLayerParamsDefaultTypeInternal;
extern TanLayerParamsDefaultTypeInternal _TanLayerParams_default_instance_;
class TanhLayerParams;
struct TanhLayerParamsDefaultTypeInternal;
extern TanhLayerParamsDefaultTypeInternal _TanhLayerParams_default_instance_;
class Tensor;
struct TensorDefaultTypeInternal;
extern TensorDefaultTypeInternal _Tensor_default_instance_;
class TileLayerParams;
struct TileLayerParamsDefaultTypeInternal;
extern TileLayerParamsDefaultTypeInternal _TileLayerParams_default_instance_;
class TopKLayerParams;
struct TopKLayerParamsDefaultTypeInternal;
extern TopKLayerParamsDefaultTypeInternal _TopKLayerParams_default_instance_;
class TransposeLayerParams;
struct TransposeLayerParamsDefaultTypeInternal;
extern TransposeLayerParamsDefaultTypeInternal _TransposeLayerParams_default_instance_;
class UnaryFunctionLayerParams;
struct UnaryFunctionLayerParamsDefaultTypeInternal;
extern UnaryFunctionLayerParamsDefaultTypeInternal _UnaryFunctionLayerParams_default_instance_;
class UniDirectionalLSTMLayerParams;
struct UniDirectionalLSTMLayerParamsDefaultTypeInternal;
extern UniDirectionalLSTMLayerParamsDefaultTypeInternal _UniDirectionalLSTMLayerParams_default_instance_;
class UpperTriangularLayerParams;
struct UpperTriangularLayerParamsDefaultTypeInternal;
extern UpperTriangularLayerParamsDefaultTypeInternal _UpperTriangularLayerParams_default_instance_;
class UpsampleLayerParams;
struct UpsampleLayerParamsDefaultTypeInternal;
extern UpsampleLayerParamsDefaultTypeInternal _UpsampleLayerParams_default_instance_;
class ValidPadding;
struct ValidPaddingDefaultTypeInternal;
extern ValidPaddingDefaultTypeInternal _ValidPadding_default_instance_;
class WeightParams;
struct WeightParamsDefaultTypeInternal;
extern WeightParamsDefaultTypeInternal _WeightParams_default_instance_;
class WhereBroadcastableLayerParams;
struct WhereBroadcastableLayerParamsDefaultTypeInternal;
extern WhereBroadcastableLayerParamsDefaultTypeInternal _WhereBroadcastableLayerParams_default_instance_;
class WhereNonZeroLayerParams;
struct WhereNonZeroLayerParamsDefaultTypeInternal;
extern WhereNonZeroLayerParamsDefaultTypeInternal _WhereNonZeroLayerParams_default_instance_;
}  // namespace Specification
}  // namespace CoreML
PROTOBUF_NAMESPACE_OPEN
template<> ::CoreML::Specification::AcosLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::AcosLayerParams>(Arena*);
template<> ::CoreML::Specification::AcoshLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::AcoshLayerParams>(Arena*);
template<> ::CoreML::Specification::ActivationELU* Arena::CreateMaybeMessage<::CoreML::Specification::ActivationELU>(Arena*);
template<> ::CoreML::Specification::ActivationLeakyReLU* Arena::CreateMaybeMessage<::CoreML::Specification::ActivationLeakyReLU>(Arena*);
template<> ::CoreML::Specification::ActivationLinear* Arena::CreateMaybeMessage<::CoreML::Specification::ActivationLinear>(Arena*);
template<> ::CoreML::Specification::ActivationPReLU* Arena::CreateMaybeMessage<::CoreML::Specification::ActivationPReLU>(Arena*);
template<> ::CoreML::Specification::ActivationParametricSoftplus* Arena::CreateMaybeMessage<::CoreML::Specification::ActivationParametricSoftplus>(Arena*);
template<> ::CoreML::Specification::ActivationParams* Arena::CreateMaybeMessage<::CoreML::Specification::ActivationParams>(Arena*);
template<> ::CoreML::Specification::ActivationReLU* Arena::CreateMaybeMessage<::CoreML::Specification::ActivationReLU>(Arena*);
template<> ::CoreML::Specification::ActivationScaledTanh* Arena::CreateMaybeMessage<::CoreML::Specification::ActivationScaledTanh>(Arena*);
template<> ::CoreML::Specification::ActivationSigmoid* Arena::CreateMaybeMessage<::CoreML::Specification::ActivationSigmoid>(Arena*);
template<> ::CoreML::Specification::ActivationSigmoidHard* Arena::CreateMaybeMessage<::CoreML::Specification::ActivationSigmoidHard>(Arena*);
template<> ::CoreML::Specification::ActivationSoftplus* Arena::CreateMaybeMessage<::CoreML::Specification::ActivationSoftplus>(Arena*);
template<> ::CoreML::Specification::ActivationSoftsign* Arena::CreateMaybeMessage<::CoreML::Specification::ActivationSoftsign>(Arena*);
template<> ::CoreML::Specification::ActivationTanh* Arena::CreateMaybeMessage<::CoreML::Specification::ActivationTanh>(Arena*);
template<> ::CoreML::Specification::ActivationThresholdedReLU* Arena::CreateMaybeMessage<::CoreML::Specification::ActivationThresholdedReLU>(Arena*);
template<> ::CoreML::Specification::AdamOptimizer* Arena::CreateMaybeMessage<::CoreML::Specification::AdamOptimizer>(Arena*);
template<> ::CoreML::Specification::AddBroadcastableLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::AddBroadcastableLayerParams>(Arena*);
template<> ::CoreML::Specification::AddLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::AddLayerParams>(Arena*);
template<> ::CoreML::Specification::ArgMaxLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ArgMaxLayerParams>(Arena*);
template<> ::CoreML::Specification::ArgMinLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ArgMinLayerParams>(Arena*);
template<> ::CoreML::Specification::ArgSortLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ArgSortLayerParams>(Arena*);
template<> ::CoreML::Specification::AsinLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::AsinLayerParams>(Arena*);
template<> ::CoreML::Specification::AsinhLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::AsinhLayerParams>(Arena*);
template<> ::CoreML::Specification::AtanLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::AtanLayerParams>(Arena*);
template<> ::CoreML::Specification::AtanhLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::AtanhLayerParams>(Arena*);
template<> ::CoreML::Specification::AverageLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::AverageLayerParams>(Arena*);
template<> ::CoreML::Specification::BatchedMatMulLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::BatchedMatMulLayerParams>(Arena*);
template<> ::CoreML::Specification::BatchnormLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::BatchnormLayerParams>(Arena*);
template<> ::CoreML::Specification::BiDirectionalLSTMLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::BiDirectionalLSTMLayerParams>(Arena*);
template<> ::CoreML::Specification::BiasLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::BiasLayerParams>(Arena*);
template<> ::CoreML::Specification::BorderAmounts* Arena::CreateMaybeMessage<::CoreML::Specification::BorderAmounts>(Arena*);
template<> ::CoreML::Specification::BorderAmounts_EdgeSizes* Arena::CreateMaybeMessage<::CoreML::Specification::BorderAmounts_EdgeSizes>(Arena*);
template<> ::CoreML::Specification::BoxCoordinatesMode* Arena::CreateMaybeMessage<::CoreML::Specification::BoxCoordinatesMode>(Arena*);
template<> ::CoreML::Specification::BranchLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::BranchLayerParams>(Arena*);
template<> ::CoreML::Specification::BroadcastToDynamicLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::BroadcastToDynamicLayerParams>(Arena*);
template<> ::CoreML::Specification::BroadcastToLikeLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::BroadcastToLikeLayerParams>(Arena*);
template<> ::CoreML::Specification::BroadcastToStaticLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::BroadcastToStaticLayerParams>(Arena*);
template<> ::CoreML::Specification::CategoricalCrossEntropyLossLayer* Arena::CreateMaybeMessage<::CoreML::Specification::CategoricalCrossEntropyLossLayer>(Arena*);
template<> ::CoreML::Specification::CategoricalDistributionLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::CategoricalDistributionLayerParams>(Arena*);
template<> ::CoreML::Specification::CeilLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::CeilLayerParams>(Arena*);
template<> ::CoreML::Specification::ClampedReLULayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ClampedReLULayerParams>(Arena*);
template<> ::CoreML::Specification::ClipLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ClipLayerParams>(Arena*);
template<> ::CoreML::Specification::ConcatLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ConcatLayerParams>(Arena*);
template<> ::CoreML::Specification::ConcatNDLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ConcatNDLayerParams>(Arena*);
template<> ::CoreML::Specification::ConstantPaddingLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ConstantPaddingLayerParams>(Arena*);
template<> ::CoreML::Specification::Convolution3DLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::Convolution3DLayerParams>(Arena*);
template<> ::CoreML::Specification::ConvolutionLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ConvolutionLayerParams>(Arena*);
template<> ::CoreML::Specification::CopyLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::CopyLayerParams>(Arena*);
template<> ::CoreML::Specification::CosLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::CosLayerParams>(Arena*);
template<> ::CoreML::Specification::CoshLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::CoshLayerParams>(Arena*);
template<> ::CoreML::Specification::CropLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::CropLayerParams>(Arena*);
template<> ::CoreML::Specification::CropResizeLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::CropResizeLayerParams>(Arena*);
template<> ::CoreML::Specification::CumSumLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::CumSumLayerParams>(Arena*);
template<> ::CoreML::Specification::CustomLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::CustomLayerParams>(Arena*);
template<> ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue* Arena::CreateMaybeMessage<::CoreML::Specification::CustomLayerParams_CustomLayerParamValue>(Arena*);
template<> ::CoreML::Specification::CustomLayerParams_ParametersEntry_DoNotUse* Arena::CreateMaybeMessage<::CoreML::Specification::CustomLayerParams_ParametersEntry_DoNotUse>(Arena*);
template<> ::CoreML::Specification::DivideBroadcastableLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::DivideBroadcastableLayerParams>(Arena*);
template<> ::CoreML::Specification::DotProductLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::DotProductLayerParams>(Arena*);
template<> ::CoreML::Specification::EmbeddingLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::EmbeddingLayerParams>(Arena*);
template<> ::CoreML::Specification::EmbeddingNDLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::EmbeddingNDLayerParams>(Arena*);
template<> ::CoreML::Specification::EqualLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::EqualLayerParams>(Arena*);
template<> ::CoreML::Specification::ErfLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ErfLayerParams>(Arena*);
template<> ::CoreML::Specification::Exp2LayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::Exp2LayerParams>(Arena*);
template<> ::CoreML::Specification::ExpandDimsLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ExpandDimsLayerParams>(Arena*);
template<> ::CoreML::Specification::FillDynamicLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::FillDynamicLayerParams>(Arena*);
template<> ::CoreML::Specification::FillLikeLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::FillLikeLayerParams>(Arena*);
template<> ::CoreML::Specification::FillStaticLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::FillStaticLayerParams>(Arena*);
template<> ::CoreML::Specification::FlattenLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::FlattenLayerParams>(Arena*);
template<> ::CoreML::Specification::FlattenTo2DLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::FlattenTo2DLayerParams>(Arena*);
template<> ::CoreML::Specification::FloorDivBroadcastableLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::FloorDivBroadcastableLayerParams>(Arena*);
template<> ::CoreML::Specification::FloorLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::FloorLayerParams>(Arena*);
template<> ::CoreML::Specification::GRULayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::GRULayerParams>(Arena*);
template<> ::CoreML::Specification::GatherAlongAxisLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::GatherAlongAxisLayerParams>(Arena*);
template<> ::CoreML::Specification::GatherLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::GatherLayerParams>(Arena*);
template<> ::CoreML::Specification::GatherNDLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::GatherNDLayerParams>(Arena*);
template<> ::CoreML::Specification::GeluLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::GeluLayerParams>(Arena*);
template<> ::CoreML::Specification::GetShapeLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::GetShapeLayerParams>(Arena*);
template<> ::CoreML::Specification::GlobalPooling3DLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::GlobalPooling3DLayerParams>(Arena*);
template<> ::CoreML::Specification::GreaterEqualLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::GreaterEqualLayerParams>(Arena*);
template<> ::CoreML::Specification::GreaterThanLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::GreaterThanLayerParams>(Arena*);
template<> ::CoreML::Specification::InnerProductLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::InnerProductLayerParams>(Arena*);
template<> ::CoreML::Specification::L2NormalizeLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::L2NormalizeLayerParams>(Arena*);
template<> ::CoreML::Specification::LRNLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::LRNLayerParams>(Arena*);
template<> ::CoreML::Specification::LSTMParams* Arena::CreateMaybeMessage<::CoreML::Specification::LSTMParams>(Arena*);
template<> ::CoreML::Specification::LSTMWeightParams* Arena::CreateMaybeMessage<::CoreML::Specification::LSTMWeightParams>(Arena*);
template<> ::CoreML::Specification::LayerNormalizationLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::LayerNormalizationLayerParams>(Arena*);
template<> ::CoreML::Specification::LessEqualLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::LessEqualLayerParams>(Arena*);
template<> ::CoreML::Specification::LessThanLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::LessThanLayerParams>(Arena*);
template<> ::CoreML::Specification::LinearQuantizationParams* Arena::CreateMaybeMessage<::CoreML::Specification::LinearQuantizationParams>(Arena*);
template<> ::CoreML::Specification::LoadConstantLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::LoadConstantLayerParams>(Arena*);
template<> ::CoreML::Specification::LoadConstantNDLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::LoadConstantNDLayerParams>(Arena*);
template<> ::CoreML::Specification::LogicalAndLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::LogicalAndLayerParams>(Arena*);
template<> ::CoreML::Specification::LogicalNotLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::LogicalNotLayerParams>(Arena*);
template<> ::CoreML::Specification::LogicalOrLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::LogicalOrLayerParams>(Arena*);
template<> ::CoreML::Specification::LogicalXorLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::LogicalXorLayerParams>(Arena*);
template<> ::CoreML::Specification::LookUpTableQuantizationParams* Arena::CreateMaybeMessage<::CoreML::Specification::LookUpTableQuantizationParams>(Arena*);
template<> ::CoreML::Specification::LoopBreakLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::LoopBreakLayerParams>(Arena*);
template<> ::CoreML::Specification::LoopContinueLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::LoopContinueLayerParams>(Arena*);
template<> ::CoreML::Specification::LoopLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::LoopLayerParams>(Arena*);
template<> ::CoreML::Specification::LossLayer* Arena::CreateMaybeMessage<::CoreML::Specification::LossLayer>(Arena*);
template<> ::CoreML::Specification::LowerTriangularLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::LowerTriangularLayerParams>(Arena*);
template<> ::CoreML::Specification::MatrixBandPartLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::MatrixBandPartLayerParams>(Arena*);
template<> ::CoreML::Specification::MaxBroadcastableLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::MaxBroadcastableLayerParams>(Arena*);
template<> ::CoreML::Specification::MaxLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::MaxLayerParams>(Arena*);
template<> ::CoreML::Specification::MeanSquaredErrorLossLayer* Arena::CreateMaybeMessage<::CoreML::Specification::MeanSquaredErrorLossLayer>(Arena*);
template<> ::CoreML::Specification::MeanVarianceNormalizeLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::MeanVarianceNormalizeLayerParams>(Arena*);
template<> ::CoreML::Specification::MinBroadcastableLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::MinBroadcastableLayerParams>(Arena*);
template<> ::CoreML::Specification::MinLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::MinLayerParams>(Arena*);
template<> ::CoreML::Specification::ModBroadcastableLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ModBroadcastableLayerParams>(Arena*);
template<> ::CoreML::Specification::MultiplyBroadcastableLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::MultiplyBroadcastableLayerParams>(Arena*);
template<> ::CoreML::Specification::MultiplyLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::MultiplyLayerParams>(Arena*);
template<> ::CoreML::Specification::NetworkUpdateParameters* Arena::CreateMaybeMessage<::CoreML::Specification::NetworkUpdateParameters>(Arena*);
template<> ::CoreML::Specification::NeuralNetwork* Arena::CreateMaybeMessage<::CoreML::Specification::NeuralNetwork>(Arena*);
template<> ::CoreML::Specification::NeuralNetworkClassifier* Arena::CreateMaybeMessage<::CoreML::Specification::NeuralNetworkClassifier>(Arena*);
template<> ::CoreML::Specification::NeuralNetworkImageScaler* Arena::CreateMaybeMessage<::CoreML::Specification::NeuralNetworkImageScaler>(Arena*);
template<> ::CoreML::Specification::NeuralNetworkLayer* Arena::CreateMaybeMessage<::CoreML::Specification::NeuralNetworkLayer>(Arena*);
template<> ::CoreML::Specification::NeuralNetworkMeanImage* Arena::CreateMaybeMessage<::CoreML::Specification::NeuralNetworkMeanImage>(Arena*);
template<> ::CoreML::Specification::NeuralNetworkPreprocessing* Arena::CreateMaybeMessage<::CoreML::Specification::NeuralNetworkPreprocessing>(Arena*);
template<> ::CoreML::Specification::NeuralNetworkRegressor* Arena::CreateMaybeMessage<::CoreML::Specification::NeuralNetworkRegressor>(Arena*);
template<> ::CoreML::Specification::NonMaximumSuppressionLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::NonMaximumSuppressionLayerParams>(Arena*);
template<> ::CoreML::Specification::NotEqualLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::NotEqualLayerParams>(Arena*);
template<> ::CoreML::Specification::OneHotLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::OneHotLayerParams>(Arena*);
template<> ::CoreML::Specification::Optimizer* Arena::CreateMaybeMessage<::CoreML::Specification::Optimizer>(Arena*);
template<> ::CoreML::Specification::PaddingLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::PaddingLayerParams>(Arena*);
template<> ::CoreML::Specification::PaddingLayerParams_PaddingConstant* Arena::CreateMaybeMessage<::CoreML::Specification::PaddingLayerParams_PaddingConstant>(Arena*);
template<> ::CoreML::Specification::PaddingLayerParams_PaddingReflection* Arena::CreateMaybeMessage<::CoreML::Specification::PaddingLayerParams_PaddingReflection>(Arena*);
template<> ::CoreML::Specification::PaddingLayerParams_PaddingReplication* Arena::CreateMaybeMessage<::CoreML::Specification::PaddingLayerParams_PaddingReplication>(Arena*);
template<> ::CoreML::Specification::PermuteLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::PermuteLayerParams>(Arena*);
template<> ::CoreML::Specification::Pooling3DLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::Pooling3DLayerParams>(Arena*);
template<> ::CoreML::Specification::PoolingLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::PoolingLayerParams>(Arena*);
template<> ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* Arena::CreateMaybeMessage<::CoreML::Specification::PoolingLayerParams_ValidCompletePadding>(Arena*);
template<> ::CoreML::Specification::PowBroadcastableLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::PowBroadcastableLayerParams>(Arena*);
template<> ::CoreML::Specification::QuantizationParams* Arena::CreateMaybeMessage<::CoreML::Specification::QuantizationParams>(Arena*);
template<> ::CoreML::Specification::RandomBernoulliDynamicLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::RandomBernoulliDynamicLayerParams>(Arena*);
template<> ::CoreML::Specification::RandomBernoulliLikeLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::RandomBernoulliLikeLayerParams>(Arena*);
template<> ::CoreML::Specification::RandomBernoulliStaticLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::RandomBernoulliStaticLayerParams>(Arena*);
template<> ::CoreML::Specification::RandomNormalDynamicLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::RandomNormalDynamicLayerParams>(Arena*);
template<> ::CoreML::Specification::RandomNormalLikeLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::RandomNormalLikeLayerParams>(Arena*);
template<> ::CoreML::Specification::RandomNormalStaticLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::RandomNormalStaticLayerParams>(Arena*);
template<> ::CoreML::Specification::RandomUniformDynamicLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::RandomUniformDynamicLayerParams>(Arena*);
template<> ::CoreML::Specification::RandomUniformLikeLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::RandomUniformLikeLayerParams>(Arena*);
template<> ::CoreML::Specification::RandomUniformStaticLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::RandomUniformStaticLayerParams>(Arena*);
template<> ::CoreML::Specification::RangeDynamicLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::RangeDynamicLayerParams>(Arena*);
template<> ::CoreML::Specification::RangeStaticLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::RangeStaticLayerParams>(Arena*);
template<> ::CoreML::Specification::RankPreservingReshapeLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::RankPreservingReshapeLayerParams>(Arena*);
template<> ::CoreML::Specification::ReduceL1LayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ReduceL1LayerParams>(Arena*);
template<> ::CoreML::Specification::ReduceL2LayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ReduceL2LayerParams>(Arena*);
template<> ::CoreML::Specification::ReduceLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ReduceLayerParams>(Arena*);
template<> ::CoreML::Specification::ReduceLogSumExpLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ReduceLogSumExpLayerParams>(Arena*);
template<> ::CoreML::Specification::ReduceLogSumLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ReduceLogSumLayerParams>(Arena*);
template<> ::CoreML::Specification::ReduceMaxLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ReduceMaxLayerParams>(Arena*);
template<> ::CoreML::Specification::ReduceMeanLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ReduceMeanLayerParams>(Arena*);
template<> ::CoreML::Specification::ReduceMinLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ReduceMinLayerParams>(Arena*);
template<> ::CoreML::Specification::ReduceProdLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ReduceProdLayerParams>(Arena*);
template<> ::CoreML::Specification::ReduceSumLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ReduceSumLayerParams>(Arena*);
template<> ::CoreML::Specification::ReduceSumSquareLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ReduceSumSquareLayerParams>(Arena*);
template<> ::CoreML::Specification::ReorganizeDataLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ReorganizeDataLayerParams>(Arena*);
template<> ::CoreML::Specification::ReshapeDynamicLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ReshapeDynamicLayerParams>(Arena*);
template<> ::CoreML::Specification::ReshapeLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ReshapeLayerParams>(Arena*);
template<> ::CoreML::Specification::ReshapeLikeLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ReshapeLikeLayerParams>(Arena*);
template<> ::CoreML::Specification::ReshapeStaticLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ReshapeStaticLayerParams>(Arena*);
template<> ::CoreML::Specification::ResizeBilinearLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ResizeBilinearLayerParams>(Arena*);
template<> ::CoreML::Specification::ReverseLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ReverseLayerParams>(Arena*);
template<> ::CoreML::Specification::ReverseSeqLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ReverseSeqLayerParams>(Arena*);
template<> ::CoreML::Specification::RoundLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::RoundLayerParams>(Arena*);
template<> ::CoreML::Specification::SGDOptimizer* Arena::CreateMaybeMessage<::CoreML::Specification::SGDOptimizer>(Arena*);
template<> ::CoreML::Specification::SamePadding* Arena::CreateMaybeMessage<::CoreML::Specification::SamePadding>(Arena*);
template<> ::CoreML::Specification::SamplingMode* Arena::CreateMaybeMessage<::CoreML::Specification::SamplingMode>(Arena*);
template<> ::CoreML::Specification::ScaleLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ScaleLayerParams>(Arena*);
template<> ::CoreML::Specification::ScatterAlongAxisLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ScatterAlongAxisLayerParams>(Arena*);
template<> ::CoreML::Specification::ScatterLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ScatterLayerParams>(Arena*);
template<> ::CoreML::Specification::ScatterNDLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::ScatterNDLayerParams>(Arena*);
template<> ::CoreML::Specification::SequenceRepeatLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::SequenceRepeatLayerParams>(Arena*);
template<> ::CoreML::Specification::SignLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::SignLayerParams>(Arena*);
template<> ::CoreML::Specification::SimpleRecurrentLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::SimpleRecurrentLayerParams>(Arena*);
template<> ::CoreML::Specification::SinLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::SinLayerParams>(Arena*);
template<> ::CoreML::Specification::SinhLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::SinhLayerParams>(Arena*);
template<> ::CoreML::Specification::SliceBySizeLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::SliceBySizeLayerParams>(Arena*);
template<> ::CoreML::Specification::SliceDynamicLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::SliceDynamicLayerParams>(Arena*);
template<> ::CoreML::Specification::SliceLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::SliceLayerParams>(Arena*);
template<> ::CoreML::Specification::SliceStaticLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::SliceStaticLayerParams>(Arena*);
template<> ::CoreML::Specification::SlidingWindowsLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::SlidingWindowsLayerParams>(Arena*);
template<> ::CoreML::Specification::SoftmaxLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::SoftmaxLayerParams>(Arena*);
template<> ::CoreML::Specification::SoftmaxNDLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::SoftmaxNDLayerParams>(Arena*);
template<> ::CoreML::Specification::SplitLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::SplitLayerParams>(Arena*);
template<> ::CoreML::Specification::SplitNDLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::SplitNDLayerParams>(Arena*);
template<> ::CoreML::Specification::SqueezeLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::SqueezeLayerParams>(Arena*);
template<> ::CoreML::Specification::StackLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::StackLayerParams>(Arena*);
template<> ::CoreML::Specification::SubtractBroadcastableLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::SubtractBroadcastableLayerParams>(Arena*);
template<> ::CoreML::Specification::TanLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::TanLayerParams>(Arena*);
template<> ::CoreML::Specification::TanhLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::TanhLayerParams>(Arena*);
template<> ::CoreML::Specification::Tensor* Arena::CreateMaybeMessage<::CoreML::Specification::Tensor>(Arena*);
template<> ::CoreML::Specification::TileLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::TileLayerParams>(Arena*);
template<> ::CoreML::Specification::TopKLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::TopKLayerParams>(Arena*);
template<> ::CoreML::Specification::TransposeLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::TransposeLayerParams>(Arena*);
template<> ::CoreML::Specification::UnaryFunctionLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::UnaryFunctionLayerParams>(Arena*);
template<> ::CoreML::Specification::UniDirectionalLSTMLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::UniDirectionalLSTMLayerParams>(Arena*);
template<> ::CoreML::Specification::UpperTriangularLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::UpperTriangularLayerParams>(Arena*);
template<> ::CoreML::Specification::UpsampleLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::UpsampleLayerParams>(Arena*);
template<> ::CoreML::Specification::ValidPadding* Arena::CreateMaybeMessage<::CoreML::Specification::ValidPadding>(Arena*);
template<> ::CoreML::Specification::WeightParams* Arena::CreateMaybeMessage<::CoreML::Specification::WeightParams>(Arena*);
template<> ::CoreML::Specification::WhereBroadcastableLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::WhereBroadcastableLayerParams>(Arena*);
template<> ::CoreML::Specification::WhereNonZeroLayerParams* Arena::CreateMaybeMessage<::CoreML::Specification::WhereNonZeroLayerParams>(Arena*);
PROTOBUF_NAMESPACE_CLOSE
namespace CoreML {
namespace Specification {

enum SamePadding_SamePaddingMode : int {
  SamePadding_SamePaddingMode_BOTTOM_RIGHT_HEAVY = 0,
  SamePadding_SamePaddingMode_TOP_LEFT_HEAVY = 1,
  SamePadding_SamePaddingMode_SamePadding_SamePaddingMode_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  SamePadding_SamePaddingMode_SamePadding_SamePaddingMode_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool SamePadding_SamePaddingMode_IsValid(int value);
constexpr SamePadding_SamePaddingMode SamePadding_SamePaddingMode_SamePaddingMode_MIN = SamePadding_SamePaddingMode_BOTTOM_RIGHT_HEAVY;
constexpr SamePadding_SamePaddingMode SamePadding_SamePaddingMode_SamePaddingMode_MAX = SamePadding_SamePaddingMode_TOP_LEFT_HEAVY;
constexpr int SamePadding_SamePaddingMode_SamePaddingMode_ARRAYSIZE = SamePadding_SamePaddingMode_SamePaddingMode_MAX + 1;

const std::string& SamePadding_SamePaddingMode_Name(SamePadding_SamePaddingMode value);
template<typename T>
inline const std::string& SamePadding_SamePaddingMode_Name(T enum_t_value) {
  static_assert(::std::is_same<T, SamePadding_SamePaddingMode>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function SamePadding_SamePaddingMode_Name.");
  return SamePadding_SamePaddingMode_Name(static_cast<SamePadding_SamePaddingMode>(enum_t_value));
}
bool SamePadding_SamePaddingMode_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, SamePadding_SamePaddingMode* value);
enum SamplingMode_Method : int {
  SamplingMode_Method_STRICT_ALIGN_ENDPOINTS_MODE = 0,
  SamplingMode_Method_ALIGN_ENDPOINTS_MODE = 1,
  SamplingMode_Method_UPSAMPLE_MODE = 2,
  SamplingMode_Method_ROI_ALIGN_MODE = 3,
  SamplingMode_Method_SamplingMode_Method_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  SamplingMode_Method_SamplingMode_Method_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool SamplingMode_Method_IsValid(int value);
constexpr SamplingMode_Method SamplingMode_Method_Method_MIN = SamplingMode_Method_STRICT_ALIGN_ENDPOINTS_MODE;
constexpr SamplingMode_Method SamplingMode_Method_Method_MAX = SamplingMode_Method_ROI_ALIGN_MODE;
constexpr int SamplingMode_Method_Method_ARRAYSIZE = SamplingMode_Method_Method_MAX + 1;

const std::string& SamplingMode_Method_Name(SamplingMode_Method value);
template<typename T>
inline const std::string& SamplingMode_Method_Name(T enum_t_value) {
  static_assert(::std::is_same<T, SamplingMode_Method>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function SamplingMode_Method_Name.");
  return SamplingMode_Method_Name(static_cast<SamplingMode_Method>(enum_t_value));
}
bool SamplingMode_Method_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, SamplingMode_Method* value);
enum BoxCoordinatesMode_Coordinates : int {
  BoxCoordinatesMode_Coordinates_CORNERS_HEIGHT_FIRST = 0,
  BoxCoordinatesMode_Coordinates_CORNERS_WIDTH_FIRST = 1,
  BoxCoordinatesMode_Coordinates_CENTER_SIZE_HEIGHT_FIRST = 2,
  BoxCoordinatesMode_Coordinates_CENTER_SIZE_WIDTH_FIRST = 3,
  BoxCoordinatesMode_Coordinates_BoxCoordinatesMode_Coordinates_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  BoxCoordinatesMode_Coordinates_BoxCoordinatesMode_Coordinates_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool BoxCoordinatesMode_Coordinates_IsValid(int value);
constexpr BoxCoordinatesMode_Coordinates BoxCoordinatesMode_Coordinates_Coordinates_MIN = BoxCoordinatesMode_Coordinates_CORNERS_HEIGHT_FIRST;
constexpr BoxCoordinatesMode_Coordinates BoxCoordinatesMode_Coordinates_Coordinates_MAX = BoxCoordinatesMode_Coordinates_CENTER_SIZE_WIDTH_FIRST;
constexpr int BoxCoordinatesMode_Coordinates_Coordinates_ARRAYSIZE = BoxCoordinatesMode_Coordinates_Coordinates_MAX + 1;

const std::string& BoxCoordinatesMode_Coordinates_Name(BoxCoordinatesMode_Coordinates value);
template<typename T>
inline const std::string& BoxCoordinatesMode_Coordinates_Name(T enum_t_value) {
  static_assert(::std::is_same<T, BoxCoordinatesMode_Coordinates>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function BoxCoordinatesMode_Coordinates_Name.");
  return BoxCoordinatesMode_Coordinates_Name(static_cast<BoxCoordinatesMode_Coordinates>(enum_t_value));
}
bool BoxCoordinatesMode_Coordinates_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, BoxCoordinatesMode_Coordinates* value);
enum Convolution3DLayerParams_PaddingType : int {
  Convolution3DLayerParams_PaddingType_CUSTOM = 0,
  Convolution3DLayerParams_PaddingType_VALID = 1,
  Convolution3DLayerParams_PaddingType_SAME = 2,
  Convolution3DLayerParams_PaddingType_Convolution3DLayerParams_PaddingType_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  Convolution3DLayerParams_PaddingType_Convolution3DLayerParams_PaddingType_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool Convolution3DLayerParams_PaddingType_IsValid(int value);
constexpr Convolution3DLayerParams_PaddingType Convolution3DLayerParams_PaddingType_PaddingType_MIN = Convolution3DLayerParams_PaddingType_CUSTOM;
constexpr Convolution3DLayerParams_PaddingType Convolution3DLayerParams_PaddingType_PaddingType_MAX = Convolution3DLayerParams_PaddingType_SAME;
constexpr int Convolution3DLayerParams_PaddingType_PaddingType_ARRAYSIZE = Convolution3DLayerParams_PaddingType_PaddingType_MAX + 1;

const std::string& Convolution3DLayerParams_PaddingType_Name(Convolution3DLayerParams_PaddingType value);
template<typename T>
inline const std::string& Convolution3DLayerParams_PaddingType_Name(T enum_t_value) {
  static_assert(::std::is_same<T, Convolution3DLayerParams_PaddingType>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function Convolution3DLayerParams_PaddingType_Name.");
  return Convolution3DLayerParams_PaddingType_Name(static_cast<Convolution3DLayerParams_PaddingType>(enum_t_value));
}
bool Convolution3DLayerParams_PaddingType_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, Convolution3DLayerParams_PaddingType* value);
enum PoolingLayerParams_PoolingType : int {
  PoolingLayerParams_PoolingType_MAX = 0,
  PoolingLayerParams_PoolingType_AVERAGE = 1,
  PoolingLayerParams_PoolingType_L2 = 2,
  PoolingLayerParams_PoolingType_PoolingLayerParams_PoolingType_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  PoolingLayerParams_PoolingType_PoolingLayerParams_PoolingType_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool PoolingLayerParams_PoolingType_IsValid(int value);
constexpr PoolingLayerParams_PoolingType PoolingLayerParams_PoolingType_PoolingType_MIN = PoolingLayerParams_PoolingType_MAX;
constexpr PoolingLayerParams_PoolingType PoolingLayerParams_PoolingType_PoolingType_MAX = PoolingLayerParams_PoolingType_L2;
constexpr int PoolingLayerParams_PoolingType_PoolingType_ARRAYSIZE = PoolingLayerParams_PoolingType_PoolingType_MAX + 1;

const std::string& PoolingLayerParams_PoolingType_Name(PoolingLayerParams_PoolingType value);
template<typename T>
inline const std::string& PoolingLayerParams_PoolingType_Name(T enum_t_value) {
  static_assert(::std::is_same<T, PoolingLayerParams_PoolingType>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function PoolingLayerParams_PoolingType_Name.");
  return PoolingLayerParams_PoolingType_Name(static_cast<PoolingLayerParams_PoolingType>(enum_t_value));
}
bool PoolingLayerParams_PoolingType_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, PoolingLayerParams_PoolingType* value);
enum Pooling3DLayerParams_PoolingType3D : int {
  Pooling3DLayerParams_PoolingType3D_MAX = 0,
  Pooling3DLayerParams_PoolingType3D_AVERAGE = 1,
  Pooling3DLayerParams_PoolingType3D_Pooling3DLayerParams_PoolingType3D_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  Pooling3DLayerParams_PoolingType3D_Pooling3DLayerParams_PoolingType3D_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool Pooling3DLayerParams_PoolingType3D_IsValid(int value);
constexpr Pooling3DLayerParams_PoolingType3D Pooling3DLayerParams_PoolingType3D_PoolingType3D_MIN = Pooling3DLayerParams_PoolingType3D_MAX;
constexpr Pooling3DLayerParams_PoolingType3D Pooling3DLayerParams_PoolingType3D_PoolingType3D_MAX = Pooling3DLayerParams_PoolingType3D_AVERAGE;
constexpr int Pooling3DLayerParams_PoolingType3D_PoolingType3D_ARRAYSIZE = Pooling3DLayerParams_PoolingType3D_PoolingType3D_MAX + 1;

const std::string& Pooling3DLayerParams_PoolingType3D_Name(Pooling3DLayerParams_PoolingType3D value);
template<typename T>
inline const std::string& Pooling3DLayerParams_PoolingType3D_Name(T enum_t_value) {
  static_assert(::std::is_same<T, Pooling3DLayerParams_PoolingType3D>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function Pooling3DLayerParams_PoolingType3D_Name.");
  return Pooling3DLayerParams_PoolingType3D_Name(static_cast<Pooling3DLayerParams_PoolingType3D>(enum_t_value));
}
bool Pooling3DLayerParams_PoolingType3D_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, Pooling3DLayerParams_PoolingType3D* value);
enum Pooling3DLayerParams_Pooling3DPaddingType : int {
  Pooling3DLayerParams_Pooling3DPaddingType_CUSTOM = 0,
  Pooling3DLayerParams_Pooling3DPaddingType_VALID = 1,
  Pooling3DLayerParams_Pooling3DPaddingType_SAME = 2,
  Pooling3DLayerParams_Pooling3DPaddingType_Pooling3DLayerParams_Pooling3DPaddingType_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  Pooling3DLayerParams_Pooling3DPaddingType_Pooling3DLayerParams_Pooling3DPaddingType_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool Pooling3DLayerParams_Pooling3DPaddingType_IsValid(int value);
constexpr Pooling3DLayerParams_Pooling3DPaddingType Pooling3DLayerParams_Pooling3DPaddingType_Pooling3DPaddingType_MIN = Pooling3DLayerParams_Pooling3DPaddingType_CUSTOM;
constexpr Pooling3DLayerParams_Pooling3DPaddingType Pooling3DLayerParams_Pooling3DPaddingType_Pooling3DPaddingType_MAX = Pooling3DLayerParams_Pooling3DPaddingType_SAME;
constexpr int Pooling3DLayerParams_Pooling3DPaddingType_Pooling3DPaddingType_ARRAYSIZE = Pooling3DLayerParams_Pooling3DPaddingType_Pooling3DPaddingType_MAX + 1;

const std::string& Pooling3DLayerParams_Pooling3DPaddingType_Name(Pooling3DLayerParams_Pooling3DPaddingType value);
template<typename T>
inline const std::string& Pooling3DLayerParams_Pooling3DPaddingType_Name(T enum_t_value) {
  static_assert(::std::is_same<T, Pooling3DLayerParams_Pooling3DPaddingType>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function Pooling3DLayerParams_Pooling3DPaddingType_Name.");
  return Pooling3DLayerParams_Pooling3DPaddingType_Name(static_cast<Pooling3DLayerParams_Pooling3DPaddingType>(enum_t_value));
}
bool Pooling3DLayerParams_Pooling3DPaddingType_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, Pooling3DLayerParams_Pooling3DPaddingType* value);
enum GlobalPooling3DLayerParams_GlobalPoolingType3D : int {
  GlobalPooling3DLayerParams_GlobalPoolingType3D_MAX = 0,
  GlobalPooling3DLayerParams_GlobalPoolingType3D_AVERAGE = 1,
  GlobalPooling3DLayerParams_GlobalPoolingType3D_GlobalPooling3DLayerParams_GlobalPoolingType3D_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  GlobalPooling3DLayerParams_GlobalPoolingType3D_GlobalPooling3DLayerParams_GlobalPoolingType3D_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool GlobalPooling3DLayerParams_GlobalPoolingType3D_IsValid(int value);
constexpr GlobalPooling3DLayerParams_GlobalPoolingType3D GlobalPooling3DLayerParams_GlobalPoolingType3D_GlobalPoolingType3D_MIN = GlobalPooling3DLayerParams_GlobalPoolingType3D_MAX;
constexpr GlobalPooling3DLayerParams_GlobalPoolingType3D GlobalPooling3DLayerParams_GlobalPoolingType3D_GlobalPoolingType3D_MAX = GlobalPooling3DLayerParams_GlobalPoolingType3D_AVERAGE;
constexpr int GlobalPooling3DLayerParams_GlobalPoolingType3D_GlobalPoolingType3D_ARRAYSIZE = GlobalPooling3DLayerParams_GlobalPoolingType3D_GlobalPoolingType3D_MAX + 1;

const std::string& GlobalPooling3DLayerParams_GlobalPoolingType3D_Name(GlobalPooling3DLayerParams_GlobalPoolingType3D value);
template<typename T>
inline const std::string& GlobalPooling3DLayerParams_GlobalPoolingType3D_Name(T enum_t_value) {
  static_assert(::std::is_same<T, GlobalPooling3DLayerParams_GlobalPoolingType3D>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function GlobalPooling3DLayerParams_GlobalPoolingType3D_Name.");
  return GlobalPooling3DLayerParams_GlobalPoolingType3D_Name(static_cast<GlobalPooling3DLayerParams_GlobalPoolingType3D>(enum_t_value));
}
bool GlobalPooling3DLayerParams_GlobalPoolingType3D_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, GlobalPooling3DLayerParams_GlobalPoolingType3D* value);
enum UnaryFunctionLayerParams_Operation : int {
  UnaryFunctionLayerParams_Operation_SQRT = 0,
  UnaryFunctionLayerParams_Operation_RSQRT = 1,
  UnaryFunctionLayerParams_Operation_INVERSE = 2,
  UnaryFunctionLayerParams_Operation_POWER = 3,
  UnaryFunctionLayerParams_Operation_EXP = 4,
  UnaryFunctionLayerParams_Operation_LOG = 5,
  UnaryFunctionLayerParams_Operation_ABS = 6,
  UnaryFunctionLayerParams_Operation_THRESHOLD = 7,
  UnaryFunctionLayerParams_Operation_UnaryFunctionLayerParams_Operation_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  UnaryFunctionLayerParams_Operation_UnaryFunctionLayerParams_Operation_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool UnaryFunctionLayerParams_Operation_IsValid(int value);
constexpr UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams_Operation_Operation_MIN = UnaryFunctionLayerParams_Operation_SQRT;
constexpr UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams_Operation_Operation_MAX = UnaryFunctionLayerParams_Operation_THRESHOLD;
constexpr int UnaryFunctionLayerParams_Operation_Operation_ARRAYSIZE = UnaryFunctionLayerParams_Operation_Operation_MAX + 1;

const std::string& UnaryFunctionLayerParams_Operation_Name(UnaryFunctionLayerParams_Operation value);
template<typename T>
inline const std::string& UnaryFunctionLayerParams_Operation_Name(T enum_t_value) {
  static_assert(::std::is_same<T, UnaryFunctionLayerParams_Operation>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function UnaryFunctionLayerParams_Operation_Name.");
  return UnaryFunctionLayerParams_Operation_Name(static_cast<UnaryFunctionLayerParams_Operation>(enum_t_value));
}
bool UnaryFunctionLayerParams_Operation_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, UnaryFunctionLayerParams_Operation* value);
enum UpsampleLayerParams_InterpolationMode : int {
  UpsampleLayerParams_InterpolationMode_NN = 0,
  UpsampleLayerParams_InterpolationMode_BILINEAR = 1,
  UpsampleLayerParams_InterpolationMode_UpsampleLayerParams_InterpolationMode_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  UpsampleLayerParams_InterpolationMode_UpsampleLayerParams_InterpolationMode_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool UpsampleLayerParams_InterpolationMode_IsValid(int value);
constexpr UpsampleLayerParams_InterpolationMode UpsampleLayerParams_InterpolationMode_InterpolationMode_MIN = UpsampleLayerParams_InterpolationMode_NN;
constexpr UpsampleLayerParams_InterpolationMode UpsampleLayerParams_InterpolationMode_InterpolationMode_MAX = UpsampleLayerParams_InterpolationMode_BILINEAR;
constexpr int UpsampleLayerParams_InterpolationMode_InterpolationMode_ARRAYSIZE = UpsampleLayerParams_InterpolationMode_InterpolationMode_MAX + 1;

const std::string& UpsampleLayerParams_InterpolationMode_Name(UpsampleLayerParams_InterpolationMode value);
template<typename T>
inline const std::string& UpsampleLayerParams_InterpolationMode_Name(T enum_t_value) {
  static_assert(::std::is_same<T, UpsampleLayerParams_InterpolationMode>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function UpsampleLayerParams_InterpolationMode_Name.");
  return UpsampleLayerParams_InterpolationMode_Name(static_cast<UpsampleLayerParams_InterpolationMode>(enum_t_value));
}
bool UpsampleLayerParams_InterpolationMode_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, UpsampleLayerParams_InterpolationMode* value);
enum UpsampleLayerParams_LinearUpsampleMode : int {
  UpsampleLayerParams_LinearUpsampleMode_DEFAULT = 0,
  UpsampleLayerParams_LinearUpsampleMode_ALIGN_CORNERS_TRUE = 1,
  UpsampleLayerParams_LinearUpsampleMode_ALIGN_CORNERS_FALSE = 2,
  UpsampleLayerParams_LinearUpsampleMode_UpsampleLayerParams_LinearUpsampleMode_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  UpsampleLayerParams_LinearUpsampleMode_UpsampleLayerParams_LinearUpsampleMode_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool UpsampleLayerParams_LinearUpsampleMode_IsValid(int value);
constexpr UpsampleLayerParams_LinearUpsampleMode UpsampleLayerParams_LinearUpsampleMode_LinearUpsampleMode_MIN = UpsampleLayerParams_LinearUpsampleMode_DEFAULT;
constexpr UpsampleLayerParams_LinearUpsampleMode UpsampleLayerParams_LinearUpsampleMode_LinearUpsampleMode_MAX = UpsampleLayerParams_LinearUpsampleMode_ALIGN_CORNERS_FALSE;
constexpr int UpsampleLayerParams_LinearUpsampleMode_LinearUpsampleMode_ARRAYSIZE = UpsampleLayerParams_LinearUpsampleMode_LinearUpsampleMode_MAX + 1;

const std::string& UpsampleLayerParams_LinearUpsampleMode_Name(UpsampleLayerParams_LinearUpsampleMode value);
template<typename T>
inline const std::string& UpsampleLayerParams_LinearUpsampleMode_Name(T enum_t_value) {
  static_assert(::std::is_same<T, UpsampleLayerParams_LinearUpsampleMode>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function UpsampleLayerParams_LinearUpsampleMode_Name.");
  return UpsampleLayerParams_LinearUpsampleMode_Name(static_cast<UpsampleLayerParams_LinearUpsampleMode>(enum_t_value));
}
bool UpsampleLayerParams_LinearUpsampleMode_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, UpsampleLayerParams_LinearUpsampleMode* value);
enum FlattenLayerParams_FlattenOrder : int {
  FlattenLayerParams_FlattenOrder_CHANNEL_FIRST = 0,
  FlattenLayerParams_FlattenOrder_CHANNEL_LAST = 1,
  FlattenLayerParams_FlattenOrder_FlattenLayerParams_FlattenOrder_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  FlattenLayerParams_FlattenOrder_FlattenLayerParams_FlattenOrder_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool FlattenLayerParams_FlattenOrder_IsValid(int value);
constexpr FlattenLayerParams_FlattenOrder FlattenLayerParams_FlattenOrder_FlattenOrder_MIN = FlattenLayerParams_FlattenOrder_CHANNEL_FIRST;
constexpr FlattenLayerParams_FlattenOrder FlattenLayerParams_FlattenOrder_FlattenOrder_MAX = FlattenLayerParams_FlattenOrder_CHANNEL_LAST;
constexpr int FlattenLayerParams_FlattenOrder_FlattenOrder_ARRAYSIZE = FlattenLayerParams_FlattenOrder_FlattenOrder_MAX + 1;

const std::string& FlattenLayerParams_FlattenOrder_Name(FlattenLayerParams_FlattenOrder value);
template<typename T>
inline const std::string& FlattenLayerParams_FlattenOrder_Name(T enum_t_value) {
  static_assert(::std::is_same<T, FlattenLayerParams_FlattenOrder>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function FlattenLayerParams_FlattenOrder_Name.");
  return FlattenLayerParams_FlattenOrder_Name(static_cast<FlattenLayerParams_FlattenOrder>(enum_t_value));
}
bool FlattenLayerParams_FlattenOrder_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, FlattenLayerParams_FlattenOrder* value);
enum ReshapeLayerParams_ReshapeOrder : int {
  ReshapeLayerParams_ReshapeOrder_CHANNEL_FIRST = 0,
  ReshapeLayerParams_ReshapeOrder_CHANNEL_LAST = 1,
  ReshapeLayerParams_ReshapeOrder_ReshapeLayerParams_ReshapeOrder_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  ReshapeLayerParams_ReshapeOrder_ReshapeLayerParams_ReshapeOrder_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool ReshapeLayerParams_ReshapeOrder_IsValid(int value);
constexpr ReshapeLayerParams_ReshapeOrder ReshapeLayerParams_ReshapeOrder_ReshapeOrder_MIN = ReshapeLayerParams_ReshapeOrder_CHANNEL_FIRST;
constexpr ReshapeLayerParams_ReshapeOrder ReshapeLayerParams_ReshapeOrder_ReshapeOrder_MAX = ReshapeLayerParams_ReshapeOrder_CHANNEL_LAST;
constexpr int ReshapeLayerParams_ReshapeOrder_ReshapeOrder_ARRAYSIZE = ReshapeLayerParams_ReshapeOrder_ReshapeOrder_MAX + 1;

const std::string& ReshapeLayerParams_ReshapeOrder_Name(ReshapeLayerParams_ReshapeOrder value);
template<typename T>
inline const std::string& ReshapeLayerParams_ReshapeOrder_Name(T enum_t_value) {
  static_assert(::std::is_same<T, ReshapeLayerParams_ReshapeOrder>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function ReshapeLayerParams_ReshapeOrder_Name.");
  return ReshapeLayerParams_ReshapeOrder_Name(static_cast<ReshapeLayerParams_ReshapeOrder>(enum_t_value));
}
bool ReshapeLayerParams_ReshapeOrder_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, ReshapeLayerParams_ReshapeOrder* value);
enum ReorganizeDataLayerParams_ReorganizationType : int {
  ReorganizeDataLayerParams_ReorganizationType_SPACE_TO_DEPTH = 0,
  ReorganizeDataLayerParams_ReorganizationType_DEPTH_TO_SPACE = 1,
  ReorganizeDataLayerParams_ReorganizationType_PIXEL_SHUFFLE = 2,
  ReorganizeDataLayerParams_ReorganizationType_ReorganizeDataLayerParams_ReorganizationType_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  ReorganizeDataLayerParams_ReorganizationType_ReorganizeDataLayerParams_ReorganizationType_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool ReorganizeDataLayerParams_ReorganizationType_IsValid(int value);
constexpr ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams_ReorganizationType_ReorganizationType_MIN = ReorganizeDataLayerParams_ReorganizationType_SPACE_TO_DEPTH;
constexpr ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams_ReorganizationType_ReorganizationType_MAX = ReorganizeDataLayerParams_ReorganizationType_PIXEL_SHUFFLE;
constexpr int ReorganizeDataLayerParams_ReorganizationType_ReorganizationType_ARRAYSIZE = ReorganizeDataLayerParams_ReorganizationType_ReorganizationType_MAX + 1;

const std::string& ReorganizeDataLayerParams_ReorganizationType_Name(ReorganizeDataLayerParams_ReorganizationType value);
template<typename T>
inline const std::string& ReorganizeDataLayerParams_ReorganizationType_Name(T enum_t_value) {
  static_assert(::std::is_same<T, ReorganizeDataLayerParams_ReorganizationType>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function ReorganizeDataLayerParams_ReorganizationType_Name.");
  return ReorganizeDataLayerParams_ReorganizationType_Name(static_cast<ReorganizeDataLayerParams_ReorganizationType>(enum_t_value));
}
bool ReorganizeDataLayerParams_ReorganizationType_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, ReorganizeDataLayerParams_ReorganizationType* value);
enum SliceLayerParams_SliceAxis : int {
  SliceLayerParams_SliceAxis_CHANNEL_AXIS = 0,
  SliceLayerParams_SliceAxis_HEIGHT_AXIS = 1,
  SliceLayerParams_SliceAxis_WIDTH_AXIS = 2,
  SliceLayerParams_SliceAxis_SliceLayerParams_SliceAxis_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  SliceLayerParams_SliceAxis_SliceLayerParams_SliceAxis_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool SliceLayerParams_SliceAxis_IsValid(int value);
constexpr SliceLayerParams_SliceAxis SliceLayerParams_SliceAxis_SliceAxis_MIN = SliceLayerParams_SliceAxis_CHANNEL_AXIS;
constexpr SliceLayerParams_SliceAxis SliceLayerParams_SliceAxis_SliceAxis_MAX = SliceLayerParams_SliceAxis_WIDTH_AXIS;
constexpr int SliceLayerParams_SliceAxis_SliceAxis_ARRAYSIZE = SliceLayerParams_SliceAxis_SliceAxis_MAX + 1;

const std::string& SliceLayerParams_SliceAxis_Name(SliceLayerParams_SliceAxis value);
template<typename T>
inline const std::string& SliceLayerParams_SliceAxis_Name(T enum_t_value) {
  static_assert(::std::is_same<T, SliceLayerParams_SliceAxis>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function SliceLayerParams_SliceAxis_Name.");
  return SliceLayerParams_SliceAxis_Name(static_cast<SliceLayerParams_SliceAxis>(enum_t_value));
}
bool SliceLayerParams_SliceAxis_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, SliceLayerParams_SliceAxis* value);
enum ReduceLayerParams_ReduceOperation : int {
  ReduceLayerParams_ReduceOperation_SUM = 0,
  ReduceLayerParams_ReduceOperation_AVG = 1,
  ReduceLayerParams_ReduceOperation_PROD = 2,
  ReduceLayerParams_ReduceOperation_LOGSUM = 3,
  ReduceLayerParams_ReduceOperation_SUMSQUARE = 4,
  ReduceLayerParams_ReduceOperation_L1 = 5,
  ReduceLayerParams_ReduceOperation_L2 = 6,
  ReduceLayerParams_ReduceOperation_MAX = 7,
  ReduceLayerParams_ReduceOperation_MIN = 8,
  ReduceLayerParams_ReduceOperation_ARGMAX = 9,
  ReduceLayerParams_ReduceOperation_ReduceLayerParams_ReduceOperation_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  ReduceLayerParams_ReduceOperation_ReduceLayerParams_ReduceOperation_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool ReduceLayerParams_ReduceOperation_IsValid(int value);
constexpr ReduceLayerParams_ReduceOperation ReduceLayerParams_ReduceOperation_ReduceOperation_MIN = ReduceLayerParams_ReduceOperation_SUM;
constexpr ReduceLayerParams_ReduceOperation ReduceLayerParams_ReduceOperation_ReduceOperation_MAX = ReduceLayerParams_ReduceOperation_ARGMAX;
constexpr int ReduceLayerParams_ReduceOperation_ReduceOperation_ARRAYSIZE = ReduceLayerParams_ReduceOperation_ReduceOperation_MAX + 1;

const std::string& ReduceLayerParams_ReduceOperation_Name(ReduceLayerParams_ReduceOperation value);
template<typename T>
inline const std::string& ReduceLayerParams_ReduceOperation_Name(T enum_t_value) {
  static_assert(::std::is_same<T, ReduceLayerParams_ReduceOperation>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function ReduceLayerParams_ReduceOperation_Name.");
  return ReduceLayerParams_ReduceOperation_Name(static_cast<ReduceLayerParams_ReduceOperation>(enum_t_value));
}
bool ReduceLayerParams_ReduceOperation_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, ReduceLayerParams_ReduceOperation* value);
enum ReduceLayerParams_ReduceAxis : int {
  ReduceLayerParams_ReduceAxis_CHW = 0,
  ReduceLayerParams_ReduceAxis_HW = 1,
  ReduceLayerParams_ReduceAxis_C = 2,
  ReduceLayerParams_ReduceAxis_H = 3,
  ReduceLayerParams_ReduceAxis_W = 4,
  ReduceLayerParams_ReduceAxis_ReduceLayerParams_ReduceAxis_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  ReduceLayerParams_ReduceAxis_ReduceLayerParams_ReduceAxis_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool ReduceLayerParams_ReduceAxis_IsValid(int value);
constexpr ReduceLayerParams_ReduceAxis ReduceLayerParams_ReduceAxis_ReduceAxis_MIN = ReduceLayerParams_ReduceAxis_CHW;
constexpr ReduceLayerParams_ReduceAxis ReduceLayerParams_ReduceAxis_ReduceAxis_MAX = ReduceLayerParams_ReduceAxis_W;
constexpr int ReduceLayerParams_ReduceAxis_ReduceAxis_ARRAYSIZE = ReduceLayerParams_ReduceAxis_ReduceAxis_MAX + 1;

const std::string& ReduceLayerParams_ReduceAxis_Name(ReduceLayerParams_ReduceAxis value);
template<typename T>
inline const std::string& ReduceLayerParams_ReduceAxis_Name(T enum_t_value) {
  static_assert(::std::is_same<T, ReduceLayerParams_ReduceAxis>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function ReduceLayerParams_ReduceAxis_Name.");
  return ReduceLayerParams_ReduceAxis_Name(static_cast<ReduceLayerParams_ReduceAxis>(enum_t_value));
}
bool ReduceLayerParams_ReduceAxis_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, ReduceLayerParams_ReduceAxis* value);
enum GeluLayerParams_GeluMode : int {
  GeluLayerParams_GeluMode_EXACT = 0,
  GeluLayerParams_GeluMode_TANH_APPROXIMATION = 1,
  GeluLayerParams_GeluMode_SIGMOID_APPROXIMATION = 2,
  GeluLayerParams_GeluMode_GeluLayerParams_GeluMode_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  GeluLayerParams_GeluMode_GeluLayerParams_GeluMode_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool GeluLayerParams_GeluMode_IsValid(int value);
constexpr GeluLayerParams_GeluMode GeluLayerParams_GeluMode_GeluMode_MIN = GeluLayerParams_GeluMode_EXACT;
constexpr GeluLayerParams_GeluMode GeluLayerParams_GeluMode_GeluMode_MAX = GeluLayerParams_GeluMode_SIGMOID_APPROXIMATION;
constexpr int GeluLayerParams_GeluMode_GeluMode_ARRAYSIZE = GeluLayerParams_GeluMode_GeluMode_MAX + 1;

const std::string& GeluLayerParams_GeluMode_Name(GeluLayerParams_GeluMode value);
template<typename T>
inline const std::string& GeluLayerParams_GeluMode_Name(T enum_t_value) {
  static_assert(::std::is_same<T, GeluLayerParams_GeluMode>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function GeluLayerParams_GeluMode_Name.");
  return GeluLayerParams_GeluMode_Name(static_cast<GeluLayerParams_GeluMode>(enum_t_value));
}
bool GeluLayerParams_GeluMode_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, GeluLayerParams_GeluMode* value);
enum NeuralNetworkMultiArrayShapeMapping : int {
  RANK5_ARRAY_MAPPING = 0,
  EXACT_ARRAY_MAPPING = 1,
  NeuralNetworkMultiArrayShapeMapping_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  NeuralNetworkMultiArrayShapeMapping_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool NeuralNetworkMultiArrayShapeMapping_IsValid(int value);
constexpr NeuralNetworkMultiArrayShapeMapping NeuralNetworkMultiArrayShapeMapping_MIN = RANK5_ARRAY_MAPPING;
constexpr NeuralNetworkMultiArrayShapeMapping NeuralNetworkMultiArrayShapeMapping_MAX = EXACT_ARRAY_MAPPING;
constexpr int NeuralNetworkMultiArrayShapeMapping_ARRAYSIZE = NeuralNetworkMultiArrayShapeMapping_MAX + 1;

const std::string& NeuralNetworkMultiArrayShapeMapping_Name(NeuralNetworkMultiArrayShapeMapping value);
template<typename T>
inline const std::string& NeuralNetworkMultiArrayShapeMapping_Name(T enum_t_value) {
  static_assert(::std::is_same<T, NeuralNetworkMultiArrayShapeMapping>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function NeuralNetworkMultiArrayShapeMapping_Name.");
  return NeuralNetworkMultiArrayShapeMapping_Name(static_cast<NeuralNetworkMultiArrayShapeMapping>(enum_t_value));
}
bool NeuralNetworkMultiArrayShapeMapping_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, NeuralNetworkMultiArrayShapeMapping* value);
enum NeuralNetworkImageShapeMapping : int {
  RANK5_IMAGE_MAPPING = 0,
  RANK4_IMAGE_MAPPING = 1,
  NeuralNetworkImageShapeMapping_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  NeuralNetworkImageShapeMapping_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool NeuralNetworkImageShapeMapping_IsValid(int value);
constexpr NeuralNetworkImageShapeMapping NeuralNetworkImageShapeMapping_MIN = RANK5_IMAGE_MAPPING;
constexpr NeuralNetworkImageShapeMapping NeuralNetworkImageShapeMapping_MAX = RANK4_IMAGE_MAPPING;
constexpr int NeuralNetworkImageShapeMapping_ARRAYSIZE = NeuralNetworkImageShapeMapping_MAX + 1;

const std::string& NeuralNetworkImageShapeMapping_Name(NeuralNetworkImageShapeMapping value);
template<typename T>
inline const std::string& NeuralNetworkImageShapeMapping_Name(T enum_t_value) {
  static_assert(::std::is_same<T, NeuralNetworkImageShapeMapping>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function NeuralNetworkImageShapeMapping_Name.");
  return NeuralNetworkImageShapeMapping_Name(static_cast<NeuralNetworkImageShapeMapping>(enum_t_value));
}
bool NeuralNetworkImageShapeMapping_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, NeuralNetworkImageShapeMapping* value);
enum ScatterMode : int {
  SCATTER_UPDATE = 0,
  SCATTER_ADD = 1,
  SCATTER_SUB = 2,
  SCATTER_MUL = 3,
  SCATTER_DIV = 4,
  SCATTER_MAX = 5,
  SCATTER_MIN = 6,
  ScatterMode_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  ScatterMode_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool ScatterMode_IsValid(int value);
constexpr ScatterMode ScatterMode_MIN = SCATTER_UPDATE;
constexpr ScatterMode ScatterMode_MAX = SCATTER_MIN;
constexpr int ScatterMode_ARRAYSIZE = ScatterMode_MAX + 1;

const std::string& ScatterMode_Name(ScatterMode value);
template<typename T>
inline const std::string& ScatterMode_Name(T enum_t_value) {
  static_assert(::std::is_same<T, ScatterMode>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function ScatterMode_Name.");
  return ScatterMode_Name(static_cast<ScatterMode>(enum_t_value));
}
bool ScatterMode_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, ScatterMode* value);
// ===================================================================

class NeuralNetwork final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NeuralNetwork) */ {
 public:
  inline NeuralNetwork() : NeuralNetwork(nullptr) {}
  ~NeuralNetwork() override;
  explicit constexpr NeuralNetwork(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  NeuralNetwork(const NeuralNetwork& from);
  NeuralNetwork(NeuralNetwork&& from) noexcept
    : NeuralNetwork() {
    *this = ::std::move(from);
  }

  inline NeuralNetwork& operator=(const NeuralNetwork& from) {
    CopyFrom(from);
    return *this;
  }
  inline NeuralNetwork& operator=(NeuralNetwork&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const NeuralNetwork& default_instance() {
    return *internal_default_instance();
  }
  static inline const NeuralNetwork* internal_default_instance() {
    return reinterpret_cast<const NeuralNetwork*>(
               &_NeuralNetwork_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    0;

  friend void swap(NeuralNetwork& a, NeuralNetwork& b) {
    a.Swap(&b);
  }
  inline void Swap(NeuralNetwork* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(NeuralNetwork* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  NeuralNetwork* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<NeuralNetwork>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const NeuralNetwork& from);
  void MergeFrom(const NeuralNetwork& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NeuralNetwork* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.NeuralNetwork";
  }
  protected:
  explicit NeuralNetwork(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kLayersFieldNumber = 1,
    kPreprocessingFieldNumber = 2,
    kUpdateParamsFieldNumber = 10,
    kArrayInputShapeMappingFieldNumber = 5,
    kImageInputShapeMappingFieldNumber = 6,
  };
  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  int layers_size() const;
  private:
  int _internal_layers_size() const;
  public:
  void clear_layers();
  ::CoreML::Specification::NeuralNetworkLayer* mutable_layers(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
      mutable_layers();
  private:
  const ::CoreML::Specification::NeuralNetworkLayer& _internal_layers(int index) const;
  ::CoreML::Specification::NeuralNetworkLayer* _internal_add_layers();
  public:
  const ::CoreML::Specification::NeuralNetworkLayer& layers(int index) const;
  ::CoreML::Specification::NeuralNetworkLayer* add_layers();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
      layers() const;

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  int preprocessing_size() const;
  private:
  int _internal_preprocessing_size() const;
  public:
  void clear_preprocessing();
  ::CoreML::Specification::NeuralNetworkPreprocessing* mutable_preprocessing(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
      mutable_preprocessing();
  private:
  const ::CoreML::Specification::NeuralNetworkPreprocessing& _internal_preprocessing(int index) const;
  ::CoreML::Specification::NeuralNetworkPreprocessing* _internal_add_preprocessing();
  public:
  const ::CoreML::Specification::NeuralNetworkPreprocessing& preprocessing(int index) const;
  ::CoreML::Specification::NeuralNetworkPreprocessing* add_preprocessing();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
      preprocessing() const;

  // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
  bool has_updateparams() const;
  private:
  bool _internal_has_updateparams() const;
  public:
  void clear_updateparams();
  const ::CoreML::Specification::NetworkUpdateParameters& updateparams() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::NetworkUpdateParameters* release_updateparams();
  ::CoreML::Specification::NetworkUpdateParameters* mutable_updateparams();
  void set_allocated_updateparams(::CoreML::Specification::NetworkUpdateParameters* updateparams);
  private:
  const ::CoreML::Specification::NetworkUpdateParameters& _internal_updateparams() const;
  ::CoreML::Specification::NetworkUpdateParameters* _internal_mutable_updateparams();
  public:
  void unsafe_arena_set_allocated_updateparams(
      ::CoreML::Specification::NetworkUpdateParameters* updateparams);
  ::CoreML::Specification::NetworkUpdateParameters* unsafe_arena_release_updateparams();

  // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
  void clear_arrayinputshapemapping();
  ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping arrayinputshapemapping() const;
  void set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value);
  private:
  ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping _internal_arrayinputshapemapping() const;
  void _internal_set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value);
  public:

  // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
  void clear_imageinputshapemapping();
  ::CoreML::Specification::NeuralNetworkImageShapeMapping imageinputshapemapping() const;
  void set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value);
  private:
  ::CoreML::Specification::NeuralNetworkImageShapeMapping _internal_imageinputshapemapping() const;
  void _internal_set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.NeuralNetwork)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer > layers_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing > preprocessing_;
  ::CoreML::Specification::NetworkUpdateParameters* updateparams_;
  int arrayinputshapemapping_;
  int imageinputshapemapping_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class NeuralNetworkImageScaler final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NeuralNetworkImageScaler) */ {
 public:
  inline NeuralNetworkImageScaler() : NeuralNetworkImageScaler(nullptr) {}
  ~NeuralNetworkImageScaler() override;
  explicit constexpr NeuralNetworkImageScaler(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  NeuralNetworkImageScaler(const NeuralNetworkImageScaler& from);
  NeuralNetworkImageScaler(NeuralNetworkImageScaler&& from) noexcept
    : NeuralNetworkImageScaler() {
    *this = ::std::move(from);
  }

  inline NeuralNetworkImageScaler& operator=(const NeuralNetworkImageScaler& from) {
    CopyFrom(from);
    return *this;
  }
  inline NeuralNetworkImageScaler& operator=(NeuralNetworkImageScaler&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const NeuralNetworkImageScaler& default_instance() {
    return *internal_default_instance();
  }
  static inline const NeuralNetworkImageScaler* internal_default_instance() {
    return reinterpret_cast<const NeuralNetworkImageScaler*>(
               &_NeuralNetworkImageScaler_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    1;

  friend void swap(NeuralNetworkImageScaler& a, NeuralNetworkImageScaler& b) {
    a.Swap(&b);
  }
  inline void Swap(NeuralNetworkImageScaler* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(NeuralNetworkImageScaler* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  NeuralNetworkImageScaler* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<NeuralNetworkImageScaler>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const NeuralNetworkImageScaler& from);
  void MergeFrom(const NeuralNetworkImageScaler& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NeuralNetworkImageScaler* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.NeuralNetworkImageScaler";
  }
  protected:
  explicit NeuralNetworkImageScaler(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kGrayBiasFieldNumber = 30,
    kChannelScaleFieldNumber = 10,
    kBlueBiasFieldNumber = 20,
    kGreenBiasFieldNumber = 21,
    kRedBiasFieldNumber = 22,
  };
  // float grayBias = 30;
  void clear_graybias();
  float graybias() const;
  void set_graybias(float value);
  private:
  float _internal_graybias() const;
  void _internal_set_graybias(float value);
  public:

  // float channelScale = 10;
  void clear_channelscale();
  float channelscale() const;
  void set_channelscale(float value);
  private:
  float _internal_channelscale() const;
  void _internal_set_channelscale(float value);
  public:

  // float blueBias = 20;
  void clear_bluebias();
  float bluebias() const;
  void set_bluebias(float value);
  private:
  float _internal_bluebias() const;
  void _internal_set_bluebias(float value);
  public:

  // float greenBias = 21;
  void clear_greenbias();
  float greenbias() const;
  void set_greenbias(float value);
  private:
  float _internal_greenbias() const;
  void _internal_set_greenbias(float value);
  public:

  // float redBias = 22;
  void clear_redbias();
  float redbias() const;
  void set_redbias(float value);
  private:
  float _internal_redbias() const;
  void _internal_set_redbias(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.NeuralNetworkImageScaler)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float graybias_;
  float channelscale_;
  float bluebias_;
  float greenbias_;
  float redbias_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class NeuralNetworkMeanImage final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NeuralNetworkMeanImage) */ {
 public:
  inline NeuralNetworkMeanImage() : NeuralNetworkMeanImage(nullptr) {}
  ~NeuralNetworkMeanImage() override;
  explicit constexpr NeuralNetworkMeanImage(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  NeuralNetworkMeanImage(const NeuralNetworkMeanImage& from);
  NeuralNetworkMeanImage(NeuralNetworkMeanImage&& from) noexcept
    : NeuralNetworkMeanImage() {
    *this = ::std::move(from);
  }

  inline NeuralNetworkMeanImage& operator=(const NeuralNetworkMeanImage& from) {
    CopyFrom(from);
    return *this;
  }
  inline NeuralNetworkMeanImage& operator=(NeuralNetworkMeanImage&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const NeuralNetworkMeanImage& default_instance() {
    return *internal_default_instance();
  }
  static inline const NeuralNetworkMeanImage* internal_default_instance() {
    return reinterpret_cast<const NeuralNetworkMeanImage*>(
               &_NeuralNetworkMeanImage_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    2;

  friend void swap(NeuralNetworkMeanImage& a, NeuralNetworkMeanImage& b) {
    a.Swap(&b);
  }
  inline void Swap(NeuralNetworkMeanImage* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(NeuralNetworkMeanImage* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  NeuralNetworkMeanImage* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<NeuralNetworkMeanImage>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const NeuralNetworkMeanImage& from);
  void MergeFrom(const NeuralNetworkMeanImage& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NeuralNetworkMeanImage* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.NeuralNetworkMeanImage";
  }
  protected:
  explicit NeuralNetworkMeanImage(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kMeanImageFieldNumber = 1,
  };
  // repeated float meanImage = 1;
  int meanimage_size() const;
  private:
  int _internal_meanimage_size() const;
  public:
  void clear_meanimage();
  private:
  float _internal_meanimage(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
      _internal_meanimage() const;
  void _internal_add_meanimage(float value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
      _internal_mutable_meanimage();
  public:
  float meanimage(int index) const;
  void set_meanimage(int index, float value);
  void add_meanimage(float value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
      meanimage() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
      mutable_meanimage();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.NeuralNetworkMeanImage)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< float > meanimage_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class NeuralNetworkPreprocessing final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NeuralNetworkPreprocessing) */ {
 public:
  inline NeuralNetworkPreprocessing() : NeuralNetworkPreprocessing(nullptr) {}
  ~NeuralNetworkPreprocessing() override;
  explicit constexpr NeuralNetworkPreprocessing(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  NeuralNetworkPreprocessing(const NeuralNetworkPreprocessing& from);
  NeuralNetworkPreprocessing(NeuralNetworkPreprocessing&& from) noexcept
    : NeuralNetworkPreprocessing() {
    *this = ::std::move(from);
  }

  inline NeuralNetworkPreprocessing& operator=(const NeuralNetworkPreprocessing& from) {
    CopyFrom(from);
    return *this;
  }
  inline NeuralNetworkPreprocessing& operator=(NeuralNetworkPreprocessing&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const NeuralNetworkPreprocessing& default_instance() {
    return *internal_default_instance();
  }
  enum PreprocessorCase {
    kScaler = 10,
    kMeanImage = 11,
    PREPROCESSOR_NOT_SET = 0,
  };

  static inline const NeuralNetworkPreprocessing* internal_default_instance() {
    return reinterpret_cast<const NeuralNetworkPreprocessing*>(
               &_NeuralNetworkPreprocessing_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    3;

  friend void swap(NeuralNetworkPreprocessing& a, NeuralNetworkPreprocessing& b) {
    a.Swap(&b);
  }
  inline void Swap(NeuralNetworkPreprocessing* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(NeuralNetworkPreprocessing* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  NeuralNetworkPreprocessing* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<NeuralNetworkPreprocessing>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const NeuralNetworkPreprocessing& from);
  void MergeFrom(const NeuralNetworkPreprocessing& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NeuralNetworkPreprocessing* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.NeuralNetworkPreprocessing";
  }
  protected:
  explicit NeuralNetworkPreprocessing(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kFeatureNameFieldNumber = 1,
    kScalerFieldNumber = 10,
    kMeanImageFieldNumber = 11,
  };
  // string featureName = 1;
  void clear_featurename();
  const std::string& featurename() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_featurename(ArgT0&& arg0, ArgT... args);
  std::string* mutable_featurename();
  PROTOBUF_NODISCARD std::string* release_featurename();
  void set_allocated_featurename(std::string* featurename);
  private:
  const std::string& _internal_featurename() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_featurename(const std::string& value);
  std::string* _internal_mutable_featurename();
  public:

  // .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
  bool has_scaler() const;
  private:
  bool _internal_has_scaler() const;
  public:
  void clear_scaler();
  const ::CoreML::Specification::NeuralNetworkImageScaler& scaler() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::NeuralNetworkImageScaler* release_scaler();
  ::CoreML::Specification::NeuralNetworkImageScaler* mutable_scaler();
  void set_allocated_scaler(::CoreML::Specification::NeuralNetworkImageScaler* scaler);
  private:
  const ::CoreML::Specification::NeuralNetworkImageScaler& _internal_scaler() const;
  ::CoreML::Specification::NeuralNetworkImageScaler* _internal_mutable_scaler();
  public:
  void unsafe_arena_set_allocated_scaler(
      ::CoreML::Specification::NeuralNetworkImageScaler* scaler);
  ::CoreML::Specification::NeuralNetworkImageScaler* unsafe_arena_release_scaler();

  // .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
  bool has_meanimage() const;
  private:
  bool _internal_has_meanimage() const;
  public:
  void clear_meanimage();
  const ::CoreML::Specification::NeuralNetworkMeanImage& meanimage() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::NeuralNetworkMeanImage* release_meanimage();
  ::CoreML::Specification::NeuralNetworkMeanImage* mutable_meanimage();
  void set_allocated_meanimage(::CoreML::Specification::NeuralNetworkMeanImage* meanimage);
  private:
  const ::CoreML::Specification::NeuralNetworkMeanImage& _internal_meanimage() const;
  ::CoreML::Specification::NeuralNetworkMeanImage* _internal_mutable_meanimage();
  public:
  void unsafe_arena_set_allocated_meanimage(
      ::CoreML::Specification::NeuralNetworkMeanImage* meanimage);
  ::CoreML::Specification::NeuralNetworkMeanImage* unsafe_arena_release_meanimage();

  void clear_preprocessor();
  PreprocessorCase preprocessor_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.NeuralNetworkPreprocessing)
 private:
  class _Internal;
  void set_has_scaler();
  void set_has_meanimage();

  inline bool has_preprocessor() const;
  inline void clear_has_preprocessor();

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr featurename_;
  union PreprocessorUnion {
    constexpr PreprocessorUnion() : _constinit_{} {}
      ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized _constinit_;
    ::CoreML::Specification::NeuralNetworkImageScaler* scaler_;
    ::CoreML::Specification::NeuralNetworkMeanImage* meanimage_;
  } preprocessor_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  uint32_t _oneof_case_[1];

  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ActivationReLU final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationReLU) */ {
 public:
  inline ActivationReLU() : ActivationReLU(nullptr) {}
  ~ActivationReLU() override;
  explicit constexpr ActivationReLU(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ActivationReLU(const ActivationReLU& from);
  ActivationReLU(ActivationReLU&& from) noexcept
    : ActivationReLU() {
    *this = ::std::move(from);
  }

  inline ActivationReLU& operator=(const ActivationReLU& from) {
    CopyFrom(from);
    return *this;
  }
  inline ActivationReLU& operator=(ActivationReLU&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ActivationReLU& default_instance() {
    return *internal_default_instance();
  }
  static inline const ActivationReLU* internal_default_instance() {
    return reinterpret_cast<const ActivationReLU*>(
               &_ActivationReLU_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    4;

  friend void swap(ActivationReLU& a, ActivationReLU& b) {
    a.Swap(&b);
  }
  inline void Swap(ActivationReLU* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ActivationReLU* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ActivationReLU* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ActivationReLU>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ActivationReLU& from);
  void MergeFrom(const ActivationReLU& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationReLU* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ActivationReLU";
  }
  protected:
  explicit ActivationReLU(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationReLU)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ActivationLeakyReLU final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationLeakyReLU) */ {
 public:
  inline ActivationLeakyReLU() : ActivationLeakyReLU(nullptr) {}
  ~ActivationLeakyReLU() override;
  explicit constexpr ActivationLeakyReLU(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ActivationLeakyReLU(const ActivationLeakyReLU& from);
  ActivationLeakyReLU(ActivationLeakyReLU&& from) noexcept
    : ActivationLeakyReLU() {
    *this = ::std::move(from);
  }

  inline ActivationLeakyReLU& operator=(const ActivationLeakyReLU& from) {
    CopyFrom(from);
    return *this;
  }
  inline ActivationLeakyReLU& operator=(ActivationLeakyReLU&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ActivationLeakyReLU& default_instance() {
    return *internal_default_instance();
  }
  static inline const ActivationLeakyReLU* internal_default_instance() {
    return reinterpret_cast<const ActivationLeakyReLU*>(
               &_ActivationLeakyReLU_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    5;

  friend void swap(ActivationLeakyReLU& a, ActivationLeakyReLU& b) {
    a.Swap(&b);
  }
  inline void Swap(ActivationLeakyReLU* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ActivationLeakyReLU* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ActivationLeakyReLU* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ActivationLeakyReLU>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ActivationLeakyReLU& from);
  void MergeFrom(const ActivationLeakyReLU& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationLeakyReLU* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ActivationLeakyReLU";
  }
  protected:
  explicit ActivationLeakyReLU(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAlphaFieldNumber = 1,
  };
  // float alpha = 1;
  void clear_alpha();
  float alpha() const;
  void set_alpha(float value);
  private:
  float _internal_alpha() const;
  void _internal_set_alpha(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationLeakyReLU)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float alpha_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ActivationTanh final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationTanh) */ {
 public:
  inline ActivationTanh() : ActivationTanh(nullptr) {}
  ~ActivationTanh() override;
  explicit constexpr ActivationTanh(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ActivationTanh(const ActivationTanh& from);
  ActivationTanh(ActivationTanh&& from) noexcept
    : ActivationTanh() {
    *this = ::std::move(from);
  }

  inline ActivationTanh& operator=(const ActivationTanh& from) {
    CopyFrom(from);
    return *this;
  }
  inline ActivationTanh& operator=(ActivationTanh&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ActivationTanh& default_instance() {
    return *internal_default_instance();
  }
  static inline const ActivationTanh* internal_default_instance() {
    return reinterpret_cast<const ActivationTanh*>(
               &_ActivationTanh_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    6;

  friend void swap(ActivationTanh& a, ActivationTanh& b) {
    a.Swap(&b);
  }
  inline void Swap(ActivationTanh* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ActivationTanh* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ActivationTanh* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ActivationTanh>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ActivationTanh& from);
  void MergeFrom(const ActivationTanh& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationTanh* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ActivationTanh";
  }
  protected:
  explicit ActivationTanh(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationTanh)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ActivationScaledTanh final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationScaledTanh) */ {
 public:
  inline ActivationScaledTanh() : ActivationScaledTanh(nullptr) {}
  ~ActivationScaledTanh() override;
  explicit constexpr ActivationScaledTanh(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ActivationScaledTanh(const ActivationScaledTanh& from);
  ActivationScaledTanh(ActivationScaledTanh&& from) noexcept
    : ActivationScaledTanh() {
    *this = ::std::move(from);
  }

  inline ActivationScaledTanh& operator=(const ActivationScaledTanh& from) {
    CopyFrom(from);
    return *this;
  }
  inline ActivationScaledTanh& operator=(ActivationScaledTanh&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ActivationScaledTanh& default_instance() {
    return *internal_default_instance();
  }
  static inline const ActivationScaledTanh* internal_default_instance() {
    return reinterpret_cast<const ActivationScaledTanh*>(
               &_ActivationScaledTanh_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    7;

  friend void swap(ActivationScaledTanh& a, ActivationScaledTanh& b) {
    a.Swap(&b);
  }
  inline void Swap(ActivationScaledTanh* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ActivationScaledTanh* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ActivationScaledTanh* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ActivationScaledTanh>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ActivationScaledTanh& from);
  void MergeFrom(const ActivationScaledTanh& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationScaledTanh* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ActivationScaledTanh";
  }
  protected:
  explicit ActivationScaledTanh(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAlphaFieldNumber = 1,
    kBetaFieldNumber = 2,
  };
  // float alpha = 1;
  void clear_alpha();
  float alpha() const;
  void set_alpha(float value);
  private:
  float _internal_alpha() const;
  void _internal_set_alpha(float value);
  public:

  // float beta = 2;
  void clear_beta();
  float beta() const;
  void set_beta(float value);
  private:
  float _internal_beta() const;
  void _internal_set_beta(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationScaledTanh)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float alpha_;
  float beta_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ActivationSigmoid final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationSigmoid) */ {
 public:
  inline ActivationSigmoid() : ActivationSigmoid(nullptr) {}
  ~ActivationSigmoid() override;
  explicit constexpr ActivationSigmoid(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ActivationSigmoid(const ActivationSigmoid& from);
  ActivationSigmoid(ActivationSigmoid&& from) noexcept
    : ActivationSigmoid() {
    *this = ::std::move(from);
  }

  inline ActivationSigmoid& operator=(const ActivationSigmoid& from) {
    CopyFrom(from);
    return *this;
  }
  inline ActivationSigmoid& operator=(ActivationSigmoid&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ActivationSigmoid& default_instance() {
    return *internal_default_instance();
  }
  static inline const ActivationSigmoid* internal_default_instance() {
    return reinterpret_cast<const ActivationSigmoid*>(
               &_ActivationSigmoid_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    8;

  friend void swap(ActivationSigmoid& a, ActivationSigmoid& b) {
    a.Swap(&b);
  }
  inline void Swap(ActivationSigmoid* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ActivationSigmoid* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ActivationSigmoid* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ActivationSigmoid>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ActivationSigmoid& from);
  void MergeFrom(const ActivationSigmoid& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationSigmoid* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ActivationSigmoid";
  }
  protected:
  explicit ActivationSigmoid(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationSigmoid)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ActivationLinear final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationLinear) */ {
 public:
  inline ActivationLinear() : ActivationLinear(nullptr) {}
  ~ActivationLinear() override;
  explicit constexpr ActivationLinear(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ActivationLinear(const ActivationLinear& from);
  ActivationLinear(ActivationLinear&& from) noexcept
    : ActivationLinear() {
    *this = ::std::move(from);
  }

  inline ActivationLinear& operator=(const ActivationLinear& from) {
    CopyFrom(from);
    return *this;
  }
  inline ActivationLinear& operator=(ActivationLinear&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ActivationLinear& default_instance() {
    return *internal_default_instance();
  }
  static inline const ActivationLinear* internal_default_instance() {
    return reinterpret_cast<const ActivationLinear*>(
               &_ActivationLinear_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    9;

  friend void swap(ActivationLinear& a, ActivationLinear& b) {
    a.Swap(&b);
  }
  inline void Swap(ActivationLinear* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ActivationLinear* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ActivationLinear* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ActivationLinear>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ActivationLinear& from);
  void MergeFrom(const ActivationLinear& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationLinear* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ActivationLinear";
  }
  protected:
  explicit ActivationLinear(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAlphaFieldNumber = 1,
    kBetaFieldNumber = 2,
  };
  // float alpha = 1;
  void clear_alpha();
  float alpha() const;
  void set_alpha(float value);
  private:
  float _internal_alpha() const;
  void _internal_set_alpha(float value);
  public:

  // float beta = 2;
  void clear_beta();
  float beta() const;
  void set_beta(float value);
  private:
  float _internal_beta() const;
  void _internal_set_beta(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationLinear)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float alpha_;
  float beta_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ActivationSigmoidHard final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationSigmoidHard) */ {
 public:
  inline ActivationSigmoidHard() : ActivationSigmoidHard(nullptr) {}
  ~ActivationSigmoidHard() override;
  explicit constexpr ActivationSigmoidHard(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ActivationSigmoidHard(const ActivationSigmoidHard& from);
  ActivationSigmoidHard(ActivationSigmoidHard&& from) noexcept
    : ActivationSigmoidHard() {
    *this = ::std::move(from);
  }

  inline ActivationSigmoidHard& operator=(const ActivationSigmoidHard& from) {
    CopyFrom(from);
    return *this;
  }
  inline ActivationSigmoidHard& operator=(ActivationSigmoidHard&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ActivationSigmoidHard& default_instance() {
    return *internal_default_instance();
  }
  static inline const ActivationSigmoidHard* internal_default_instance() {
    return reinterpret_cast<const ActivationSigmoidHard*>(
               &_ActivationSigmoidHard_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    10;

  friend void swap(ActivationSigmoidHard& a, ActivationSigmoidHard& b) {
    a.Swap(&b);
  }
  inline void Swap(ActivationSigmoidHard* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ActivationSigmoidHard* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ActivationSigmoidHard* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ActivationSigmoidHard>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ActivationSigmoidHard& from);
  void MergeFrom(const ActivationSigmoidHard& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationSigmoidHard* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ActivationSigmoidHard";
  }
  protected:
  explicit ActivationSigmoidHard(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAlphaFieldNumber = 1,
    kBetaFieldNumber = 2,
  };
  // float alpha = 1;
  void clear_alpha();
  float alpha() const;
  void set_alpha(float value);
  private:
  float _internal_alpha() const;
  void _internal_set_alpha(float value);
  public:

  // float beta = 2;
  void clear_beta();
  float beta() const;
  void set_beta(float value);
  private:
  float _internal_beta() const;
  void _internal_set_beta(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationSigmoidHard)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float alpha_;
  float beta_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ActivationPReLU final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationPReLU) */ {
 public:
  inline ActivationPReLU() : ActivationPReLU(nullptr) {}
  ~ActivationPReLU() override;
  explicit constexpr ActivationPReLU(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ActivationPReLU(const ActivationPReLU& from);
  ActivationPReLU(ActivationPReLU&& from) noexcept
    : ActivationPReLU() {
    *this = ::std::move(from);
  }

  inline ActivationPReLU& operator=(const ActivationPReLU& from) {
    CopyFrom(from);
    return *this;
  }
  inline ActivationPReLU& operator=(ActivationPReLU&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ActivationPReLU& default_instance() {
    return *internal_default_instance();
  }
  static inline const ActivationPReLU* internal_default_instance() {
    return reinterpret_cast<const ActivationPReLU*>(
               &_ActivationPReLU_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    11;

  friend void swap(ActivationPReLU& a, ActivationPReLU& b) {
    a.Swap(&b);
  }
  inline void Swap(ActivationPReLU* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ActivationPReLU* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ActivationPReLU* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ActivationPReLU>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ActivationPReLU& from);
  void MergeFrom(const ActivationPReLU& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationPReLU* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ActivationPReLU";
  }
  protected:
  explicit ActivationPReLU(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAlphaFieldNumber = 1,
  };
  // .CoreML.Specification.WeightParams alpha = 1;
  bool has_alpha() const;
  private:
  bool _internal_has_alpha() const;
  public:
  void clear_alpha();
  const ::CoreML::Specification::WeightParams& alpha() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_alpha();
  ::CoreML::Specification::WeightParams* mutable_alpha();
  void set_allocated_alpha(::CoreML::Specification::WeightParams* alpha);
  private:
  const ::CoreML::Specification::WeightParams& _internal_alpha() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_alpha();
  public:
  void unsafe_arena_set_allocated_alpha(
      ::CoreML::Specification::WeightParams* alpha);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_alpha();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationPReLU)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::CoreML::Specification::WeightParams* alpha_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ActivationELU final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationELU) */ {
 public:
  inline ActivationELU() : ActivationELU(nullptr) {}
  ~ActivationELU() override;
  explicit constexpr ActivationELU(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ActivationELU(const ActivationELU& from);
  ActivationELU(ActivationELU&& from) noexcept
    : ActivationELU() {
    *this = ::std::move(from);
  }

  inline ActivationELU& operator=(const ActivationELU& from) {
    CopyFrom(from);
    return *this;
  }
  inline ActivationELU& operator=(ActivationELU&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ActivationELU& default_instance() {
    return *internal_default_instance();
  }
  static inline const ActivationELU* internal_default_instance() {
    return reinterpret_cast<const ActivationELU*>(
               &_ActivationELU_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    12;

  friend void swap(ActivationELU& a, ActivationELU& b) {
    a.Swap(&b);
  }
  inline void Swap(ActivationELU* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ActivationELU* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ActivationELU* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ActivationELU>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ActivationELU& from);
  void MergeFrom(const ActivationELU& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationELU* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ActivationELU";
  }
  protected:
  explicit ActivationELU(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAlphaFieldNumber = 1,
  };
  // float alpha = 1;
  void clear_alpha();
  float alpha() const;
  void set_alpha(float value);
  private:
  float _internal_alpha() const;
  void _internal_set_alpha(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationELU)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float alpha_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ActivationThresholdedReLU final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationThresholdedReLU) */ {
 public:
  inline ActivationThresholdedReLU() : ActivationThresholdedReLU(nullptr) {}
  ~ActivationThresholdedReLU() override;
  explicit constexpr ActivationThresholdedReLU(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ActivationThresholdedReLU(const ActivationThresholdedReLU& from);
  ActivationThresholdedReLU(ActivationThresholdedReLU&& from) noexcept
    : ActivationThresholdedReLU() {
    *this = ::std::move(from);
  }

  inline ActivationThresholdedReLU& operator=(const ActivationThresholdedReLU& from) {
    CopyFrom(from);
    return *this;
  }
  inline ActivationThresholdedReLU& operator=(ActivationThresholdedReLU&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ActivationThresholdedReLU& default_instance() {
    return *internal_default_instance();
  }
  static inline const ActivationThresholdedReLU* internal_default_instance() {
    return reinterpret_cast<const ActivationThresholdedReLU*>(
               &_ActivationThresholdedReLU_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    13;

  friend void swap(ActivationThresholdedReLU& a, ActivationThresholdedReLU& b) {
    a.Swap(&b);
  }
  inline void Swap(ActivationThresholdedReLU* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ActivationThresholdedReLU* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ActivationThresholdedReLU* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ActivationThresholdedReLU>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ActivationThresholdedReLU& from);
  void MergeFrom(const ActivationThresholdedReLU& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationThresholdedReLU* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ActivationThresholdedReLU";
  }
  protected:
  explicit ActivationThresholdedReLU(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAlphaFieldNumber = 1,
  };
  // float alpha = 1;
  void clear_alpha();
  float alpha() const;
  void set_alpha(float value);
  private:
  float _internal_alpha() const;
  void _internal_set_alpha(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationThresholdedReLU)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float alpha_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ActivationSoftsign final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationSoftsign) */ {
 public:
  inline ActivationSoftsign() : ActivationSoftsign(nullptr) {}
  ~ActivationSoftsign() override;
  explicit constexpr ActivationSoftsign(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ActivationSoftsign(const ActivationSoftsign& from);
  ActivationSoftsign(ActivationSoftsign&& from) noexcept
    : ActivationSoftsign() {
    *this = ::std::move(from);
  }

  inline ActivationSoftsign& operator=(const ActivationSoftsign& from) {
    CopyFrom(from);
    return *this;
  }
  inline ActivationSoftsign& operator=(ActivationSoftsign&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ActivationSoftsign& default_instance() {
    return *internal_default_instance();
  }
  static inline const ActivationSoftsign* internal_default_instance() {
    return reinterpret_cast<const ActivationSoftsign*>(
               &_ActivationSoftsign_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    14;

  friend void swap(ActivationSoftsign& a, ActivationSoftsign& b) {
    a.Swap(&b);
  }
  inline void Swap(ActivationSoftsign* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ActivationSoftsign* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ActivationSoftsign* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ActivationSoftsign>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ActivationSoftsign& from);
  void MergeFrom(const ActivationSoftsign& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationSoftsign* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ActivationSoftsign";
  }
  protected:
  explicit ActivationSoftsign(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationSoftsign)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ActivationSoftplus final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationSoftplus) */ {
 public:
  inline ActivationSoftplus() : ActivationSoftplus(nullptr) {}
  ~ActivationSoftplus() override;
  explicit constexpr ActivationSoftplus(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ActivationSoftplus(const ActivationSoftplus& from);
  ActivationSoftplus(ActivationSoftplus&& from) noexcept
    : ActivationSoftplus() {
    *this = ::std::move(from);
  }

  inline ActivationSoftplus& operator=(const ActivationSoftplus& from) {
    CopyFrom(from);
    return *this;
  }
  inline ActivationSoftplus& operator=(ActivationSoftplus&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ActivationSoftplus& default_instance() {
    return *internal_default_instance();
  }
  static inline const ActivationSoftplus* internal_default_instance() {
    return reinterpret_cast<const ActivationSoftplus*>(
               &_ActivationSoftplus_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    15;

  friend void swap(ActivationSoftplus& a, ActivationSoftplus& b) {
    a.Swap(&b);
  }
  inline void Swap(ActivationSoftplus* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ActivationSoftplus* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ActivationSoftplus* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ActivationSoftplus>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ActivationSoftplus& from);
  void MergeFrom(const ActivationSoftplus& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationSoftplus* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ActivationSoftplus";
  }
  protected:
  explicit ActivationSoftplus(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationSoftplus)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ActivationParametricSoftplus final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationParametricSoftplus) */ {
 public:
  inline ActivationParametricSoftplus() : ActivationParametricSoftplus(nullptr) {}
  ~ActivationParametricSoftplus() override;
  explicit constexpr ActivationParametricSoftplus(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ActivationParametricSoftplus(const ActivationParametricSoftplus& from);
  ActivationParametricSoftplus(ActivationParametricSoftplus&& from) noexcept
    : ActivationParametricSoftplus() {
    *this = ::std::move(from);
  }

  inline ActivationParametricSoftplus& operator=(const ActivationParametricSoftplus& from) {
    CopyFrom(from);
    return *this;
  }
  inline ActivationParametricSoftplus& operator=(ActivationParametricSoftplus&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ActivationParametricSoftplus& default_instance() {
    return *internal_default_instance();
  }
  static inline const ActivationParametricSoftplus* internal_default_instance() {
    return reinterpret_cast<const ActivationParametricSoftplus*>(
               &_ActivationParametricSoftplus_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    16;

  friend void swap(ActivationParametricSoftplus& a, ActivationParametricSoftplus& b) {
    a.Swap(&b);
  }
  inline void Swap(ActivationParametricSoftplus* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ActivationParametricSoftplus* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ActivationParametricSoftplus* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ActivationParametricSoftplus>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ActivationParametricSoftplus& from);
  void MergeFrom(const ActivationParametricSoftplus& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationParametricSoftplus* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ActivationParametricSoftplus";
  }
  protected:
  explicit ActivationParametricSoftplus(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAlphaFieldNumber = 1,
    kBetaFieldNumber = 2,
  };
  // .CoreML.Specification.WeightParams alpha = 1;
  bool has_alpha() const;
  private:
  bool _internal_has_alpha() const;
  public:
  void clear_alpha();
  const ::CoreML::Specification::WeightParams& alpha() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_alpha();
  ::CoreML::Specification::WeightParams* mutable_alpha();
  void set_allocated_alpha(::CoreML::Specification::WeightParams* alpha);
  private:
  const ::CoreML::Specification::WeightParams& _internal_alpha() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_alpha();
  public:
  void unsafe_arena_set_allocated_alpha(
      ::CoreML::Specification::WeightParams* alpha);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_alpha();

  // .CoreML.Specification.WeightParams beta = 2;
  bool has_beta() const;
  private:
  bool _internal_has_beta() const;
  public:
  void clear_beta();
  const ::CoreML::Specification::WeightParams& beta() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_beta();
  ::CoreML::Specification::WeightParams* mutable_beta();
  void set_allocated_beta(::CoreML::Specification::WeightParams* beta);
  private:
  const ::CoreML::Specification::WeightParams& _internal_beta() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_beta();
  public:
  void unsafe_arena_set_allocated_beta(
      ::CoreML::Specification::WeightParams* beta);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_beta();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationParametricSoftplus)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::CoreML::Specification::WeightParams* alpha_;
  ::CoreML::Specification::WeightParams* beta_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ActivationParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationParams) */ {
 public:
  inline ActivationParams() : ActivationParams(nullptr) {}
  ~ActivationParams() override;
  explicit constexpr ActivationParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ActivationParams(const ActivationParams& from);
  ActivationParams(ActivationParams&& from) noexcept
    : ActivationParams() {
    *this = ::std::move(from);
  }

  inline ActivationParams& operator=(const ActivationParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ActivationParams& operator=(ActivationParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ActivationParams& default_instance() {
    return *internal_default_instance();
  }
  enum NonlinearityTypeCase {
    kLinear = 5,
    kReLU = 10,
    kLeakyReLU = 15,
    kThresholdedReLU = 20,
    kPReLU = 25,
    kTanh = 30,
    kScaledTanh = 31,
    kSigmoid = 40,
    kSigmoidHard = 41,
    kELU = 50,
    kSoftsign = 60,
    kSoftplus = 70,
    kParametricSoftplus = 71,
    NONLINEARITYTYPE_NOT_SET = 0,
  };

  static inline const ActivationParams* internal_default_instance() {
    return reinterpret_cast<const ActivationParams*>(
               &_ActivationParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    17;

  friend void swap(ActivationParams& a, ActivationParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ActivationParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ActivationParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ActivationParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ActivationParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ActivationParams& from);
  void MergeFrom(const ActivationParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ActivationParams";
  }
  protected:
  explicit ActivationParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kLinearFieldNumber = 5,
    kReLUFieldNumber = 10,
    kLeakyReLUFieldNumber = 15,
    kThresholdedReLUFieldNumber = 20,
    kPReLUFieldNumber = 25,
    kTanhFieldNumber = 30,
    kScaledTanhFieldNumber = 31,
    kSigmoidFieldNumber = 40,
    kSigmoidHardFieldNumber = 41,
    kELUFieldNumber = 50,
    kSoftsignFieldNumber = 60,
    kSoftplusFieldNumber = 70,
    kParametricSoftplusFieldNumber = 71,
  };
  // .CoreML.Specification.ActivationLinear linear = 5;
  bool has_linear() const;
  private:
  bool _internal_has_linear() const;
  public:
  void clear_linear();
  const ::CoreML::Specification::ActivationLinear& linear() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ActivationLinear* release_linear();
  ::CoreML::Specification::ActivationLinear* mutable_linear();
  void set_allocated_linear(::CoreML::Specification::ActivationLinear* linear);
  private:
  const ::CoreML::Specification::ActivationLinear& _internal_linear() const;
  ::CoreML::Specification::ActivationLinear* _internal_mutable_linear();
  public:
  void unsafe_arena_set_allocated_linear(
      ::CoreML::Specification::ActivationLinear* linear);
  ::CoreML::Specification::ActivationLinear* unsafe_arena_release_linear();

  // .CoreML.Specification.ActivationReLU ReLU = 10;
  bool has_relu() const;
  private:
  bool _internal_has_relu() const;
  public:
  void clear_relu();
  const ::CoreML::Specification::ActivationReLU& relu() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ActivationReLU* release_relu();
  ::CoreML::Specification::ActivationReLU* mutable_relu();
  void set_allocated_relu(::CoreML::Specification::ActivationReLU* relu);
  private:
  const ::CoreML::Specification::ActivationReLU& _internal_relu() const;
  ::CoreML::Specification::ActivationReLU* _internal_mutable_relu();
  public:
  void unsafe_arena_set_allocated_relu(
      ::CoreML::Specification::ActivationReLU* relu);
  ::CoreML::Specification::ActivationReLU* unsafe_arena_release_relu();

  // .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
  bool has_leakyrelu() const;
  private:
  bool _internal_has_leakyrelu() const;
  public:
  void clear_leakyrelu();
  const ::CoreML::Specification::ActivationLeakyReLU& leakyrelu() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ActivationLeakyReLU* release_leakyrelu();
  ::CoreML::Specification::ActivationLeakyReLU* mutable_leakyrelu();
  void set_allocated_leakyrelu(::CoreML::Specification::ActivationLeakyReLU* leakyrelu);
  private:
  const ::CoreML::Specification::ActivationLeakyReLU& _internal_leakyrelu() const;
  ::CoreML::Specification::ActivationLeakyReLU* _internal_mutable_leakyrelu();
  public:
  void unsafe_arena_set_allocated_leakyrelu(
      ::CoreML::Specification::ActivationLeakyReLU* leakyrelu);
  ::CoreML::Specification::ActivationLeakyReLU* unsafe_arena_release_leakyrelu();

  // .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
  bool has_thresholdedrelu() const;
  private:
  bool _internal_has_thresholdedrelu() const;
  public:
  void clear_thresholdedrelu();
  const ::CoreML::Specification::ActivationThresholdedReLU& thresholdedrelu() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ActivationThresholdedReLU* release_thresholdedrelu();
  ::CoreML::Specification::ActivationThresholdedReLU* mutable_thresholdedrelu();
  void set_allocated_thresholdedrelu(::CoreML::Specification::ActivationThresholdedReLU* thresholdedrelu);
  private:
  const ::CoreML::Specification::ActivationThresholdedReLU& _internal_thresholdedrelu() const;
  ::CoreML::Specification::ActivationThresholdedReLU* _internal_mutable_thresholdedrelu();
  public:
  void unsafe_arena_set_allocated_thresholdedrelu(
      ::CoreML::Specification::ActivationThresholdedReLU* thresholdedrelu);
  ::CoreML::Specification::ActivationThresholdedReLU* unsafe_arena_release_thresholdedrelu();

  // .CoreML.Specification.ActivationPReLU PReLU = 25;
  bool has_prelu() const;
  private:
  bool _internal_has_prelu() const;
  public:
  void clear_prelu();
  const ::CoreML::Specification::ActivationPReLU& prelu() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ActivationPReLU* release_prelu();
  ::CoreML::Specification::ActivationPReLU* mutable_prelu();
  void set_allocated_prelu(::CoreML::Specification::ActivationPReLU* prelu);
  private:
  const ::CoreML::Specification::ActivationPReLU& _internal_prelu() const;
  ::CoreML::Specification::ActivationPReLU* _internal_mutable_prelu();
  public:
  void unsafe_arena_set_allocated_prelu(
      ::CoreML::Specification::ActivationPReLU* prelu);
  ::CoreML::Specification::ActivationPReLU* unsafe_arena_release_prelu();

  // .CoreML.Specification.ActivationTanh tanh = 30;
  bool has_tanh() const;
  private:
  bool _internal_has_tanh() const;
  public:
  void clear_tanh();
  const ::CoreML::Specification::ActivationTanh& tanh() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ActivationTanh* release_tanh();
  ::CoreML::Specification::ActivationTanh* mutable_tanh();
  void set_allocated_tanh(::CoreML::Specification::ActivationTanh* tanh);
  private:
  const ::CoreML::Specification::ActivationTanh& _internal_tanh() const;
  ::CoreML::Specification::ActivationTanh* _internal_mutable_tanh();
  public:
  void unsafe_arena_set_allocated_tanh(
      ::CoreML::Specification::ActivationTanh* tanh);
  ::CoreML::Specification::ActivationTanh* unsafe_arena_release_tanh();

  // .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
  bool has_scaledtanh() const;
  private:
  bool _internal_has_scaledtanh() const;
  public:
  void clear_scaledtanh();
  const ::CoreML::Specification::ActivationScaledTanh& scaledtanh() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ActivationScaledTanh* release_scaledtanh();
  ::CoreML::Specification::ActivationScaledTanh* mutable_scaledtanh();
  void set_allocated_scaledtanh(::CoreML::Specification::ActivationScaledTanh* scaledtanh);
  private:
  const ::CoreML::Specification::ActivationScaledTanh& _internal_scaledtanh() const;
  ::CoreML::Specification::ActivationScaledTanh* _internal_mutable_scaledtanh();
  public:
  void unsafe_arena_set_allocated_scaledtanh(
      ::CoreML::Specification::ActivationScaledTanh* scaledtanh);
  ::CoreML::Specification::ActivationScaledTanh* unsafe_arena_release_scaledtanh();

  // .CoreML.Specification.ActivationSigmoid sigmoid = 40;
  bool has_sigmoid() const;
  private:
  bool _internal_has_sigmoid() const;
  public:
  void clear_sigmoid();
  const ::CoreML::Specification::ActivationSigmoid& sigmoid() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ActivationSigmoid* release_sigmoid();
  ::CoreML::Specification::ActivationSigmoid* mutable_sigmoid();
  void set_allocated_sigmoid(::CoreML::Specification::ActivationSigmoid* sigmoid);
  private:
  const ::CoreML::Specification::ActivationSigmoid& _internal_sigmoid() const;
  ::CoreML::Specification::ActivationSigmoid* _internal_mutable_sigmoid();
  public:
  void unsafe_arena_set_allocated_sigmoid(
      ::CoreML::Specification::ActivationSigmoid* sigmoid);
  ::CoreML::Specification::ActivationSigmoid* unsafe_arena_release_sigmoid();

  // .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
  bool has_sigmoidhard() const;
  private:
  bool _internal_has_sigmoidhard() const;
  public:
  void clear_sigmoidhard();
  const ::CoreML::Specification::ActivationSigmoidHard& sigmoidhard() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ActivationSigmoidHard* release_sigmoidhard();
  ::CoreML::Specification::ActivationSigmoidHard* mutable_sigmoidhard();
  void set_allocated_sigmoidhard(::CoreML::Specification::ActivationSigmoidHard* sigmoidhard);
  private:
  const ::CoreML::Specification::ActivationSigmoidHard& _internal_sigmoidhard() const;
  ::CoreML::Specification::ActivationSigmoidHard* _internal_mutable_sigmoidhard();
  public:
  void unsafe_arena_set_allocated_sigmoidhard(
      ::CoreML::Specification::ActivationSigmoidHard* sigmoidhard);
  ::CoreML::Specification::ActivationSigmoidHard* unsafe_arena_release_sigmoidhard();

  // .CoreML.Specification.ActivationELU ELU = 50;
  bool has_elu() const;
  private:
  bool _internal_has_elu() const;
  public:
  void clear_elu();
  const ::CoreML::Specification::ActivationELU& elu() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ActivationELU* release_elu();
  ::CoreML::Specification::ActivationELU* mutable_elu();
  void set_allocated_elu(::CoreML::Specification::ActivationELU* elu);
  private:
  const ::CoreML::Specification::ActivationELU& _internal_elu() const;
  ::CoreML::Specification::ActivationELU* _internal_mutable_elu();
  public:
  void unsafe_arena_set_allocated_elu(
      ::CoreML::Specification::ActivationELU* elu);
  ::CoreML::Specification::ActivationELU* unsafe_arena_release_elu();

  // .CoreML.Specification.ActivationSoftsign softsign = 60;
  bool has_softsign() const;
  private:
  bool _internal_has_softsign() const;
  public:
  void clear_softsign();
  const ::CoreML::Specification::ActivationSoftsign& softsign() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ActivationSoftsign* release_softsign();
  ::CoreML::Specification::ActivationSoftsign* mutable_softsign();
  void set_allocated_softsign(::CoreML::Specification::ActivationSoftsign* softsign);
  private:
  const ::CoreML::Specification::ActivationSoftsign& _internal_softsign() const;
  ::CoreML::Specification::ActivationSoftsign* _internal_mutable_softsign();
  public:
  void unsafe_arena_set_allocated_softsign(
      ::CoreML::Specification::ActivationSoftsign* softsign);
  ::CoreML::Specification::ActivationSoftsign* unsafe_arena_release_softsign();

  // .CoreML.Specification.ActivationSoftplus softplus = 70;
  bool has_softplus() const;
  private:
  bool _internal_has_softplus() const;
  public:
  void clear_softplus();
  const ::CoreML::Specification::ActivationSoftplus& softplus() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ActivationSoftplus* release_softplus();
  ::CoreML::Specification::ActivationSoftplus* mutable_softplus();
  void set_allocated_softplus(::CoreML::Specification::ActivationSoftplus* softplus);
  private:
  const ::CoreML::Specification::ActivationSoftplus& _internal_softplus() const;
  ::CoreML::Specification::ActivationSoftplus* _internal_mutable_softplus();
  public:
  void unsafe_arena_set_allocated_softplus(
      ::CoreML::Specification::ActivationSoftplus* softplus);
  ::CoreML::Specification::ActivationSoftplus* unsafe_arena_release_softplus();

  // .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
  bool has_parametricsoftplus() const;
  private:
  bool _internal_has_parametricsoftplus() const;
  public:
  void clear_parametricsoftplus();
  const ::CoreML::Specification::ActivationParametricSoftplus& parametricsoftplus() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ActivationParametricSoftplus* release_parametricsoftplus();
  ::CoreML::Specification::ActivationParametricSoftplus* mutable_parametricsoftplus();
  void set_allocated_parametricsoftplus(::CoreML::Specification::ActivationParametricSoftplus* parametricsoftplus);
  private:
  const ::CoreML::Specification::ActivationParametricSoftplus& _internal_parametricsoftplus() const;
  ::CoreML::Specification::ActivationParametricSoftplus* _internal_mutable_parametricsoftplus();
  public:
  void unsafe_arena_set_allocated_parametricsoftplus(
      ::CoreML::Specification::ActivationParametricSoftplus* parametricsoftplus);
  ::CoreML::Specification::ActivationParametricSoftplus* unsafe_arena_release_parametricsoftplus();

  void clear_NonlinearityType();
  NonlinearityTypeCase NonlinearityType_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationParams)
 private:
  class _Internal;
  void set_has_linear();
  void set_has_relu();
  void set_has_leakyrelu();
  void set_has_thresholdedrelu();
  void set_has_prelu();
  void set_has_tanh();
  void set_has_scaledtanh();
  void set_has_sigmoid();
  void set_has_sigmoidhard();
  void set_has_elu();
  void set_has_softsign();
  void set_has_softplus();
  void set_has_parametricsoftplus();

  inline bool has_NonlinearityType() const;
  inline void clear_has_NonlinearityType();

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  union NonlinearityTypeUnion {
    constexpr NonlinearityTypeUnion() : _constinit_{} {}
      ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized _constinit_;
    ::CoreML::Specification::ActivationLinear* linear_;
    ::CoreML::Specification::ActivationReLU* relu_;
    ::CoreML::Specification::ActivationLeakyReLU* leakyrelu_;
    ::CoreML::Specification::ActivationThresholdedReLU* thresholdedrelu_;
    ::CoreML::Specification::ActivationPReLU* prelu_;
    ::CoreML::Specification::ActivationTanh* tanh_;
    ::CoreML::Specification::ActivationScaledTanh* scaledtanh_;
    ::CoreML::Specification::ActivationSigmoid* sigmoid_;
    ::CoreML::Specification::ActivationSigmoidHard* sigmoidhard_;
    ::CoreML::Specification::ActivationELU* elu_;
    ::CoreML::Specification::ActivationSoftsign* softsign_;
    ::CoreML::Specification::ActivationSoftplus* softplus_;
    ::CoreML::Specification::ActivationParametricSoftplus* parametricsoftplus_;
  } NonlinearityType_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  uint32_t _oneof_case_[1];

  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class Tensor final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.Tensor) */ {
 public:
  inline Tensor() : Tensor(nullptr) {}
  ~Tensor() override;
  explicit constexpr Tensor(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  Tensor(const Tensor& from);
  Tensor(Tensor&& from) noexcept
    : Tensor() {
    *this = ::std::move(from);
  }

  inline Tensor& operator=(const Tensor& from) {
    CopyFrom(from);
    return *this;
  }
  inline Tensor& operator=(Tensor&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const Tensor& default_instance() {
    return *internal_default_instance();
  }
  static inline const Tensor* internal_default_instance() {
    return reinterpret_cast<const Tensor*>(
               &_Tensor_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    18;

  friend void swap(Tensor& a, Tensor& b) {
    a.Swap(&b);
  }
  inline void Swap(Tensor* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(Tensor* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  Tensor* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<Tensor>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const Tensor& from);
  void MergeFrom(const Tensor& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(Tensor* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.Tensor";
  }
  protected:
  explicit Tensor(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kDimValueFieldNumber = 2,
    kRankFieldNumber = 1,
  };
  // repeated int64 dimValue = 2;
  int dimvalue_size() const;
  private:
  int _internal_dimvalue_size() const;
  public:
  void clear_dimvalue();
  private:
  int64_t _internal_dimvalue(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_dimvalue() const;
  void _internal_add_dimvalue(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_dimvalue();
  public:
  int64_t dimvalue(int index) const;
  void set_dimvalue(int index, int64_t value);
  void add_dimvalue(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      dimvalue() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_dimvalue();

  // uint32 rank = 1;
  void clear_rank();
  uint32_t rank() const;
  void set_rank(uint32_t value);
  private:
  uint32_t _internal_rank() const;
  void _internal_set_rank(uint32_t value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.Tensor)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > dimvalue_;
  mutable std::atomic<int> _dimvalue_cached_byte_size_;
  uint32_t rank_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class NeuralNetworkLayer final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NeuralNetworkLayer) */ {
 public:
  inline NeuralNetworkLayer() : NeuralNetworkLayer(nullptr) {}
  ~NeuralNetworkLayer() override;
  explicit constexpr NeuralNetworkLayer(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  NeuralNetworkLayer(const NeuralNetworkLayer& from);
  NeuralNetworkLayer(NeuralNetworkLayer&& from) noexcept
    : NeuralNetworkLayer() {
    *this = ::std::move(from);
  }

  inline NeuralNetworkLayer& operator=(const NeuralNetworkLayer& from) {
    CopyFrom(from);
    return *this;
  }
  inline NeuralNetworkLayer& operator=(NeuralNetworkLayer&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const NeuralNetworkLayer& default_instance() {
    return *internal_default_instance();
  }
  enum LayerCase {
    kConvolution = 100,
    kPooling = 120,
    kActivation = 130,
    kInnerProduct = 140,
    kEmbedding = 150,
    kBatchnorm = 160,
    kMvn = 165,
    kL2Normalize = 170,
    kSoftmax = 175,
    kLrn = 180,
    kCrop = 190,
    kPadding = 200,
    kUpsample = 210,
    kResizeBilinear = 211,
    kCropResize = 212,
    kUnary = 220,
    kAdd = 230,
    kMultiply = 231,
    kAverage = 240,
    kScale = 245,
    kBias = 250,
    kMax = 260,
    kMin = 261,
    kDot = 270,
    kReduce = 280,
    kLoadConstant = 290,
    kReshape = 300,
    kFlatten = 301,
    kPermute = 310,
    kConcat = 320,
    kSplit = 330,
    kSequenceRepeat = 340,
    kReorganizeData = 345,
    kSlice = 350,
    kSimpleRecurrent = 400,
    kGru = 410,
    kUniDirectionalLSTM = 420,
    kBiDirectionalLSTM = 430,
    kCustom = 500,
    kCopy = 600,
    kBranch = 605,
    kLoop = 615,
    kLoopBreak = 620,
    kLoopContinue = 625,
    kRangeStatic = 635,
    kRangeDynamic = 640,
    kClip = 660,
    kCeil = 665,
    kFloor = 670,
    kSign = 680,
    kRound = 685,
    kExp2 = 700,
    kSin = 710,
    kCos = 715,
    kTan = 720,
    kAsin = 730,
    kAcos = 735,
    kAtan = 740,
    kSinh = 750,
    kCosh = 755,
    kTanh = 760,
    kAsinh = 770,
    kAcosh = 775,
    kAtanh = 780,
    kErf = 790,
    kGelu = 795,
    kEqual = 815,
    kNotEqual = 820,
    kLessThan = 825,
    kLessEqual = 827,
    kGreaterThan = 830,
    kGreaterEqual = 832,
    kLogicalOr = 840,
    kLogicalXor = 845,
    kLogicalNot = 850,
    kLogicalAnd = 855,
    kModBroadcastable = 865,
    kMinBroadcastable = 870,
    kMaxBroadcastable = 875,
    kAddBroadcastable = 880,
    kPowBroadcastable = 885,
    kDivideBroadcastable = 890,
    kFloorDivBroadcastable = 895,
    kMultiplyBroadcastable = 900,
    kSubtractBroadcastable = 905,
    kTile = 920,
    kStack = 925,
    kGather = 930,
    kScatter = 935,
    kGatherND = 940,
    kScatterND = 945,
    kSoftmaxND = 950,
    kGatherAlongAxis = 952,
    kScatterAlongAxis = 954,
    kReverse = 960,
    kReverseSeq = 965,
    kSplitND = 975,
    kConcatND = 980,
    kTranspose = 985,
    kSliceStatic = 995,
    kSliceDynamic = 1000,
    kSlidingWindows = 1005,
    kTopK = 1015,
    kArgMin = 1020,
    kArgMax = 1025,
    kEmbeddingND = 1040,
    kBatchedMatmul = 1045,
    kGetShape = 1065,
    kLoadConstantND = 1070,
    kFillLike = 1080,
    kFillStatic = 1085,
    kFillDynamic = 1090,
    kBroadcastToLike = 1100,
    kBroadcastToStatic = 1105,
    kBroadcastToDynamic = 1110,
    kSqueeze = 1120,
    kExpandDims = 1125,
    kFlattenTo2D = 1130,
    kReshapeLike = 1135,
    kReshapeStatic = 1140,
    kReshapeDynamic = 1145,
    kRankPreservingReshape = 1150,
    kConstantPad = 1155,
    kRandomNormalLike = 1170,
    kRandomNormalStatic = 1175,
    kRandomNormalDynamic = 1180,
    kRandomUniformLike = 1190,
    kRandomUniformStatic = 1195,
    kRandomUniformDynamic = 1200,
    kRandomBernoulliLike = 1210,
    kRandomBernoulliStatic = 1215,
    kRandomBernoulliDynamic = 1220,
    kCategoricalDistribution = 1230,
    kReduceL1 = 1250,
    kReduceL2 = 1255,
    kReduceMax = 1260,
    kReduceMin = 1265,
    kReduceSum = 1270,
    kReduceProd = 1275,
    kReduceMean = 1280,
    kReduceLogSum = 1285,
    kReduceSumSquare = 1290,
    kReduceLogSumExp = 1295,
    kWhereNonZero = 1313,
    kMatrixBandPart = 1315,
    kLowerTriangular = 1320,
    kUpperTriangular = 1325,
    kWhereBroadcastable = 1330,
    kLayerNormalization = 1350,
    kNonMaximumSuppression = 1400,
    kOneHot = 1450,
    kCumSum = 1455,
    kClampedReLU = 1460,
    kArgSort = 1461,
    kPooling3D = 1465,
    kGlobalPooling3D = 1466,
    kSliceBySize = 1470,
    kConvolution3D = 1471,
    LAYER_NOT_SET = 0,
  };

  static inline const NeuralNetworkLayer* internal_default_instance() {
    return reinterpret_cast<const NeuralNetworkLayer*>(
               &_NeuralNetworkLayer_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    19;

  friend void swap(NeuralNetworkLayer& a, NeuralNetworkLayer& b) {
    a.Swap(&b);
  }
  inline void Swap(NeuralNetworkLayer* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(NeuralNetworkLayer* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  NeuralNetworkLayer* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<NeuralNetworkLayer>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const NeuralNetworkLayer& from);
  void MergeFrom(const NeuralNetworkLayer& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NeuralNetworkLayer* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.NeuralNetworkLayer";
  }
  protected:
  explicit NeuralNetworkLayer(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kInputFieldNumber = 2,
    kOutputFieldNumber = 3,
    kInputTensorFieldNumber = 4,
    kOutputTensorFieldNumber = 5,
    kNameFieldNumber = 1,
    kIsUpdatableFieldNumber = 10,
    kConvolutionFieldNumber = 100,
    kPoolingFieldNumber = 120,
    kActivationFieldNumber = 130,
    kInnerProductFieldNumber = 140,
    kEmbeddingFieldNumber = 150,
    kBatchnormFieldNumber = 160,
    kMvnFieldNumber = 165,
    kL2NormalizeFieldNumber = 170,
    kSoftmaxFieldNumber = 175,
    kLrnFieldNumber = 180,
    kCropFieldNumber = 190,
    kPaddingFieldNumber = 200,
    kUpsampleFieldNumber = 210,
    kResizeBilinearFieldNumber = 211,
    kCropResizeFieldNumber = 212,
    kUnaryFieldNumber = 220,
    kAddFieldNumber = 230,
    kMultiplyFieldNumber = 231,
    kAverageFieldNumber = 240,
    kScaleFieldNumber = 245,
    kBiasFieldNumber = 250,
    kMaxFieldNumber = 260,
    kMinFieldNumber = 261,
    kDotFieldNumber = 270,
    kReduceFieldNumber = 280,
    kLoadConstantFieldNumber = 290,
    kReshapeFieldNumber = 300,
    kFlattenFieldNumber = 301,
    kPermuteFieldNumber = 310,
    kConcatFieldNumber = 320,
    kSplitFieldNumber = 330,
    kSequenceRepeatFieldNumber = 340,
    kReorganizeDataFieldNumber = 345,
    kSliceFieldNumber = 350,
    kSimpleRecurrentFieldNumber = 400,
    kGruFieldNumber = 410,
    kUniDirectionalLSTMFieldNumber = 420,
    kBiDirectionalLSTMFieldNumber = 430,
    kCustomFieldNumber = 500,
    kCopyFieldNumber = 600,
    kBranchFieldNumber = 605,
    kLoopFieldNumber = 615,
    kLoopBreakFieldNumber = 620,
    kLoopContinueFieldNumber = 625,
    kRangeStaticFieldNumber = 635,
    kRangeDynamicFieldNumber = 640,
    kClipFieldNumber = 660,
    kCeilFieldNumber = 665,
    kFloorFieldNumber = 670,
    kSignFieldNumber = 680,
    kRoundFieldNumber = 685,
    kExp2FieldNumber = 700,
    kSinFieldNumber = 710,
    kCosFieldNumber = 715,
    kTanFieldNumber = 720,
    kAsinFieldNumber = 730,
    kAcosFieldNumber = 735,
    kAtanFieldNumber = 740,
    kSinhFieldNumber = 750,
    kCoshFieldNumber = 755,
    kTanhFieldNumber = 760,
    kAsinhFieldNumber = 770,
    kAcoshFieldNumber = 775,
    kAtanhFieldNumber = 780,
    kErfFieldNumber = 790,
    kGeluFieldNumber = 795,
    kEqualFieldNumber = 815,
    kNotEqualFieldNumber = 820,
    kLessThanFieldNumber = 825,
    kLessEqualFieldNumber = 827,
    kGreaterThanFieldNumber = 830,
    kGreaterEqualFieldNumber = 832,
    kLogicalOrFieldNumber = 840,
    kLogicalXorFieldNumber = 845,
    kLogicalNotFieldNumber = 850,
    kLogicalAndFieldNumber = 855,
    kModBroadcastableFieldNumber = 865,
    kMinBroadcastableFieldNumber = 870,
    kMaxBroadcastableFieldNumber = 875,
    kAddBroadcastableFieldNumber = 880,
    kPowBroadcastableFieldNumber = 885,
    kDivideBroadcastableFieldNumber = 890,
    kFloorDivBroadcastableFieldNumber = 895,
    kMultiplyBroadcastableFieldNumber = 900,
    kSubtractBroadcastableFieldNumber = 905,
    kTileFieldNumber = 920,
    kStackFieldNumber = 925,
    kGatherFieldNumber = 930,
    kScatterFieldNumber = 935,
    kGatherNDFieldNumber = 940,
    kScatterNDFieldNumber = 945,
    kSoftmaxNDFieldNumber = 950,
    kGatherAlongAxisFieldNumber = 952,
    kScatterAlongAxisFieldNumber = 954,
    kReverseFieldNumber = 960,
    kReverseSeqFieldNumber = 965,
    kSplitNDFieldNumber = 975,
    kConcatNDFieldNumber = 980,
    kTransposeFieldNumber = 985,
    kSliceStaticFieldNumber = 995,
    kSliceDynamicFieldNumber = 1000,
    kSlidingWindowsFieldNumber = 1005,
    kTopKFieldNumber = 1015,
    kArgMinFieldNumber = 1020,
    kArgMaxFieldNumber = 1025,
    kEmbeddingNDFieldNumber = 1040,
    kBatchedMatmulFieldNumber = 1045,
    kGetShapeFieldNumber = 1065,
    kLoadConstantNDFieldNumber = 1070,
    kFillLikeFieldNumber = 1080,
    kFillStaticFieldNumber = 1085,
    kFillDynamicFieldNumber = 1090,
    kBroadcastToLikeFieldNumber = 1100,
    kBroadcastToStaticFieldNumber = 1105,
    kBroadcastToDynamicFieldNumber = 1110,
    kSqueezeFieldNumber = 1120,
    kExpandDimsFieldNumber = 1125,
    kFlattenTo2DFieldNumber = 1130,
    kReshapeLikeFieldNumber = 1135,
    kReshapeStaticFieldNumber = 1140,
    kReshapeDynamicFieldNumber = 1145,
    kRankPreservingReshapeFieldNumber = 1150,
    kConstantPadFieldNumber = 1155,
    kRandomNormalLikeFieldNumber = 1170,
    kRandomNormalStaticFieldNumber = 1175,
    kRandomNormalDynamicFieldNumber = 1180,
    kRandomUniformLikeFieldNumber = 1190,
    kRandomUniformStaticFieldNumber = 1195,
    kRandomUniformDynamicFieldNumber = 1200,
    kRandomBernoulliLikeFieldNumber = 1210,
    kRandomBernoulliStaticFieldNumber = 1215,
    kRandomBernoulliDynamicFieldNumber = 1220,
    kCategoricalDistributionFieldNumber = 1230,
    kReduceL1FieldNumber = 1250,
    kReduceL2FieldNumber = 1255,
    kReduceMaxFieldNumber = 1260,
    kReduceMinFieldNumber = 1265,
    kReduceSumFieldNumber = 1270,
    kReduceProdFieldNumber = 1275,
    kReduceMeanFieldNumber = 1280,
    kReduceLogSumFieldNumber = 1285,
    kReduceSumSquareFieldNumber = 1290,
    kReduceLogSumExpFieldNumber = 1295,
    kWhereNonZeroFieldNumber = 1313,
    kMatrixBandPartFieldNumber = 1315,
    kLowerTriangularFieldNumber = 1320,
    kUpperTriangularFieldNumber = 1325,
    kWhereBroadcastableFieldNumber = 1330,
    kLayerNormalizationFieldNumber = 1350,
    kNonMaximumSuppressionFieldNumber = 1400,
    kOneHotFieldNumber = 1450,
    kCumSumFieldNumber = 1455,
    kClampedReLUFieldNumber = 1460,
    kArgSortFieldNumber = 1461,
    kPooling3DFieldNumber = 1465,
    kGlobalPooling3DFieldNumber = 1466,
    kSliceBySizeFieldNumber = 1470,
    kConvolution3DFieldNumber = 1471,
  };
  // repeated string input = 2;
  int input_size() const;
  private:
  int _internal_input_size() const;
  public:
  void clear_input();
  const std::string& input(int index) const;
  std::string* mutable_input(int index);
  void set_input(int index, const std::string& value);
  void set_input(int index, std::string&& value);
  void set_input(int index, const char* value);
  void set_input(int index, const char* value, size_t size);
  std::string* add_input();
  void add_input(const std::string& value);
  void add_input(std::string&& value);
  void add_input(const char* value);
  void add_input(const char* value, size_t size);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField<std::string>& input() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField<std::string>* mutable_input();
  private:
  const std::string& _internal_input(int index) const;
  std::string* _internal_add_input();
  public:

  // repeated string output = 3;
  int output_size() const;
  private:
  int _internal_output_size() const;
  public:
  void clear_output();
  const std::string& output(int index) const;
  std::string* mutable_output(int index);
  void set_output(int index, const std::string& value);
  void set_output(int index, std::string&& value);
  void set_output(int index, const char* value);
  void set_output(int index, const char* value, size_t size);
  std::string* add_output();
  void add_output(const std::string& value);
  void add_output(std::string&& value);
  void add_output(const char* value);
  void add_output(const char* value, size_t size);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField<std::string>& output() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField<std::string>* mutable_output();
  private:
  const std::string& _internal_output(int index) const;
  std::string* _internal_add_output();
  public:

  // repeated .CoreML.Specification.Tensor inputTensor = 4;
  int inputtensor_size() const;
  private:
  int _internal_inputtensor_size() const;
  public:
  void clear_inputtensor();
  ::CoreML::Specification::Tensor* mutable_inputtensor(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::Tensor >*
      mutable_inputtensor();
  private:
  const ::CoreML::Specification::Tensor& _internal_inputtensor(int index) const;
  ::CoreML::Specification::Tensor* _internal_add_inputtensor();
  public:
  const ::CoreML::Specification::Tensor& inputtensor(int index) const;
  ::CoreML::Specification::Tensor* add_inputtensor();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::Tensor >&
      inputtensor() const;

  // repeated .CoreML.Specification.Tensor outputTensor = 5;
  int outputtensor_size() const;
  private:
  int _internal_outputtensor_size() const;
  public:
  void clear_outputtensor();
  ::CoreML::Specification::Tensor* mutable_outputtensor(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::Tensor >*
      mutable_outputtensor();
  private:
  const ::CoreML::Specification::Tensor& _internal_outputtensor(int index) const;
  ::CoreML::Specification::Tensor* _internal_add_outputtensor();
  public:
  const ::CoreML::Specification::Tensor& outputtensor(int index) const;
  ::CoreML::Specification::Tensor* add_outputtensor();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::Tensor >&
      outputtensor() const;

  // string name = 1;
  void clear_name();
  const std::string& name() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_name(ArgT0&& arg0, ArgT... args);
  std::string* mutable_name();
  PROTOBUF_NODISCARD std::string* release_name();
  void set_allocated_name(std::string* name);
  private:
  const std::string& _internal_name() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_name(const std::string& value);
  std::string* _internal_mutable_name();
  public:

  // bool isUpdatable = 10;
  void clear_isupdatable();
  bool isupdatable() const;
  void set_isupdatable(bool value);
  private:
  bool _internal_isupdatable() const;
  void _internal_set_isupdatable(bool value);
  public:

  // .CoreML.Specification.ConvolutionLayerParams convolution = 100;
  bool has_convolution() const;
  private:
  bool _internal_has_convolution() const;
  public:
  void clear_convolution();
  const ::CoreML::Specification::ConvolutionLayerParams& convolution() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ConvolutionLayerParams* release_convolution();
  ::CoreML::Specification::ConvolutionLayerParams* mutable_convolution();
  void set_allocated_convolution(::CoreML::Specification::ConvolutionLayerParams* convolution);
  private:
  const ::CoreML::Specification::ConvolutionLayerParams& _internal_convolution() const;
  ::CoreML::Specification::ConvolutionLayerParams* _internal_mutable_convolution();
  public:
  void unsafe_arena_set_allocated_convolution(
      ::CoreML::Specification::ConvolutionLayerParams* convolution);
  ::CoreML::Specification::ConvolutionLayerParams* unsafe_arena_release_convolution();

  // .CoreML.Specification.PoolingLayerParams pooling = 120;
  bool has_pooling() const;
  private:
  bool _internal_has_pooling() const;
  public:
  void clear_pooling();
  const ::CoreML::Specification::PoolingLayerParams& pooling() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::PoolingLayerParams* release_pooling();
  ::CoreML::Specification::PoolingLayerParams* mutable_pooling();
  void set_allocated_pooling(::CoreML::Specification::PoolingLayerParams* pooling);
  private:
  const ::CoreML::Specification::PoolingLayerParams& _internal_pooling() const;
  ::CoreML::Specification::PoolingLayerParams* _internal_mutable_pooling();
  public:
  void unsafe_arena_set_allocated_pooling(
      ::CoreML::Specification::PoolingLayerParams* pooling);
  ::CoreML::Specification::PoolingLayerParams* unsafe_arena_release_pooling();

  // .CoreML.Specification.ActivationParams activation = 130;
  bool has_activation() const;
  private:
  bool _internal_has_activation() const;
  public:
  void clear_activation();
  const ::CoreML::Specification::ActivationParams& activation() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ActivationParams* release_activation();
  ::CoreML::Specification::ActivationParams* mutable_activation();
  void set_allocated_activation(::CoreML::Specification::ActivationParams* activation);
  private:
  const ::CoreML::Specification::ActivationParams& _internal_activation() const;
  ::CoreML::Specification::ActivationParams* _internal_mutable_activation();
  public:
  void unsafe_arena_set_allocated_activation(
      ::CoreML::Specification::ActivationParams* activation);
  ::CoreML::Specification::ActivationParams* unsafe_arena_release_activation();

  // .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
  bool has_innerproduct() const;
  private:
  bool _internal_has_innerproduct() const;
  public:
  void clear_innerproduct();
  const ::CoreML::Specification::InnerProductLayerParams& innerproduct() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::InnerProductLayerParams* release_innerproduct();
  ::CoreML::Specification::InnerProductLayerParams* mutable_innerproduct();
  void set_allocated_innerproduct(::CoreML::Specification::InnerProductLayerParams* innerproduct);
  private:
  const ::CoreML::Specification::InnerProductLayerParams& _internal_innerproduct() const;
  ::CoreML::Specification::InnerProductLayerParams* _internal_mutable_innerproduct();
  public:
  void unsafe_arena_set_allocated_innerproduct(
      ::CoreML::Specification::InnerProductLayerParams* innerproduct);
  ::CoreML::Specification::InnerProductLayerParams* unsafe_arena_release_innerproduct();

  // .CoreML.Specification.EmbeddingLayerParams embedding = 150;
  bool has_embedding() const;
  private:
  bool _internal_has_embedding() const;
  public:
  void clear_embedding();
  const ::CoreML::Specification::EmbeddingLayerParams& embedding() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::EmbeddingLayerParams* release_embedding();
  ::CoreML::Specification::EmbeddingLayerParams* mutable_embedding();
  void set_allocated_embedding(::CoreML::Specification::EmbeddingLayerParams* embedding);
  private:
  const ::CoreML::Specification::EmbeddingLayerParams& _internal_embedding() const;
  ::CoreML::Specification::EmbeddingLayerParams* _internal_mutable_embedding();
  public:
  void unsafe_arena_set_allocated_embedding(
      ::CoreML::Specification::EmbeddingLayerParams* embedding);
  ::CoreML::Specification::EmbeddingLayerParams* unsafe_arena_release_embedding();

  // .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
  bool has_batchnorm() const;
  private:
  bool _internal_has_batchnorm() const;
  public:
  void clear_batchnorm();
  const ::CoreML::Specification::BatchnormLayerParams& batchnorm() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::BatchnormLayerParams* release_batchnorm();
  ::CoreML::Specification::BatchnormLayerParams* mutable_batchnorm();
  void set_allocated_batchnorm(::CoreML::Specification::BatchnormLayerParams* batchnorm);
  private:
  const ::CoreML::Specification::BatchnormLayerParams& _internal_batchnorm() const;
  ::CoreML::Specification::BatchnormLayerParams* _internal_mutable_batchnorm();
  public:
  void unsafe_arena_set_allocated_batchnorm(
      ::CoreML::Specification::BatchnormLayerParams* batchnorm);
  ::CoreML::Specification::BatchnormLayerParams* unsafe_arena_release_batchnorm();

  // .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
  bool has_mvn() const;
  private:
  bool _internal_has_mvn() const;
  public:
  void clear_mvn();
  const ::CoreML::Specification::MeanVarianceNormalizeLayerParams& mvn() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::MeanVarianceNormalizeLayerParams* release_mvn();
  ::CoreML::Specification::MeanVarianceNormalizeLayerParams* mutable_mvn();
  void set_allocated_mvn(::CoreML::Specification::MeanVarianceNormalizeLayerParams* mvn);
  private:
  const ::CoreML::Specification::MeanVarianceNormalizeLayerParams& _internal_mvn() const;
  ::CoreML::Specification::MeanVarianceNormalizeLayerParams* _internal_mutable_mvn();
  public:
  void unsafe_arena_set_allocated_mvn(
      ::CoreML::Specification::MeanVarianceNormalizeLayerParams* mvn);
  ::CoreML::Specification::MeanVarianceNormalizeLayerParams* unsafe_arena_release_mvn();

  // .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
  bool has_l2normalize() const;
  private:
  bool _internal_has_l2normalize() const;
  public:
  void clear_l2normalize();
  const ::CoreML::Specification::L2NormalizeLayerParams& l2normalize() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::L2NormalizeLayerParams* release_l2normalize();
  ::CoreML::Specification::L2NormalizeLayerParams* mutable_l2normalize();
  void set_allocated_l2normalize(::CoreML::Specification::L2NormalizeLayerParams* l2normalize);
  private:
  const ::CoreML::Specification::L2NormalizeLayerParams& _internal_l2normalize() const;
  ::CoreML::Specification::L2NormalizeLayerParams* _internal_mutable_l2normalize();
  public:
  void unsafe_arena_set_allocated_l2normalize(
      ::CoreML::Specification::L2NormalizeLayerParams* l2normalize);
  ::CoreML::Specification::L2NormalizeLayerParams* unsafe_arena_release_l2normalize();

  // .CoreML.Specification.SoftmaxLayerParams softmax = 175;
  bool has_softmax() const;
  private:
  bool _internal_has_softmax() const;
  public:
  void clear_softmax();
  const ::CoreML::Specification::SoftmaxLayerParams& softmax() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::SoftmaxLayerParams* release_softmax();
  ::CoreML::Specification::SoftmaxLayerParams* mutable_softmax();
  void set_allocated_softmax(::CoreML::Specification::SoftmaxLayerParams* softmax);
  private:
  const ::CoreML::Specification::SoftmaxLayerParams& _internal_softmax() const;
  ::CoreML::Specification::SoftmaxLayerParams* _internal_mutable_softmax();
  public:
  void unsafe_arena_set_allocated_softmax(
      ::CoreML::Specification::SoftmaxLayerParams* softmax);
  ::CoreML::Specification::SoftmaxLayerParams* unsafe_arena_release_softmax();

  // .CoreML.Specification.LRNLayerParams lrn = 180;
  bool has_lrn() const;
  private:
  bool _internal_has_lrn() const;
  public:
  void clear_lrn();
  const ::CoreML::Specification::LRNLayerParams& lrn() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::LRNLayerParams* release_lrn();
  ::CoreML::Specification::LRNLayerParams* mutable_lrn();
  void set_allocated_lrn(::CoreML::Specification::LRNLayerParams* lrn);
  private:
  const ::CoreML::Specification::LRNLayerParams& _internal_lrn() const;
  ::CoreML::Specification::LRNLayerParams* _internal_mutable_lrn();
  public:
  void unsafe_arena_set_allocated_lrn(
      ::CoreML::Specification::LRNLayerParams* lrn);
  ::CoreML::Specification::LRNLayerParams* unsafe_arena_release_lrn();

  // .CoreML.Specification.CropLayerParams crop = 190;
  bool has_crop() const;
  private:
  bool _internal_has_crop() const;
  public:
  void clear_crop();
  const ::CoreML::Specification::CropLayerParams& crop() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::CropLayerParams* release_crop();
  ::CoreML::Specification::CropLayerParams* mutable_crop();
  void set_allocated_crop(::CoreML::Specification::CropLayerParams* crop);
  private:
  const ::CoreML::Specification::CropLayerParams& _internal_crop() const;
  ::CoreML::Specification::CropLayerParams* _internal_mutable_crop();
  public:
  void unsafe_arena_set_allocated_crop(
      ::CoreML::Specification::CropLayerParams* crop);
  ::CoreML::Specification::CropLayerParams* unsafe_arena_release_crop();

  // .CoreML.Specification.PaddingLayerParams padding = 200;
  bool has_padding() const;
  private:
  bool _internal_has_padding() const;
  public:
  void clear_padding();
  const ::CoreML::Specification::PaddingLayerParams& padding() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::PaddingLayerParams* release_padding();
  ::CoreML::Specification::PaddingLayerParams* mutable_padding();
  void set_allocated_padding(::CoreML::Specification::PaddingLayerParams* padding);
  private:
  const ::CoreML::Specification::PaddingLayerParams& _internal_padding() const;
  ::CoreML::Specification::PaddingLayerParams* _internal_mutable_padding();
  public:
  void unsafe_arena_set_allocated_padding(
      ::CoreML::Specification::PaddingLayerParams* padding);
  ::CoreML::Specification::PaddingLayerParams* unsafe_arena_release_padding();

  // .CoreML.Specification.UpsampleLayerParams upsample = 210;
  bool has_upsample() const;
  private:
  bool _internal_has_upsample() const;
  public:
  void clear_upsample();
  const ::CoreML::Specification::UpsampleLayerParams& upsample() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::UpsampleLayerParams* release_upsample();
  ::CoreML::Specification::UpsampleLayerParams* mutable_upsample();
  void set_allocated_upsample(::CoreML::Specification::UpsampleLayerParams* upsample);
  private:
  const ::CoreML::Specification::UpsampleLayerParams& _internal_upsample() const;
  ::CoreML::Specification::UpsampleLayerParams* _internal_mutable_upsample();
  public:
  void unsafe_arena_set_allocated_upsample(
      ::CoreML::Specification::UpsampleLayerParams* upsample);
  ::CoreML::Specification::UpsampleLayerParams* unsafe_arena_release_upsample();

  // .CoreML.Specification.ResizeBilinearLayerParams resizeBilinear = 211;
  bool has_resizebilinear() const;
  private:
  bool _internal_has_resizebilinear() const;
  public:
  void clear_resizebilinear();
  const ::CoreML::Specification::ResizeBilinearLayerParams& resizebilinear() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ResizeBilinearLayerParams* release_resizebilinear();
  ::CoreML::Specification::ResizeBilinearLayerParams* mutable_resizebilinear();
  void set_allocated_resizebilinear(::CoreML::Specification::ResizeBilinearLayerParams* resizebilinear);
  private:
  const ::CoreML::Specification::ResizeBilinearLayerParams& _internal_resizebilinear() const;
  ::CoreML::Specification::ResizeBilinearLayerParams* _internal_mutable_resizebilinear();
  public:
  void unsafe_arena_set_allocated_resizebilinear(
      ::CoreML::Specification::ResizeBilinearLayerParams* resizebilinear);
  ::CoreML::Specification::ResizeBilinearLayerParams* unsafe_arena_release_resizebilinear();

  // .CoreML.Specification.CropResizeLayerParams cropResize = 212;
  bool has_cropresize() const;
  private:
  bool _internal_has_cropresize() const;
  public:
  void clear_cropresize();
  const ::CoreML::Specification::CropResizeLayerParams& cropresize() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::CropResizeLayerParams* release_cropresize();
  ::CoreML::Specification::CropResizeLayerParams* mutable_cropresize();
  void set_allocated_cropresize(::CoreML::Specification::CropResizeLayerParams* cropresize);
  private:
  const ::CoreML::Specification::CropResizeLayerParams& _internal_cropresize() const;
  ::CoreML::Specification::CropResizeLayerParams* _internal_mutable_cropresize();
  public:
  void unsafe_arena_set_allocated_cropresize(
      ::CoreML::Specification::CropResizeLayerParams* cropresize);
  ::CoreML::Specification::CropResizeLayerParams* unsafe_arena_release_cropresize();

  // .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
  bool has_unary() const;
  private:
  bool _internal_has_unary() const;
  public:
  void clear_unary();
  const ::CoreML::Specification::UnaryFunctionLayerParams& unary() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::UnaryFunctionLayerParams* release_unary();
  ::CoreML::Specification::UnaryFunctionLayerParams* mutable_unary();
  void set_allocated_unary(::CoreML::Specification::UnaryFunctionLayerParams* unary);
  private:
  const ::CoreML::Specification::UnaryFunctionLayerParams& _internal_unary() const;
  ::CoreML::Specification::UnaryFunctionLayerParams* _internal_mutable_unary();
  public:
  void unsafe_arena_set_allocated_unary(
      ::CoreML::Specification::UnaryFunctionLayerParams* unary);
  ::CoreML::Specification::UnaryFunctionLayerParams* unsafe_arena_release_unary();

  // .CoreML.Specification.AddLayerParams add = 230;
  bool has_add() const;
  private:
  bool _internal_has_add() const;
  public:
  void clear_add();
  const ::CoreML::Specification::AddLayerParams& add() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::AddLayerParams* release_add();
  ::CoreML::Specification::AddLayerParams* mutable_add();
  void set_allocated_add(::CoreML::Specification::AddLayerParams* add);
  private:
  const ::CoreML::Specification::AddLayerParams& _internal_add() const;
  ::CoreML::Specification::AddLayerParams* _internal_mutable_add();
  public:
  void unsafe_arena_set_allocated_add(
      ::CoreML::Specification::AddLayerParams* add);
  ::CoreML::Specification::AddLayerParams* unsafe_arena_release_add();

  // .CoreML.Specification.MultiplyLayerParams multiply = 231;
  bool has_multiply() const;
  private:
  bool _internal_has_multiply() const;
  public:
  void clear_multiply();
  const ::CoreML::Specification::MultiplyLayerParams& multiply() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::MultiplyLayerParams* release_multiply();
  ::CoreML::Specification::MultiplyLayerParams* mutable_multiply();
  void set_allocated_multiply(::CoreML::Specification::MultiplyLayerParams* multiply);
  private:
  const ::CoreML::Specification::MultiplyLayerParams& _internal_multiply() const;
  ::CoreML::Specification::MultiplyLayerParams* _internal_mutable_multiply();
  public:
  void unsafe_arena_set_allocated_multiply(
      ::CoreML::Specification::MultiplyLayerParams* multiply);
  ::CoreML::Specification::MultiplyLayerParams* unsafe_arena_release_multiply();

  // .CoreML.Specification.AverageLayerParams average = 240;
  bool has_average() const;
  private:
  bool _internal_has_average() const;
  public:
  void clear_average();
  const ::CoreML::Specification::AverageLayerParams& average() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::AverageLayerParams* release_average();
  ::CoreML::Specification::AverageLayerParams* mutable_average();
  void set_allocated_average(::CoreML::Specification::AverageLayerParams* average);
  private:
  const ::CoreML::Specification::AverageLayerParams& _internal_average() const;
  ::CoreML::Specification::AverageLayerParams* _internal_mutable_average();
  public:
  void unsafe_arena_set_allocated_average(
      ::CoreML::Specification::AverageLayerParams* average);
  ::CoreML::Specification::AverageLayerParams* unsafe_arena_release_average();

  // .CoreML.Specification.ScaleLayerParams scale = 245;
  bool has_scale() const;
  private:
  bool _internal_has_scale() const;
  public:
  void clear_scale();
  const ::CoreML::Specification::ScaleLayerParams& scale() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ScaleLayerParams* release_scale();
  ::CoreML::Specification::ScaleLayerParams* mutable_scale();
  void set_allocated_scale(::CoreML::Specification::ScaleLayerParams* scale);
  private:
  const ::CoreML::Specification::ScaleLayerParams& _internal_scale() const;
  ::CoreML::Specification::ScaleLayerParams* _internal_mutable_scale();
  public:
  void unsafe_arena_set_allocated_scale(
      ::CoreML::Specification::ScaleLayerParams* scale);
  ::CoreML::Specification::ScaleLayerParams* unsafe_arena_release_scale();

  // .CoreML.Specification.BiasLayerParams bias = 250;
  bool has_bias() const;
  private:
  bool _internal_has_bias() const;
  public:
  void clear_bias();
  const ::CoreML::Specification::BiasLayerParams& bias() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::BiasLayerParams* release_bias();
  ::CoreML::Specification::BiasLayerParams* mutable_bias();
  void set_allocated_bias(::CoreML::Specification::BiasLayerParams* bias);
  private:
  const ::CoreML::Specification::BiasLayerParams& _internal_bias() const;
  ::CoreML::Specification::BiasLayerParams* _internal_mutable_bias();
  public:
  void unsafe_arena_set_allocated_bias(
      ::CoreML::Specification::BiasLayerParams* bias);
  ::CoreML::Specification::BiasLayerParams* unsafe_arena_release_bias();

  // .CoreML.Specification.MaxLayerParams max = 260;
  bool has_max() const;
  private:
  bool _internal_has_max() const;
  public:
  void clear_max();
  const ::CoreML::Specification::MaxLayerParams& max() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::MaxLayerParams* release_max();
  ::CoreML::Specification::MaxLayerParams* mutable_max();
  void set_allocated_max(::CoreML::Specification::MaxLayerParams* max);
  private:
  const ::CoreML::Specification::MaxLayerParams& _internal_max() const;
  ::CoreML::Specification::MaxLayerParams* _internal_mutable_max();
  public:
  void unsafe_arena_set_allocated_max(
      ::CoreML::Specification::MaxLayerParams* max);
  ::CoreML::Specification::MaxLayerParams* unsafe_arena_release_max();

  // .CoreML.Specification.MinLayerParams min = 261;
  bool has_min() const;
  private:
  bool _internal_has_min() const;
  public:
  void clear_min();
  const ::CoreML::Specification::MinLayerParams& min() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::MinLayerParams* release_min();
  ::CoreML::Specification::MinLayerParams* mutable_min();
  void set_allocated_min(::CoreML::Specification::MinLayerParams* min);
  private:
  const ::CoreML::Specification::MinLayerParams& _internal_min() const;
  ::CoreML::Specification::MinLayerParams* _internal_mutable_min();
  public:
  void unsafe_arena_set_allocated_min(
      ::CoreML::Specification::MinLayerParams* min);
  ::CoreML::Specification::MinLayerParams* unsafe_arena_release_min();

  // .CoreML.Specification.DotProductLayerParams dot = 270;
  bool has_dot() const;
  private:
  bool _internal_has_dot() const;
  public:
  void clear_dot();
  const ::CoreML::Specification::DotProductLayerParams& dot() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::DotProductLayerParams* release_dot();
  ::CoreML::Specification::DotProductLayerParams* mutable_dot();
  void set_allocated_dot(::CoreML::Specification::DotProductLayerParams* dot);
  private:
  const ::CoreML::Specification::DotProductLayerParams& _internal_dot() const;
  ::CoreML::Specification::DotProductLayerParams* _internal_mutable_dot();
  public:
  void unsafe_arena_set_allocated_dot(
      ::CoreML::Specification::DotProductLayerParams* dot);
  ::CoreML::Specification::DotProductLayerParams* unsafe_arena_release_dot();

  // .CoreML.Specification.ReduceLayerParams reduce = 280;
  bool has_reduce() const;
  private:
  bool _internal_has_reduce() const;
  public:
  void clear_reduce();
  const ::CoreML::Specification::ReduceLayerParams& reduce() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ReduceLayerParams* release_reduce();
  ::CoreML::Specification::ReduceLayerParams* mutable_reduce();
  void set_allocated_reduce(::CoreML::Specification::ReduceLayerParams* reduce);
  private:
  const ::CoreML::Specification::ReduceLayerParams& _internal_reduce() const;
  ::CoreML::Specification::ReduceLayerParams* _internal_mutable_reduce();
  public:
  void unsafe_arena_set_allocated_reduce(
      ::CoreML::Specification::ReduceLayerParams* reduce);
  ::CoreML::Specification::ReduceLayerParams* unsafe_arena_release_reduce();

  // .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
  bool has_loadconstant() const;
  private:
  bool _internal_has_loadconstant() const;
  public:
  void clear_loadconstant();
  const ::CoreML::Specification::LoadConstantLayerParams& loadconstant() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::LoadConstantLayerParams* release_loadconstant();
  ::CoreML::Specification::LoadConstantLayerParams* mutable_loadconstant();
  void set_allocated_loadconstant(::CoreML::Specification::LoadConstantLayerParams* loadconstant);
  private:
  const ::CoreML::Specification::LoadConstantLayerParams& _internal_loadconstant() const;
  ::CoreML::Specification::LoadConstantLayerParams* _internal_mutable_loadconstant();
  public:
  void unsafe_arena_set_allocated_loadconstant(
      ::CoreML::Specification::LoadConstantLayerParams* loadconstant);
  ::CoreML::Specification::LoadConstantLayerParams* unsafe_arena_release_loadconstant();

  // .CoreML.Specification.ReshapeLayerParams reshape = 300;
  bool has_reshape() const;
  private:
  bool _internal_has_reshape() const;
  public:
  void clear_reshape();
  const ::CoreML::Specification::ReshapeLayerParams& reshape() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ReshapeLayerParams* release_reshape();
  ::CoreML::Specification::ReshapeLayerParams* mutable_reshape();
  void set_allocated_reshape(::CoreML::Specification::ReshapeLayerParams* reshape);
  private:
  const ::CoreML::Specification::ReshapeLayerParams& _internal_reshape() const;
  ::CoreML::Specification::ReshapeLayerParams* _internal_mutable_reshape();
  public:
  void unsafe_arena_set_allocated_reshape(
      ::CoreML::Specification::ReshapeLayerParams* reshape);
  ::CoreML::Specification::ReshapeLayerParams* unsafe_arena_release_reshape();

  // .CoreML.Specification.FlattenLayerParams flatten = 301;
  bool has_flatten() const;
  private:
  bool _internal_has_flatten() const;
  public:
  void clear_flatten();
  const ::CoreML::Specification::FlattenLayerParams& flatten() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::FlattenLayerParams* release_flatten();
  ::CoreML::Specification::FlattenLayerParams* mutable_flatten();
  void set_allocated_flatten(::CoreML::Specification::FlattenLayerParams* flatten);
  private:
  const ::CoreML::Specification::FlattenLayerParams& _internal_flatten() const;
  ::CoreML::Specification::FlattenLayerParams* _internal_mutable_flatten();
  public:
  void unsafe_arena_set_allocated_flatten(
      ::CoreML::Specification::FlattenLayerParams* flatten);
  ::CoreML::Specification::FlattenLayerParams* unsafe_arena_release_flatten();

  // .CoreML.Specification.PermuteLayerParams permute = 310;
  bool has_permute() const;
  private:
  bool _internal_has_permute() const;
  public:
  void clear_permute();
  const ::CoreML::Specification::PermuteLayerParams& permute() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::PermuteLayerParams* release_permute();
  ::CoreML::Specification::PermuteLayerParams* mutable_permute();
  void set_allocated_permute(::CoreML::Specification::PermuteLayerParams* permute);
  private:
  const ::CoreML::Specification::PermuteLayerParams& _internal_permute() const;
  ::CoreML::Specification::PermuteLayerParams* _internal_mutable_permute();
  public:
  void unsafe_arena_set_allocated_permute(
      ::CoreML::Specification::PermuteLayerParams* permute);
  ::CoreML::Specification::PermuteLayerParams* unsafe_arena_release_permute();

  // .CoreML.Specification.ConcatLayerParams concat = 320;
  bool has_concat() const;
  private:
  bool _internal_has_concat() const;
  public:
  void clear_concat();
  const ::CoreML::Specification::ConcatLayerParams& concat() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ConcatLayerParams* release_concat();
  ::CoreML::Specification::ConcatLayerParams* mutable_concat();
  void set_allocated_concat(::CoreML::Specification::ConcatLayerParams* concat);
  private:
  const ::CoreML::Specification::ConcatLayerParams& _internal_concat() const;
  ::CoreML::Specification::ConcatLayerParams* _internal_mutable_concat();
  public:
  void unsafe_arena_set_allocated_concat(
      ::CoreML::Specification::ConcatLayerParams* concat);
  ::CoreML::Specification::ConcatLayerParams* unsafe_arena_release_concat();

  // .CoreML.Specification.SplitLayerParams split = 330;
  bool has_split() const;
  private:
  bool _internal_has_split() const;
  public:
  void clear_split();
  const ::CoreML::Specification::SplitLayerParams& split() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::SplitLayerParams* release_split();
  ::CoreML::Specification::SplitLayerParams* mutable_split();
  void set_allocated_split(::CoreML::Specification::SplitLayerParams* split);
  private:
  const ::CoreML::Specification::SplitLayerParams& _internal_split() const;
  ::CoreML::Specification::SplitLayerParams* _internal_mutable_split();
  public:
  void unsafe_arena_set_allocated_split(
      ::CoreML::Specification::SplitLayerParams* split);
  ::CoreML::Specification::SplitLayerParams* unsafe_arena_release_split();

  // .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
  bool has_sequencerepeat() const;
  private:
  bool _internal_has_sequencerepeat() const;
  public:
  void clear_sequencerepeat();
  const ::CoreML::Specification::SequenceRepeatLayerParams& sequencerepeat() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::SequenceRepeatLayerParams* release_sequencerepeat();
  ::CoreML::Specification::SequenceRepeatLayerParams* mutable_sequencerepeat();
  void set_allocated_sequencerepeat(::CoreML::Specification::SequenceRepeatLayerParams* sequencerepeat);
  private:
  const ::CoreML::Specification::SequenceRepeatLayerParams& _internal_sequencerepeat() const;
  ::CoreML::Specification::SequenceRepeatLayerParams* _internal_mutable_sequencerepeat();
  public:
  void unsafe_arena_set_allocated_sequencerepeat(
      ::CoreML::Specification::SequenceRepeatLayerParams* sequencerepeat);
  ::CoreML::Specification::SequenceRepeatLayerParams* unsafe_arena_release_sequencerepeat();

  // .CoreML.Specification.ReorganizeDataLayerParams reorganizeData = 345;
  bool has_reorganizedata() const;
  private:
  bool _internal_has_reorganizedata() const;
  public:
  void clear_reorganizedata();
  const ::CoreML::Specification::ReorganizeDataLayerParams& reorganizedata() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ReorganizeDataLayerParams* release_reorganizedata();
  ::CoreML::Specification::ReorganizeDataLayerParams* mutable_reorganizedata();
  void set_allocated_reorganizedata(::CoreML::Specification::ReorganizeDataLayerParams* reorganizedata);
  private:
  const ::CoreML::Specification::ReorganizeDataLayerParams& _internal_reorganizedata() const;
  ::CoreML::Specification::ReorganizeDataLayerParams* _internal_mutable_reorganizedata();
  public:
  void unsafe_arena_set_allocated_reorganizedata(
      ::CoreML::Specification::ReorganizeDataLayerParams* reorganizedata);
  ::CoreML::Specification::ReorganizeDataLayerParams* unsafe_arena_release_reorganizedata();

  // .CoreML.Specification.SliceLayerParams slice = 350;
  bool has_slice() const;
  private:
  bool _internal_has_slice() const;
  public:
  void clear_slice();
  const ::CoreML::Specification::SliceLayerParams& slice() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::SliceLayerParams* release_slice();
  ::CoreML::Specification::SliceLayerParams* mutable_slice();
  void set_allocated_slice(::CoreML::Specification::SliceLayerParams* slice);
  private:
  const ::CoreML::Specification::SliceLayerParams& _internal_slice() const;
  ::CoreML::Specification::SliceLayerParams* _internal_mutable_slice();
  public:
  void unsafe_arena_set_allocated_slice(
      ::CoreML::Specification::SliceLayerParams* slice);
  ::CoreML::Specification::SliceLayerParams* unsafe_arena_release_slice();

  // .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
  bool has_simplerecurrent() const;
  private:
  bool _internal_has_simplerecurrent() const;
  public:
  void clear_simplerecurrent();
  const ::CoreML::Specification::SimpleRecurrentLayerParams& simplerecurrent() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::SimpleRecurrentLayerParams* release_simplerecurrent();
  ::CoreML::Specification::SimpleRecurrentLayerParams* mutable_simplerecurrent();
  void set_allocated_simplerecurrent(::CoreML::Specification::SimpleRecurrentLayerParams* simplerecurrent);
  private:
  const ::CoreML::Specification::SimpleRecurrentLayerParams& _internal_simplerecurrent() const;
  ::CoreML::Specification::SimpleRecurrentLayerParams* _internal_mutable_simplerecurrent();
  public:
  void unsafe_arena_set_allocated_simplerecurrent(
      ::CoreML::Specification::SimpleRecurrentLayerParams* simplerecurrent);
  ::CoreML::Specification::SimpleRecurrentLayerParams* unsafe_arena_release_simplerecurrent();

  // .CoreML.Specification.GRULayerParams gru = 410;
  bool has_gru() const;
  private:
  bool _internal_has_gru() const;
  public:
  void clear_gru();
  const ::CoreML::Specification::GRULayerParams& gru() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::GRULayerParams* release_gru();
  ::CoreML::Specification::GRULayerParams* mutable_gru();
  void set_allocated_gru(::CoreML::Specification::GRULayerParams* gru);
  private:
  const ::CoreML::Specification::GRULayerParams& _internal_gru() const;
  ::CoreML::Specification::GRULayerParams* _internal_mutable_gru();
  public:
  void unsafe_arena_set_allocated_gru(
      ::CoreML::Specification::GRULayerParams* gru);
  ::CoreML::Specification::GRULayerParams* unsafe_arena_release_gru();

  // .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
  bool has_unidirectionallstm() const;
  private:
  bool _internal_has_unidirectionallstm() const;
  public:
  void clear_unidirectionallstm();
  const ::CoreML::Specification::UniDirectionalLSTMLayerParams& unidirectionallstm() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::UniDirectionalLSTMLayerParams* release_unidirectionallstm();
  ::CoreML::Specification::UniDirectionalLSTMLayerParams* mutable_unidirectionallstm();
  void set_allocated_unidirectionallstm(::CoreML::Specification::UniDirectionalLSTMLayerParams* unidirectionallstm);
  private:
  const ::CoreML::Specification::UniDirectionalLSTMLayerParams& _internal_unidirectionallstm() const;
  ::CoreML::Specification::UniDirectionalLSTMLayerParams* _internal_mutable_unidirectionallstm();
  public:
  void unsafe_arena_set_allocated_unidirectionallstm(
      ::CoreML::Specification::UniDirectionalLSTMLayerParams* unidirectionallstm);
  ::CoreML::Specification::UniDirectionalLSTMLayerParams* unsafe_arena_release_unidirectionallstm();

  // .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
  bool has_bidirectionallstm() const;
  private:
  bool _internal_has_bidirectionallstm() const;
  public:
  void clear_bidirectionallstm();
  const ::CoreML::Specification::BiDirectionalLSTMLayerParams& bidirectionallstm() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::BiDirectionalLSTMLayerParams* release_bidirectionallstm();
  ::CoreML::Specification::BiDirectionalLSTMLayerParams* mutable_bidirectionallstm();
  void set_allocated_bidirectionallstm(::CoreML::Specification::BiDirectionalLSTMLayerParams* bidirectionallstm);
  private:
  const ::CoreML::Specification::BiDirectionalLSTMLayerParams& _internal_bidirectionallstm() const;
  ::CoreML::Specification::BiDirectionalLSTMLayerParams* _internal_mutable_bidirectionallstm();
  public:
  void unsafe_arena_set_allocated_bidirectionallstm(
      ::CoreML::Specification::BiDirectionalLSTMLayerParams* bidirectionallstm);
  ::CoreML::Specification::BiDirectionalLSTMLayerParams* unsafe_arena_release_bidirectionallstm();

  // .CoreML.Specification.CustomLayerParams custom = 500;
  bool has_custom() const;
  private:
  bool _internal_has_custom() const;
  public:
  void clear_custom();
  const ::CoreML::Specification::CustomLayerParams& custom() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::CustomLayerParams* release_custom();
  ::CoreML::Specification::CustomLayerParams* mutable_custom();
  void set_allocated_custom(::CoreML::Specification::CustomLayerParams* custom);
  private:
  const ::CoreML::Specification::CustomLayerParams& _internal_custom() const;
  ::CoreML::Specification::CustomLayerParams* _internal_mutable_custom();
  public:
  void unsafe_arena_set_allocated_custom(
      ::CoreML::Specification::CustomLayerParams* custom);
  ::CoreML::Specification::CustomLayerParams* unsafe_arena_release_custom();

  // .CoreML.Specification.CopyLayerParams copy = 600;
  bool has_copy() const;
  private:
  bool _internal_has_copy() const;
  public:
  void clear_copy();
  const ::CoreML::Specification::CopyLayerParams& copy() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::CopyLayerParams* release_copy();
  ::CoreML::Specification::CopyLayerParams* mutable_copy();
  void set_allocated_copy(::CoreML::Specification::CopyLayerParams* copy);
  private:
  const ::CoreML::Specification::CopyLayerParams& _internal_copy() const;
  ::CoreML::Specification::CopyLayerParams* _internal_mutable_copy();
  public:
  void unsafe_arena_set_allocated_copy(
      ::CoreML::Specification::CopyLayerParams* copy);
  ::CoreML::Specification::CopyLayerParams* unsafe_arena_release_copy();

  // .CoreML.Specification.BranchLayerParams branch = 605;
  bool has_branch() const;
  private:
  bool _internal_has_branch() const;
  public:
  void clear_branch();
  const ::CoreML::Specification::BranchLayerParams& branch() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::BranchLayerParams* release_branch();
  ::CoreML::Specification::BranchLayerParams* mutable_branch();
  void set_allocated_branch(::CoreML::Specification::BranchLayerParams* branch);
  private:
  const ::CoreML::Specification::BranchLayerParams& _internal_branch() const;
  ::CoreML::Specification::BranchLayerParams* _internal_mutable_branch();
  public:
  void unsafe_arena_set_allocated_branch(
      ::CoreML::Specification::BranchLayerParams* branch);
  ::CoreML::Specification::BranchLayerParams* unsafe_arena_release_branch();

  // .CoreML.Specification.LoopLayerParams loop = 615;
  bool has_loop() const;
  private:
  bool _internal_has_loop() const;
  public:
  void clear_loop();
  const ::CoreML::Specification::LoopLayerParams& loop() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::LoopLayerParams* release_loop();
  ::CoreML::Specification::LoopLayerParams* mutable_loop();
  void set_allocated_loop(::CoreML::Specification::LoopLayerParams* loop);
  private:
  const ::CoreML::Specification::LoopLayerParams& _internal_loop() const;
  ::CoreML::Specification::LoopLayerParams* _internal_mutable_loop();
  public:
  void unsafe_arena_set_allocated_loop(
      ::CoreML::Specification::LoopLayerParams* loop);
  ::CoreML::Specification::LoopLayerParams* unsafe_arena_release_loop();

  // .CoreML.Specification.LoopBreakLayerParams loopBreak = 620;
  bool has_loopbreak() const;
  private:
  bool _internal_has_loopbreak() const;
  public:
  void clear_loopbreak();
  const ::CoreML::Specification::LoopBreakLayerParams& loopbreak() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::LoopBreakLayerParams* release_loopbreak();
  ::CoreML::Specification::LoopBreakLayerParams* mutable_loopbreak();
  void set_allocated_loopbreak(::CoreML::Specification::LoopBreakLayerParams* loopbreak);
  private:
  const ::CoreML::Specification::LoopBreakLayerParams& _internal_loopbreak() const;
  ::CoreML::Specification::LoopBreakLayerParams* _internal_mutable_loopbreak();
  public:
  void unsafe_arena_set_allocated_loopbreak(
      ::CoreML::Specification::LoopBreakLayerParams* loopbreak);
  ::CoreML::Specification::LoopBreakLayerParams* unsafe_arena_release_loopbreak();

  // .CoreML.Specification.LoopContinueLayerParams loopContinue = 625;
  bool has_loopcontinue() const;
  private:
  bool _internal_has_loopcontinue() const;
  public:
  void clear_loopcontinue();
  const ::CoreML::Specification::LoopContinueLayerParams& loopcontinue() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::LoopContinueLayerParams* release_loopcontinue();
  ::CoreML::Specification::LoopContinueLayerParams* mutable_loopcontinue();
  void set_allocated_loopcontinue(::CoreML::Specification::LoopContinueLayerParams* loopcontinue);
  private:
  const ::CoreML::Specification::LoopContinueLayerParams& _internal_loopcontinue() const;
  ::CoreML::Specification::LoopContinueLayerParams* _internal_mutable_loopcontinue();
  public:
  void unsafe_arena_set_allocated_loopcontinue(
      ::CoreML::Specification::LoopContinueLayerParams* loopcontinue);
  ::CoreML::Specification::LoopContinueLayerParams* unsafe_arena_release_loopcontinue();

  // .CoreML.Specification.RangeStaticLayerParams rangeStatic = 635;
  bool has_rangestatic() const;
  private:
  bool _internal_has_rangestatic() const;
  public:
  void clear_rangestatic();
  const ::CoreML::Specification::RangeStaticLayerParams& rangestatic() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::RangeStaticLayerParams* release_rangestatic();
  ::CoreML::Specification::RangeStaticLayerParams* mutable_rangestatic();
  void set_allocated_rangestatic(::CoreML::Specification::RangeStaticLayerParams* rangestatic);
  private:
  const ::CoreML::Specification::RangeStaticLayerParams& _internal_rangestatic() const;
  ::CoreML::Specification::RangeStaticLayerParams* _internal_mutable_rangestatic();
  public:
  void unsafe_arena_set_allocated_rangestatic(
      ::CoreML::Specification::RangeStaticLayerParams* rangestatic);
  ::CoreML::Specification::RangeStaticLayerParams* unsafe_arena_release_rangestatic();

  // .CoreML.Specification.RangeDynamicLayerParams rangeDynamic = 640;
  bool has_rangedynamic() const;
  private:
  bool _internal_has_rangedynamic() const;
  public:
  void clear_rangedynamic();
  const ::CoreML::Specification::RangeDynamicLayerParams& rangedynamic() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::RangeDynamicLayerParams* release_rangedynamic();
  ::CoreML::Specification::RangeDynamicLayerParams* mutable_rangedynamic();
  void set_allocated_rangedynamic(::CoreML::Specification::RangeDynamicLayerParams* rangedynamic);
  private:
  const ::CoreML::Specification::RangeDynamicLayerParams& _internal_rangedynamic() const;
  ::CoreML::Specification::RangeDynamicLayerParams* _internal_mutable_rangedynamic();
  public:
  void unsafe_arena_set_allocated_rangedynamic(
      ::CoreML::Specification::RangeDynamicLayerParams* rangedynamic);
  ::CoreML::Specification::RangeDynamicLayerParams* unsafe_arena_release_rangedynamic();

  // .CoreML.Specification.ClipLayerParams clip = 660;
  bool has_clip() const;
  private:
  bool _internal_has_clip() const;
  public:
  void clear_clip();
  const ::CoreML::Specification::ClipLayerParams& clip() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ClipLayerParams* release_clip();
  ::CoreML::Specification::ClipLayerParams* mutable_clip();
  void set_allocated_clip(::CoreML::Specification::ClipLayerParams* clip);
  private:
  const ::CoreML::Specification::ClipLayerParams& _internal_clip() const;
  ::CoreML::Specification::ClipLayerParams* _internal_mutable_clip();
  public:
  void unsafe_arena_set_allocated_clip(
      ::CoreML::Specification::ClipLayerParams* clip);
  ::CoreML::Specification::ClipLayerParams* unsafe_arena_release_clip();

  // .CoreML.Specification.CeilLayerParams ceil = 665;
  bool has_ceil() const;
  private:
  bool _internal_has_ceil() const;
  public:
  void clear_ceil();
  const ::CoreML::Specification::CeilLayerParams& ceil() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::CeilLayerParams* release_ceil();
  ::CoreML::Specification::CeilLayerParams* mutable_ceil();
  void set_allocated_ceil(::CoreML::Specification::CeilLayerParams* ceil);
  private:
  const ::CoreML::Specification::CeilLayerParams& _internal_ceil() const;
  ::CoreML::Specification::CeilLayerParams* _internal_mutable_ceil();
  public:
  void unsafe_arena_set_allocated_ceil(
      ::CoreML::Specification::CeilLayerParams* ceil);
  ::CoreML::Specification::CeilLayerParams* unsafe_arena_release_ceil();

  // .CoreML.Specification.FloorLayerParams floor = 670;
  bool has_floor() const;
  private:
  bool _internal_has_floor() const;
  public:
  void clear_floor();
  const ::CoreML::Specification::FloorLayerParams& floor() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::FloorLayerParams* release_floor();
  ::CoreML::Specification::FloorLayerParams* mutable_floor();
  void set_allocated_floor(::CoreML::Specification::FloorLayerParams* floor);
  private:
  const ::CoreML::Specification::FloorLayerParams& _internal_floor() const;
  ::CoreML::Specification::FloorLayerParams* _internal_mutable_floor();
  public:
  void unsafe_arena_set_allocated_floor(
      ::CoreML::Specification::FloorLayerParams* floor);
  ::CoreML::Specification::FloorLayerParams* unsafe_arena_release_floor();

  // .CoreML.Specification.SignLayerParams sign = 680;
  bool has_sign() const;
  private:
  bool _internal_has_sign() const;
  public:
  void clear_sign();
  const ::CoreML::Specification::SignLayerParams& sign() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::SignLayerParams* release_sign();
  ::CoreML::Specification::SignLayerParams* mutable_sign();
  void set_allocated_sign(::CoreML::Specification::SignLayerParams* sign);
  private:
  const ::CoreML::Specification::SignLayerParams& _internal_sign() const;
  ::CoreML::Specification::SignLayerParams* _internal_mutable_sign();
  public:
  void unsafe_arena_set_allocated_sign(
      ::CoreML::Specification::SignLayerParams* sign);
  ::CoreML::Specification::SignLayerParams* unsafe_arena_release_sign();

  // .CoreML.Specification.RoundLayerParams round = 685;
  bool has_round() const;
  private:
  bool _internal_has_round() const;
  public:
  void clear_round();
  const ::CoreML::Specification::RoundLayerParams& round() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::RoundLayerParams* release_round();
  ::CoreML::Specification::RoundLayerParams* mutable_round();
  void set_allocated_round(::CoreML::Specification::RoundLayerParams* round);
  private:
  const ::CoreML::Specification::RoundLayerParams& _internal_round() const;
  ::CoreML::Specification::RoundLayerParams* _internal_mutable_round();
  public:
  void unsafe_arena_set_allocated_round(
      ::CoreML::Specification::RoundLayerParams* round);
  ::CoreML::Specification::RoundLayerParams* unsafe_arena_release_round();

  // .CoreML.Specification.Exp2LayerParams exp2 = 700;
  bool has_exp2() const;
  private:
  bool _internal_has_exp2() const;
  public:
  void clear_exp2();
  const ::CoreML::Specification::Exp2LayerParams& exp2() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::Exp2LayerParams* release_exp2();
  ::CoreML::Specification::Exp2LayerParams* mutable_exp2();
  void set_allocated_exp2(::CoreML::Specification::Exp2LayerParams* exp2);
  private:
  const ::CoreML::Specification::Exp2LayerParams& _internal_exp2() const;
  ::CoreML::Specification::Exp2LayerParams* _internal_mutable_exp2();
  public:
  void unsafe_arena_set_allocated_exp2(
      ::CoreML::Specification::Exp2LayerParams* exp2);
  ::CoreML::Specification::Exp2LayerParams* unsafe_arena_release_exp2();

  // .CoreML.Specification.SinLayerParams sin = 710;
  bool has_sin() const;
  private:
  bool _internal_has_sin() const;
  public:
  void clear_sin();
  const ::CoreML::Specification::SinLayerParams& sin() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::SinLayerParams* release_sin();
  ::CoreML::Specification::SinLayerParams* mutable_sin();
  void set_allocated_sin(::CoreML::Specification::SinLayerParams* sin);
  private:
  const ::CoreML::Specification::SinLayerParams& _internal_sin() const;
  ::CoreML::Specification::SinLayerParams* _internal_mutable_sin();
  public:
  void unsafe_arena_set_allocated_sin(
      ::CoreML::Specification::SinLayerParams* sin);
  ::CoreML::Specification::SinLayerParams* unsafe_arena_release_sin();

  // .CoreML.Specification.CosLayerParams cos = 715;
  bool has_cos() const;
  private:
  bool _internal_has_cos() const;
  public:
  void clear_cos();
  const ::CoreML::Specification::CosLayerParams& cos() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::CosLayerParams* release_cos();
  ::CoreML::Specification::CosLayerParams* mutable_cos();
  void set_allocated_cos(::CoreML::Specification::CosLayerParams* cos);
  private:
  const ::CoreML::Specification::CosLayerParams& _internal_cos() const;
  ::CoreML::Specification::CosLayerParams* _internal_mutable_cos();
  public:
  void unsafe_arena_set_allocated_cos(
      ::CoreML::Specification::CosLayerParams* cos);
  ::CoreML::Specification::CosLayerParams* unsafe_arena_release_cos();

  // .CoreML.Specification.TanLayerParams tan = 720;
  bool has_tan() const;
  private:
  bool _internal_has_tan() const;
  public:
  void clear_tan();
  const ::CoreML::Specification::TanLayerParams& tan() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::TanLayerParams* release_tan();
  ::CoreML::Specification::TanLayerParams* mutable_tan();
  void set_allocated_tan(::CoreML::Specification::TanLayerParams* tan);
  private:
  const ::CoreML::Specification::TanLayerParams& _internal_tan() const;
  ::CoreML::Specification::TanLayerParams* _internal_mutable_tan();
  public:
  void unsafe_arena_set_allocated_tan(
      ::CoreML::Specification::TanLayerParams* tan);
  ::CoreML::Specification::TanLayerParams* unsafe_arena_release_tan();

  // .CoreML.Specification.AsinLayerParams asin = 730;
  bool has_asin() const;
  private:
  bool _internal_has_asin() const;
  public:
  void clear_asin();
  const ::CoreML::Specification::AsinLayerParams& asin() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::AsinLayerParams* release_asin();
  ::CoreML::Specification::AsinLayerParams* mutable_asin();
  void set_allocated_asin(::CoreML::Specification::AsinLayerParams* asin);
  private:
  const ::CoreML::Specification::AsinLayerParams& _internal_asin() const;
  ::CoreML::Specification::AsinLayerParams* _internal_mutable_asin();
  public:
  void unsafe_arena_set_allocated_asin(
      ::CoreML::Specification::AsinLayerParams* asin);
  ::CoreML::Specification::AsinLayerParams* unsafe_arena_release_asin();

  // .CoreML.Specification.AcosLayerParams acos = 735;
  bool has_acos() const;
  private:
  bool _internal_has_acos() const;
  public:
  void clear_acos();
  const ::CoreML::Specification::AcosLayerParams& acos() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::AcosLayerParams* release_acos();
  ::CoreML::Specification::AcosLayerParams* mutable_acos();
  void set_allocated_acos(::CoreML::Specification::AcosLayerParams* acos);
  private:
  const ::CoreML::Specification::AcosLayerParams& _internal_acos() const;
  ::CoreML::Specification::AcosLayerParams* _internal_mutable_acos();
  public:
  void unsafe_arena_set_allocated_acos(
      ::CoreML::Specification::AcosLayerParams* acos);
  ::CoreML::Specification::AcosLayerParams* unsafe_arena_release_acos();

  // .CoreML.Specification.AtanLayerParams atan = 740;
  bool has_atan() const;
  private:
  bool _internal_has_atan() const;
  public:
  void clear_atan();
  const ::CoreML::Specification::AtanLayerParams& atan() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::AtanLayerParams* release_atan();
  ::CoreML::Specification::AtanLayerParams* mutable_atan();
  void set_allocated_atan(::CoreML::Specification::AtanLayerParams* atan);
  private:
  const ::CoreML::Specification::AtanLayerParams& _internal_atan() const;
  ::CoreML::Specification::AtanLayerParams* _internal_mutable_atan();
  public:
  void unsafe_arena_set_allocated_atan(
      ::CoreML::Specification::AtanLayerParams* atan);
  ::CoreML::Specification::AtanLayerParams* unsafe_arena_release_atan();

  // .CoreML.Specification.SinhLayerParams sinh = 750;
  bool has_sinh() const;
  private:
  bool _internal_has_sinh() const;
  public:
  void clear_sinh();
  const ::CoreML::Specification::SinhLayerParams& sinh() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::SinhLayerParams* release_sinh();
  ::CoreML::Specification::SinhLayerParams* mutable_sinh();
  void set_allocated_sinh(::CoreML::Specification::SinhLayerParams* sinh);
  private:
  const ::CoreML::Specification::SinhLayerParams& _internal_sinh() const;
  ::CoreML::Specification::SinhLayerParams* _internal_mutable_sinh();
  public:
  void unsafe_arena_set_allocated_sinh(
      ::CoreML::Specification::SinhLayerParams* sinh);
  ::CoreML::Specification::SinhLayerParams* unsafe_arena_release_sinh();

  // .CoreML.Specification.CoshLayerParams cosh = 755;
  bool has_cosh() const;
  private:
  bool _internal_has_cosh() const;
  public:
  void clear_cosh();
  const ::CoreML::Specification::CoshLayerParams& cosh() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::CoshLayerParams* release_cosh();
  ::CoreML::Specification::CoshLayerParams* mutable_cosh();
  void set_allocated_cosh(::CoreML::Specification::CoshLayerParams* cosh);
  private:
  const ::CoreML::Specification::CoshLayerParams& _internal_cosh() const;
  ::CoreML::Specification::CoshLayerParams* _internal_mutable_cosh();
  public:
  void unsafe_arena_set_allocated_cosh(
      ::CoreML::Specification::CoshLayerParams* cosh);
  ::CoreML::Specification::CoshLayerParams* unsafe_arena_release_cosh();

  // .CoreML.Specification.TanhLayerParams tanh = 760;
  bool has_tanh() const;
  private:
  bool _internal_has_tanh() const;
  public:
  void clear_tanh();
  const ::CoreML::Specification::TanhLayerParams& tanh() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::TanhLayerParams* release_tanh();
  ::CoreML::Specification::TanhLayerParams* mutable_tanh();
  void set_allocated_tanh(::CoreML::Specification::TanhLayerParams* tanh);
  private:
  const ::CoreML::Specification::TanhLayerParams& _internal_tanh() const;
  ::CoreML::Specification::TanhLayerParams* _internal_mutable_tanh();
  public:
  void unsafe_arena_set_allocated_tanh(
      ::CoreML::Specification::TanhLayerParams* tanh);
  ::CoreML::Specification::TanhLayerParams* unsafe_arena_release_tanh();

  // .CoreML.Specification.AsinhLayerParams asinh = 770;
  bool has_asinh() const;
  private:
  bool _internal_has_asinh() const;
  public:
  void clear_asinh();
  const ::CoreML::Specification::AsinhLayerParams& asinh() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::AsinhLayerParams* release_asinh();
  ::CoreML::Specification::AsinhLayerParams* mutable_asinh();
  void set_allocated_asinh(::CoreML::Specification::AsinhLayerParams* asinh);
  private:
  const ::CoreML::Specification::AsinhLayerParams& _internal_asinh() const;
  ::CoreML::Specification::AsinhLayerParams* _internal_mutable_asinh();
  public:
  void unsafe_arena_set_allocated_asinh(
      ::CoreML::Specification::AsinhLayerParams* asinh);
  ::CoreML::Specification::AsinhLayerParams* unsafe_arena_release_asinh();

  // .CoreML.Specification.AcoshLayerParams acosh = 775;
  bool has_acosh() const;
  private:
  bool _internal_has_acosh() const;
  public:
  void clear_acosh();
  const ::CoreML::Specification::AcoshLayerParams& acosh() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::AcoshLayerParams* release_acosh();
  ::CoreML::Specification::AcoshLayerParams* mutable_acosh();
  void set_allocated_acosh(::CoreML::Specification::AcoshLayerParams* acosh);
  private:
  const ::CoreML::Specification::AcoshLayerParams& _internal_acosh() const;
  ::CoreML::Specification::AcoshLayerParams* _internal_mutable_acosh();
  public:
  void unsafe_arena_set_allocated_acosh(
      ::CoreML::Specification::AcoshLayerParams* acosh);
  ::CoreML::Specification::AcoshLayerParams* unsafe_arena_release_acosh();

  // .CoreML.Specification.AtanhLayerParams atanh = 780;
  bool has_atanh() const;
  private:
  bool _internal_has_atanh() const;
  public:
  void clear_atanh();
  const ::CoreML::Specification::AtanhLayerParams& atanh() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::AtanhLayerParams* release_atanh();
  ::CoreML::Specification::AtanhLayerParams* mutable_atanh();
  void set_allocated_atanh(::CoreML::Specification::AtanhLayerParams* atanh);
  private:
  const ::CoreML::Specification::AtanhLayerParams& _internal_atanh() const;
  ::CoreML::Specification::AtanhLayerParams* _internal_mutable_atanh();
  public:
  void unsafe_arena_set_allocated_atanh(
      ::CoreML::Specification::AtanhLayerParams* atanh);
  ::CoreML::Specification::AtanhLayerParams* unsafe_arena_release_atanh();

  // .CoreML.Specification.ErfLayerParams erf = 790;
  bool has_erf() const;
  private:
  bool _internal_has_erf() const;
  public:
  void clear_erf();
  const ::CoreML::Specification::ErfLayerParams& erf() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ErfLayerParams* release_erf();
  ::CoreML::Specification::ErfLayerParams* mutable_erf();
  void set_allocated_erf(::CoreML::Specification::ErfLayerParams* erf);
  private:
  const ::CoreML::Specification::ErfLayerParams& _internal_erf() const;
  ::CoreML::Specification::ErfLayerParams* _internal_mutable_erf();
  public:
  void unsafe_arena_set_allocated_erf(
      ::CoreML::Specification::ErfLayerParams* erf);
  ::CoreML::Specification::ErfLayerParams* unsafe_arena_release_erf();

  // .CoreML.Specification.GeluLayerParams gelu = 795;
  bool has_gelu() const;
  private:
  bool _internal_has_gelu() const;
  public:
  void clear_gelu();
  const ::CoreML::Specification::GeluLayerParams& gelu() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::GeluLayerParams* release_gelu();
  ::CoreML::Specification::GeluLayerParams* mutable_gelu();
  void set_allocated_gelu(::CoreML::Specification::GeluLayerParams* gelu);
  private:
  const ::CoreML::Specification::GeluLayerParams& _internal_gelu() const;
  ::CoreML::Specification::GeluLayerParams* _internal_mutable_gelu();
  public:
  void unsafe_arena_set_allocated_gelu(
      ::CoreML::Specification::GeluLayerParams* gelu);
  ::CoreML::Specification::GeluLayerParams* unsafe_arena_release_gelu();

  // .CoreML.Specification.EqualLayerParams equal = 815;
  bool has_equal() const;
  private:
  bool _internal_has_equal() const;
  public:
  void clear_equal();
  const ::CoreML::Specification::EqualLayerParams& equal() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::EqualLayerParams* release_equal();
  ::CoreML::Specification::EqualLayerParams* mutable_equal();
  void set_allocated_equal(::CoreML::Specification::EqualLayerParams* equal);
  private:
  const ::CoreML::Specification::EqualLayerParams& _internal_equal() const;
  ::CoreML::Specification::EqualLayerParams* _internal_mutable_equal();
  public:
  void unsafe_arena_set_allocated_equal(
      ::CoreML::Specification::EqualLayerParams* equal);
  ::CoreML::Specification::EqualLayerParams* unsafe_arena_release_equal();

  // .CoreML.Specification.NotEqualLayerParams notEqual = 820;
  bool has_notequal() const;
  private:
  bool _internal_has_notequal() const;
  public:
  void clear_notequal();
  const ::CoreML::Specification::NotEqualLayerParams& notequal() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::NotEqualLayerParams* release_notequal();
  ::CoreML::Specification::NotEqualLayerParams* mutable_notequal();
  void set_allocated_notequal(::CoreML::Specification::NotEqualLayerParams* notequal);
  private:
  const ::CoreML::Specification::NotEqualLayerParams& _internal_notequal() const;
  ::CoreML::Specification::NotEqualLayerParams* _internal_mutable_notequal();
  public:
  void unsafe_arena_set_allocated_notequal(
      ::CoreML::Specification::NotEqualLayerParams* notequal);
  ::CoreML::Specification::NotEqualLayerParams* unsafe_arena_release_notequal();

  // .CoreML.Specification.LessThanLayerParams lessThan = 825;
  bool has_lessthan() const;
  private:
  bool _internal_has_lessthan() const;
  public:
  void clear_lessthan();
  const ::CoreML::Specification::LessThanLayerParams& lessthan() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::LessThanLayerParams* release_lessthan();
  ::CoreML::Specification::LessThanLayerParams* mutable_lessthan();
  void set_allocated_lessthan(::CoreML::Specification::LessThanLayerParams* lessthan);
  private:
  const ::CoreML::Specification::LessThanLayerParams& _internal_lessthan() const;
  ::CoreML::Specification::LessThanLayerParams* _internal_mutable_lessthan();
  public:
  void unsafe_arena_set_allocated_lessthan(
      ::CoreML::Specification::LessThanLayerParams* lessthan);
  ::CoreML::Specification::LessThanLayerParams* unsafe_arena_release_lessthan();

  // .CoreML.Specification.LessEqualLayerParams lessEqual = 827;
  bool has_lessequal() const;
  private:
  bool _internal_has_lessequal() const;
  public:
  void clear_lessequal();
  const ::CoreML::Specification::LessEqualLayerParams& lessequal() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::LessEqualLayerParams* release_lessequal();
  ::CoreML::Specification::LessEqualLayerParams* mutable_lessequal();
  void set_allocated_lessequal(::CoreML::Specification::LessEqualLayerParams* lessequal);
  private:
  const ::CoreML::Specification::LessEqualLayerParams& _internal_lessequal() const;
  ::CoreML::Specification::LessEqualLayerParams* _internal_mutable_lessequal();
  public:
  void unsafe_arena_set_allocated_lessequal(
      ::CoreML::Specification::LessEqualLayerParams* lessequal);
  ::CoreML::Specification::LessEqualLayerParams* unsafe_arena_release_lessequal();

  // .CoreML.Specification.GreaterThanLayerParams greaterThan = 830;
  bool has_greaterthan() const;
  private:
  bool _internal_has_greaterthan() const;
  public:
  void clear_greaterthan();
  const ::CoreML::Specification::GreaterThanLayerParams& greaterthan() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::GreaterThanLayerParams* release_greaterthan();
  ::CoreML::Specification::GreaterThanLayerParams* mutable_greaterthan();
  void set_allocated_greaterthan(::CoreML::Specification::GreaterThanLayerParams* greaterthan);
  private:
  const ::CoreML::Specification::GreaterThanLayerParams& _internal_greaterthan() const;
  ::CoreML::Specification::GreaterThanLayerParams* _internal_mutable_greaterthan();
  public:
  void unsafe_arena_set_allocated_greaterthan(
      ::CoreML::Specification::GreaterThanLayerParams* greaterthan);
  ::CoreML::Specification::GreaterThanLayerParams* unsafe_arena_release_greaterthan();

  // .CoreML.Specification.GreaterEqualLayerParams greaterEqual = 832;
  bool has_greaterequal() const;
  private:
  bool _internal_has_greaterequal() const;
  public:
  void clear_greaterequal();
  const ::CoreML::Specification::GreaterEqualLayerParams& greaterequal() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::GreaterEqualLayerParams* release_greaterequal();
  ::CoreML::Specification::GreaterEqualLayerParams* mutable_greaterequal();
  void set_allocated_greaterequal(::CoreML::Specification::GreaterEqualLayerParams* greaterequal);
  private:
  const ::CoreML::Specification::GreaterEqualLayerParams& _internal_greaterequal() const;
  ::CoreML::Specification::GreaterEqualLayerParams* _internal_mutable_greaterequal();
  public:
  void unsafe_arena_set_allocated_greaterequal(
      ::CoreML::Specification::GreaterEqualLayerParams* greaterequal);
  ::CoreML::Specification::GreaterEqualLayerParams* unsafe_arena_release_greaterequal();

  // .CoreML.Specification.LogicalOrLayerParams logicalOr = 840;
  bool has_logicalor() const;
  private:
  bool _internal_has_logicalor() const;
  public:
  void clear_logicalor();
  const ::CoreML::Specification::LogicalOrLayerParams& logicalor() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::LogicalOrLayerParams* release_logicalor();
  ::CoreML::Specification::LogicalOrLayerParams* mutable_logicalor();
  void set_allocated_logicalor(::CoreML::Specification::LogicalOrLayerParams* logicalor);
  private:
  const ::CoreML::Specification::LogicalOrLayerParams& _internal_logicalor() const;
  ::CoreML::Specification::LogicalOrLayerParams* _internal_mutable_logicalor();
  public:
  void unsafe_arena_set_allocated_logicalor(
      ::CoreML::Specification::LogicalOrLayerParams* logicalor);
  ::CoreML::Specification::LogicalOrLayerParams* unsafe_arena_release_logicalor();

  // .CoreML.Specification.LogicalXorLayerParams logicalXor = 845;
  bool has_logicalxor() const;
  private:
  bool _internal_has_logicalxor() const;
  public:
  void clear_logicalxor();
  const ::CoreML::Specification::LogicalXorLayerParams& logicalxor() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::LogicalXorLayerParams* release_logicalxor();
  ::CoreML::Specification::LogicalXorLayerParams* mutable_logicalxor();
  void set_allocated_logicalxor(::CoreML::Specification::LogicalXorLayerParams* logicalxor);
  private:
  const ::CoreML::Specification::LogicalXorLayerParams& _internal_logicalxor() const;
  ::CoreML::Specification::LogicalXorLayerParams* _internal_mutable_logicalxor();
  public:
  void unsafe_arena_set_allocated_logicalxor(
      ::CoreML::Specification::LogicalXorLayerParams* logicalxor);
  ::CoreML::Specification::LogicalXorLayerParams* unsafe_arena_release_logicalxor();

  // .CoreML.Specification.LogicalNotLayerParams logicalNot = 850;
  bool has_logicalnot() const;
  private:
  bool _internal_has_logicalnot() const;
  public:
  void clear_logicalnot();
  const ::CoreML::Specification::LogicalNotLayerParams& logicalnot() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::LogicalNotLayerParams* release_logicalnot();
  ::CoreML::Specification::LogicalNotLayerParams* mutable_logicalnot();
  void set_allocated_logicalnot(::CoreML::Specification::LogicalNotLayerParams* logicalnot);
  private:
  const ::CoreML::Specification::LogicalNotLayerParams& _internal_logicalnot() const;
  ::CoreML::Specification::LogicalNotLayerParams* _internal_mutable_logicalnot();
  public:
  void unsafe_arena_set_allocated_logicalnot(
      ::CoreML::Specification::LogicalNotLayerParams* logicalnot);
  ::CoreML::Specification::LogicalNotLayerParams* unsafe_arena_release_logicalnot();

  // .CoreML.Specification.LogicalAndLayerParams logicalAnd = 855;
  bool has_logicaland() const;
  private:
  bool _internal_has_logicaland() const;
  public:
  void clear_logicaland();
  const ::CoreML::Specification::LogicalAndLayerParams& logicaland() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::LogicalAndLayerParams* release_logicaland();
  ::CoreML::Specification::LogicalAndLayerParams* mutable_logicaland();
  void set_allocated_logicaland(::CoreML::Specification::LogicalAndLayerParams* logicaland);
  private:
  const ::CoreML::Specification::LogicalAndLayerParams& _internal_logicaland() const;
  ::CoreML::Specification::LogicalAndLayerParams* _internal_mutable_logicaland();
  public:
  void unsafe_arena_set_allocated_logicaland(
      ::CoreML::Specification::LogicalAndLayerParams* logicaland);
  ::CoreML::Specification::LogicalAndLayerParams* unsafe_arena_release_logicaland();

  // .CoreML.Specification.ModBroadcastableLayerParams modBroadcastable = 865;
  bool has_modbroadcastable() const;
  private:
  bool _internal_has_modbroadcastable() const;
  public:
  void clear_modbroadcastable();
  const ::CoreML::Specification::ModBroadcastableLayerParams& modbroadcastable() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ModBroadcastableLayerParams* release_modbroadcastable();
  ::CoreML::Specification::ModBroadcastableLayerParams* mutable_modbroadcastable();
  void set_allocated_modbroadcastable(::CoreML::Specification::ModBroadcastableLayerParams* modbroadcastable);
  private:
  const ::CoreML::Specification::ModBroadcastableLayerParams& _internal_modbroadcastable() const;
  ::CoreML::Specification::ModBroadcastableLayerParams* _internal_mutable_modbroadcastable();
  public:
  void unsafe_arena_set_allocated_modbroadcastable(
      ::CoreML::Specification::ModBroadcastableLayerParams* modbroadcastable);
  ::CoreML::Specification::ModBroadcastableLayerParams* unsafe_arena_release_modbroadcastable();

  // .CoreML.Specification.MinBroadcastableLayerParams minBroadcastable = 870;
  bool has_minbroadcastable() const;
  private:
  bool _internal_has_minbroadcastable() const;
  public:
  void clear_minbroadcastable();
  const ::CoreML::Specification::MinBroadcastableLayerParams& minbroadcastable() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::MinBroadcastableLayerParams* release_minbroadcastable();
  ::CoreML::Specification::MinBroadcastableLayerParams* mutable_minbroadcastable();
  void set_allocated_minbroadcastable(::CoreML::Specification::MinBroadcastableLayerParams* minbroadcastable);
  private:
  const ::CoreML::Specification::MinBroadcastableLayerParams& _internal_minbroadcastable() const;
  ::CoreML::Specification::MinBroadcastableLayerParams* _internal_mutable_minbroadcastable();
  public:
  void unsafe_arena_set_allocated_minbroadcastable(
      ::CoreML::Specification::MinBroadcastableLayerParams* minbroadcastable);
  ::CoreML::Specification::MinBroadcastableLayerParams* unsafe_arena_release_minbroadcastable();

  // .CoreML.Specification.MaxBroadcastableLayerParams maxBroadcastable = 875;
  bool has_maxbroadcastable() const;
  private:
  bool _internal_has_maxbroadcastable() const;
  public:
  void clear_maxbroadcastable();
  const ::CoreML::Specification::MaxBroadcastableLayerParams& maxbroadcastable() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::MaxBroadcastableLayerParams* release_maxbroadcastable();
  ::CoreML::Specification::MaxBroadcastableLayerParams* mutable_maxbroadcastable();
  void set_allocated_maxbroadcastable(::CoreML::Specification::MaxBroadcastableLayerParams* maxbroadcastable);
  private:
  const ::CoreML::Specification::MaxBroadcastableLayerParams& _internal_maxbroadcastable() const;
  ::CoreML::Specification::MaxBroadcastableLayerParams* _internal_mutable_maxbroadcastable();
  public:
  void unsafe_arena_set_allocated_maxbroadcastable(
      ::CoreML::Specification::MaxBroadcastableLayerParams* maxbroadcastable);
  ::CoreML::Specification::MaxBroadcastableLayerParams* unsafe_arena_release_maxbroadcastable();

  // .CoreML.Specification.AddBroadcastableLayerParams addBroadcastable = 880;
  bool has_addbroadcastable() const;
  private:
  bool _internal_has_addbroadcastable() const;
  public:
  void clear_addbroadcastable();
  const ::CoreML::Specification::AddBroadcastableLayerParams& addbroadcastable() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::AddBroadcastableLayerParams* release_addbroadcastable();
  ::CoreML::Specification::AddBroadcastableLayerParams* mutable_addbroadcastable();
  void set_allocated_addbroadcastable(::CoreML::Specification::AddBroadcastableLayerParams* addbroadcastable);
  private:
  const ::CoreML::Specification::AddBroadcastableLayerParams& _internal_addbroadcastable() const;
  ::CoreML::Specification::AddBroadcastableLayerParams* _internal_mutable_addbroadcastable();
  public:
  void unsafe_arena_set_allocated_addbroadcastable(
      ::CoreML::Specification::AddBroadcastableLayerParams* addbroadcastable);
  ::CoreML::Specification::AddBroadcastableLayerParams* unsafe_arena_release_addbroadcastable();

  // .CoreML.Specification.PowBroadcastableLayerParams powBroadcastable = 885;
  bool has_powbroadcastable() const;
  private:
  bool _internal_has_powbroadcastable() const;
  public:
  void clear_powbroadcastable();
  const ::CoreML::Specification::PowBroadcastableLayerParams& powbroadcastable() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::PowBroadcastableLayerParams* release_powbroadcastable();
  ::CoreML::Specification::PowBroadcastableLayerParams* mutable_powbroadcastable();
  void set_allocated_powbroadcastable(::CoreML::Specification::PowBroadcastableLayerParams* powbroadcastable);
  private:
  const ::CoreML::Specification::PowBroadcastableLayerParams& _internal_powbroadcastable() const;
  ::CoreML::Specification::PowBroadcastableLayerParams* _internal_mutable_powbroadcastable();
  public:
  void unsafe_arena_set_allocated_powbroadcastable(
      ::CoreML::Specification::PowBroadcastableLayerParams* powbroadcastable);
  ::CoreML::Specification::PowBroadcastableLayerParams* unsafe_arena_release_powbroadcastable();

  // .CoreML.Specification.DivideBroadcastableLayerParams divideBroadcastable = 890;
  bool has_dividebroadcastable() const;
  private:
  bool _internal_has_dividebroadcastable() const;
  public:
  void clear_dividebroadcastable();
  const ::CoreML::Specification::DivideBroadcastableLayerParams& dividebroadcastable() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::DivideBroadcastableLayerParams* release_dividebroadcastable();
  ::CoreML::Specification::DivideBroadcastableLayerParams* mutable_dividebroadcastable();
  void set_allocated_dividebroadcastable(::CoreML::Specification::DivideBroadcastableLayerParams* dividebroadcastable);
  private:
  const ::CoreML::Specification::DivideBroadcastableLayerParams& _internal_dividebroadcastable() const;
  ::CoreML::Specification::DivideBroadcastableLayerParams* _internal_mutable_dividebroadcastable();
  public:
  void unsafe_arena_set_allocated_dividebroadcastable(
      ::CoreML::Specification::DivideBroadcastableLayerParams* dividebroadcastable);
  ::CoreML::Specification::DivideBroadcastableLayerParams* unsafe_arena_release_dividebroadcastable();

  // .CoreML.Specification.FloorDivBroadcastableLayerParams floorDivBroadcastable = 895;
  bool has_floordivbroadcastable() const;
  private:
  bool _internal_has_floordivbroadcastable() const;
  public:
  void clear_floordivbroadcastable();
  const ::CoreML::Specification::FloorDivBroadcastableLayerParams& floordivbroadcastable() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::FloorDivBroadcastableLayerParams* release_floordivbroadcastable();
  ::CoreML::Specification::FloorDivBroadcastableLayerParams* mutable_floordivbroadcastable();
  void set_allocated_floordivbroadcastable(::CoreML::Specification::FloorDivBroadcastableLayerParams* floordivbroadcastable);
  private:
  const ::CoreML::Specification::FloorDivBroadcastableLayerParams& _internal_floordivbroadcastable() const;
  ::CoreML::Specification::FloorDivBroadcastableLayerParams* _internal_mutable_floordivbroadcastable();
  public:
  void unsafe_arena_set_allocated_floordivbroadcastable(
      ::CoreML::Specification::FloorDivBroadcastableLayerParams* floordivbroadcastable);
  ::CoreML::Specification::FloorDivBroadcastableLayerParams* unsafe_arena_release_floordivbroadcastable();

  // .CoreML.Specification.MultiplyBroadcastableLayerParams multiplyBroadcastable = 900;
  bool has_multiplybroadcastable() const;
  private:
  bool _internal_has_multiplybroadcastable() const;
  public:
  void clear_multiplybroadcastable();
  const ::CoreML::Specification::MultiplyBroadcastableLayerParams& multiplybroadcastable() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::MultiplyBroadcastableLayerParams* release_multiplybroadcastable();
  ::CoreML::Specification::MultiplyBroadcastableLayerParams* mutable_multiplybroadcastable();
  void set_allocated_multiplybroadcastable(::CoreML::Specification::MultiplyBroadcastableLayerParams* multiplybroadcastable);
  private:
  const ::CoreML::Specification::MultiplyBroadcastableLayerParams& _internal_multiplybroadcastable() const;
  ::CoreML::Specification::MultiplyBroadcastableLayerParams* _internal_mutable_multiplybroadcastable();
  public:
  void unsafe_arena_set_allocated_multiplybroadcastable(
      ::CoreML::Specification::MultiplyBroadcastableLayerParams* multiplybroadcastable);
  ::CoreML::Specification::MultiplyBroadcastableLayerParams* unsafe_arena_release_multiplybroadcastable();

  // .CoreML.Specification.SubtractBroadcastableLayerParams subtractBroadcastable = 905;
  bool has_subtractbroadcastable() const;
  private:
  bool _internal_has_subtractbroadcastable() const;
  public:
  void clear_subtractbroadcastable();
  const ::CoreML::Specification::SubtractBroadcastableLayerParams& subtractbroadcastable() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::SubtractBroadcastableLayerParams* release_subtractbroadcastable();
  ::CoreML::Specification::SubtractBroadcastableLayerParams* mutable_subtractbroadcastable();
  void set_allocated_subtractbroadcastable(::CoreML::Specification::SubtractBroadcastableLayerParams* subtractbroadcastable);
  private:
  const ::CoreML::Specification::SubtractBroadcastableLayerParams& _internal_subtractbroadcastable() const;
  ::CoreML::Specification::SubtractBroadcastableLayerParams* _internal_mutable_subtractbroadcastable();
  public:
  void unsafe_arena_set_allocated_subtractbroadcastable(
      ::CoreML::Specification::SubtractBroadcastableLayerParams* subtractbroadcastable);
  ::CoreML::Specification::SubtractBroadcastableLayerParams* unsafe_arena_release_subtractbroadcastable();

  // .CoreML.Specification.TileLayerParams tile = 920;
  bool has_tile() const;
  private:
  bool _internal_has_tile() const;
  public:
  void clear_tile();
  const ::CoreML::Specification::TileLayerParams& tile() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::TileLayerParams* release_tile();
  ::CoreML::Specification::TileLayerParams* mutable_tile();
  void set_allocated_tile(::CoreML::Specification::TileLayerParams* tile);
  private:
  const ::CoreML::Specification::TileLayerParams& _internal_tile() const;
  ::CoreML::Specification::TileLayerParams* _internal_mutable_tile();
  public:
  void unsafe_arena_set_allocated_tile(
      ::CoreML::Specification::TileLayerParams* tile);
  ::CoreML::Specification::TileLayerParams* unsafe_arena_release_tile();

  // .CoreML.Specification.StackLayerParams stack = 925;
  bool has_stack() const;
  private:
  bool _internal_has_stack() const;
  public:
  void clear_stack();
  const ::CoreML::Specification::StackLayerParams& stack() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::StackLayerParams* release_stack();
  ::CoreML::Specification::StackLayerParams* mutable_stack();
  void set_allocated_stack(::CoreML::Specification::StackLayerParams* stack);
  private:
  const ::CoreML::Specification::StackLayerParams& _internal_stack() const;
  ::CoreML::Specification::StackLayerParams* _internal_mutable_stack();
  public:
  void unsafe_arena_set_allocated_stack(
      ::CoreML::Specification::StackLayerParams* stack);
  ::CoreML::Specification::StackLayerParams* unsafe_arena_release_stack();

  // .CoreML.Specification.GatherLayerParams gather = 930;
  bool has_gather() const;
  private:
  bool _internal_has_gather() const;
  public:
  void clear_gather();
  const ::CoreML::Specification::GatherLayerParams& gather() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::GatherLayerParams* release_gather();
  ::CoreML::Specification::GatherLayerParams* mutable_gather();
  void set_allocated_gather(::CoreML::Specification::GatherLayerParams* gather);
  private:
  const ::CoreML::Specification::GatherLayerParams& _internal_gather() const;
  ::CoreML::Specification::GatherLayerParams* _internal_mutable_gather();
  public:
  void unsafe_arena_set_allocated_gather(
      ::CoreML::Specification::GatherLayerParams* gather);
  ::CoreML::Specification::GatherLayerParams* unsafe_arena_release_gather();

  // .CoreML.Specification.ScatterLayerParams scatter = 935;
  bool has_scatter() const;
  private:
  bool _internal_has_scatter() const;
  public:
  void clear_scatter();
  const ::CoreML::Specification::ScatterLayerParams& scatter() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ScatterLayerParams* release_scatter();
  ::CoreML::Specification::ScatterLayerParams* mutable_scatter();
  void set_allocated_scatter(::CoreML::Specification::ScatterLayerParams* scatter);
  private:
  const ::CoreML::Specification::ScatterLayerParams& _internal_scatter() const;
  ::CoreML::Specification::ScatterLayerParams* _internal_mutable_scatter();
  public:
  void unsafe_arena_set_allocated_scatter(
      ::CoreML::Specification::ScatterLayerParams* scatter);
  ::CoreML::Specification::ScatterLayerParams* unsafe_arena_release_scatter();

  // .CoreML.Specification.GatherNDLayerParams gatherND = 940;
  bool has_gathernd() const;
  private:
  bool _internal_has_gathernd() const;
  public:
  void clear_gathernd();
  const ::CoreML::Specification::GatherNDLayerParams& gathernd() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::GatherNDLayerParams* release_gathernd();
  ::CoreML::Specification::GatherNDLayerParams* mutable_gathernd();
  void set_allocated_gathernd(::CoreML::Specification::GatherNDLayerParams* gathernd);
  private:
  const ::CoreML::Specification::GatherNDLayerParams& _internal_gathernd() const;
  ::CoreML::Specification::GatherNDLayerParams* _internal_mutable_gathernd();
  public:
  void unsafe_arena_set_allocated_gathernd(
      ::CoreML::Specification::GatherNDLayerParams* gathernd);
  ::CoreML::Specification::GatherNDLayerParams* unsafe_arena_release_gathernd();

  // .CoreML.Specification.ScatterNDLayerParams scatterND = 945;
  bool has_scatternd() const;
  private:
  bool _internal_has_scatternd() const;
  public:
  void clear_scatternd();
  const ::CoreML::Specification::ScatterNDLayerParams& scatternd() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ScatterNDLayerParams* release_scatternd();
  ::CoreML::Specification::ScatterNDLayerParams* mutable_scatternd();
  void set_allocated_scatternd(::CoreML::Specification::ScatterNDLayerParams* scatternd);
  private:
  const ::CoreML::Specification::ScatterNDLayerParams& _internal_scatternd() const;
  ::CoreML::Specification::ScatterNDLayerParams* _internal_mutable_scatternd();
  public:
  void unsafe_arena_set_allocated_scatternd(
      ::CoreML::Specification::ScatterNDLayerParams* scatternd);
  ::CoreML::Specification::ScatterNDLayerParams* unsafe_arena_release_scatternd();

  // .CoreML.Specification.SoftmaxNDLayerParams softmaxND = 950;
  bool has_softmaxnd() const;
  private:
  bool _internal_has_softmaxnd() const;
  public:
  void clear_softmaxnd();
  const ::CoreML::Specification::SoftmaxNDLayerParams& softmaxnd() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::SoftmaxNDLayerParams* release_softmaxnd();
  ::CoreML::Specification::SoftmaxNDLayerParams* mutable_softmaxnd();
  void set_allocated_softmaxnd(::CoreML::Specification::SoftmaxNDLayerParams* softmaxnd);
  private:
  const ::CoreML::Specification::SoftmaxNDLayerParams& _internal_softmaxnd() const;
  ::CoreML::Specification::SoftmaxNDLayerParams* _internal_mutable_softmaxnd();
  public:
  void unsafe_arena_set_allocated_softmaxnd(
      ::CoreML::Specification::SoftmaxNDLayerParams* softmaxnd);
  ::CoreML::Specification::SoftmaxNDLayerParams* unsafe_arena_release_softmaxnd();

  // .CoreML.Specification.GatherAlongAxisLayerParams gatherAlongAxis = 952;
  bool has_gatheralongaxis() const;
  private:
  bool _internal_has_gatheralongaxis() const;
  public:
  void clear_gatheralongaxis();
  const ::CoreML::Specification::GatherAlongAxisLayerParams& gatheralongaxis() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::GatherAlongAxisLayerParams* release_gatheralongaxis();
  ::CoreML::Specification::GatherAlongAxisLayerParams* mutable_gatheralongaxis();
  void set_allocated_gatheralongaxis(::CoreML::Specification::GatherAlongAxisLayerParams* gatheralongaxis);
  private:
  const ::CoreML::Specification::GatherAlongAxisLayerParams& _internal_gatheralongaxis() const;
  ::CoreML::Specification::GatherAlongAxisLayerParams* _internal_mutable_gatheralongaxis();
  public:
  void unsafe_arena_set_allocated_gatheralongaxis(
      ::CoreML::Specification::GatherAlongAxisLayerParams* gatheralongaxis);
  ::CoreML::Specification::GatherAlongAxisLayerParams* unsafe_arena_release_gatheralongaxis();

  // .CoreML.Specification.ScatterAlongAxisLayerParams scatterAlongAxis = 954;
  bool has_scatteralongaxis() const;
  private:
  bool _internal_has_scatteralongaxis() const;
  public:
  void clear_scatteralongaxis();
  const ::CoreML::Specification::ScatterAlongAxisLayerParams& scatteralongaxis() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ScatterAlongAxisLayerParams* release_scatteralongaxis();
  ::CoreML::Specification::ScatterAlongAxisLayerParams* mutable_scatteralongaxis();
  void set_allocated_scatteralongaxis(::CoreML::Specification::ScatterAlongAxisLayerParams* scatteralongaxis);
  private:
  const ::CoreML::Specification::ScatterAlongAxisLayerParams& _internal_scatteralongaxis() const;
  ::CoreML::Specification::ScatterAlongAxisLayerParams* _internal_mutable_scatteralongaxis();
  public:
  void unsafe_arena_set_allocated_scatteralongaxis(
      ::CoreML::Specification::ScatterAlongAxisLayerParams* scatteralongaxis);
  ::CoreML::Specification::ScatterAlongAxisLayerParams* unsafe_arena_release_scatteralongaxis();

  // .CoreML.Specification.ReverseLayerParams reverse = 960;
  bool has_reverse() const;
  private:
  bool _internal_has_reverse() const;
  public:
  void clear_reverse();
  const ::CoreML::Specification::ReverseLayerParams& reverse() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ReverseLayerParams* release_reverse();
  ::CoreML::Specification::ReverseLayerParams* mutable_reverse();
  void set_allocated_reverse(::CoreML::Specification::ReverseLayerParams* reverse);
  private:
  const ::CoreML::Specification::ReverseLayerParams& _internal_reverse() const;
  ::CoreML::Specification::ReverseLayerParams* _internal_mutable_reverse();
  public:
  void unsafe_arena_set_allocated_reverse(
      ::CoreML::Specification::ReverseLayerParams* reverse);
  ::CoreML::Specification::ReverseLayerParams* unsafe_arena_release_reverse();

  // .CoreML.Specification.ReverseSeqLayerParams reverseSeq = 965;
  bool has_reverseseq() const;
  private:
  bool _internal_has_reverseseq() const;
  public:
  void clear_reverseseq();
  const ::CoreML::Specification::ReverseSeqLayerParams& reverseseq() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ReverseSeqLayerParams* release_reverseseq();
  ::CoreML::Specification::ReverseSeqLayerParams* mutable_reverseseq();
  void set_allocated_reverseseq(::CoreML::Specification::ReverseSeqLayerParams* reverseseq);
  private:
  const ::CoreML::Specification::ReverseSeqLayerParams& _internal_reverseseq() const;
  ::CoreML::Specification::ReverseSeqLayerParams* _internal_mutable_reverseseq();
  public:
  void unsafe_arena_set_allocated_reverseseq(
      ::CoreML::Specification::ReverseSeqLayerParams* reverseseq);
  ::CoreML::Specification::ReverseSeqLayerParams* unsafe_arena_release_reverseseq();

  // .CoreML.Specification.SplitNDLayerParams splitND = 975;
  bool has_splitnd() const;
  private:
  bool _internal_has_splitnd() const;
  public:
  void clear_splitnd();
  const ::CoreML::Specification::SplitNDLayerParams& splitnd() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::SplitNDLayerParams* release_splitnd();
  ::CoreML::Specification::SplitNDLayerParams* mutable_splitnd();
  void set_allocated_splitnd(::CoreML::Specification::SplitNDLayerParams* splitnd);
  private:
  const ::CoreML::Specification::SplitNDLayerParams& _internal_splitnd() const;
  ::CoreML::Specification::SplitNDLayerParams* _internal_mutable_splitnd();
  public:
  void unsafe_arena_set_allocated_splitnd(
      ::CoreML::Specification::SplitNDLayerParams* splitnd);
  ::CoreML::Specification::SplitNDLayerParams* unsafe_arena_release_splitnd();

  // .CoreML.Specification.ConcatNDLayerParams concatND = 980;
  bool has_concatnd() const;
  private:
  bool _internal_has_concatnd() const;
  public:
  void clear_concatnd();
  const ::CoreML::Specification::ConcatNDLayerParams& concatnd() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ConcatNDLayerParams* release_concatnd();
  ::CoreML::Specification::ConcatNDLayerParams* mutable_concatnd();
  void set_allocated_concatnd(::CoreML::Specification::ConcatNDLayerParams* concatnd);
  private:
  const ::CoreML::Specification::ConcatNDLayerParams& _internal_concatnd() const;
  ::CoreML::Specification::ConcatNDLayerParams* _internal_mutable_concatnd();
  public:
  void unsafe_arena_set_allocated_concatnd(
      ::CoreML::Specification::ConcatNDLayerParams* concatnd);
  ::CoreML::Specification::ConcatNDLayerParams* unsafe_arena_release_concatnd();

  // .CoreML.Specification.TransposeLayerParams transpose = 985;
  bool has_transpose() const;
  private:
  bool _internal_has_transpose() const;
  public:
  void clear_transpose();
  const ::CoreML::Specification::TransposeLayerParams& transpose() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::TransposeLayerParams* release_transpose();
  ::CoreML::Specification::TransposeLayerParams* mutable_transpose();
  void set_allocated_transpose(::CoreML::Specification::TransposeLayerParams* transpose);
  private:
  const ::CoreML::Specification::TransposeLayerParams& _internal_transpose() const;
  ::CoreML::Specification::TransposeLayerParams* _internal_mutable_transpose();
  public:
  void unsafe_arena_set_allocated_transpose(
      ::CoreML::Specification::TransposeLayerParams* transpose);
  ::CoreML::Specification::TransposeLayerParams* unsafe_arena_release_transpose();

  // .CoreML.Specification.SliceStaticLayerParams sliceStatic = 995;
  bool has_slicestatic() const;
  private:
  bool _internal_has_slicestatic() const;
  public:
  void clear_slicestatic();
  const ::CoreML::Specification::SliceStaticLayerParams& slicestatic() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::SliceStaticLayerParams* release_slicestatic();
  ::CoreML::Specification::SliceStaticLayerParams* mutable_slicestatic();
  void set_allocated_slicestatic(::CoreML::Specification::SliceStaticLayerParams* slicestatic);
  private:
  const ::CoreML::Specification::SliceStaticLayerParams& _internal_slicestatic() const;
  ::CoreML::Specification::SliceStaticLayerParams* _internal_mutable_slicestatic();
  public:
  void unsafe_arena_set_allocated_slicestatic(
      ::CoreML::Specification::SliceStaticLayerParams* slicestatic);
  ::CoreML::Specification::SliceStaticLayerParams* unsafe_arena_release_slicestatic();

  // .CoreML.Specification.SliceDynamicLayerParams sliceDynamic = 1000;
  bool has_slicedynamic() const;
  private:
  bool _internal_has_slicedynamic() const;
  public:
  void clear_slicedynamic();
  const ::CoreML::Specification::SliceDynamicLayerParams& slicedynamic() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::SliceDynamicLayerParams* release_slicedynamic();
  ::CoreML::Specification::SliceDynamicLayerParams* mutable_slicedynamic();
  void set_allocated_slicedynamic(::CoreML::Specification::SliceDynamicLayerParams* slicedynamic);
  private:
  const ::CoreML::Specification::SliceDynamicLayerParams& _internal_slicedynamic() const;
  ::CoreML::Specification::SliceDynamicLayerParams* _internal_mutable_slicedynamic();
  public:
  void unsafe_arena_set_allocated_slicedynamic(
      ::CoreML::Specification::SliceDynamicLayerParams* slicedynamic);
  ::CoreML::Specification::SliceDynamicLayerParams* unsafe_arena_release_slicedynamic();

  // .CoreML.Specification.SlidingWindowsLayerParams slidingWindows = 1005;
  bool has_slidingwindows() const;
  private:
  bool _internal_has_slidingwindows() const;
  public:
  void clear_slidingwindows();
  const ::CoreML::Specification::SlidingWindowsLayerParams& slidingwindows() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::SlidingWindowsLayerParams* release_slidingwindows();
  ::CoreML::Specification::SlidingWindowsLayerParams* mutable_slidingwindows();
  void set_allocated_slidingwindows(::CoreML::Specification::SlidingWindowsLayerParams* slidingwindows);
  private:
  const ::CoreML::Specification::SlidingWindowsLayerParams& _internal_slidingwindows() const;
  ::CoreML::Specification::SlidingWindowsLayerParams* _internal_mutable_slidingwindows();
  public:
  void unsafe_arena_set_allocated_slidingwindows(
      ::CoreML::Specification::SlidingWindowsLayerParams* slidingwindows);
  ::CoreML::Specification::SlidingWindowsLayerParams* unsafe_arena_release_slidingwindows();

  // .CoreML.Specification.TopKLayerParams topK = 1015;
  bool has_topk() const;
  private:
  bool _internal_has_topk() const;
  public:
  void clear_topk();
  const ::CoreML::Specification::TopKLayerParams& topk() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::TopKLayerParams* release_topk();
  ::CoreML::Specification::TopKLayerParams* mutable_topk();
  void set_allocated_topk(::CoreML::Specification::TopKLayerParams* topk);
  private:
  const ::CoreML::Specification::TopKLayerParams& _internal_topk() const;
  ::CoreML::Specification::TopKLayerParams* _internal_mutable_topk();
  public:
  void unsafe_arena_set_allocated_topk(
      ::CoreML::Specification::TopKLayerParams* topk);
  ::CoreML::Specification::TopKLayerParams* unsafe_arena_release_topk();

  // .CoreML.Specification.ArgMinLayerParams argMin = 1020;
  bool has_argmin() const;
  private:
  bool _internal_has_argmin() const;
  public:
  void clear_argmin();
  const ::CoreML::Specification::ArgMinLayerParams& argmin() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ArgMinLayerParams* release_argmin();
  ::CoreML::Specification::ArgMinLayerParams* mutable_argmin();
  void set_allocated_argmin(::CoreML::Specification::ArgMinLayerParams* argmin);
  private:
  const ::CoreML::Specification::ArgMinLayerParams& _internal_argmin() const;
  ::CoreML::Specification::ArgMinLayerParams* _internal_mutable_argmin();
  public:
  void unsafe_arena_set_allocated_argmin(
      ::CoreML::Specification::ArgMinLayerParams* argmin);
  ::CoreML::Specification::ArgMinLayerParams* unsafe_arena_release_argmin();

  // .CoreML.Specification.ArgMaxLayerParams argMax = 1025;
  bool has_argmax() const;
  private:
  bool _internal_has_argmax() const;
  public:
  void clear_argmax();
  const ::CoreML::Specification::ArgMaxLayerParams& argmax() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ArgMaxLayerParams* release_argmax();
  ::CoreML::Specification::ArgMaxLayerParams* mutable_argmax();
  void set_allocated_argmax(::CoreML::Specification::ArgMaxLayerParams* argmax);
  private:
  const ::CoreML::Specification::ArgMaxLayerParams& _internal_argmax() const;
  ::CoreML::Specification::ArgMaxLayerParams* _internal_mutable_argmax();
  public:
  void unsafe_arena_set_allocated_argmax(
      ::CoreML::Specification::ArgMaxLayerParams* argmax);
  ::CoreML::Specification::ArgMaxLayerParams* unsafe_arena_release_argmax();

  // .CoreML.Specification.EmbeddingNDLayerParams embeddingND = 1040;
  bool has_embeddingnd() const;
  private:
  bool _internal_has_embeddingnd() const;
  public:
  void clear_embeddingnd();
  const ::CoreML::Specification::EmbeddingNDLayerParams& embeddingnd() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::EmbeddingNDLayerParams* release_embeddingnd();
  ::CoreML::Specification::EmbeddingNDLayerParams* mutable_embeddingnd();
  void set_allocated_embeddingnd(::CoreML::Specification::EmbeddingNDLayerParams* embeddingnd);
  private:
  const ::CoreML::Specification::EmbeddingNDLayerParams& _internal_embeddingnd() const;
  ::CoreML::Specification::EmbeddingNDLayerParams* _internal_mutable_embeddingnd();
  public:
  void unsafe_arena_set_allocated_embeddingnd(
      ::CoreML::Specification::EmbeddingNDLayerParams* embeddingnd);
  ::CoreML::Specification::EmbeddingNDLayerParams* unsafe_arena_release_embeddingnd();

  // .CoreML.Specification.BatchedMatMulLayerParams batchedMatmul = 1045;
  bool has_batchedmatmul() const;
  private:
  bool _internal_has_batchedmatmul() const;
  public:
  void clear_batchedmatmul();
  const ::CoreML::Specification::BatchedMatMulLayerParams& batchedmatmul() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::BatchedMatMulLayerParams* release_batchedmatmul();
  ::CoreML::Specification::BatchedMatMulLayerParams* mutable_batchedmatmul();
  void set_allocated_batchedmatmul(::CoreML::Specification::BatchedMatMulLayerParams* batchedmatmul);
  private:
  const ::CoreML::Specification::BatchedMatMulLayerParams& _internal_batchedmatmul() const;
  ::CoreML::Specification::BatchedMatMulLayerParams* _internal_mutable_batchedmatmul();
  public:
  void unsafe_arena_set_allocated_batchedmatmul(
      ::CoreML::Specification::BatchedMatMulLayerParams* batchedmatmul);
  ::CoreML::Specification::BatchedMatMulLayerParams* unsafe_arena_release_batchedmatmul();

  // .CoreML.Specification.GetShapeLayerParams getShape = 1065;
  bool has_getshape() const;
  private:
  bool _internal_has_getshape() const;
  public:
  void clear_getshape();
  const ::CoreML::Specification::GetShapeLayerParams& getshape() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::GetShapeLayerParams* release_getshape();
  ::CoreML::Specification::GetShapeLayerParams* mutable_getshape();
  void set_allocated_getshape(::CoreML::Specification::GetShapeLayerParams* getshape);
  private:
  const ::CoreML::Specification::GetShapeLayerParams& _internal_getshape() const;
  ::CoreML::Specification::GetShapeLayerParams* _internal_mutable_getshape();
  public:
  void unsafe_arena_set_allocated_getshape(
      ::CoreML::Specification::GetShapeLayerParams* getshape);
  ::CoreML::Specification::GetShapeLayerParams* unsafe_arena_release_getshape();

  // .CoreML.Specification.LoadConstantNDLayerParams loadConstantND = 1070;
  bool has_loadconstantnd() const;
  private:
  bool _internal_has_loadconstantnd() const;
  public:
  void clear_loadconstantnd();
  const ::CoreML::Specification::LoadConstantNDLayerParams& loadconstantnd() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::LoadConstantNDLayerParams* release_loadconstantnd();
  ::CoreML::Specification::LoadConstantNDLayerParams* mutable_loadconstantnd();
  void set_allocated_loadconstantnd(::CoreML::Specification::LoadConstantNDLayerParams* loadconstantnd);
  private:
  const ::CoreML::Specification::LoadConstantNDLayerParams& _internal_loadconstantnd() const;
  ::CoreML::Specification::LoadConstantNDLayerParams* _internal_mutable_loadconstantnd();
  public:
  void unsafe_arena_set_allocated_loadconstantnd(
      ::CoreML::Specification::LoadConstantNDLayerParams* loadconstantnd);
  ::CoreML::Specification::LoadConstantNDLayerParams* unsafe_arena_release_loadconstantnd();

  // .CoreML.Specification.FillLikeLayerParams fillLike = 1080;
  bool has_filllike() const;
  private:
  bool _internal_has_filllike() const;
  public:
  void clear_filllike();
  const ::CoreML::Specification::FillLikeLayerParams& filllike() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::FillLikeLayerParams* release_filllike();
  ::CoreML::Specification::FillLikeLayerParams* mutable_filllike();
  void set_allocated_filllike(::CoreML::Specification::FillLikeLayerParams* filllike);
  private:
  const ::CoreML::Specification::FillLikeLayerParams& _internal_filllike() const;
  ::CoreML::Specification::FillLikeLayerParams* _internal_mutable_filllike();
  public:
  void unsafe_arena_set_allocated_filllike(
      ::CoreML::Specification::FillLikeLayerParams* filllike);
  ::CoreML::Specification::FillLikeLayerParams* unsafe_arena_release_filllike();

  // .CoreML.Specification.FillStaticLayerParams fillStatic = 1085;
  bool has_fillstatic() const;
  private:
  bool _internal_has_fillstatic() const;
  public:
  void clear_fillstatic();
  const ::CoreML::Specification::FillStaticLayerParams& fillstatic() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::FillStaticLayerParams* release_fillstatic();
  ::CoreML::Specification::FillStaticLayerParams* mutable_fillstatic();
  void set_allocated_fillstatic(::CoreML::Specification::FillStaticLayerParams* fillstatic);
  private:
  const ::CoreML::Specification::FillStaticLayerParams& _internal_fillstatic() const;
  ::CoreML::Specification::FillStaticLayerParams* _internal_mutable_fillstatic();
  public:
  void unsafe_arena_set_allocated_fillstatic(
      ::CoreML::Specification::FillStaticLayerParams* fillstatic);
  ::CoreML::Specification::FillStaticLayerParams* unsafe_arena_release_fillstatic();

  // .CoreML.Specification.FillDynamicLayerParams fillDynamic = 1090;
  bool has_filldynamic() const;
  private:
  bool _internal_has_filldynamic() const;
  public:
  void clear_filldynamic();
  const ::CoreML::Specification::FillDynamicLayerParams& filldynamic() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::FillDynamicLayerParams* release_filldynamic();
  ::CoreML::Specification::FillDynamicLayerParams* mutable_filldynamic();
  void set_allocated_filldynamic(::CoreML::Specification::FillDynamicLayerParams* filldynamic);
  private:
  const ::CoreML::Specification::FillDynamicLayerParams& _internal_filldynamic() const;
  ::CoreML::Specification::FillDynamicLayerParams* _internal_mutable_filldynamic();
  public:
  void unsafe_arena_set_allocated_filldynamic(
      ::CoreML::Specification::FillDynamicLayerParams* filldynamic);
  ::CoreML::Specification::FillDynamicLayerParams* unsafe_arena_release_filldynamic();

  // .CoreML.Specification.BroadcastToLikeLayerParams broadcastToLike = 1100;
  bool has_broadcasttolike() const;
  private:
  bool _internal_has_broadcasttolike() const;
  public:
  void clear_broadcasttolike();
  const ::CoreML::Specification::BroadcastToLikeLayerParams& broadcasttolike() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::BroadcastToLikeLayerParams* release_broadcasttolike();
  ::CoreML::Specification::BroadcastToLikeLayerParams* mutable_broadcasttolike();
  void set_allocated_broadcasttolike(::CoreML::Specification::BroadcastToLikeLayerParams* broadcasttolike);
  private:
  const ::CoreML::Specification::BroadcastToLikeLayerParams& _internal_broadcasttolike() const;
  ::CoreML::Specification::BroadcastToLikeLayerParams* _internal_mutable_broadcasttolike();
  public:
  void unsafe_arena_set_allocated_broadcasttolike(
      ::CoreML::Specification::BroadcastToLikeLayerParams* broadcasttolike);
  ::CoreML::Specification::BroadcastToLikeLayerParams* unsafe_arena_release_broadcasttolike();

  // .CoreML.Specification.BroadcastToStaticLayerParams broadcastToStatic = 1105;
  bool has_broadcasttostatic() const;
  private:
  bool _internal_has_broadcasttostatic() const;
  public:
  void clear_broadcasttostatic();
  const ::CoreML::Specification::BroadcastToStaticLayerParams& broadcasttostatic() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::BroadcastToStaticLayerParams* release_broadcasttostatic();
  ::CoreML::Specification::BroadcastToStaticLayerParams* mutable_broadcasttostatic();
  void set_allocated_broadcasttostatic(::CoreML::Specification::BroadcastToStaticLayerParams* broadcasttostatic);
  private:
  const ::CoreML::Specification::BroadcastToStaticLayerParams& _internal_broadcasttostatic() const;
  ::CoreML::Specification::BroadcastToStaticLayerParams* _internal_mutable_broadcasttostatic();
  public:
  void unsafe_arena_set_allocated_broadcasttostatic(
      ::CoreML::Specification::BroadcastToStaticLayerParams* broadcasttostatic);
  ::CoreML::Specification::BroadcastToStaticLayerParams* unsafe_arena_release_broadcasttostatic();

  // .CoreML.Specification.BroadcastToDynamicLayerParams broadcastToDynamic = 1110;
  bool has_broadcasttodynamic() const;
  private:
  bool _internal_has_broadcasttodynamic() const;
  public:
  void clear_broadcasttodynamic();
  const ::CoreML::Specification::BroadcastToDynamicLayerParams& broadcasttodynamic() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::BroadcastToDynamicLayerParams* release_broadcasttodynamic();
  ::CoreML::Specification::BroadcastToDynamicLayerParams* mutable_broadcasttodynamic();
  void set_allocated_broadcasttodynamic(::CoreML::Specification::BroadcastToDynamicLayerParams* broadcasttodynamic);
  private:
  const ::CoreML::Specification::BroadcastToDynamicLayerParams& _internal_broadcasttodynamic() const;
  ::CoreML::Specification::BroadcastToDynamicLayerParams* _internal_mutable_broadcasttodynamic();
  public:
  void unsafe_arena_set_allocated_broadcasttodynamic(
      ::CoreML::Specification::BroadcastToDynamicLayerParams* broadcasttodynamic);
  ::CoreML::Specification::BroadcastToDynamicLayerParams* unsafe_arena_release_broadcasttodynamic();

  // .CoreML.Specification.SqueezeLayerParams squeeze = 1120;
  bool has_squeeze() const;
  private:
  bool _internal_has_squeeze() const;
  public:
  void clear_squeeze();
  const ::CoreML::Specification::SqueezeLayerParams& squeeze() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::SqueezeLayerParams* release_squeeze();
  ::CoreML::Specification::SqueezeLayerParams* mutable_squeeze();
  void set_allocated_squeeze(::CoreML::Specification::SqueezeLayerParams* squeeze);
  private:
  const ::CoreML::Specification::SqueezeLayerParams& _internal_squeeze() const;
  ::CoreML::Specification::SqueezeLayerParams* _internal_mutable_squeeze();
  public:
  void unsafe_arena_set_allocated_squeeze(
      ::CoreML::Specification::SqueezeLayerParams* squeeze);
  ::CoreML::Specification::SqueezeLayerParams* unsafe_arena_release_squeeze();

  // .CoreML.Specification.ExpandDimsLayerParams expandDims = 1125;
  bool has_expanddims() const;
  private:
  bool _internal_has_expanddims() const;
  public:
  void clear_expanddims();
  const ::CoreML::Specification::ExpandDimsLayerParams& expanddims() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ExpandDimsLayerParams* release_expanddims();
  ::CoreML::Specification::ExpandDimsLayerParams* mutable_expanddims();
  void set_allocated_expanddims(::CoreML::Specification::ExpandDimsLayerParams* expanddims);
  private:
  const ::CoreML::Specification::ExpandDimsLayerParams& _internal_expanddims() const;
  ::CoreML::Specification::ExpandDimsLayerParams* _internal_mutable_expanddims();
  public:
  void unsafe_arena_set_allocated_expanddims(
      ::CoreML::Specification::ExpandDimsLayerParams* expanddims);
  ::CoreML::Specification::ExpandDimsLayerParams* unsafe_arena_release_expanddims();

  // .CoreML.Specification.FlattenTo2DLayerParams flattenTo2D = 1130;
  bool has_flattento2d() const;
  private:
  bool _internal_has_flattento2d() const;
  public:
  void clear_flattento2d();
  const ::CoreML::Specification::FlattenTo2DLayerParams& flattento2d() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::FlattenTo2DLayerParams* release_flattento2d();
  ::CoreML::Specification::FlattenTo2DLayerParams* mutable_flattento2d();
  void set_allocated_flattento2d(::CoreML::Specification::FlattenTo2DLayerParams* flattento2d);
  private:
  const ::CoreML::Specification::FlattenTo2DLayerParams& _internal_flattento2d() const;
  ::CoreML::Specification::FlattenTo2DLayerParams* _internal_mutable_flattento2d();
  public:
  void unsafe_arena_set_allocated_flattento2d(
      ::CoreML::Specification::FlattenTo2DLayerParams* flattento2d);
  ::CoreML::Specification::FlattenTo2DLayerParams* unsafe_arena_release_flattento2d();

  // .CoreML.Specification.ReshapeLikeLayerParams reshapeLike = 1135;
  bool has_reshapelike() const;
  private:
  bool _internal_has_reshapelike() const;
  public:
  void clear_reshapelike();
  const ::CoreML::Specification::ReshapeLikeLayerParams& reshapelike() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ReshapeLikeLayerParams* release_reshapelike();
  ::CoreML::Specification::ReshapeLikeLayerParams* mutable_reshapelike();
  void set_allocated_reshapelike(::CoreML::Specification::ReshapeLikeLayerParams* reshapelike);
  private:
  const ::CoreML::Specification::ReshapeLikeLayerParams& _internal_reshapelike() const;
  ::CoreML::Specification::ReshapeLikeLayerParams* _internal_mutable_reshapelike();
  public:
  void unsafe_arena_set_allocated_reshapelike(
      ::CoreML::Specification::ReshapeLikeLayerParams* reshapelike);
  ::CoreML::Specification::ReshapeLikeLayerParams* unsafe_arena_release_reshapelike();

  // .CoreML.Specification.ReshapeStaticLayerParams reshapeStatic = 1140;
  bool has_reshapestatic() const;
  private:
  bool _internal_has_reshapestatic() const;
  public:
  void clear_reshapestatic();
  const ::CoreML::Specification::ReshapeStaticLayerParams& reshapestatic() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ReshapeStaticLayerParams* release_reshapestatic();
  ::CoreML::Specification::ReshapeStaticLayerParams* mutable_reshapestatic();
  void set_allocated_reshapestatic(::CoreML::Specification::ReshapeStaticLayerParams* reshapestatic);
  private:
  const ::CoreML::Specification::ReshapeStaticLayerParams& _internal_reshapestatic() const;
  ::CoreML::Specification::ReshapeStaticLayerParams* _internal_mutable_reshapestatic();
  public:
  void unsafe_arena_set_allocated_reshapestatic(
      ::CoreML::Specification::ReshapeStaticLayerParams* reshapestatic);
  ::CoreML::Specification::ReshapeStaticLayerParams* unsafe_arena_release_reshapestatic();

  // .CoreML.Specification.ReshapeDynamicLayerParams reshapeDynamic = 1145;
  bool has_reshapedynamic() const;
  private:
  bool _internal_has_reshapedynamic() const;
  public:
  void clear_reshapedynamic();
  const ::CoreML::Specification::ReshapeDynamicLayerParams& reshapedynamic() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ReshapeDynamicLayerParams* release_reshapedynamic();
  ::CoreML::Specification::ReshapeDynamicLayerParams* mutable_reshapedynamic();
  void set_allocated_reshapedynamic(::CoreML::Specification::ReshapeDynamicLayerParams* reshapedynamic);
  private:
  const ::CoreML::Specification::ReshapeDynamicLayerParams& _internal_reshapedynamic() const;
  ::CoreML::Specification::ReshapeDynamicLayerParams* _internal_mutable_reshapedynamic();
  public:
  void unsafe_arena_set_allocated_reshapedynamic(
      ::CoreML::Specification::ReshapeDynamicLayerParams* reshapedynamic);
  ::CoreML::Specification::ReshapeDynamicLayerParams* unsafe_arena_release_reshapedynamic();

  // .CoreML.Specification.RankPreservingReshapeLayerParams rankPreservingReshape = 1150;
  bool has_rankpreservingreshape() const;
  private:
  bool _internal_has_rankpreservingreshape() const;
  public:
  void clear_rankpreservingreshape();
  const ::CoreML::Specification::RankPreservingReshapeLayerParams& rankpreservingreshape() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::RankPreservingReshapeLayerParams* release_rankpreservingreshape();
  ::CoreML::Specification::RankPreservingReshapeLayerParams* mutable_rankpreservingreshape();
  void set_allocated_rankpreservingreshape(::CoreML::Specification::RankPreservingReshapeLayerParams* rankpreservingreshape);
  private:
  const ::CoreML::Specification::RankPreservingReshapeLayerParams& _internal_rankpreservingreshape() const;
  ::CoreML::Specification::RankPreservingReshapeLayerParams* _internal_mutable_rankpreservingreshape();
  public:
  void unsafe_arena_set_allocated_rankpreservingreshape(
      ::CoreML::Specification::RankPreservingReshapeLayerParams* rankpreservingreshape);
  ::CoreML::Specification::RankPreservingReshapeLayerParams* unsafe_arena_release_rankpreservingreshape();

  // .CoreML.Specification.ConstantPaddingLayerParams constantPad = 1155;
  bool has_constantpad() const;
  private:
  bool _internal_has_constantpad() const;
  public:
  void clear_constantpad();
  const ::CoreML::Specification::ConstantPaddingLayerParams& constantpad() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ConstantPaddingLayerParams* release_constantpad();
  ::CoreML::Specification::ConstantPaddingLayerParams* mutable_constantpad();
  void set_allocated_constantpad(::CoreML::Specification::ConstantPaddingLayerParams* constantpad);
  private:
  const ::CoreML::Specification::ConstantPaddingLayerParams& _internal_constantpad() const;
  ::CoreML::Specification::ConstantPaddingLayerParams* _internal_mutable_constantpad();
  public:
  void unsafe_arena_set_allocated_constantpad(
      ::CoreML::Specification::ConstantPaddingLayerParams* constantpad);
  ::CoreML::Specification::ConstantPaddingLayerParams* unsafe_arena_release_constantpad();

  // .CoreML.Specification.RandomNormalLikeLayerParams randomNormalLike = 1170;
  bool has_randomnormallike() const;
  private:
  bool _internal_has_randomnormallike() const;
  public:
  void clear_randomnormallike();
  const ::CoreML::Specification::RandomNormalLikeLayerParams& randomnormallike() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::RandomNormalLikeLayerParams* release_randomnormallike();
  ::CoreML::Specification::RandomNormalLikeLayerParams* mutable_randomnormallike();
  void set_allocated_randomnormallike(::CoreML::Specification::RandomNormalLikeLayerParams* randomnormallike);
  private:
  const ::CoreML::Specification::RandomNormalLikeLayerParams& _internal_randomnormallike() const;
  ::CoreML::Specification::RandomNormalLikeLayerParams* _internal_mutable_randomnormallike();
  public:
  void unsafe_arena_set_allocated_randomnormallike(
      ::CoreML::Specification::RandomNormalLikeLayerParams* randomnormallike);
  ::CoreML::Specification::RandomNormalLikeLayerParams* unsafe_arena_release_randomnormallike();

  // .CoreML.Specification.RandomNormalStaticLayerParams randomNormalStatic = 1175;
  bool has_randomnormalstatic() const;
  private:
  bool _internal_has_randomnormalstatic() const;
  public:
  void clear_randomnormalstatic();
  const ::CoreML::Specification::RandomNormalStaticLayerParams& randomnormalstatic() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::RandomNormalStaticLayerParams* release_randomnormalstatic();
  ::CoreML::Specification::RandomNormalStaticLayerParams* mutable_randomnormalstatic();
  void set_allocated_randomnormalstatic(::CoreML::Specification::RandomNormalStaticLayerParams* randomnormalstatic);
  private:
  const ::CoreML::Specification::RandomNormalStaticLayerParams& _internal_randomnormalstatic() const;
  ::CoreML::Specification::RandomNormalStaticLayerParams* _internal_mutable_randomnormalstatic();
  public:
  void unsafe_arena_set_allocated_randomnormalstatic(
      ::CoreML::Specification::RandomNormalStaticLayerParams* randomnormalstatic);
  ::CoreML::Specification::RandomNormalStaticLayerParams* unsafe_arena_release_randomnormalstatic();

  // .CoreML.Specification.RandomNormalDynamicLayerParams randomNormalDynamic = 1180;
  bool has_randomnormaldynamic() const;
  private:
  bool _internal_has_randomnormaldynamic() const;
  public:
  void clear_randomnormaldynamic();
  const ::CoreML::Specification::RandomNormalDynamicLayerParams& randomnormaldynamic() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::RandomNormalDynamicLayerParams* release_randomnormaldynamic();
  ::CoreML::Specification::RandomNormalDynamicLayerParams* mutable_randomnormaldynamic();
  void set_allocated_randomnormaldynamic(::CoreML::Specification::RandomNormalDynamicLayerParams* randomnormaldynamic);
  private:
  const ::CoreML::Specification::RandomNormalDynamicLayerParams& _internal_randomnormaldynamic() const;
  ::CoreML::Specification::RandomNormalDynamicLayerParams* _internal_mutable_randomnormaldynamic();
  public:
  void unsafe_arena_set_allocated_randomnormaldynamic(
      ::CoreML::Specification::RandomNormalDynamicLayerParams* randomnormaldynamic);
  ::CoreML::Specification::RandomNormalDynamicLayerParams* unsafe_arena_release_randomnormaldynamic();

  // .CoreML.Specification.RandomUniformLikeLayerParams randomUniformLike = 1190;
  bool has_randomuniformlike() const;
  private:
  bool _internal_has_randomuniformlike() const;
  public:
  void clear_randomuniformlike();
  const ::CoreML::Specification::RandomUniformLikeLayerParams& randomuniformlike() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::RandomUniformLikeLayerParams* release_randomuniformlike();
  ::CoreML::Specification::RandomUniformLikeLayerParams* mutable_randomuniformlike();
  void set_allocated_randomuniformlike(::CoreML::Specification::RandomUniformLikeLayerParams* randomuniformlike);
  private:
  const ::CoreML::Specification::RandomUniformLikeLayerParams& _internal_randomuniformlike() const;
  ::CoreML::Specification::RandomUniformLikeLayerParams* _internal_mutable_randomuniformlike();
  public:
  void unsafe_arena_set_allocated_randomuniformlike(
      ::CoreML::Specification::RandomUniformLikeLayerParams* randomuniformlike);
  ::CoreML::Specification::RandomUniformLikeLayerParams* unsafe_arena_release_randomuniformlike();

  // .CoreML.Specification.RandomUniformStaticLayerParams randomUniformStatic = 1195;
  bool has_randomuniformstatic() const;
  private:
  bool _internal_has_randomuniformstatic() const;
  public:
  void clear_randomuniformstatic();
  const ::CoreML::Specification::RandomUniformStaticLayerParams& randomuniformstatic() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::RandomUniformStaticLayerParams* release_randomuniformstatic();
  ::CoreML::Specification::RandomUniformStaticLayerParams* mutable_randomuniformstatic();
  void set_allocated_randomuniformstatic(::CoreML::Specification::RandomUniformStaticLayerParams* randomuniformstatic);
  private:
  const ::CoreML::Specification::RandomUniformStaticLayerParams& _internal_randomuniformstatic() const;
  ::CoreML::Specification::RandomUniformStaticLayerParams* _internal_mutable_randomuniformstatic();
  public:
  void unsafe_arena_set_allocated_randomuniformstatic(
      ::CoreML::Specification::RandomUniformStaticLayerParams* randomuniformstatic);
  ::CoreML::Specification::RandomUniformStaticLayerParams* unsafe_arena_release_randomuniformstatic();

  // .CoreML.Specification.RandomUniformDynamicLayerParams randomUniformDynamic = 1200;
  bool has_randomuniformdynamic() const;
  private:
  bool _internal_has_randomuniformdynamic() const;
  public:
  void clear_randomuniformdynamic();
  const ::CoreML::Specification::RandomUniformDynamicLayerParams& randomuniformdynamic() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::RandomUniformDynamicLayerParams* release_randomuniformdynamic();
  ::CoreML::Specification::RandomUniformDynamicLayerParams* mutable_randomuniformdynamic();
  void set_allocated_randomuniformdynamic(::CoreML::Specification::RandomUniformDynamicLayerParams* randomuniformdynamic);
  private:
  const ::CoreML::Specification::RandomUniformDynamicLayerParams& _internal_randomuniformdynamic() const;
  ::CoreML::Specification::RandomUniformDynamicLayerParams* _internal_mutable_randomuniformdynamic();
  public:
  void unsafe_arena_set_allocated_randomuniformdynamic(
      ::CoreML::Specification::RandomUniformDynamicLayerParams* randomuniformdynamic);
  ::CoreML::Specification::RandomUniformDynamicLayerParams* unsafe_arena_release_randomuniformdynamic();

  // .CoreML.Specification.RandomBernoulliLikeLayerParams randomBernoulliLike = 1210;
  bool has_randombernoullilike() const;
  private:
  bool _internal_has_randombernoullilike() const;
  public:
  void clear_randombernoullilike();
  const ::CoreML::Specification::RandomBernoulliLikeLayerParams& randombernoullilike() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::RandomBernoulliLikeLayerParams* release_randombernoullilike();
  ::CoreML::Specification::RandomBernoulliLikeLayerParams* mutable_randombernoullilike();
  void set_allocated_randombernoullilike(::CoreML::Specification::RandomBernoulliLikeLayerParams* randombernoullilike);
  private:
  const ::CoreML::Specification::RandomBernoulliLikeLayerParams& _internal_randombernoullilike() const;
  ::CoreML::Specification::RandomBernoulliLikeLayerParams* _internal_mutable_randombernoullilike();
  public:
  void unsafe_arena_set_allocated_randombernoullilike(
      ::CoreML::Specification::RandomBernoulliLikeLayerParams* randombernoullilike);
  ::CoreML::Specification::RandomBernoulliLikeLayerParams* unsafe_arena_release_randombernoullilike();

  // .CoreML.Specification.RandomBernoulliStaticLayerParams randomBernoulliStatic = 1215;
  bool has_randombernoullistatic() const;
  private:
  bool _internal_has_randombernoullistatic() const;
  public:
  void clear_randombernoullistatic();
  const ::CoreML::Specification::RandomBernoulliStaticLayerParams& randombernoullistatic() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::RandomBernoulliStaticLayerParams* release_randombernoullistatic();
  ::CoreML::Specification::RandomBernoulliStaticLayerParams* mutable_randombernoullistatic();
  void set_allocated_randombernoullistatic(::CoreML::Specification::RandomBernoulliStaticLayerParams* randombernoullistatic);
  private:
  const ::CoreML::Specification::RandomBernoulliStaticLayerParams& _internal_randombernoullistatic() const;
  ::CoreML::Specification::RandomBernoulliStaticLayerParams* _internal_mutable_randombernoullistatic();
  public:
  void unsafe_arena_set_allocated_randombernoullistatic(
      ::CoreML::Specification::RandomBernoulliStaticLayerParams* randombernoullistatic);
  ::CoreML::Specification::RandomBernoulliStaticLayerParams* unsafe_arena_release_randombernoullistatic();

  // .CoreML.Specification.RandomBernoulliDynamicLayerParams randomBernoulliDynamic = 1220;
  bool has_randombernoullidynamic() const;
  private:
  bool _internal_has_randombernoullidynamic() const;
  public:
  void clear_randombernoullidynamic();
  const ::CoreML::Specification::RandomBernoulliDynamicLayerParams& randombernoullidynamic() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::RandomBernoulliDynamicLayerParams* release_randombernoullidynamic();
  ::CoreML::Specification::RandomBernoulliDynamicLayerParams* mutable_randombernoullidynamic();
  void set_allocated_randombernoullidynamic(::CoreML::Specification::RandomBernoulliDynamicLayerParams* randombernoullidynamic);
  private:
  const ::CoreML::Specification::RandomBernoulliDynamicLayerParams& _internal_randombernoullidynamic() const;
  ::CoreML::Specification::RandomBernoulliDynamicLayerParams* _internal_mutable_randombernoullidynamic();
  public:
  void unsafe_arena_set_allocated_randombernoullidynamic(
      ::CoreML::Specification::RandomBernoulliDynamicLayerParams* randombernoullidynamic);
  ::CoreML::Specification::RandomBernoulliDynamicLayerParams* unsafe_arena_release_randombernoullidynamic();

  // .CoreML.Specification.CategoricalDistributionLayerParams categoricalDistribution = 1230;
  bool has_categoricaldistribution() const;
  private:
  bool _internal_has_categoricaldistribution() const;
  public:
  void clear_categoricaldistribution();
  const ::CoreML::Specification::CategoricalDistributionLayerParams& categoricaldistribution() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::CategoricalDistributionLayerParams* release_categoricaldistribution();
  ::CoreML::Specification::CategoricalDistributionLayerParams* mutable_categoricaldistribution();
  void set_allocated_categoricaldistribution(::CoreML::Specification::CategoricalDistributionLayerParams* categoricaldistribution);
  private:
  const ::CoreML::Specification::CategoricalDistributionLayerParams& _internal_categoricaldistribution() const;
  ::CoreML::Specification::CategoricalDistributionLayerParams* _internal_mutable_categoricaldistribution();
  public:
  void unsafe_arena_set_allocated_categoricaldistribution(
      ::CoreML::Specification::CategoricalDistributionLayerParams* categoricaldistribution);
  ::CoreML::Specification::CategoricalDistributionLayerParams* unsafe_arena_release_categoricaldistribution();

  // .CoreML.Specification.ReduceL1LayerParams reduceL1 = 1250;
  bool has_reducel1() const;
  private:
  bool _internal_has_reducel1() const;
  public:
  void clear_reducel1();
  const ::CoreML::Specification::ReduceL1LayerParams& reducel1() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ReduceL1LayerParams* release_reducel1();
  ::CoreML::Specification::ReduceL1LayerParams* mutable_reducel1();
  void set_allocated_reducel1(::CoreML::Specification::ReduceL1LayerParams* reducel1);
  private:
  const ::CoreML::Specification::ReduceL1LayerParams& _internal_reducel1() const;
  ::CoreML::Specification::ReduceL1LayerParams* _internal_mutable_reducel1();
  public:
  void unsafe_arena_set_allocated_reducel1(
      ::CoreML::Specification::ReduceL1LayerParams* reducel1);
  ::CoreML::Specification::ReduceL1LayerParams* unsafe_arena_release_reducel1();

  // .CoreML.Specification.ReduceL2LayerParams reduceL2 = 1255;
  bool has_reducel2() const;
  private:
  bool _internal_has_reducel2() const;
  public:
  void clear_reducel2();
  const ::CoreML::Specification::ReduceL2LayerParams& reducel2() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ReduceL2LayerParams* release_reducel2();
  ::CoreML::Specification::ReduceL2LayerParams* mutable_reducel2();
  void set_allocated_reducel2(::CoreML::Specification::ReduceL2LayerParams* reducel2);
  private:
  const ::CoreML::Specification::ReduceL2LayerParams& _internal_reducel2() const;
  ::CoreML::Specification::ReduceL2LayerParams* _internal_mutable_reducel2();
  public:
  void unsafe_arena_set_allocated_reducel2(
      ::CoreML::Specification::ReduceL2LayerParams* reducel2);
  ::CoreML::Specification::ReduceL2LayerParams* unsafe_arena_release_reducel2();

  // .CoreML.Specification.ReduceMaxLayerParams reduceMax = 1260;
  bool has_reducemax() const;
  private:
  bool _internal_has_reducemax() const;
  public:
  void clear_reducemax();
  const ::CoreML::Specification::ReduceMaxLayerParams& reducemax() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ReduceMaxLayerParams* release_reducemax();
  ::CoreML::Specification::ReduceMaxLayerParams* mutable_reducemax();
  void set_allocated_reducemax(::CoreML::Specification::ReduceMaxLayerParams* reducemax);
  private:
  const ::CoreML::Specification::ReduceMaxLayerParams& _internal_reducemax() const;
  ::CoreML::Specification::ReduceMaxLayerParams* _internal_mutable_reducemax();
  public:
  void unsafe_arena_set_allocated_reducemax(
      ::CoreML::Specification::ReduceMaxLayerParams* reducemax);
  ::CoreML::Specification::ReduceMaxLayerParams* unsafe_arena_release_reducemax();

  // .CoreML.Specification.ReduceMinLayerParams reduceMin = 1265;
  bool has_reducemin() const;
  private:
  bool _internal_has_reducemin() const;
  public:
  void clear_reducemin();
  const ::CoreML::Specification::ReduceMinLayerParams& reducemin() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ReduceMinLayerParams* release_reducemin();
  ::CoreML::Specification::ReduceMinLayerParams* mutable_reducemin();
  void set_allocated_reducemin(::CoreML::Specification::ReduceMinLayerParams* reducemin);
  private:
  const ::CoreML::Specification::ReduceMinLayerParams& _internal_reducemin() const;
  ::CoreML::Specification::ReduceMinLayerParams* _internal_mutable_reducemin();
  public:
  void unsafe_arena_set_allocated_reducemin(
      ::CoreML::Specification::ReduceMinLayerParams* reducemin);
  ::CoreML::Specification::ReduceMinLayerParams* unsafe_arena_release_reducemin();

  // .CoreML.Specification.ReduceSumLayerParams reduceSum = 1270;
  bool has_reducesum() const;
  private:
  bool _internal_has_reducesum() const;
  public:
  void clear_reducesum();
  const ::CoreML::Specification::ReduceSumLayerParams& reducesum() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ReduceSumLayerParams* release_reducesum();
  ::CoreML::Specification::ReduceSumLayerParams* mutable_reducesum();
  void set_allocated_reducesum(::CoreML::Specification::ReduceSumLayerParams* reducesum);
  private:
  const ::CoreML::Specification::ReduceSumLayerParams& _internal_reducesum() const;
  ::CoreML::Specification::ReduceSumLayerParams* _internal_mutable_reducesum();
  public:
  void unsafe_arena_set_allocated_reducesum(
      ::CoreML::Specification::ReduceSumLayerParams* reducesum);
  ::CoreML::Specification::ReduceSumLayerParams* unsafe_arena_release_reducesum();

  // .CoreML.Specification.ReduceProdLayerParams reduceProd = 1275;
  bool has_reduceprod() const;
  private:
  bool _internal_has_reduceprod() const;
  public:
  void clear_reduceprod();
  const ::CoreML::Specification::ReduceProdLayerParams& reduceprod() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ReduceProdLayerParams* release_reduceprod();
  ::CoreML::Specification::ReduceProdLayerParams* mutable_reduceprod();
  void set_allocated_reduceprod(::CoreML::Specification::ReduceProdLayerParams* reduceprod);
  private:
  const ::CoreML::Specification::ReduceProdLayerParams& _internal_reduceprod() const;
  ::CoreML::Specification::ReduceProdLayerParams* _internal_mutable_reduceprod();
  public:
  void unsafe_arena_set_allocated_reduceprod(
      ::CoreML::Specification::ReduceProdLayerParams* reduceprod);
  ::CoreML::Specification::ReduceProdLayerParams* unsafe_arena_release_reduceprod();

  // .CoreML.Specification.ReduceMeanLayerParams reduceMean = 1280;
  bool has_reducemean() const;
  private:
  bool _internal_has_reducemean() const;
  public:
  void clear_reducemean();
  const ::CoreML::Specification::ReduceMeanLayerParams& reducemean() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ReduceMeanLayerParams* release_reducemean();
  ::CoreML::Specification::ReduceMeanLayerParams* mutable_reducemean();
  void set_allocated_reducemean(::CoreML::Specification::ReduceMeanLayerParams* reducemean);
  private:
  const ::CoreML::Specification::ReduceMeanLayerParams& _internal_reducemean() const;
  ::CoreML::Specification::ReduceMeanLayerParams* _internal_mutable_reducemean();
  public:
  void unsafe_arena_set_allocated_reducemean(
      ::CoreML::Specification::ReduceMeanLayerParams* reducemean);
  ::CoreML::Specification::ReduceMeanLayerParams* unsafe_arena_release_reducemean();

  // .CoreML.Specification.ReduceLogSumLayerParams reduceLogSum = 1285;
  bool has_reducelogsum() const;
  private:
  bool _internal_has_reducelogsum() const;
  public:
  void clear_reducelogsum();
  const ::CoreML::Specification::ReduceLogSumLayerParams& reducelogsum() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ReduceLogSumLayerParams* release_reducelogsum();
  ::CoreML::Specification::ReduceLogSumLayerParams* mutable_reducelogsum();
  void set_allocated_reducelogsum(::CoreML::Specification::ReduceLogSumLayerParams* reducelogsum);
  private:
  const ::CoreML::Specification::ReduceLogSumLayerParams& _internal_reducelogsum() const;
  ::CoreML::Specification::ReduceLogSumLayerParams* _internal_mutable_reducelogsum();
  public:
  void unsafe_arena_set_allocated_reducelogsum(
      ::CoreML::Specification::ReduceLogSumLayerParams* reducelogsum);
  ::CoreML::Specification::ReduceLogSumLayerParams* unsafe_arena_release_reducelogsum();

  // .CoreML.Specification.ReduceSumSquareLayerParams reduceSumSquare = 1290;
  bool has_reducesumsquare() const;
  private:
  bool _internal_has_reducesumsquare() const;
  public:
  void clear_reducesumsquare();
  const ::CoreML::Specification::ReduceSumSquareLayerParams& reducesumsquare() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ReduceSumSquareLayerParams* release_reducesumsquare();
  ::CoreML::Specification::ReduceSumSquareLayerParams* mutable_reducesumsquare();
  void set_allocated_reducesumsquare(::CoreML::Specification::ReduceSumSquareLayerParams* reducesumsquare);
  private:
  const ::CoreML::Specification::ReduceSumSquareLayerParams& _internal_reducesumsquare() const;
  ::CoreML::Specification::ReduceSumSquareLayerParams* _internal_mutable_reducesumsquare();
  public:
  void unsafe_arena_set_allocated_reducesumsquare(
      ::CoreML::Specification::ReduceSumSquareLayerParams* reducesumsquare);
  ::CoreML::Specification::ReduceSumSquareLayerParams* unsafe_arena_release_reducesumsquare();

  // .CoreML.Specification.ReduceLogSumExpLayerParams reduceLogSumExp = 1295;
  bool has_reducelogsumexp() const;
  private:
  bool _internal_has_reducelogsumexp() const;
  public:
  void clear_reducelogsumexp();
  const ::CoreML::Specification::ReduceLogSumExpLayerParams& reducelogsumexp() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ReduceLogSumExpLayerParams* release_reducelogsumexp();
  ::CoreML::Specification::ReduceLogSumExpLayerParams* mutable_reducelogsumexp();
  void set_allocated_reducelogsumexp(::CoreML::Specification::ReduceLogSumExpLayerParams* reducelogsumexp);
  private:
  const ::CoreML::Specification::ReduceLogSumExpLayerParams& _internal_reducelogsumexp() const;
  ::CoreML::Specification::ReduceLogSumExpLayerParams* _internal_mutable_reducelogsumexp();
  public:
  void unsafe_arena_set_allocated_reducelogsumexp(
      ::CoreML::Specification::ReduceLogSumExpLayerParams* reducelogsumexp);
  ::CoreML::Specification::ReduceLogSumExpLayerParams* unsafe_arena_release_reducelogsumexp();

  // .CoreML.Specification.WhereNonZeroLayerParams whereNonZero = 1313;
  bool has_wherenonzero() const;
  private:
  bool _internal_has_wherenonzero() const;
  public:
  void clear_wherenonzero();
  const ::CoreML::Specification::WhereNonZeroLayerParams& wherenonzero() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WhereNonZeroLayerParams* release_wherenonzero();
  ::CoreML::Specification::WhereNonZeroLayerParams* mutable_wherenonzero();
  void set_allocated_wherenonzero(::CoreML::Specification::WhereNonZeroLayerParams* wherenonzero);
  private:
  const ::CoreML::Specification::WhereNonZeroLayerParams& _internal_wherenonzero() const;
  ::CoreML::Specification::WhereNonZeroLayerParams* _internal_mutable_wherenonzero();
  public:
  void unsafe_arena_set_allocated_wherenonzero(
      ::CoreML::Specification::WhereNonZeroLayerParams* wherenonzero);
  ::CoreML::Specification::WhereNonZeroLayerParams* unsafe_arena_release_wherenonzero();

  // .CoreML.Specification.MatrixBandPartLayerParams matrixBandPart = 1315;
  bool has_matrixbandpart() const;
  private:
  bool _internal_has_matrixbandpart() const;
  public:
  void clear_matrixbandpart();
  const ::CoreML::Specification::MatrixBandPartLayerParams& matrixbandpart() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::MatrixBandPartLayerParams* release_matrixbandpart();
  ::CoreML::Specification::MatrixBandPartLayerParams* mutable_matrixbandpart();
  void set_allocated_matrixbandpart(::CoreML::Specification::MatrixBandPartLayerParams* matrixbandpart);
  private:
  const ::CoreML::Specification::MatrixBandPartLayerParams& _internal_matrixbandpart() const;
  ::CoreML::Specification::MatrixBandPartLayerParams* _internal_mutable_matrixbandpart();
  public:
  void unsafe_arena_set_allocated_matrixbandpart(
      ::CoreML::Specification::MatrixBandPartLayerParams* matrixbandpart);
  ::CoreML::Specification::MatrixBandPartLayerParams* unsafe_arena_release_matrixbandpart();

  // .CoreML.Specification.LowerTriangularLayerParams lowerTriangular = 1320;
  bool has_lowertriangular() const;
  private:
  bool _internal_has_lowertriangular() const;
  public:
  void clear_lowertriangular();
  const ::CoreML::Specification::LowerTriangularLayerParams& lowertriangular() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::LowerTriangularLayerParams* release_lowertriangular();
  ::CoreML::Specification::LowerTriangularLayerParams* mutable_lowertriangular();
  void set_allocated_lowertriangular(::CoreML::Specification::LowerTriangularLayerParams* lowertriangular);
  private:
  const ::CoreML::Specification::LowerTriangularLayerParams& _internal_lowertriangular() const;
  ::CoreML::Specification::LowerTriangularLayerParams* _internal_mutable_lowertriangular();
  public:
  void unsafe_arena_set_allocated_lowertriangular(
      ::CoreML::Specification::LowerTriangularLayerParams* lowertriangular);
  ::CoreML::Specification::LowerTriangularLayerParams* unsafe_arena_release_lowertriangular();

  // .CoreML.Specification.UpperTriangularLayerParams upperTriangular = 1325;
  bool has_uppertriangular() const;
  private:
  bool _internal_has_uppertriangular() const;
  public:
  void clear_uppertriangular();
  const ::CoreML::Specification::UpperTriangularLayerParams& uppertriangular() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::UpperTriangularLayerParams* release_uppertriangular();
  ::CoreML::Specification::UpperTriangularLayerParams* mutable_uppertriangular();
  void set_allocated_uppertriangular(::CoreML::Specification::UpperTriangularLayerParams* uppertriangular);
  private:
  const ::CoreML::Specification::UpperTriangularLayerParams& _internal_uppertriangular() const;
  ::CoreML::Specification::UpperTriangularLayerParams* _internal_mutable_uppertriangular();
  public:
  void unsafe_arena_set_allocated_uppertriangular(
      ::CoreML::Specification::UpperTriangularLayerParams* uppertriangular);
  ::CoreML::Specification::UpperTriangularLayerParams* unsafe_arena_release_uppertriangular();

  // .CoreML.Specification.WhereBroadcastableLayerParams whereBroadcastable = 1330;
  bool has_wherebroadcastable() const;
  private:
  bool _internal_has_wherebroadcastable() const;
  public:
  void clear_wherebroadcastable();
  const ::CoreML::Specification::WhereBroadcastableLayerParams& wherebroadcastable() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WhereBroadcastableLayerParams* release_wherebroadcastable();
  ::CoreML::Specification::WhereBroadcastableLayerParams* mutable_wherebroadcastable();
  void set_allocated_wherebroadcastable(::CoreML::Specification::WhereBroadcastableLayerParams* wherebroadcastable);
  private:
  const ::CoreML::Specification::WhereBroadcastableLayerParams& _internal_wherebroadcastable() const;
  ::CoreML::Specification::WhereBroadcastableLayerParams* _internal_mutable_wherebroadcastable();
  public:
  void unsafe_arena_set_allocated_wherebroadcastable(
      ::CoreML::Specification::WhereBroadcastableLayerParams* wherebroadcastable);
  ::CoreML::Specification::WhereBroadcastableLayerParams* unsafe_arena_release_wherebroadcastable();

  // .CoreML.Specification.LayerNormalizationLayerParams layerNormalization = 1350;
  bool has_layernormalization() const;
  private:
  bool _internal_has_layernormalization() const;
  public:
  void clear_layernormalization();
  const ::CoreML::Specification::LayerNormalizationLayerParams& layernormalization() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::LayerNormalizationLayerParams* release_layernormalization();
  ::CoreML::Specification::LayerNormalizationLayerParams* mutable_layernormalization();
  void set_allocated_layernormalization(::CoreML::Specification::LayerNormalizationLayerParams* layernormalization);
  private:
  const ::CoreML::Specification::LayerNormalizationLayerParams& _internal_layernormalization() const;
  ::CoreML::Specification::LayerNormalizationLayerParams* _internal_mutable_layernormalization();
  public:
  void unsafe_arena_set_allocated_layernormalization(
      ::CoreML::Specification::LayerNormalizationLayerParams* layernormalization);
  ::CoreML::Specification::LayerNormalizationLayerParams* unsafe_arena_release_layernormalization();

  // .CoreML.Specification.NonMaximumSuppressionLayerParams NonMaximumSuppression = 1400;
  bool has_nonmaximumsuppression() const;
  private:
  bool _internal_has_nonmaximumsuppression() const;
  public:
  void clear_nonmaximumsuppression();
  const ::CoreML::Specification::NonMaximumSuppressionLayerParams& nonmaximumsuppression() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::NonMaximumSuppressionLayerParams* release_nonmaximumsuppression();
  ::CoreML::Specification::NonMaximumSuppressionLayerParams* mutable_nonmaximumsuppression();
  void set_allocated_nonmaximumsuppression(::CoreML::Specification::NonMaximumSuppressionLayerParams* nonmaximumsuppression);
  private:
  const ::CoreML::Specification::NonMaximumSuppressionLayerParams& _internal_nonmaximumsuppression() const;
  ::CoreML::Specification::NonMaximumSuppressionLayerParams* _internal_mutable_nonmaximumsuppression();
  public:
  void unsafe_arena_set_allocated_nonmaximumsuppression(
      ::CoreML::Specification::NonMaximumSuppressionLayerParams* nonmaximumsuppression);
  ::CoreML::Specification::NonMaximumSuppressionLayerParams* unsafe_arena_release_nonmaximumsuppression();

  // .CoreML.Specification.OneHotLayerParams oneHot = 1450;
  bool has_onehot() const;
  private:
  bool _internal_has_onehot() const;
  public:
  void clear_onehot();
  const ::CoreML::Specification::OneHotLayerParams& onehot() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::OneHotLayerParams* release_onehot();
  ::CoreML::Specification::OneHotLayerParams* mutable_onehot();
  void set_allocated_onehot(::CoreML::Specification::OneHotLayerParams* onehot);
  private:
  const ::CoreML::Specification::OneHotLayerParams& _internal_onehot() const;
  ::CoreML::Specification::OneHotLayerParams* _internal_mutable_onehot();
  public:
  void unsafe_arena_set_allocated_onehot(
      ::CoreML::Specification::OneHotLayerParams* onehot);
  ::CoreML::Specification::OneHotLayerParams* unsafe_arena_release_onehot();

  // .CoreML.Specification.CumSumLayerParams cumSum = 1455;
  bool has_cumsum() const;
  private:
  bool _internal_has_cumsum() const;
  public:
  void clear_cumsum();
  const ::CoreML::Specification::CumSumLayerParams& cumsum() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::CumSumLayerParams* release_cumsum();
  ::CoreML::Specification::CumSumLayerParams* mutable_cumsum();
  void set_allocated_cumsum(::CoreML::Specification::CumSumLayerParams* cumsum);
  private:
  const ::CoreML::Specification::CumSumLayerParams& _internal_cumsum() const;
  ::CoreML::Specification::CumSumLayerParams* _internal_mutable_cumsum();
  public:
  void unsafe_arena_set_allocated_cumsum(
      ::CoreML::Specification::CumSumLayerParams* cumsum);
  ::CoreML::Specification::CumSumLayerParams* unsafe_arena_release_cumsum();

  // .CoreML.Specification.ClampedReLULayerParams clampedReLU = 1460;
  bool has_clampedrelu() const;
  private:
  bool _internal_has_clampedrelu() const;
  public:
  void clear_clampedrelu();
  const ::CoreML::Specification::ClampedReLULayerParams& clampedrelu() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ClampedReLULayerParams* release_clampedrelu();
  ::CoreML::Specification::ClampedReLULayerParams* mutable_clampedrelu();
  void set_allocated_clampedrelu(::CoreML::Specification::ClampedReLULayerParams* clampedrelu);
  private:
  const ::CoreML::Specification::ClampedReLULayerParams& _internal_clampedrelu() const;
  ::CoreML::Specification::ClampedReLULayerParams* _internal_mutable_clampedrelu();
  public:
  void unsafe_arena_set_allocated_clampedrelu(
      ::CoreML::Specification::ClampedReLULayerParams* clampedrelu);
  ::CoreML::Specification::ClampedReLULayerParams* unsafe_arena_release_clampedrelu();

  // .CoreML.Specification.ArgSortLayerParams argSort = 1461;
  bool has_argsort() const;
  private:
  bool _internal_has_argsort() const;
  public:
  void clear_argsort();
  const ::CoreML::Specification::ArgSortLayerParams& argsort() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ArgSortLayerParams* release_argsort();
  ::CoreML::Specification::ArgSortLayerParams* mutable_argsort();
  void set_allocated_argsort(::CoreML::Specification::ArgSortLayerParams* argsort);
  private:
  const ::CoreML::Specification::ArgSortLayerParams& _internal_argsort() const;
  ::CoreML::Specification::ArgSortLayerParams* _internal_mutable_argsort();
  public:
  void unsafe_arena_set_allocated_argsort(
      ::CoreML::Specification::ArgSortLayerParams* argsort);
  ::CoreML::Specification::ArgSortLayerParams* unsafe_arena_release_argsort();

  // .CoreML.Specification.Pooling3DLayerParams pooling3d = 1465;
  bool has_pooling3d() const;
  private:
  bool _internal_has_pooling3d() const;
  public:
  void clear_pooling3d();
  const ::CoreML::Specification::Pooling3DLayerParams& pooling3d() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::Pooling3DLayerParams* release_pooling3d();
  ::CoreML::Specification::Pooling3DLayerParams* mutable_pooling3d();
  void set_allocated_pooling3d(::CoreML::Specification::Pooling3DLayerParams* pooling3d);
  private:
  const ::CoreML::Specification::Pooling3DLayerParams& _internal_pooling3d() const;
  ::CoreML::Specification::Pooling3DLayerParams* _internal_mutable_pooling3d();
  public:
  void unsafe_arena_set_allocated_pooling3d(
      ::CoreML::Specification::Pooling3DLayerParams* pooling3d);
  ::CoreML::Specification::Pooling3DLayerParams* unsafe_arena_release_pooling3d();

  // .CoreML.Specification.GlobalPooling3DLayerParams globalPooling3d = 1466;
  bool has_globalpooling3d() const;
  private:
  bool _internal_has_globalpooling3d() const;
  public:
  void clear_globalpooling3d();
  const ::CoreML::Specification::GlobalPooling3DLayerParams& globalpooling3d() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::GlobalPooling3DLayerParams* release_globalpooling3d();
  ::CoreML::Specification::GlobalPooling3DLayerParams* mutable_globalpooling3d();
  void set_allocated_globalpooling3d(::CoreML::Specification::GlobalPooling3DLayerParams* globalpooling3d);
  private:
  const ::CoreML::Specification::GlobalPooling3DLayerParams& _internal_globalpooling3d() const;
  ::CoreML::Specification::GlobalPooling3DLayerParams* _internal_mutable_globalpooling3d();
  public:
  void unsafe_arena_set_allocated_globalpooling3d(
      ::CoreML::Specification::GlobalPooling3DLayerParams* globalpooling3d);
  ::CoreML::Specification::GlobalPooling3DLayerParams* unsafe_arena_release_globalpooling3d();

  // .CoreML.Specification.SliceBySizeLayerParams sliceBySize = 1470;
  bool has_slicebysize() const;
  private:
  bool _internal_has_slicebysize() const;
  public:
  void clear_slicebysize();
  const ::CoreML::Specification::SliceBySizeLayerParams& slicebysize() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::SliceBySizeLayerParams* release_slicebysize();
  ::CoreML::Specification::SliceBySizeLayerParams* mutable_slicebysize();
  void set_allocated_slicebysize(::CoreML::Specification::SliceBySizeLayerParams* slicebysize);
  private:
  const ::CoreML::Specification::SliceBySizeLayerParams& _internal_slicebysize() const;
  ::CoreML::Specification::SliceBySizeLayerParams* _internal_mutable_slicebysize();
  public:
  void unsafe_arena_set_allocated_slicebysize(
      ::CoreML::Specification::SliceBySizeLayerParams* slicebysize);
  ::CoreML::Specification::SliceBySizeLayerParams* unsafe_arena_release_slicebysize();

  // .CoreML.Specification.Convolution3DLayerParams convolution3d = 1471;
  bool has_convolution3d() const;
  private:
  bool _internal_has_convolution3d() const;
  public:
  void clear_convolution3d();
  const ::CoreML::Specification::Convolution3DLayerParams& convolution3d() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::Convolution3DLayerParams* release_convolution3d();
  ::CoreML::Specification::Convolution3DLayerParams* mutable_convolution3d();
  void set_allocated_convolution3d(::CoreML::Specification::Convolution3DLayerParams* convolution3d);
  private:
  const ::CoreML::Specification::Convolution3DLayerParams& _internal_convolution3d() const;
  ::CoreML::Specification::Convolution3DLayerParams* _internal_mutable_convolution3d();
  public:
  void unsafe_arena_set_allocated_convolution3d(
      ::CoreML::Specification::Convolution3DLayerParams* convolution3d);
  ::CoreML::Specification::Convolution3DLayerParams* unsafe_arena_release_convolution3d();

  void clear_layer();
  LayerCase layer_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.NeuralNetworkLayer)
 private:
  class _Internal;
  void set_has_convolution();
  void set_has_pooling();
  void set_has_activation();
  void set_has_innerproduct();
  void set_has_embedding();
  void set_has_batchnorm();
  void set_has_mvn();
  void set_has_l2normalize();
  void set_has_softmax();
  void set_has_lrn();
  void set_has_crop();
  void set_has_padding();
  void set_has_upsample();
  void set_has_resizebilinear();
  void set_has_cropresize();
  void set_has_unary();
  void set_has_add();
  void set_has_multiply();
  void set_has_average();
  void set_has_scale();
  void set_has_bias();
  void set_has_max();
  void set_has_min();
  void set_has_dot();
  void set_has_reduce();
  void set_has_loadconstant();
  void set_has_reshape();
  void set_has_flatten();
  void set_has_permute();
  void set_has_concat();
  void set_has_split();
  void set_has_sequencerepeat();
  void set_has_reorganizedata();
  void set_has_slice();
  void set_has_simplerecurrent();
  void set_has_gru();
  void set_has_unidirectionallstm();
  void set_has_bidirectionallstm();
  void set_has_custom();
  void set_has_copy();
  void set_has_branch();
  void set_has_loop();
  void set_has_loopbreak();
  void set_has_loopcontinue();
  void set_has_rangestatic();
  void set_has_rangedynamic();
  void set_has_clip();
  void set_has_ceil();
  void set_has_floor();
  void set_has_sign();
  void set_has_round();
  void set_has_exp2();
  void set_has_sin();
  void set_has_cos();
  void set_has_tan();
  void set_has_asin();
  void set_has_acos();
  void set_has_atan();
  void set_has_sinh();
  void set_has_cosh();
  void set_has_tanh();
  void set_has_asinh();
  void set_has_acosh();
  void set_has_atanh();
  void set_has_erf();
  void set_has_gelu();
  void set_has_equal();
  void set_has_notequal();
  void set_has_lessthan();
  void set_has_lessequal();
  void set_has_greaterthan();
  void set_has_greaterequal();
  void set_has_logicalor();
  void set_has_logicalxor();
  void set_has_logicalnot();
  void set_has_logicaland();
  void set_has_modbroadcastable();
  void set_has_minbroadcastable();
  void set_has_maxbroadcastable();
  void set_has_addbroadcastable();
  void set_has_powbroadcastable();
  void set_has_dividebroadcastable();
  void set_has_floordivbroadcastable();
  void set_has_multiplybroadcastable();
  void set_has_subtractbroadcastable();
  void set_has_tile();
  void set_has_stack();
  void set_has_gather();
  void set_has_scatter();
  void set_has_gathernd();
  void set_has_scatternd();
  void set_has_softmaxnd();
  void set_has_gatheralongaxis();
  void set_has_scatteralongaxis();
  void set_has_reverse();
  void set_has_reverseseq();
  void set_has_splitnd();
  void set_has_concatnd();
  void set_has_transpose();
  void set_has_slicestatic();
  void set_has_slicedynamic();
  void set_has_slidingwindows();
  void set_has_topk();
  void set_has_argmin();
  void set_has_argmax();
  void set_has_embeddingnd();
  void set_has_batchedmatmul();
  void set_has_getshape();
  void set_has_loadconstantnd();
  void set_has_filllike();
  void set_has_fillstatic();
  void set_has_filldynamic();
  void set_has_broadcasttolike();
  void set_has_broadcasttostatic();
  void set_has_broadcasttodynamic();
  void set_has_squeeze();
  void set_has_expanddims();
  void set_has_flattento2d();
  void set_has_reshapelike();
  void set_has_reshapestatic();
  void set_has_reshapedynamic();
  void set_has_rankpreservingreshape();
  void set_has_constantpad();
  void set_has_randomnormallike();
  void set_has_randomnormalstatic();
  void set_has_randomnormaldynamic();
  void set_has_randomuniformlike();
  void set_has_randomuniformstatic();
  void set_has_randomuniformdynamic();
  void set_has_randombernoullilike();
  void set_has_randombernoullistatic();
  void set_has_randombernoullidynamic();
  void set_has_categoricaldistribution();
  void set_has_reducel1();
  void set_has_reducel2();
  void set_has_reducemax();
  void set_has_reducemin();
  void set_has_reducesum();
  void set_has_reduceprod();
  void set_has_reducemean();
  void set_has_reducelogsum();
  void set_has_reducesumsquare();
  void set_has_reducelogsumexp();
  void set_has_wherenonzero();
  void set_has_matrixbandpart();
  void set_has_lowertriangular();
  void set_has_uppertriangular();
  void set_has_wherebroadcastable();
  void set_has_layernormalization();
  void set_has_nonmaximumsuppression();
  void set_has_onehot();
  void set_has_cumsum();
  void set_has_clampedrelu();
  void set_has_argsort();
  void set_has_pooling3d();
  void set_has_globalpooling3d();
  void set_has_slicebysize();
  void set_has_convolution3d();

  inline bool has_layer() const;
  inline void clear_has_layer();

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField<std::string> input_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField<std::string> output_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::Tensor > inputtensor_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::Tensor > outputtensor_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr name_;
  bool isupdatable_;
  union LayerUnion {
    constexpr LayerUnion() : _constinit_{} {}
      ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized _constinit_;
    ::CoreML::Specification::ConvolutionLayerParams* convolution_;
    ::CoreML::Specification::PoolingLayerParams* pooling_;
    ::CoreML::Specification::ActivationParams* activation_;
    ::CoreML::Specification::InnerProductLayerParams* innerproduct_;
    ::CoreML::Specification::EmbeddingLayerParams* embedding_;
    ::CoreML::Specification::BatchnormLayerParams* batchnorm_;
    ::CoreML::Specification::MeanVarianceNormalizeLayerParams* mvn_;
    ::CoreML::Specification::L2NormalizeLayerParams* l2normalize_;
    ::CoreML::Specification::SoftmaxLayerParams* softmax_;
    ::CoreML::Specification::LRNLayerParams* lrn_;
    ::CoreML::Specification::CropLayerParams* crop_;
    ::CoreML::Specification::PaddingLayerParams* padding_;
    ::CoreML::Specification::UpsampleLayerParams* upsample_;
    ::CoreML::Specification::ResizeBilinearLayerParams* resizebilinear_;
    ::CoreML::Specification::CropResizeLayerParams* cropresize_;
    ::CoreML::Specification::UnaryFunctionLayerParams* unary_;
    ::CoreML::Specification::AddLayerParams* add_;
    ::CoreML::Specification::MultiplyLayerParams* multiply_;
    ::CoreML::Specification::AverageLayerParams* average_;
    ::CoreML::Specification::ScaleLayerParams* scale_;
    ::CoreML::Specification::BiasLayerParams* bias_;
    ::CoreML::Specification::MaxLayerParams* max_;
    ::CoreML::Specification::MinLayerParams* min_;
    ::CoreML::Specification::DotProductLayerParams* dot_;
    ::CoreML::Specification::ReduceLayerParams* reduce_;
    ::CoreML::Specification::LoadConstantLayerParams* loadconstant_;
    ::CoreML::Specification::ReshapeLayerParams* reshape_;
    ::CoreML::Specification::FlattenLayerParams* flatten_;
    ::CoreML::Specification::PermuteLayerParams* permute_;
    ::CoreML::Specification::ConcatLayerParams* concat_;
    ::CoreML::Specification::SplitLayerParams* split_;
    ::CoreML::Specification::SequenceRepeatLayerParams* sequencerepeat_;
    ::CoreML::Specification::ReorganizeDataLayerParams* reorganizedata_;
    ::CoreML::Specification::SliceLayerParams* slice_;
    ::CoreML::Specification::SimpleRecurrentLayerParams* simplerecurrent_;
    ::CoreML::Specification::GRULayerParams* gru_;
    ::CoreML::Specification::UniDirectionalLSTMLayerParams* unidirectionallstm_;
    ::CoreML::Specification::BiDirectionalLSTMLayerParams* bidirectionallstm_;
    ::CoreML::Specification::CustomLayerParams* custom_;
    ::CoreML::Specification::CopyLayerParams* copy_;
    ::CoreML::Specification::BranchLayerParams* branch_;
    ::CoreML::Specification::LoopLayerParams* loop_;
    ::CoreML::Specification::LoopBreakLayerParams* loopbreak_;
    ::CoreML::Specification::LoopContinueLayerParams* loopcontinue_;
    ::CoreML::Specification::RangeStaticLayerParams* rangestatic_;
    ::CoreML::Specification::RangeDynamicLayerParams* rangedynamic_;
    ::CoreML::Specification::ClipLayerParams* clip_;
    ::CoreML::Specification::CeilLayerParams* ceil_;
    ::CoreML::Specification::FloorLayerParams* floor_;
    ::CoreML::Specification::SignLayerParams* sign_;
    ::CoreML::Specification::RoundLayerParams* round_;
    ::CoreML::Specification::Exp2LayerParams* exp2_;
    ::CoreML::Specification::SinLayerParams* sin_;
    ::CoreML::Specification::CosLayerParams* cos_;
    ::CoreML::Specification::TanLayerParams* tan_;
    ::CoreML::Specification::AsinLayerParams* asin_;
    ::CoreML::Specification::AcosLayerParams* acos_;
    ::CoreML::Specification::AtanLayerParams* atan_;
    ::CoreML::Specification::SinhLayerParams* sinh_;
    ::CoreML::Specification::CoshLayerParams* cosh_;
    ::CoreML::Specification::TanhLayerParams* tanh_;
    ::CoreML::Specification::AsinhLayerParams* asinh_;
    ::CoreML::Specification::AcoshLayerParams* acosh_;
    ::CoreML::Specification::AtanhLayerParams* atanh_;
    ::CoreML::Specification::ErfLayerParams* erf_;
    ::CoreML::Specification::GeluLayerParams* gelu_;
    ::CoreML::Specification::EqualLayerParams* equal_;
    ::CoreML::Specification::NotEqualLayerParams* notequal_;
    ::CoreML::Specification::LessThanLayerParams* lessthan_;
    ::CoreML::Specification::LessEqualLayerParams* lessequal_;
    ::CoreML::Specification::GreaterThanLayerParams* greaterthan_;
    ::CoreML::Specification::GreaterEqualLayerParams* greaterequal_;
    ::CoreML::Specification::LogicalOrLayerParams* logicalor_;
    ::CoreML::Specification::LogicalXorLayerParams* logicalxor_;
    ::CoreML::Specification::LogicalNotLayerParams* logicalnot_;
    ::CoreML::Specification::LogicalAndLayerParams* logicaland_;
    ::CoreML::Specification::ModBroadcastableLayerParams* modbroadcastable_;
    ::CoreML::Specification::MinBroadcastableLayerParams* minbroadcastable_;
    ::CoreML::Specification::MaxBroadcastableLayerParams* maxbroadcastable_;
    ::CoreML::Specification::AddBroadcastableLayerParams* addbroadcastable_;
    ::CoreML::Specification::PowBroadcastableLayerParams* powbroadcastable_;
    ::CoreML::Specification::DivideBroadcastableLayerParams* dividebroadcastable_;
    ::CoreML::Specification::FloorDivBroadcastableLayerParams* floordivbroadcastable_;
    ::CoreML::Specification::MultiplyBroadcastableLayerParams* multiplybroadcastable_;
    ::CoreML::Specification::SubtractBroadcastableLayerParams* subtractbroadcastable_;
    ::CoreML::Specification::TileLayerParams* tile_;
    ::CoreML::Specification::StackLayerParams* stack_;
    ::CoreML::Specification::GatherLayerParams* gather_;
    ::CoreML::Specification::ScatterLayerParams* scatter_;
    ::CoreML::Specification::GatherNDLayerParams* gathernd_;
    ::CoreML::Specification::ScatterNDLayerParams* scatternd_;
    ::CoreML::Specification::SoftmaxNDLayerParams* softmaxnd_;
    ::CoreML::Specification::GatherAlongAxisLayerParams* gatheralongaxis_;
    ::CoreML::Specification::ScatterAlongAxisLayerParams* scatteralongaxis_;
    ::CoreML::Specification::ReverseLayerParams* reverse_;
    ::CoreML::Specification::ReverseSeqLayerParams* reverseseq_;
    ::CoreML::Specification::SplitNDLayerParams* splitnd_;
    ::CoreML::Specification::ConcatNDLayerParams* concatnd_;
    ::CoreML::Specification::TransposeLayerParams* transpose_;
    ::CoreML::Specification::SliceStaticLayerParams* slicestatic_;
    ::CoreML::Specification::SliceDynamicLayerParams* slicedynamic_;
    ::CoreML::Specification::SlidingWindowsLayerParams* slidingwindows_;
    ::CoreML::Specification::TopKLayerParams* topk_;
    ::CoreML::Specification::ArgMinLayerParams* argmin_;
    ::CoreML::Specification::ArgMaxLayerParams* argmax_;
    ::CoreML::Specification::EmbeddingNDLayerParams* embeddingnd_;
    ::CoreML::Specification::BatchedMatMulLayerParams* batchedmatmul_;
    ::CoreML::Specification::GetShapeLayerParams* getshape_;
    ::CoreML::Specification::LoadConstantNDLayerParams* loadconstantnd_;
    ::CoreML::Specification::FillLikeLayerParams* filllike_;
    ::CoreML::Specification::FillStaticLayerParams* fillstatic_;
    ::CoreML::Specification::FillDynamicLayerParams* filldynamic_;
    ::CoreML::Specification::BroadcastToLikeLayerParams* broadcasttolike_;
    ::CoreML::Specification::BroadcastToStaticLayerParams* broadcasttostatic_;
    ::CoreML::Specification::BroadcastToDynamicLayerParams* broadcasttodynamic_;
    ::CoreML::Specification::SqueezeLayerParams* squeeze_;
    ::CoreML::Specification::ExpandDimsLayerParams* expanddims_;
    ::CoreML::Specification::FlattenTo2DLayerParams* flattento2d_;
    ::CoreML::Specification::ReshapeLikeLayerParams* reshapelike_;
    ::CoreML::Specification::ReshapeStaticLayerParams* reshapestatic_;
    ::CoreML::Specification::ReshapeDynamicLayerParams* reshapedynamic_;
    ::CoreML::Specification::RankPreservingReshapeLayerParams* rankpreservingreshape_;
    ::CoreML::Specification::ConstantPaddingLayerParams* constantpad_;
    ::CoreML::Specification::RandomNormalLikeLayerParams* randomnormallike_;
    ::CoreML::Specification::RandomNormalStaticLayerParams* randomnormalstatic_;
    ::CoreML::Specification::RandomNormalDynamicLayerParams* randomnormaldynamic_;
    ::CoreML::Specification::RandomUniformLikeLayerParams* randomuniformlike_;
    ::CoreML::Specification::RandomUniformStaticLayerParams* randomuniformstatic_;
    ::CoreML::Specification::RandomUniformDynamicLayerParams* randomuniformdynamic_;
    ::CoreML::Specification::RandomBernoulliLikeLayerParams* randombernoullilike_;
    ::CoreML::Specification::RandomBernoulliStaticLayerParams* randombernoullistatic_;
    ::CoreML::Specification::RandomBernoulliDynamicLayerParams* randombernoullidynamic_;
    ::CoreML::Specification::CategoricalDistributionLayerParams* categoricaldistribution_;
    ::CoreML::Specification::ReduceL1LayerParams* reducel1_;
    ::CoreML::Specification::ReduceL2LayerParams* reducel2_;
    ::CoreML::Specification::ReduceMaxLayerParams* reducemax_;
    ::CoreML::Specification::ReduceMinLayerParams* reducemin_;
    ::CoreML::Specification::ReduceSumLayerParams* reducesum_;
    ::CoreML::Specification::ReduceProdLayerParams* reduceprod_;
    ::CoreML::Specification::ReduceMeanLayerParams* reducemean_;
    ::CoreML::Specification::ReduceLogSumLayerParams* reducelogsum_;
    ::CoreML::Specification::ReduceSumSquareLayerParams* reducesumsquare_;
    ::CoreML::Specification::ReduceLogSumExpLayerParams* reducelogsumexp_;
    ::CoreML::Specification::WhereNonZeroLayerParams* wherenonzero_;
    ::CoreML::Specification::MatrixBandPartLayerParams* matrixbandpart_;
    ::CoreML::Specification::LowerTriangularLayerParams* lowertriangular_;
    ::CoreML::Specification::UpperTriangularLayerParams* uppertriangular_;
    ::CoreML::Specification::WhereBroadcastableLayerParams* wherebroadcastable_;
    ::CoreML::Specification::LayerNormalizationLayerParams* layernormalization_;
    ::CoreML::Specification::NonMaximumSuppressionLayerParams* nonmaximumsuppression_;
    ::CoreML::Specification::OneHotLayerParams* onehot_;
    ::CoreML::Specification::CumSumLayerParams* cumsum_;
    ::CoreML::Specification::ClampedReLULayerParams* clampedrelu_;
    ::CoreML::Specification::ArgSortLayerParams* argsort_;
    ::CoreML::Specification::Pooling3DLayerParams* pooling3d_;
    ::CoreML::Specification::GlobalPooling3DLayerParams* globalpooling3d_;
    ::CoreML::Specification::SliceBySizeLayerParams* slicebysize_;
    ::CoreML::Specification::Convolution3DLayerParams* convolution3d_;
  } layer_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  uint32_t _oneof_case_[1];

  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class BranchLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BranchLayerParams) */ {
 public:
  inline BranchLayerParams() : BranchLayerParams(nullptr) {}
  ~BranchLayerParams() override;
  explicit constexpr BranchLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  BranchLayerParams(const BranchLayerParams& from);
  BranchLayerParams(BranchLayerParams&& from) noexcept
    : BranchLayerParams() {
    *this = ::std::move(from);
  }

  inline BranchLayerParams& operator=(const BranchLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline BranchLayerParams& operator=(BranchLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const BranchLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const BranchLayerParams* internal_default_instance() {
    return reinterpret_cast<const BranchLayerParams*>(
               &_BranchLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    20;

  friend void swap(BranchLayerParams& a, BranchLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(BranchLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(BranchLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  BranchLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<BranchLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const BranchLayerParams& from);
  void MergeFrom(const BranchLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BranchLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.BranchLayerParams";
  }
  protected:
  explicit BranchLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kIfBranchFieldNumber = 1,
    kElseBranchFieldNumber = 2,
  };
  // .CoreML.Specification.NeuralNetwork ifBranch = 1;
  bool has_ifbranch() const;
  private:
  bool _internal_has_ifbranch() const;
  public:
  void clear_ifbranch();
  const ::CoreML::Specification::NeuralNetwork& ifbranch() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::NeuralNetwork* release_ifbranch();
  ::CoreML::Specification::NeuralNetwork* mutable_ifbranch();
  void set_allocated_ifbranch(::CoreML::Specification::NeuralNetwork* ifbranch);
  private:
  const ::CoreML::Specification::NeuralNetwork& _internal_ifbranch() const;
  ::CoreML::Specification::NeuralNetwork* _internal_mutable_ifbranch();
  public:
  void unsafe_arena_set_allocated_ifbranch(
      ::CoreML::Specification::NeuralNetwork* ifbranch);
  ::CoreML::Specification::NeuralNetwork* unsafe_arena_release_ifbranch();

  // .CoreML.Specification.NeuralNetwork elseBranch = 2;
  bool has_elsebranch() const;
  private:
  bool _internal_has_elsebranch() const;
  public:
  void clear_elsebranch();
  const ::CoreML::Specification::NeuralNetwork& elsebranch() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::NeuralNetwork* release_elsebranch();
  ::CoreML::Specification::NeuralNetwork* mutable_elsebranch();
  void set_allocated_elsebranch(::CoreML::Specification::NeuralNetwork* elsebranch);
  private:
  const ::CoreML::Specification::NeuralNetwork& _internal_elsebranch() const;
  ::CoreML::Specification::NeuralNetwork* _internal_mutable_elsebranch();
  public:
  void unsafe_arena_set_allocated_elsebranch(
      ::CoreML::Specification::NeuralNetwork* elsebranch);
  ::CoreML::Specification::NeuralNetwork* unsafe_arena_release_elsebranch();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BranchLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::CoreML::Specification::NeuralNetwork* ifbranch_;
  ::CoreML::Specification::NeuralNetwork* elsebranch_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class LoopLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LoopLayerParams) */ {
 public:
  inline LoopLayerParams() : LoopLayerParams(nullptr) {}
  ~LoopLayerParams() override;
  explicit constexpr LoopLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  LoopLayerParams(const LoopLayerParams& from);
  LoopLayerParams(LoopLayerParams&& from) noexcept
    : LoopLayerParams() {
    *this = ::std::move(from);
  }

  inline LoopLayerParams& operator=(const LoopLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline LoopLayerParams& operator=(LoopLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const LoopLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const LoopLayerParams* internal_default_instance() {
    return reinterpret_cast<const LoopLayerParams*>(
               &_LoopLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    21;

  friend void swap(LoopLayerParams& a, LoopLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(LoopLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(LoopLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  LoopLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<LoopLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const LoopLayerParams& from);
  void MergeFrom(const LoopLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LoopLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.LoopLayerParams";
  }
  protected:
  explicit LoopLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kConditionVarFieldNumber = 2,
    kConditionNetworkFieldNumber = 3,
    kBodyNetworkFieldNumber = 4,
    kMaxLoopIterationsFieldNumber = 1,
  };
  // string conditionVar = 2;
  void clear_conditionvar();
  const std::string& conditionvar() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_conditionvar(ArgT0&& arg0, ArgT... args);
  std::string* mutable_conditionvar();
  PROTOBUF_NODISCARD std::string* release_conditionvar();
  void set_allocated_conditionvar(std::string* conditionvar);
  private:
  const std::string& _internal_conditionvar() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_conditionvar(const std::string& value);
  std::string* _internal_mutable_conditionvar();
  public:

  // .CoreML.Specification.NeuralNetwork conditionNetwork = 3;
  bool has_conditionnetwork() const;
  private:
  bool _internal_has_conditionnetwork() const;
  public:
  void clear_conditionnetwork();
  const ::CoreML::Specification::NeuralNetwork& conditionnetwork() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::NeuralNetwork* release_conditionnetwork();
  ::CoreML::Specification::NeuralNetwork* mutable_conditionnetwork();
  void set_allocated_conditionnetwork(::CoreML::Specification::NeuralNetwork* conditionnetwork);
  private:
  const ::CoreML::Specification::NeuralNetwork& _internal_conditionnetwork() const;
  ::CoreML::Specification::NeuralNetwork* _internal_mutable_conditionnetwork();
  public:
  void unsafe_arena_set_allocated_conditionnetwork(
      ::CoreML::Specification::NeuralNetwork* conditionnetwork);
  ::CoreML::Specification::NeuralNetwork* unsafe_arena_release_conditionnetwork();

  // .CoreML.Specification.NeuralNetwork bodyNetwork = 4;
  bool has_bodynetwork() const;
  private:
  bool _internal_has_bodynetwork() const;
  public:
  void clear_bodynetwork();
  const ::CoreML::Specification::NeuralNetwork& bodynetwork() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::NeuralNetwork* release_bodynetwork();
  ::CoreML::Specification::NeuralNetwork* mutable_bodynetwork();
  void set_allocated_bodynetwork(::CoreML::Specification::NeuralNetwork* bodynetwork);
  private:
  const ::CoreML::Specification::NeuralNetwork& _internal_bodynetwork() const;
  ::CoreML::Specification::NeuralNetwork* _internal_mutable_bodynetwork();
  public:
  void unsafe_arena_set_allocated_bodynetwork(
      ::CoreML::Specification::NeuralNetwork* bodynetwork);
  ::CoreML::Specification::NeuralNetwork* unsafe_arena_release_bodynetwork();

  // uint64 maxLoopIterations = 1;
  void clear_maxloopiterations();
  uint64_t maxloopiterations() const;
  void set_maxloopiterations(uint64_t value);
  private:
  uint64_t _internal_maxloopiterations() const;
  void _internal_set_maxloopiterations(uint64_t value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LoopLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr conditionvar_;
  ::CoreML::Specification::NeuralNetwork* conditionnetwork_;
  ::CoreML::Specification::NeuralNetwork* bodynetwork_;
  uint64_t maxloopiterations_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class LoopBreakLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LoopBreakLayerParams) */ {
 public:
  inline LoopBreakLayerParams() : LoopBreakLayerParams(nullptr) {}
  ~LoopBreakLayerParams() override;
  explicit constexpr LoopBreakLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  LoopBreakLayerParams(const LoopBreakLayerParams& from);
  LoopBreakLayerParams(LoopBreakLayerParams&& from) noexcept
    : LoopBreakLayerParams() {
    *this = ::std::move(from);
  }

  inline LoopBreakLayerParams& operator=(const LoopBreakLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline LoopBreakLayerParams& operator=(LoopBreakLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const LoopBreakLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const LoopBreakLayerParams* internal_default_instance() {
    return reinterpret_cast<const LoopBreakLayerParams*>(
               &_LoopBreakLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    22;

  friend void swap(LoopBreakLayerParams& a, LoopBreakLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(LoopBreakLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(LoopBreakLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  LoopBreakLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<LoopBreakLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const LoopBreakLayerParams& from);
  void MergeFrom(const LoopBreakLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LoopBreakLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.LoopBreakLayerParams";
  }
  protected:
  explicit LoopBreakLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LoopBreakLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class LoopContinueLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LoopContinueLayerParams) */ {
 public:
  inline LoopContinueLayerParams() : LoopContinueLayerParams(nullptr) {}
  ~LoopContinueLayerParams() override;
  explicit constexpr LoopContinueLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  LoopContinueLayerParams(const LoopContinueLayerParams& from);
  LoopContinueLayerParams(LoopContinueLayerParams&& from) noexcept
    : LoopContinueLayerParams() {
    *this = ::std::move(from);
  }

  inline LoopContinueLayerParams& operator=(const LoopContinueLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline LoopContinueLayerParams& operator=(LoopContinueLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const LoopContinueLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const LoopContinueLayerParams* internal_default_instance() {
    return reinterpret_cast<const LoopContinueLayerParams*>(
               &_LoopContinueLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    23;

  friend void swap(LoopContinueLayerParams& a, LoopContinueLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(LoopContinueLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(LoopContinueLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  LoopContinueLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<LoopContinueLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const LoopContinueLayerParams& from);
  void MergeFrom(const LoopContinueLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LoopContinueLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.LoopContinueLayerParams";
  }
  protected:
  explicit LoopContinueLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LoopContinueLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class CopyLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CopyLayerParams) */ {
 public:
  inline CopyLayerParams() : CopyLayerParams(nullptr) {}
  ~CopyLayerParams() override;
  explicit constexpr CopyLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  CopyLayerParams(const CopyLayerParams& from);
  CopyLayerParams(CopyLayerParams&& from) noexcept
    : CopyLayerParams() {
    *this = ::std::move(from);
  }

  inline CopyLayerParams& operator=(const CopyLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline CopyLayerParams& operator=(CopyLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const CopyLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const CopyLayerParams* internal_default_instance() {
    return reinterpret_cast<const CopyLayerParams*>(
               &_CopyLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    24;

  friend void swap(CopyLayerParams& a, CopyLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(CopyLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(CopyLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  CopyLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<CopyLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const CopyLayerParams& from);
  void MergeFrom(const CopyLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CopyLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.CopyLayerParams";
  }
  protected:
  explicit CopyLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.CopyLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class GreaterThanLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.GreaterThanLayerParams) */ {
 public:
  inline GreaterThanLayerParams() : GreaterThanLayerParams(nullptr) {}
  ~GreaterThanLayerParams() override;
  explicit constexpr GreaterThanLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  GreaterThanLayerParams(const GreaterThanLayerParams& from);
  GreaterThanLayerParams(GreaterThanLayerParams&& from) noexcept
    : GreaterThanLayerParams() {
    *this = ::std::move(from);
  }

  inline GreaterThanLayerParams& operator=(const GreaterThanLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline GreaterThanLayerParams& operator=(GreaterThanLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const GreaterThanLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const GreaterThanLayerParams* internal_default_instance() {
    return reinterpret_cast<const GreaterThanLayerParams*>(
               &_GreaterThanLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    25;

  friend void swap(GreaterThanLayerParams& a, GreaterThanLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(GreaterThanLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(GreaterThanLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  GreaterThanLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<GreaterThanLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const GreaterThanLayerParams& from);
  void MergeFrom(const GreaterThanLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(GreaterThanLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.GreaterThanLayerParams";
  }
  protected:
  explicit GreaterThanLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAlphaFieldNumber = 2,
  };
  // float alpha = 2;
  void clear_alpha();
  float alpha() const;
  void set_alpha(float value);
  private:
  float _internal_alpha() const;
  void _internal_set_alpha(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.GreaterThanLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float alpha_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class GreaterEqualLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.GreaterEqualLayerParams) */ {
 public:
  inline GreaterEqualLayerParams() : GreaterEqualLayerParams(nullptr) {}
  ~GreaterEqualLayerParams() override;
  explicit constexpr GreaterEqualLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  GreaterEqualLayerParams(const GreaterEqualLayerParams& from);
  GreaterEqualLayerParams(GreaterEqualLayerParams&& from) noexcept
    : GreaterEqualLayerParams() {
    *this = ::std::move(from);
  }

  inline GreaterEqualLayerParams& operator=(const GreaterEqualLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline GreaterEqualLayerParams& operator=(GreaterEqualLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const GreaterEqualLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const GreaterEqualLayerParams* internal_default_instance() {
    return reinterpret_cast<const GreaterEqualLayerParams*>(
               &_GreaterEqualLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    26;

  friend void swap(GreaterEqualLayerParams& a, GreaterEqualLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(GreaterEqualLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(GreaterEqualLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  GreaterEqualLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<GreaterEqualLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const GreaterEqualLayerParams& from);
  void MergeFrom(const GreaterEqualLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(GreaterEqualLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.GreaterEqualLayerParams";
  }
  protected:
  explicit GreaterEqualLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAlphaFieldNumber = 2,
  };
  // float alpha = 2;
  void clear_alpha();
  float alpha() const;
  void set_alpha(float value);
  private:
  float _internal_alpha() const;
  void _internal_set_alpha(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.GreaterEqualLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float alpha_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class LessThanLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LessThanLayerParams) */ {
 public:
  inline LessThanLayerParams() : LessThanLayerParams(nullptr) {}
  ~LessThanLayerParams() override;
  explicit constexpr LessThanLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  LessThanLayerParams(const LessThanLayerParams& from);
  LessThanLayerParams(LessThanLayerParams&& from) noexcept
    : LessThanLayerParams() {
    *this = ::std::move(from);
  }

  inline LessThanLayerParams& operator=(const LessThanLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline LessThanLayerParams& operator=(LessThanLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const LessThanLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const LessThanLayerParams* internal_default_instance() {
    return reinterpret_cast<const LessThanLayerParams*>(
               &_LessThanLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    27;

  friend void swap(LessThanLayerParams& a, LessThanLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(LessThanLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(LessThanLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  LessThanLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<LessThanLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const LessThanLayerParams& from);
  void MergeFrom(const LessThanLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LessThanLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.LessThanLayerParams";
  }
  protected:
  explicit LessThanLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAlphaFieldNumber = 2,
  };
  // float alpha = 2;
  void clear_alpha();
  float alpha() const;
  void set_alpha(float value);
  private:
  float _internal_alpha() const;
  void _internal_set_alpha(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LessThanLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float alpha_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class LessEqualLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LessEqualLayerParams) */ {
 public:
  inline LessEqualLayerParams() : LessEqualLayerParams(nullptr) {}
  ~LessEqualLayerParams() override;
  explicit constexpr LessEqualLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  LessEqualLayerParams(const LessEqualLayerParams& from);
  LessEqualLayerParams(LessEqualLayerParams&& from) noexcept
    : LessEqualLayerParams() {
    *this = ::std::move(from);
  }

  inline LessEqualLayerParams& operator=(const LessEqualLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline LessEqualLayerParams& operator=(LessEqualLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const LessEqualLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const LessEqualLayerParams* internal_default_instance() {
    return reinterpret_cast<const LessEqualLayerParams*>(
               &_LessEqualLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    28;

  friend void swap(LessEqualLayerParams& a, LessEqualLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(LessEqualLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(LessEqualLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  LessEqualLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<LessEqualLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const LessEqualLayerParams& from);
  void MergeFrom(const LessEqualLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LessEqualLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.LessEqualLayerParams";
  }
  protected:
  explicit LessEqualLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAlphaFieldNumber = 2,
  };
  // float alpha = 2;
  void clear_alpha();
  float alpha() const;
  void set_alpha(float value);
  private:
  float _internal_alpha() const;
  void _internal_set_alpha(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LessEqualLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float alpha_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class EqualLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.EqualLayerParams) */ {
 public:
  inline EqualLayerParams() : EqualLayerParams(nullptr) {}
  ~EqualLayerParams() override;
  explicit constexpr EqualLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  EqualLayerParams(const EqualLayerParams& from);
  EqualLayerParams(EqualLayerParams&& from) noexcept
    : EqualLayerParams() {
    *this = ::std::move(from);
  }

  inline EqualLayerParams& operator=(const EqualLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline EqualLayerParams& operator=(EqualLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const EqualLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const EqualLayerParams* internal_default_instance() {
    return reinterpret_cast<const EqualLayerParams*>(
               &_EqualLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    29;

  friend void swap(EqualLayerParams& a, EqualLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(EqualLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(EqualLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  EqualLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<EqualLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const EqualLayerParams& from);
  void MergeFrom(const EqualLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(EqualLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.EqualLayerParams";
  }
  protected:
  explicit EqualLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAlphaFieldNumber = 1,
  };
  // float alpha = 1;
  void clear_alpha();
  float alpha() const;
  void set_alpha(float value);
  private:
  float _internal_alpha() const;
  void _internal_set_alpha(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.EqualLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float alpha_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class NotEqualLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NotEqualLayerParams) */ {
 public:
  inline NotEqualLayerParams() : NotEqualLayerParams(nullptr) {}
  ~NotEqualLayerParams() override;
  explicit constexpr NotEqualLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  NotEqualLayerParams(const NotEqualLayerParams& from);
  NotEqualLayerParams(NotEqualLayerParams&& from) noexcept
    : NotEqualLayerParams() {
    *this = ::std::move(from);
  }

  inline NotEqualLayerParams& operator=(const NotEqualLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline NotEqualLayerParams& operator=(NotEqualLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const NotEqualLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const NotEqualLayerParams* internal_default_instance() {
    return reinterpret_cast<const NotEqualLayerParams*>(
               &_NotEqualLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    30;

  friend void swap(NotEqualLayerParams& a, NotEqualLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(NotEqualLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(NotEqualLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  NotEqualLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<NotEqualLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const NotEqualLayerParams& from);
  void MergeFrom(const NotEqualLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NotEqualLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.NotEqualLayerParams";
  }
  protected:
  explicit NotEqualLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAlphaFieldNumber = 1,
  };
  // float alpha = 1;
  void clear_alpha();
  float alpha() const;
  void set_alpha(float value);
  private:
  float _internal_alpha() const;
  void _internal_set_alpha(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.NotEqualLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float alpha_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class LogicalAndLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LogicalAndLayerParams) */ {
 public:
  inline LogicalAndLayerParams() : LogicalAndLayerParams(nullptr) {}
  ~LogicalAndLayerParams() override;
  explicit constexpr LogicalAndLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  LogicalAndLayerParams(const LogicalAndLayerParams& from);
  LogicalAndLayerParams(LogicalAndLayerParams&& from) noexcept
    : LogicalAndLayerParams() {
    *this = ::std::move(from);
  }

  inline LogicalAndLayerParams& operator=(const LogicalAndLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline LogicalAndLayerParams& operator=(LogicalAndLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const LogicalAndLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const LogicalAndLayerParams* internal_default_instance() {
    return reinterpret_cast<const LogicalAndLayerParams*>(
               &_LogicalAndLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    31;

  friend void swap(LogicalAndLayerParams& a, LogicalAndLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(LogicalAndLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(LogicalAndLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  LogicalAndLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<LogicalAndLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const LogicalAndLayerParams& from);
  void MergeFrom(const LogicalAndLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LogicalAndLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.LogicalAndLayerParams";
  }
  protected:
  explicit LogicalAndLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LogicalAndLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class LogicalOrLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LogicalOrLayerParams) */ {
 public:
  inline LogicalOrLayerParams() : LogicalOrLayerParams(nullptr) {}
  ~LogicalOrLayerParams() override;
  explicit constexpr LogicalOrLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  LogicalOrLayerParams(const LogicalOrLayerParams& from);
  LogicalOrLayerParams(LogicalOrLayerParams&& from) noexcept
    : LogicalOrLayerParams() {
    *this = ::std::move(from);
  }

  inline LogicalOrLayerParams& operator=(const LogicalOrLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline LogicalOrLayerParams& operator=(LogicalOrLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const LogicalOrLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const LogicalOrLayerParams* internal_default_instance() {
    return reinterpret_cast<const LogicalOrLayerParams*>(
               &_LogicalOrLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    32;

  friend void swap(LogicalOrLayerParams& a, LogicalOrLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(LogicalOrLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(LogicalOrLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  LogicalOrLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<LogicalOrLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const LogicalOrLayerParams& from);
  void MergeFrom(const LogicalOrLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LogicalOrLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.LogicalOrLayerParams";
  }
  protected:
  explicit LogicalOrLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LogicalOrLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class LogicalXorLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LogicalXorLayerParams) */ {
 public:
  inline LogicalXorLayerParams() : LogicalXorLayerParams(nullptr) {}
  ~LogicalXorLayerParams() override;
  explicit constexpr LogicalXorLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  LogicalXorLayerParams(const LogicalXorLayerParams& from);
  LogicalXorLayerParams(LogicalXorLayerParams&& from) noexcept
    : LogicalXorLayerParams() {
    *this = ::std::move(from);
  }

  inline LogicalXorLayerParams& operator=(const LogicalXorLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline LogicalXorLayerParams& operator=(LogicalXorLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const LogicalXorLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const LogicalXorLayerParams* internal_default_instance() {
    return reinterpret_cast<const LogicalXorLayerParams*>(
               &_LogicalXorLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    33;

  friend void swap(LogicalXorLayerParams& a, LogicalXorLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(LogicalXorLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(LogicalXorLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  LogicalXorLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<LogicalXorLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const LogicalXorLayerParams& from);
  void MergeFrom(const LogicalXorLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LogicalXorLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.LogicalXorLayerParams";
  }
  protected:
  explicit LogicalXorLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LogicalXorLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class LogicalNotLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LogicalNotLayerParams) */ {
 public:
  inline LogicalNotLayerParams() : LogicalNotLayerParams(nullptr) {}
  ~LogicalNotLayerParams() override;
  explicit constexpr LogicalNotLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  LogicalNotLayerParams(const LogicalNotLayerParams& from);
  LogicalNotLayerParams(LogicalNotLayerParams&& from) noexcept
    : LogicalNotLayerParams() {
    *this = ::std::move(from);
  }

  inline LogicalNotLayerParams& operator=(const LogicalNotLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline LogicalNotLayerParams& operator=(LogicalNotLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const LogicalNotLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const LogicalNotLayerParams* internal_default_instance() {
    return reinterpret_cast<const LogicalNotLayerParams*>(
               &_LogicalNotLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    34;

  friend void swap(LogicalNotLayerParams& a, LogicalNotLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(LogicalNotLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(LogicalNotLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  LogicalNotLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<LogicalNotLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const LogicalNotLayerParams& from);
  void MergeFrom(const LogicalNotLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LogicalNotLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.LogicalNotLayerParams";
  }
  protected:
  explicit LogicalNotLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LogicalNotLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class BorderAmounts_EdgeSizes final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BorderAmounts.EdgeSizes) */ {
 public:
  inline BorderAmounts_EdgeSizes() : BorderAmounts_EdgeSizes(nullptr) {}
  ~BorderAmounts_EdgeSizes() override;
  explicit constexpr BorderAmounts_EdgeSizes(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  BorderAmounts_EdgeSizes(const BorderAmounts_EdgeSizes& from);
  BorderAmounts_EdgeSizes(BorderAmounts_EdgeSizes&& from) noexcept
    : BorderAmounts_EdgeSizes() {
    *this = ::std::move(from);
  }

  inline BorderAmounts_EdgeSizes& operator=(const BorderAmounts_EdgeSizes& from) {
    CopyFrom(from);
    return *this;
  }
  inline BorderAmounts_EdgeSizes& operator=(BorderAmounts_EdgeSizes&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const BorderAmounts_EdgeSizes& default_instance() {
    return *internal_default_instance();
  }
  static inline const BorderAmounts_EdgeSizes* internal_default_instance() {
    return reinterpret_cast<const BorderAmounts_EdgeSizes*>(
               &_BorderAmounts_EdgeSizes_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    35;

  friend void swap(BorderAmounts_EdgeSizes& a, BorderAmounts_EdgeSizes& b) {
    a.Swap(&b);
  }
  inline void Swap(BorderAmounts_EdgeSizes* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(BorderAmounts_EdgeSizes* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  BorderAmounts_EdgeSizes* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<BorderAmounts_EdgeSizes>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const BorderAmounts_EdgeSizes& from);
  void MergeFrom(const BorderAmounts_EdgeSizes& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BorderAmounts_EdgeSizes* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.BorderAmounts.EdgeSizes";
  }
  protected:
  explicit BorderAmounts_EdgeSizes(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kStartEdgeSizeFieldNumber = 1,
    kEndEdgeSizeFieldNumber = 2,
  };
  // uint64 startEdgeSize = 1;
  void clear_startedgesize();
  uint64_t startedgesize() const;
  void set_startedgesize(uint64_t value);
  private:
  uint64_t _internal_startedgesize() const;
  void _internal_set_startedgesize(uint64_t value);
  public:

  // uint64 endEdgeSize = 2;
  void clear_endedgesize();
  uint64_t endedgesize() const;
  void set_endedgesize(uint64_t value);
  private:
  uint64_t _internal_endedgesize() const;
  void _internal_set_endedgesize(uint64_t value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BorderAmounts.EdgeSizes)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  uint64_t startedgesize_;
  uint64_t endedgesize_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class BorderAmounts final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BorderAmounts) */ {
 public:
  inline BorderAmounts() : BorderAmounts(nullptr) {}
  ~BorderAmounts() override;
  explicit constexpr BorderAmounts(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  BorderAmounts(const BorderAmounts& from);
  BorderAmounts(BorderAmounts&& from) noexcept
    : BorderAmounts() {
    *this = ::std::move(from);
  }

  inline BorderAmounts& operator=(const BorderAmounts& from) {
    CopyFrom(from);
    return *this;
  }
  inline BorderAmounts& operator=(BorderAmounts&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const BorderAmounts& default_instance() {
    return *internal_default_instance();
  }
  static inline const BorderAmounts* internal_default_instance() {
    return reinterpret_cast<const BorderAmounts*>(
               &_BorderAmounts_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    36;

  friend void swap(BorderAmounts& a, BorderAmounts& b) {
    a.Swap(&b);
  }
  inline void Swap(BorderAmounts* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(BorderAmounts* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  BorderAmounts* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<BorderAmounts>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const BorderAmounts& from);
  void MergeFrom(const BorderAmounts& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BorderAmounts* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.BorderAmounts";
  }
  protected:
  explicit BorderAmounts(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  typedef BorderAmounts_EdgeSizes EdgeSizes;

  // accessors -------------------------------------------------------

  enum : int {
    kBorderAmountsFieldNumber = 10,
  };
  // repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
  int borderamounts_size() const;
  private:
  int _internal_borderamounts_size() const;
  public:
  void clear_borderamounts();
  ::CoreML::Specification::BorderAmounts_EdgeSizes* mutable_borderamounts(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes >*
      mutable_borderamounts();
  private:
  const ::CoreML::Specification::BorderAmounts_EdgeSizes& _internal_borderamounts(int index) const;
  ::CoreML::Specification::BorderAmounts_EdgeSizes* _internal_add_borderamounts();
  public:
  const ::CoreML::Specification::BorderAmounts_EdgeSizes& borderamounts(int index) const;
  ::CoreML::Specification::BorderAmounts_EdgeSizes* add_borderamounts();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes >&
      borderamounts() const;

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BorderAmounts)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes > borderamounts_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ValidPadding final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ValidPadding) */ {
 public:
  inline ValidPadding() : ValidPadding(nullptr) {}
  ~ValidPadding() override;
  explicit constexpr ValidPadding(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ValidPadding(const ValidPadding& from);
  ValidPadding(ValidPadding&& from) noexcept
    : ValidPadding() {
    *this = ::std::move(from);
  }

  inline ValidPadding& operator=(const ValidPadding& from) {
    CopyFrom(from);
    return *this;
  }
  inline ValidPadding& operator=(ValidPadding&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ValidPadding& default_instance() {
    return *internal_default_instance();
  }
  static inline const ValidPadding* internal_default_instance() {
    return reinterpret_cast<const ValidPadding*>(
               &_ValidPadding_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    37;

  friend void swap(ValidPadding& a, ValidPadding& b) {
    a.Swap(&b);
  }
  inline void Swap(ValidPadding* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ValidPadding* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ValidPadding* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ValidPadding>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ValidPadding& from);
  void MergeFrom(const ValidPadding& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ValidPadding* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ValidPadding";
  }
  protected:
  explicit ValidPadding(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kPaddingAmountsFieldNumber = 1,
  };
  // .CoreML.Specification.BorderAmounts paddingAmounts = 1;
  bool has_paddingamounts() const;
  private:
  bool _internal_has_paddingamounts() const;
  public:
  void clear_paddingamounts();
  const ::CoreML::Specification::BorderAmounts& paddingamounts() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::BorderAmounts* release_paddingamounts();
  ::CoreML::Specification::BorderAmounts* mutable_paddingamounts();
  void set_allocated_paddingamounts(::CoreML::Specification::BorderAmounts* paddingamounts);
  private:
  const ::CoreML::Specification::BorderAmounts& _internal_paddingamounts() const;
  ::CoreML::Specification::BorderAmounts* _internal_mutable_paddingamounts();
  public:
  void unsafe_arena_set_allocated_paddingamounts(
      ::CoreML::Specification::BorderAmounts* paddingamounts);
  ::CoreML::Specification::BorderAmounts* unsafe_arena_release_paddingamounts();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ValidPadding)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::CoreML::Specification::BorderAmounts* paddingamounts_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class SamePadding final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SamePadding) */ {
 public:
  inline SamePadding() : SamePadding(nullptr) {}
  ~SamePadding() override;
  explicit constexpr SamePadding(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SamePadding(const SamePadding& from);
  SamePadding(SamePadding&& from) noexcept
    : SamePadding() {
    *this = ::std::move(from);
  }

  inline SamePadding& operator=(const SamePadding& from) {
    CopyFrom(from);
    return *this;
  }
  inline SamePadding& operator=(SamePadding&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const SamePadding& default_instance() {
    return *internal_default_instance();
  }
  static inline const SamePadding* internal_default_instance() {
    return reinterpret_cast<const SamePadding*>(
               &_SamePadding_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    38;

  friend void swap(SamePadding& a, SamePadding& b) {
    a.Swap(&b);
  }
  inline void Swap(SamePadding* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SamePadding* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SamePadding* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SamePadding>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const SamePadding& from);
  void MergeFrom(const SamePadding& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SamePadding* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.SamePadding";
  }
  protected:
  explicit SamePadding(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  typedef SamePadding_SamePaddingMode SamePaddingMode;
  static constexpr SamePaddingMode BOTTOM_RIGHT_HEAVY =
    SamePadding_SamePaddingMode_BOTTOM_RIGHT_HEAVY;
  static constexpr SamePaddingMode TOP_LEFT_HEAVY =
    SamePadding_SamePaddingMode_TOP_LEFT_HEAVY;
  static inline bool SamePaddingMode_IsValid(int value) {
    return SamePadding_SamePaddingMode_IsValid(value);
  }
  static constexpr SamePaddingMode SamePaddingMode_MIN =
    SamePadding_SamePaddingMode_SamePaddingMode_MIN;
  static constexpr SamePaddingMode SamePaddingMode_MAX =
    SamePadding_SamePaddingMode_SamePaddingMode_MAX;
  static constexpr int SamePaddingMode_ARRAYSIZE =
    SamePadding_SamePaddingMode_SamePaddingMode_ARRAYSIZE;
  template<typename T>
  static inline const std::string& SamePaddingMode_Name(T enum_t_value) {
    static_assert(::std::is_same<T, SamePaddingMode>::value ||
      ::std::is_integral<T>::value,
      "Incorrect type passed to function SamePaddingMode_Name.");
    return SamePadding_SamePaddingMode_Name(enum_t_value);
  }
  static inline bool SamePaddingMode_Parse(::PROTOBUF_NAMESPACE_ID::ConstStringParam name,
      SamePaddingMode* value) {
    return SamePadding_SamePaddingMode_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  enum : int {
    kAsymmetryModeFieldNumber = 1,
  };
  // .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
  void clear_asymmetrymode();
  ::CoreML::Specification::SamePadding_SamePaddingMode asymmetrymode() const;
  void set_asymmetrymode(::CoreML::Specification::SamePadding_SamePaddingMode value);
  private:
  ::CoreML::Specification::SamePadding_SamePaddingMode _internal_asymmetrymode() const;
  void _internal_set_asymmetrymode(::CoreML::Specification::SamePadding_SamePaddingMode value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SamePadding)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int asymmetrymode_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class SamplingMode final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SamplingMode) */ {
 public:
  inline SamplingMode() : SamplingMode(nullptr) {}
  ~SamplingMode() override;
  explicit constexpr SamplingMode(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SamplingMode(const SamplingMode& from);
  SamplingMode(SamplingMode&& from) noexcept
    : SamplingMode() {
    *this = ::std::move(from);
  }

  inline SamplingMode& operator=(const SamplingMode& from) {
    CopyFrom(from);
    return *this;
  }
  inline SamplingMode& operator=(SamplingMode&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const SamplingMode& default_instance() {
    return *internal_default_instance();
  }
  static inline const SamplingMode* internal_default_instance() {
    return reinterpret_cast<const SamplingMode*>(
               &_SamplingMode_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    39;

  friend void swap(SamplingMode& a, SamplingMode& b) {
    a.Swap(&b);
  }
  inline void Swap(SamplingMode* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SamplingMode* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SamplingMode* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SamplingMode>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const SamplingMode& from);
  void MergeFrom(const SamplingMode& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SamplingMode* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.SamplingMode";
  }
  protected:
  explicit SamplingMode(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  typedef SamplingMode_Method Method;
  static constexpr Method STRICT_ALIGN_ENDPOINTS_MODE =
    SamplingMode_Method_STRICT_ALIGN_ENDPOINTS_MODE;
  static constexpr Method ALIGN_ENDPOINTS_MODE =
    SamplingMode_Method_ALIGN_ENDPOINTS_MODE;
  static constexpr Method UPSAMPLE_MODE =
    SamplingMode_Method_UPSAMPLE_MODE;
  static constexpr Method ROI_ALIGN_MODE =
    SamplingMode_Method_ROI_ALIGN_MODE;
  static inline bool Method_IsValid(int value) {
    return SamplingMode_Method_IsValid(value);
  }
  static constexpr Method Method_MIN =
    SamplingMode_Method_Method_MIN;
  static constexpr Method Method_MAX =
    SamplingMode_Method_Method_MAX;
  static constexpr int Method_ARRAYSIZE =
    SamplingMode_Method_Method_ARRAYSIZE;
  template<typename T>
  static inline const std::string& Method_Name(T enum_t_value) {
    static_assert(::std::is_same<T, Method>::value ||
      ::std::is_integral<T>::value,
      "Incorrect type passed to function Method_Name.");
    return SamplingMode_Method_Name(enum_t_value);
  }
  static inline bool Method_Parse(::PROTOBUF_NAMESPACE_ID::ConstStringParam name,
      Method* value) {
    return SamplingMode_Method_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  enum : int {
    kSamplingMethodFieldNumber = 1,
  };
  // .CoreML.Specification.SamplingMode.Method samplingMethod = 1;
  void clear_samplingmethod();
  ::CoreML::Specification::SamplingMode_Method samplingmethod() const;
  void set_samplingmethod(::CoreML::Specification::SamplingMode_Method value);
  private:
  ::CoreML::Specification::SamplingMode_Method _internal_samplingmethod() const;
  void _internal_set_samplingmethod(::CoreML::Specification::SamplingMode_Method value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SamplingMode)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int samplingmethod_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class BoxCoordinatesMode final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BoxCoordinatesMode) */ {
 public:
  inline BoxCoordinatesMode() : BoxCoordinatesMode(nullptr) {}
  ~BoxCoordinatesMode() override;
  explicit constexpr BoxCoordinatesMode(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  BoxCoordinatesMode(const BoxCoordinatesMode& from);
  BoxCoordinatesMode(BoxCoordinatesMode&& from) noexcept
    : BoxCoordinatesMode() {
    *this = ::std::move(from);
  }

  inline BoxCoordinatesMode& operator=(const BoxCoordinatesMode& from) {
    CopyFrom(from);
    return *this;
  }
  inline BoxCoordinatesMode& operator=(BoxCoordinatesMode&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const BoxCoordinatesMode& default_instance() {
    return *internal_default_instance();
  }
  static inline const BoxCoordinatesMode* internal_default_instance() {
    return reinterpret_cast<const BoxCoordinatesMode*>(
               &_BoxCoordinatesMode_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    40;

  friend void swap(BoxCoordinatesMode& a, BoxCoordinatesMode& b) {
    a.Swap(&b);
  }
  inline void Swap(BoxCoordinatesMode* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(BoxCoordinatesMode* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  BoxCoordinatesMode* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<BoxCoordinatesMode>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const BoxCoordinatesMode& from);
  void MergeFrom(const BoxCoordinatesMode& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BoxCoordinatesMode* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.BoxCoordinatesMode";
  }
  protected:
  explicit BoxCoordinatesMode(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  typedef BoxCoordinatesMode_Coordinates Coordinates;
  static constexpr Coordinates CORNERS_HEIGHT_FIRST =
    BoxCoordinatesMode_Coordinates_CORNERS_HEIGHT_FIRST;
  static constexpr Coordinates CORNERS_WIDTH_FIRST =
    BoxCoordinatesMode_Coordinates_CORNERS_WIDTH_FIRST;
  static constexpr Coordinates CENTER_SIZE_HEIGHT_FIRST =
    BoxCoordinatesMode_Coordinates_CENTER_SIZE_HEIGHT_FIRST;
  static constexpr Coordinates CENTER_SIZE_WIDTH_FIRST =
    BoxCoordinatesMode_Coordinates_CENTER_SIZE_WIDTH_FIRST;
  static inline bool Coordinates_IsValid(int value) {
    return BoxCoordinatesMode_Coordinates_IsValid(value);
  }
  static constexpr Coordinates Coordinates_MIN =
    BoxCoordinatesMode_Coordinates_Coordinates_MIN;
  static constexpr Coordinates Coordinates_MAX =
    BoxCoordinatesMode_Coordinates_Coordinates_MAX;
  static constexpr int Coordinates_ARRAYSIZE =
    BoxCoordinatesMode_Coordinates_Coordinates_ARRAYSIZE;
  template<typename T>
  static inline const std::string& Coordinates_Name(T enum_t_value) {
    static_assert(::std::is_same<T, Coordinates>::value ||
      ::std::is_integral<T>::value,
      "Incorrect type passed to function Coordinates_Name.");
    return BoxCoordinatesMode_Coordinates_Name(enum_t_value);
  }
  static inline bool Coordinates_Parse(::PROTOBUF_NAMESPACE_ID::ConstStringParam name,
      Coordinates* value) {
    return BoxCoordinatesMode_Coordinates_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  enum : int {
    kBoxModeFieldNumber = 1,
  };
  // .CoreML.Specification.BoxCoordinatesMode.Coordinates boxMode = 1;
  void clear_boxmode();
  ::CoreML::Specification::BoxCoordinatesMode_Coordinates boxmode() const;
  void set_boxmode(::CoreML::Specification::BoxCoordinatesMode_Coordinates value);
  private:
  ::CoreML::Specification::BoxCoordinatesMode_Coordinates _internal_boxmode() const;
  void _internal_set_boxmode(::CoreML::Specification::BoxCoordinatesMode_Coordinates value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BoxCoordinatesMode)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int boxmode_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class WeightParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.WeightParams) */ {
 public:
  inline WeightParams() : WeightParams(nullptr) {}
  ~WeightParams() override;
  explicit constexpr WeightParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  WeightParams(const WeightParams& from);
  WeightParams(WeightParams&& from) noexcept
    : WeightParams() {
    *this = ::std::move(from);
  }

  inline WeightParams& operator=(const WeightParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline WeightParams& operator=(WeightParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const WeightParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const WeightParams* internal_default_instance() {
    return reinterpret_cast<const WeightParams*>(
               &_WeightParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    41;

  friend void swap(WeightParams& a, WeightParams& b) {
    a.Swap(&b);
  }
  inline void Swap(WeightParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(WeightParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  WeightParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<WeightParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const WeightParams& from);
  void MergeFrom(const WeightParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(WeightParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.WeightParams";
  }
  protected:
  explicit WeightParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kFloatValueFieldNumber = 1,
    kFloat16ValueFieldNumber = 2,
    kRawValueFieldNumber = 30,
    kInt8RawValueFieldNumber = 31,
    kQuantizationFieldNumber = 40,
    kIsUpdatableFieldNumber = 50,
  };
  // repeated float floatValue = 1;
  int floatvalue_size() const;
  private:
  int _internal_floatvalue_size() const;
  public:
  void clear_floatvalue();
  private:
  float _internal_floatvalue(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
      _internal_floatvalue() const;
  void _internal_add_floatvalue(float value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
      _internal_mutable_floatvalue();
  public:
  float floatvalue(int index) const;
  void set_floatvalue(int index, float value);
  void add_floatvalue(float value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
      floatvalue() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
      mutable_floatvalue();

  // bytes float16Value = 2;
  void clear_float16value();
  const std::string& float16value() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_float16value(ArgT0&& arg0, ArgT... args);
  std::string* mutable_float16value();
  PROTOBUF_NODISCARD std::string* release_float16value();
  void set_allocated_float16value(std::string* float16value);
  private:
  const std::string& _internal_float16value() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_float16value(const std::string& value);
  std::string* _internal_mutable_float16value();
  public:

  // bytes rawValue = 30;
  void clear_rawvalue();
  const std::string& rawvalue() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_rawvalue(ArgT0&& arg0, ArgT... args);
  std::string* mutable_rawvalue();
  PROTOBUF_NODISCARD std::string* release_rawvalue();
  void set_allocated_rawvalue(std::string* rawvalue);
  private:
  const std::string& _internal_rawvalue() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_rawvalue(const std::string& value);
  std::string* _internal_mutable_rawvalue();
  public:

  // bytes int8RawValue = 31;
  void clear_int8rawvalue();
  const std::string& int8rawvalue() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_int8rawvalue(ArgT0&& arg0, ArgT... args);
  std::string* mutable_int8rawvalue();
  PROTOBUF_NODISCARD std::string* release_int8rawvalue();
  void set_allocated_int8rawvalue(std::string* int8rawvalue);
  private:
  const std::string& _internal_int8rawvalue() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_int8rawvalue(const std::string& value);
  std::string* _internal_mutable_int8rawvalue();
  public:

  // .CoreML.Specification.QuantizationParams quantization = 40;
  bool has_quantization() const;
  private:
  bool _internal_has_quantization() const;
  public:
  void clear_quantization();
  const ::CoreML::Specification::QuantizationParams& quantization() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::QuantizationParams* release_quantization();
  ::CoreML::Specification::QuantizationParams* mutable_quantization();
  void set_allocated_quantization(::CoreML::Specification::QuantizationParams* quantization);
  private:
  const ::CoreML::Specification::QuantizationParams& _internal_quantization() const;
  ::CoreML::Specification::QuantizationParams* _internal_mutable_quantization();
  public:
  void unsafe_arena_set_allocated_quantization(
      ::CoreML::Specification::QuantizationParams* quantization);
  ::CoreML::Specification::QuantizationParams* unsafe_arena_release_quantization();

  // bool isUpdatable = 50;
  void clear_isupdatable();
  bool isupdatable() const;
  void set_isupdatable(bool value);
  private:
  bool _internal_isupdatable() const;
  void _internal_set_isupdatable(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.WeightParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< float > floatvalue_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr float16value_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr rawvalue_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr int8rawvalue_;
  ::CoreML::Specification::QuantizationParams* quantization_;
  bool isupdatable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class QuantizationParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.QuantizationParams) */ {
 public:
  inline QuantizationParams() : QuantizationParams(nullptr) {}
  ~QuantizationParams() override;
  explicit constexpr QuantizationParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  QuantizationParams(const QuantizationParams& from);
  QuantizationParams(QuantizationParams&& from) noexcept
    : QuantizationParams() {
    *this = ::std::move(from);
  }

  inline QuantizationParams& operator=(const QuantizationParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline QuantizationParams& operator=(QuantizationParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const QuantizationParams& default_instance() {
    return *internal_default_instance();
  }
  enum QuantizationTypeCase {
    kLinearQuantization = 101,
    kLookupTableQuantization = 102,
    QUANTIZATIONTYPE_NOT_SET = 0,
  };

  static inline const QuantizationParams* internal_default_instance() {
    return reinterpret_cast<const QuantizationParams*>(
               &_QuantizationParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    42;

  friend void swap(QuantizationParams& a, QuantizationParams& b) {
    a.Swap(&b);
  }
  inline void Swap(QuantizationParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(QuantizationParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  QuantizationParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<QuantizationParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const QuantizationParams& from);
  void MergeFrom(const QuantizationParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(QuantizationParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.QuantizationParams";
  }
  protected:
  explicit QuantizationParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kNumberOfBitsFieldNumber = 1,
    kLinearQuantizationFieldNumber = 101,
    kLookupTableQuantizationFieldNumber = 102,
  };
  // uint64 numberOfBits = 1;
  void clear_numberofbits();
  uint64_t numberofbits() const;
  void set_numberofbits(uint64_t value);
  private:
  uint64_t _internal_numberofbits() const;
  void _internal_set_numberofbits(uint64_t value);
  public:

  // .CoreML.Specification.LinearQuantizationParams linearQuantization = 101;
  bool has_linearquantization() const;
  private:
  bool _internal_has_linearquantization() const;
  public:
  void clear_linearquantization();
  const ::CoreML::Specification::LinearQuantizationParams& linearquantization() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::LinearQuantizationParams* release_linearquantization();
  ::CoreML::Specification::LinearQuantizationParams* mutable_linearquantization();
  void set_allocated_linearquantization(::CoreML::Specification::LinearQuantizationParams* linearquantization);
  private:
  const ::CoreML::Specification::LinearQuantizationParams& _internal_linearquantization() const;
  ::CoreML::Specification::LinearQuantizationParams* _internal_mutable_linearquantization();
  public:
  void unsafe_arena_set_allocated_linearquantization(
      ::CoreML::Specification::LinearQuantizationParams* linearquantization);
  ::CoreML::Specification::LinearQuantizationParams* unsafe_arena_release_linearquantization();

  // .CoreML.Specification.LookUpTableQuantizationParams lookupTableQuantization = 102;
  bool has_lookuptablequantization() const;
  private:
  bool _internal_has_lookuptablequantization() const;
  public:
  void clear_lookuptablequantization();
  const ::CoreML::Specification::LookUpTableQuantizationParams& lookuptablequantization() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::LookUpTableQuantizationParams* release_lookuptablequantization();
  ::CoreML::Specification::LookUpTableQuantizationParams* mutable_lookuptablequantization();
  void set_allocated_lookuptablequantization(::CoreML::Specification::LookUpTableQuantizationParams* lookuptablequantization);
  private:
  const ::CoreML::Specification::LookUpTableQuantizationParams& _internal_lookuptablequantization() const;
  ::CoreML::Specification::LookUpTableQuantizationParams* _internal_mutable_lookuptablequantization();
  public:
  void unsafe_arena_set_allocated_lookuptablequantization(
      ::CoreML::Specification::LookUpTableQuantizationParams* lookuptablequantization);
  ::CoreML::Specification::LookUpTableQuantizationParams* unsafe_arena_release_lookuptablequantization();

  void clear_QuantizationType();
  QuantizationTypeCase QuantizationType_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.QuantizationParams)
 private:
  class _Internal;
  void set_has_linearquantization();
  void set_has_lookuptablequantization();

  inline bool has_QuantizationType() const;
  inline void clear_has_QuantizationType();

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  uint64_t numberofbits_;
  union QuantizationTypeUnion {
    constexpr QuantizationTypeUnion() : _constinit_{} {}
      ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized _constinit_;
    ::CoreML::Specification::LinearQuantizationParams* linearquantization_;
    ::CoreML::Specification::LookUpTableQuantizationParams* lookuptablequantization_;
  } QuantizationType_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  uint32_t _oneof_case_[1];

  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class LinearQuantizationParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LinearQuantizationParams) */ {
 public:
  inline LinearQuantizationParams() : LinearQuantizationParams(nullptr) {}
  ~LinearQuantizationParams() override;
  explicit constexpr LinearQuantizationParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  LinearQuantizationParams(const LinearQuantizationParams& from);
  LinearQuantizationParams(LinearQuantizationParams&& from) noexcept
    : LinearQuantizationParams() {
    *this = ::std::move(from);
  }

  inline LinearQuantizationParams& operator=(const LinearQuantizationParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline LinearQuantizationParams& operator=(LinearQuantizationParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const LinearQuantizationParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const LinearQuantizationParams* internal_default_instance() {
    return reinterpret_cast<const LinearQuantizationParams*>(
               &_LinearQuantizationParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    43;

  friend void swap(LinearQuantizationParams& a, LinearQuantizationParams& b) {
    a.Swap(&b);
  }
  inline void Swap(LinearQuantizationParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(LinearQuantizationParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  LinearQuantizationParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<LinearQuantizationParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const LinearQuantizationParams& from);
  void MergeFrom(const LinearQuantizationParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LinearQuantizationParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.LinearQuantizationParams";
  }
  protected:
  explicit LinearQuantizationParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kScaleFieldNumber = 1,
    kBiasFieldNumber = 2,
  };
  // repeated float scale = 1;
  int scale_size() const;
  private:
  int _internal_scale_size() const;
  public:
  void clear_scale();
  private:
  float _internal_scale(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
      _internal_scale() const;
  void _internal_add_scale(float value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
      _internal_mutable_scale();
  public:
  float scale(int index) const;
  void set_scale(int index, float value);
  void add_scale(float value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
      scale() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
      mutable_scale();

  // repeated float bias = 2;
  int bias_size() const;
  private:
  int _internal_bias_size() const;
  public:
  void clear_bias();
  private:
  float _internal_bias(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
      _internal_bias() const;
  void _internal_add_bias(float value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
      _internal_mutable_bias();
  public:
  float bias(int index) const;
  void set_bias(int index, float value);
  void add_bias(float value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
      bias() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
      mutable_bias();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LinearQuantizationParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< float > scale_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< float > bias_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class LookUpTableQuantizationParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LookUpTableQuantizationParams) */ {
 public:
  inline LookUpTableQuantizationParams() : LookUpTableQuantizationParams(nullptr) {}
  ~LookUpTableQuantizationParams() override;
  explicit constexpr LookUpTableQuantizationParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  LookUpTableQuantizationParams(const LookUpTableQuantizationParams& from);
  LookUpTableQuantizationParams(LookUpTableQuantizationParams&& from) noexcept
    : LookUpTableQuantizationParams() {
    *this = ::std::move(from);
  }

  inline LookUpTableQuantizationParams& operator=(const LookUpTableQuantizationParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline LookUpTableQuantizationParams& operator=(LookUpTableQuantizationParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const LookUpTableQuantizationParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const LookUpTableQuantizationParams* internal_default_instance() {
    return reinterpret_cast<const LookUpTableQuantizationParams*>(
               &_LookUpTableQuantizationParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    44;

  friend void swap(LookUpTableQuantizationParams& a, LookUpTableQuantizationParams& b) {
    a.Swap(&b);
  }
  inline void Swap(LookUpTableQuantizationParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(LookUpTableQuantizationParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  LookUpTableQuantizationParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<LookUpTableQuantizationParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const LookUpTableQuantizationParams& from);
  void MergeFrom(const LookUpTableQuantizationParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LookUpTableQuantizationParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.LookUpTableQuantizationParams";
  }
  protected:
  explicit LookUpTableQuantizationParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kFloatValueFieldNumber = 1,
  };
  // repeated float floatValue = 1;
  int floatvalue_size() const;
  private:
  int _internal_floatvalue_size() const;
  public:
  void clear_floatvalue();
  private:
  float _internal_floatvalue(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
      _internal_floatvalue() const;
  void _internal_add_floatvalue(float value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
      _internal_mutable_floatvalue();
  public:
  float floatvalue(int index) const;
  void set_floatvalue(int index, float value);
  void add_floatvalue(float value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
      floatvalue() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
      mutable_floatvalue();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LookUpTableQuantizationParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< float > floatvalue_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ConvolutionLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ConvolutionLayerParams) */ {
 public:
  inline ConvolutionLayerParams() : ConvolutionLayerParams(nullptr) {}
  ~ConvolutionLayerParams() override;
  explicit constexpr ConvolutionLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ConvolutionLayerParams(const ConvolutionLayerParams& from);
  ConvolutionLayerParams(ConvolutionLayerParams&& from) noexcept
    : ConvolutionLayerParams() {
    *this = ::std::move(from);
  }

  inline ConvolutionLayerParams& operator=(const ConvolutionLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ConvolutionLayerParams& operator=(ConvolutionLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ConvolutionLayerParams& default_instance() {
    return *internal_default_instance();
  }
  enum ConvolutionPaddingTypeCase {
    kValid = 50,
    kSame = 51,
    CONVOLUTIONPADDINGTYPE_NOT_SET = 0,
  };

  static inline const ConvolutionLayerParams* internal_default_instance() {
    return reinterpret_cast<const ConvolutionLayerParams*>(
               &_ConvolutionLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    45;

  friend void swap(ConvolutionLayerParams& a, ConvolutionLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ConvolutionLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ConvolutionLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ConvolutionLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ConvolutionLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ConvolutionLayerParams& from);
  void MergeFrom(const ConvolutionLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ConvolutionLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ConvolutionLayerParams";
  }
  protected:
  explicit ConvolutionLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kKernelSizeFieldNumber = 20,
    kStrideFieldNumber = 30,
    kDilationFactorFieldNumber = 40,
    kOutputShapeFieldNumber = 100,
    kWeightsFieldNumber = 90,
    kBiasFieldNumber = 91,
    kOutputChannelsFieldNumber = 1,
    kKernelChannelsFieldNumber = 2,
    kNGroupsFieldNumber = 10,
    kIsDeconvolutionFieldNumber = 60,
    kHasBiasFieldNumber = 70,
    kValidFieldNumber = 50,
    kSameFieldNumber = 51,
  };
  // repeated uint64 kernelSize = 20;
  int kernelsize_size() const;
  private:
  int _internal_kernelsize_size() const;
  public:
  void clear_kernelsize();
  private:
  uint64_t _internal_kernelsize(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_kernelsize() const;
  void _internal_add_kernelsize(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_kernelsize();
  public:
  uint64_t kernelsize(int index) const;
  void set_kernelsize(int index, uint64_t value);
  void add_kernelsize(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      kernelsize() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_kernelsize();

  // repeated uint64 stride = 30;
  int stride_size() const;
  private:
  int _internal_stride_size() const;
  public:
  void clear_stride();
  private:
  uint64_t _internal_stride(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_stride() const;
  void _internal_add_stride(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_stride();
  public:
  uint64_t stride(int index) const;
  void set_stride(int index, uint64_t value);
  void add_stride(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      stride() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_stride();

  // repeated uint64 dilationFactor = 40;
  int dilationfactor_size() const;
  private:
  int _internal_dilationfactor_size() const;
  public:
  void clear_dilationfactor();
  private:
  uint64_t _internal_dilationfactor(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_dilationfactor() const;
  void _internal_add_dilationfactor(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_dilationfactor();
  public:
  uint64_t dilationfactor(int index) const;
  void set_dilationfactor(int index, uint64_t value);
  void add_dilationfactor(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      dilationfactor() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_dilationfactor();

  // repeated uint64 outputShape = 100;
  int outputshape_size() const;
  private:
  int _internal_outputshape_size() const;
  public:
  void clear_outputshape();
  private:
  uint64_t _internal_outputshape(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_outputshape() const;
  void _internal_add_outputshape(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_outputshape();
  public:
  uint64_t outputshape(int index) const;
  void set_outputshape(int index, uint64_t value);
  void add_outputshape(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      outputshape() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_outputshape();

  // .CoreML.Specification.WeightParams weights = 90;
  bool has_weights() const;
  private:
  bool _internal_has_weights() const;
  public:
  void clear_weights();
  const ::CoreML::Specification::WeightParams& weights() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_weights();
  ::CoreML::Specification::WeightParams* mutable_weights();
  void set_allocated_weights(::CoreML::Specification::WeightParams* weights);
  private:
  const ::CoreML::Specification::WeightParams& _internal_weights() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_weights();
  public:
  void unsafe_arena_set_allocated_weights(
      ::CoreML::Specification::WeightParams* weights);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_weights();

  // .CoreML.Specification.WeightParams bias = 91;
  bool has_bias() const;
  private:
  bool _internal_has_bias() const;
  public:
  void clear_bias();
  const ::CoreML::Specification::WeightParams& bias() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_bias();
  ::CoreML::Specification::WeightParams* mutable_bias();
  void set_allocated_bias(::CoreML::Specification::WeightParams* bias);
  private:
  const ::CoreML::Specification::WeightParams& _internal_bias() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_bias();
  public:
  void unsafe_arena_set_allocated_bias(
      ::CoreML::Specification::WeightParams* bias);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_bias();

  // uint64 outputChannels = 1;
  void clear_outputchannels();
  uint64_t outputchannels() const;
  void set_outputchannels(uint64_t value);
  private:
  uint64_t _internal_outputchannels() const;
  void _internal_set_outputchannels(uint64_t value);
  public:

  // uint64 kernelChannels = 2;
  void clear_kernelchannels();
  uint64_t kernelchannels() const;
  void set_kernelchannels(uint64_t value);
  private:
  uint64_t _internal_kernelchannels() const;
  void _internal_set_kernelchannels(uint64_t value);
  public:

  // uint64 nGroups = 10;
  void clear_ngroups();
  uint64_t ngroups() const;
  void set_ngroups(uint64_t value);
  private:
  uint64_t _internal_ngroups() const;
  void _internal_set_ngroups(uint64_t value);
  public:

  // bool isDeconvolution = 60;
  void clear_isdeconvolution();
  bool isdeconvolution() const;
  void set_isdeconvolution(bool value);
  private:
  bool _internal_isdeconvolution() const;
  void _internal_set_isdeconvolution(bool value);
  public:

  // bool hasBias = 70;
  void clear_hasbias();
  bool hasbias() const;
  void set_hasbias(bool value);
  private:
  bool _internal_hasbias() const;
  void _internal_set_hasbias(bool value);
  public:

  // .CoreML.Specification.ValidPadding valid = 50;
  bool has_valid() const;
  private:
  bool _internal_has_valid() const;
  public:
  void clear_valid();
  const ::CoreML::Specification::ValidPadding& valid() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ValidPadding* release_valid();
  ::CoreML::Specification::ValidPadding* mutable_valid();
  void set_allocated_valid(::CoreML::Specification::ValidPadding* valid);
  private:
  const ::CoreML::Specification::ValidPadding& _internal_valid() const;
  ::CoreML::Specification::ValidPadding* _internal_mutable_valid();
  public:
  void unsafe_arena_set_allocated_valid(
      ::CoreML::Specification::ValidPadding* valid);
  ::CoreML::Specification::ValidPadding* unsafe_arena_release_valid();

  // .CoreML.Specification.SamePadding same = 51;
  bool has_same() const;
  private:
  bool _internal_has_same() const;
  public:
  void clear_same();
  const ::CoreML::Specification::SamePadding& same() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::SamePadding* release_same();
  ::CoreML::Specification::SamePadding* mutable_same();
  void set_allocated_same(::CoreML::Specification::SamePadding* same);
  private:
  const ::CoreML::Specification::SamePadding& _internal_same() const;
  ::CoreML::Specification::SamePadding* _internal_mutable_same();
  public:
  void unsafe_arena_set_allocated_same(
      ::CoreML::Specification::SamePadding* same);
  ::CoreML::Specification::SamePadding* unsafe_arena_release_same();

  void clear_ConvolutionPaddingType();
  ConvolutionPaddingTypeCase ConvolutionPaddingType_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.ConvolutionLayerParams)
 private:
  class _Internal;
  void set_has_valid();
  void set_has_same();

  inline bool has_ConvolutionPaddingType() const;
  inline void clear_has_ConvolutionPaddingType();

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > kernelsize_;
  mutable std::atomic<int> _kernelsize_cached_byte_size_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > stride_;
  mutable std::atomic<int> _stride_cached_byte_size_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > dilationfactor_;
  mutable std::atomic<int> _dilationfactor_cached_byte_size_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > outputshape_;
  mutable std::atomic<int> _outputshape_cached_byte_size_;
  ::CoreML::Specification::WeightParams* weights_;
  ::CoreML::Specification::WeightParams* bias_;
  uint64_t outputchannels_;
  uint64_t kernelchannels_;
  uint64_t ngroups_;
  bool isdeconvolution_;
  bool hasbias_;
  union ConvolutionPaddingTypeUnion {
    constexpr ConvolutionPaddingTypeUnion() : _constinit_{} {}
      ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized _constinit_;
    ::CoreML::Specification::ValidPadding* valid_;
    ::CoreML::Specification::SamePadding* same_;
  } ConvolutionPaddingType_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  uint32_t _oneof_case_[1];

  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class Convolution3DLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.Convolution3DLayerParams) */ {
 public:
  inline Convolution3DLayerParams() : Convolution3DLayerParams(nullptr) {}
  ~Convolution3DLayerParams() override;
  explicit constexpr Convolution3DLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  Convolution3DLayerParams(const Convolution3DLayerParams& from);
  Convolution3DLayerParams(Convolution3DLayerParams&& from) noexcept
    : Convolution3DLayerParams() {
    *this = ::std::move(from);
  }

  inline Convolution3DLayerParams& operator=(const Convolution3DLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline Convolution3DLayerParams& operator=(Convolution3DLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const Convolution3DLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const Convolution3DLayerParams* internal_default_instance() {
    return reinterpret_cast<const Convolution3DLayerParams*>(
               &_Convolution3DLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    46;

  friend void swap(Convolution3DLayerParams& a, Convolution3DLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(Convolution3DLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(Convolution3DLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  Convolution3DLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<Convolution3DLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const Convolution3DLayerParams& from);
  void MergeFrom(const Convolution3DLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(Convolution3DLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.Convolution3DLayerParams";
  }
  protected:
  explicit Convolution3DLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  typedef Convolution3DLayerParams_PaddingType PaddingType;
  static constexpr PaddingType CUSTOM =
    Convolution3DLayerParams_PaddingType_CUSTOM;
  static constexpr PaddingType VALID =
    Convolution3DLayerParams_PaddingType_VALID;
  static constexpr PaddingType SAME =
    Convolution3DLayerParams_PaddingType_SAME;
  static inline bool PaddingType_IsValid(int value) {
    return Convolution3DLayerParams_PaddingType_IsValid(value);
  }
  static constexpr PaddingType PaddingType_MIN =
    Convolution3DLayerParams_PaddingType_PaddingType_MIN;
  static constexpr PaddingType PaddingType_MAX =
    Convolution3DLayerParams_PaddingType_PaddingType_MAX;
  static constexpr int PaddingType_ARRAYSIZE =
    Convolution3DLayerParams_PaddingType_PaddingType_ARRAYSIZE;
  template<typename T>
  static inline const std::string& PaddingType_Name(T enum_t_value) {
    static_assert(::std::is_same<T, PaddingType>::value ||
      ::std::is_integral<T>::value,
      "Incorrect type passed to function PaddingType_Name.");
    return Convolution3DLayerParams_PaddingType_Name(enum_t_value);
  }
  static inline bool PaddingType_Parse(::PROTOBUF_NAMESPACE_ID::ConstStringParam name,
      PaddingType* value) {
    return Convolution3DLayerParams_PaddingType_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  enum : int {
    kOutputShapeFieldNumber = 87,
    kWeightsFieldNumber = 60,
    kBiasFieldNumber = 61,
    kOutputChannelsFieldNumber = 1,
    kInputChannelsFieldNumber = 2,
    kNGroupsFieldNumber = 10,
    kKernelDepthFieldNumber = 20,
    kKernelHeightFieldNumber = 21,
    kKernelWidthFieldNumber = 22,
    kStrideDepthFieldNumber = 31,
    kStrideHeightFieldNumber = 32,
    kStrideWidthFieldNumber = 33,
    kDilationDepthFieldNumber = 40,
    kDilationHeightFieldNumber = 41,
    kDilationWidthFieldNumber = 42,
    kHasBiasFieldNumber = 50,
    kIsDeconvolutionFieldNumber = 86,
    kPaddingTypeFieldNumber = 70,
    kCustomPaddingFrontFieldNumber = 80,
    kCustomPaddingBackFieldNumber = 81,
    kCustomPaddingTopFieldNumber = 82,
    kCustomPaddingBottomFieldNumber = 83,
    kCustomPaddingLeftFieldNumber = 84,
    kCustomPaddingRightFieldNumber = 85,
  };
  // repeated uint64 outputShape = 87;
  int outputshape_size() const;
  private:
  int _internal_outputshape_size() const;
  public:
  void clear_outputshape();
  private:
  uint64_t _internal_outputshape(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_outputshape() const;
  void _internal_add_outputshape(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_outputshape();
  public:
  uint64_t outputshape(int index) const;
  void set_outputshape(int index, uint64_t value);
  void add_outputshape(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      outputshape() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_outputshape();

  // .CoreML.Specification.WeightParams weights = 60;
  bool has_weights() const;
  private:
  bool _internal_has_weights() const;
  public:
  void clear_weights();
  const ::CoreML::Specification::WeightParams& weights() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_weights();
  ::CoreML::Specification::WeightParams* mutable_weights();
  void set_allocated_weights(::CoreML::Specification::WeightParams* weights);
  private:
  const ::CoreML::Specification::WeightParams& _internal_weights() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_weights();
  public:
  void unsafe_arena_set_allocated_weights(
      ::CoreML::Specification::WeightParams* weights);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_weights();

  // .CoreML.Specification.WeightParams bias = 61;
  bool has_bias() const;
  private:
  bool _internal_has_bias() const;
  public:
  void clear_bias();
  const ::CoreML::Specification::WeightParams& bias() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_bias();
  ::CoreML::Specification::WeightParams* mutable_bias();
  void set_allocated_bias(::CoreML::Specification::WeightParams* bias);
  private:
  const ::CoreML::Specification::WeightParams& _internal_bias() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_bias();
  public:
  void unsafe_arena_set_allocated_bias(
      ::CoreML::Specification::WeightParams* bias);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_bias();

  // int32 outputChannels = 1;
  void clear_outputchannels();
  int32_t outputchannels() const;
  void set_outputchannels(int32_t value);
  private:
  int32_t _internal_outputchannels() const;
  void _internal_set_outputchannels(int32_t value);
  public:

  // int32 inputChannels = 2;
  void clear_inputchannels();
  int32_t inputchannels() const;
  void set_inputchannels(int32_t value);
  private:
  int32_t _internal_inputchannels() const;
  void _internal_set_inputchannels(int32_t value);
  public:

  // int32 nGroups = 10;
  void clear_ngroups();
  int32_t ngroups() const;
  void set_ngroups(int32_t value);
  private:
  int32_t _internal_ngroups() const;
  void _internal_set_ngroups(int32_t value);
  public:

  // int32 kernelDepth = 20;
  void clear_kerneldepth();
  int32_t kerneldepth() const;
  void set_kerneldepth(int32_t value);
  private:
  int32_t _internal_kerneldepth() const;
  void _internal_set_kerneldepth(int32_t value);
  public:

  // int32 kernelHeight = 21;
  void clear_kernelheight();
  int32_t kernelheight() const;
  void set_kernelheight(int32_t value);
  private:
  int32_t _internal_kernelheight() const;
  void _internal_set_kernelheight(int32_t value);
  public:

  // int32 kernelWidth = 22;
  void clear_kernelwidth();
  int32_t kernelwidth() const;
  void set_kernelwidth(int32_t value);
  private:
  int32_t _internal_kernelwidth() const;
  void _internal_set_kernelwidth(int32_t value);
  public:

  // int32 strideDepth = 31;
  void clear_stridedepth();
  int32_t stridedepth() const;
  void set_stridedepth(int32_t value);
  private:
  int32_t _internal_stridedepth() const;
  void _internal_set_stridedepth(int32_t value);
  public:

  // int32 strideHeight = 32;
  void clear_strideheight();
  int32_t strideheight() const;
  void set_strideheight(int32_t value);
  private:
  int32_t _internal_strideheight() const;
  void _internal_set_strideheight(int32_t value);
  public:

  // int32 strideWidth = 33;
  void clear_stridewidth();
  int32_t stridewidth() const;
  void set_stridewidth(int32_t value);
  private:
  int32_t _internal_stridewidth() const;
  void _internal_set_stridewidth(int32_t value);
  public:

  // int32 dilationDepth = 40;
  void clear_dilationdepth();
  int32_t dilationdepth() const;
  void set_dilationdepth(int32_t value);
  private:
  int32_t _internal_dilationdepth() const;
  void _internal_set_dilationdepth(int32_t value);
  public:

  // int32 dilationHeight = 41;
  void clear_dilationheight();
  int32_t dilationheight() const;
  void set_dilationheight(int32_t value);
  private:
  int32_t _internal_dilationheight() const;
  void _internal_set_dilationheight(int32_t value);
  public:

  // int32 dilationWidth = 42;
  void clear_dilationwidth();
  int32_t dilationwidth() const;
  void set_dilationwidth(int32_t value);
  private:
  int32_t _internal_dilationwidth() const;
  void _internal_set_dilationwidth(int32_t value);
  public:

  // bool hasBias = 50;
  void clear_hasbias();
  bool hasbias() const;
  void set_hasbias(bool value);
  private:
  bool _internal_hasbias() const;
  void _internal_set_hasbias(bool value);
  public:

  // bool isDeconvolution = 86;
  void clear_isdeconvolution();
  bool isdeconvolution() const;
  void set_isdeconvolution(bool value);
  private:
  bool _internal_isdeconvolution() const;
  void _internal_set_isdeconvolution(bool value);
  public:

  // .CoreML.Specification.Convolution3DLayerParams.PaddingType paddingType = 70;
  void clear_paddingtype();
  ::CoreML::Specification::Convolution3DLayerParams_PaddingType paddingtype() const;
  void set_paddingtype(::CoreML::Specification::Convolution3DLayerParams_PaddingType value);
  private:
  ::CoreML::Specification::Convolution3DLayerParams_PaddingType _internal_paddingtype() const;
  void _internal_set_paddingtype(::CoreML::Specification::Convolution3DLayerParams_PaddingType value);
  public:

  // int32 customPaddingFront = 80;
  void clear_custompaddingfront();
  int32_t custompaddingfront() const;
  void set_custompaddingfront(int32_t value);
  private:
  int32_t _internal_custompaddingfront() const;
  void _internal_set_custompaddingfront(int32_t value);
  public:

  // int32 customPaddingBack = 81;
  void clear_custompaddingback();
  int32_t custompaddingback() const;
  void set_custompaddingback(int32_t value);
  private:
  int32_t _internal_custompaddingback() const;
  void _internal_set_custompaddingback(int32_t value);
  public:

  // int32 customPaddingTop = 82;
  void clear_custompaddingtop();
  int32_t custompaddingtop() const;
  void set_custompaddingtop(int32_t value);
  private:
  int32_t _internal_custompaddingtop() const;
  void _internal_set_custompaddingtop(int32_t value);
  public:

  // int32 customPaddingBottom = 83;
  void clear_custompaddingbottom();
  int32_t custompaddingbottom() const;
  void set_custompaddingbottom(int32_t value);
  private:
  int32_t _internal_custompaddingbottom() const;
  void _internal_set_custompaddingbottom(int32_t value);
  public:

  // int32 customPaddingLeft = 84;
  void clear_custompaddingleft();
  int32_t custompaddingleft() const;
  void set_custompaddingleft(int32_t value);
  private:
  int32_t _internal_custompaddingleft() const;
  void _internal_set_custompaddingleft(int32_t value);
  public:

  // int32 customPaddingRight = 85;
  void clear_custompaddingright();
  int32_t custompaddingright() const;
  void set_custompaddingright(int32_t value);
  private:
  int32_t _internal_custompaddingright() const;
  void _internal_set_custompaddingright(int32_t value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.Convolution3DLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > outputshape_;
  mutable std::atomic<int> _outputshape_cached_byte_size_;
  ::CoreML::Specification::WeightParams* weights_;
  ::CoreML::Specification::WeightParams* bias_;
  int32_t outputchannels_;
  int32_t inputchannels_;
  int32_t ngroups_;
  int32_t kerneldepth_;
  int32_t kernelheight_;
  int32_t kernelwidth_;
  int32_t stridedepth_;
  int32_t strideheight_;
  int32_t stridewidth_;
  int32_t dilationdepth_;
  int32_t dilationheight_;
  int32_t dilationwidth_;
  bool hasbias_;
  bool isdeconvolution_;
  int paddingtype_;
  int32_t custompaddingfront_;
  int32_t custompaddingback_;
  int32_t custompaddingtop_;
  int32_t custompaddingbottom_;
  int32_t custompaddingleft_;
  int32_t custompaddingright_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class InnerProductLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.InnerProductLayerParams) */ {
 public:
  inline InnerProductLayerParams() : InnerProductLayerParams(nullptr) {}
  ~InnerProductLayerParams() override;
  explicit constexpr InnerProductLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  InnerProductLayerParams(const InnerProductLayerParams& from);
  InnerProductLayerParams(InnerProductLayerParams&& from) noexcept
    : InnerProductLayerParams() {
    *this = ::std::move(from);
  }

  inline InnerProductLayerParams& operator=(const InnerProductLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline InnerProductLayerParams& operator=(InnerProductLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const InnerProductLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const InnerProductLayerParams* internal_default_instance() {
    return reinterpret_cast<const InnerProductLayerParams*>(
               &_InnerProductLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    47;

  friend void swap(InnerProductLayerParams& a, InnerProductLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(InnerProductLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(InnerProductLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  InnerProductLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<InnerProductLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const InnerProductLayerParams& from);
  void MergeFrom(const InnerProductLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(InnerProductLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.InnerProductLayerParams";
  }
  protected:
  explicit InnerProductLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kWeightsFieldNumber = 20,
    kBiasFieldNumber = 21,
    kInputChannelsFieldNumber = 1,
    kOutputChannelsFieldNumber = 2,
    kHasBiasFieldNumber = 10,
    kInt8DynamicQuantizeFieldNumber = 22,
  };
  // .CoreML.Specification.WeightParams weights = 20;
  bool has_weights() const;
  private:
  bool _internal_has_weights() const;
  public:
  void clear_weights();
  const ::CoreML::Specification::WeightParams& weights() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_weights();
  ::CoreML::Specification::WeightParams* mutable_weights();
  void set_allocated_weights(::CoreML::Specification::WeightParams* weights);
  private:
  const ::CoreML::Specification::WeightParams& _internal_weights() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_weights();
  public:
  void unsafe_arena_set_allocated_weights(
      ::CoreML::Specification::WeightParams* weights);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_weights();

  // .CoreML.Specification.WeightParams bias = 21;
  bool has_bias() const;
  private:
  bool _internal_has_bias() const;
  public:
  void clear_bias();
  const ::CoreML::Specification::WeightParams& bias() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_bias();
  ::CoreML::Specification::WeightParams* mutable_bias();
  void set_allocated_bias(::CoreML::Specification::WeightParams* bias);
  private:
  const ::CoreML::Specification::WeightParams& _internal_bias() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_bias();
  public:
  void unsafe_arena_set_allocated_bias(
      ::CoreML::Specification::WeightParams* bias);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_bias();

  // uint64 inputChannels = 1;
  void clear_inputchannels();
  uint64_t inputchannels() const;
  void set_inputchannels(uint64_t value);
  private:
  uint64_t _internal_inputchannels() const;
  void _internal_set_inputchannels(uint64_t value);
  public:

  // uint64 outputChannels = 2;
  void clear_outputchannels();
  uint64_t outputchannels() const;
  void set_outputchannels(uint64_t value);
  private:
  uint64_t _internal_outputchannels() const;
  void _internal_set_outputchannels(uint64_t value);
  public:

  // bool hasBias = 10;
  void clear_hasbias();
  bool hasbias() const;
  void set_hasbias(bool value);
  private:
  bool _internal_hasbias() const;
  void _internal_set_hasbias(bool value);
  public:

  // bool int8DynamicQuantize = 22;
  void clear_int8dynamicquantize();
  bool int8dynamicquantize() const;
  void set_int8dynamicquantize(bool value);
  private:
  bool _internal_int8dynamicquantize() const;
  void _internal_set_int8dynamicquantize(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.InnerProductLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::CoreML::Specification::WeightParams* weights_;
  ::CoreML::Specification::WeightParams* bias_;
  uint64_t inputchannels_;
  uint64_t outputchannels_;
  bool hasbias_;
  bool int8dynamicquantize_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class EmbeddingLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.EmbeddingLayerParams) */ {
 public:
  inline EmbeddingLayerParams() : EmbeddingLayerParams(nullptr) {}
  ~EmbeddingLayerParams() override;
  explicit constexpr EmbeddingLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  EmbeddingLayerParams(const EmbeddingLayerParams& from);
  EmbeddingLayerParams(EmbeddingLayerParams&& from) noexcept
    : EmbeddingLayerParams() {
    *this = ::std::move(from);
  }

  inline EmbeddingLayerParams& operator=(const EmbeddingLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline EmbeddingLayerParams& operator=(EmbeddingLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const EmbeddingLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const EmbeddingLayerParams* internal_default_instance() {
    return reinterpret_cast<const EmbeddingLayerParams*>(
               &_EmbeddingLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    48;

  friend void swap(EmbeddingLayerParams& a, EmbeddingLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(EmbeddingLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(EmbeddingLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  EmbeddingLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<EmbeddingLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const EmbeddingLayerParams& from);
  void MergeFrom(const EmbeddingLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(EmbeddingLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.EmbeddingLayerParams";
  }
  protected:
  explicit EmbeddingLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kWeightsFieldNumber = 20,
    kBiasFieldNumber = 21,
    kInputDimFieldNumber = 1,
    kOutputChannelsFieldNumber = 2,
    kHasBiasFieldNumber = 10,
  };
  // .CoreML.Specification.WeightParams weights = 20;
  bool has_weights() const;
  private:
  bool _internal_has_weights() const;
  public:
  void clear_weights();
  const ::CoreML::Specification::WeightParams& weights() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_weights();
  ::CoreML::Specification::WeightParams* mutable_weights();
  void set_allocated_weights(::CoreML::Specification::WeightParams* weights);
  private:
  const ::CoreML::Specification::WeightParams& _internal_weights() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_weights();
  public:
  void unsafe_arena_set_allocated_weights(
      ::CoreML::Specification::WeightParams* weights);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_weights();

  // .CoreML.Specification.WeightParams bias = 21;
  bool has_bias() const;
  private:
  bool _internal_has_bias() const;
  public:
  void clear_bias();
  const ::CoreML::Specification::WeightParams& bias() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_bias();
  ::CoreML::Specification::WeightParams* mutable_bias();
  void set_allocated_bias(::CoreML::Specification::WeightParams* bias);
  private:
  const ::CoreML::Specification::WeightParams& _internal_bias() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_bias();
  public:
  void unsafe_arena_set_allocated_bias(
      ::CoreML::Specification::WeightParams* bias);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_bias();

  // uint64 inputDim = 1;
  void clear_inputdim();
  uint64_t inputdim() const;
  void set_inputdim(uint64_t value);
  private:
  uint64_t _internal_inputdim() const;
  void _internal_set_inputdim(uint64_t value);
  public:

  // uint64 outputChannels = 2;
  void clear_outputchannels();
  uint64_t outputchannels() const;
  void set_outputchannels(uint64_t value);
  private:
  uint64_t _internal_outputchannels() const;
  void _internal_set_outputchannels(uint64_t value);
  public:

  // bool hasBias = 10;
  void clear_hasbias();
  bool hasbias() const;
  void set_hasbias(bool value);
  private:
  bool _internal_hasbias() const;
  void _internal_set_hasbias(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.EmbeddingLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::CoreML::Specification::WeightParams* weights_;
  ::CoreML::Specification::WeightParams* bias_;
  uint64_t inputdim_;
  uint64_t outputchannels_;
  bool hasbias_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class EmbeddingNDLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.EmbeddingNDLayerParams) */ {
 public:
  inline EmbeddingNDLayerParams() : EmbeddingNDLayerParams(nullptr) {}
  ~EmbeddingNDLayerParams() override;
  explicit constexpr EmbeddingNDLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  EmbeddingNDLayerParams(const EmbeddingNDLayerParams& from);
  EmbeddingNDLayerParams(EmbeddingNDLayerParams&& from) noexcept
    : EmbeddingNDLayerParams() {
    *this = ::std::move(from);
  }

  inline EmbeddingNDLayerParams& operator=(const EmbeddingNDLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline EmbeddingNDLayerParams& operator=(EmbeddingNDLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const EmbeddingNDLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const EmbeddingNDLayerParams* internal_default_instance() {
    return reinterpret_cast<const EmbeddingNDLayerParams*>(
               &_EmbeddingNDLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    49;

  friend void swap(EmbeddingNDLayerParams& a, EmbeddingNDLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(EmbeddingNDLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(EmbeddingNDLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  EmbeddingNDLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<EmbeddingNDLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const EmbeddingNDLayerParams& from);
  void MergeFrom(const EmbeddingNDLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(EmbeddingNDLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.EmbeddingNDLayerParams";
  }
  protected:
  explicit EmbeddingNDLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kWeightsFieldNumber = 20,
    kBiasFieldNumber = 21,
    kVocabSizeFieldNumber = 1,
    kEmbeddingSizeFieldNumber = 2,
    kHasBiasFieldNumber = 3,
  };
  // .CoreML.Specification.WeightParams weights = 20;
  bool has_weights() const;
  private:
  bool _internal_has_weights() const;
  public:
  void clear_weights();
  const ::CoreML::Specification::WeightParams& weights() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_weights();
  ::CoreML::Specification::WeightParams* mutable_weights();
  void set_allocated_weights(::CoreML::Specification::WeightParams* weights);
  private:
  const ::CoreML::Specification::WeightParams& _internal_weights() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_weights();
  public:
  void unsafe_arena_set_allocated_weights(
      ::CoreML::Specification::WeightParams* weights);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_weights();

  // .CoreML.Specification.WeightParams bias = 21;
  bool has_bias() const;
  private:
  bool _internal_has_bias() const;
  public:
  void clear_bias();
  const ::CoreML::Specification::WeightParams& bias() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_bias();
  ::CoreML::Specification::WeightParams* mutable_bias();
  void set_allocated_bias(::CoreML::Specification::WeightParams* bias);
  private:
  const ::CoreML::Specification::WeightParams& _internal_bias() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_bias();
  public:
  void unsafe_arena_set_allocated_bias(
      ::CoreML::Specification::WeightParams* bias);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_bias();

  // uint64 vocabSize = 1;
  void clear_vocabsize();
  uint64_t vocabsize() const;
  void set_vocabsize(uint64_t value);
  private:
  uint64_t _internal_vocabsize() const;
  void _internal_set_vocabsize(uint64_t value);
  public:

  // uint64 embeddingSize = 2;
  void clear_embeddingsize();
  uint64_t embeddingsize() const;
  void set_embeddingsize(uint64_t value);
  private:
  uint64_t _internal_embeddingsize() const;
  void _internal_set_embeddingsize(uint64_t value);
  public:

  // bool hasBias = 3;
  void clear_hasbias();
  bool hasbias() const;
  void set_hasbias(bool value);
  private:
  bool _internal_hasbias() const;
  void _internal_set_hasbias(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.EmbeddingNDLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::CoreML::Specification::WeightParams* weights_;
  ::CoreML::Specification::WeightParams* bias_;
  uint64_t vocabsize_;
  uint64_t embeddingsize_;
  bool hasbias_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class BatchnormLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BatchnormLayerParams) */ {
 public:
  inline BatchnormLayerParams() : BatchnormLayerParams(nullptr) {}
  ~BatchnormLayerParams() override;
  explicit constexpr BatchnormLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  BatchnormLayerParams(const BatchnormLayerParams& from);
  BatchnormLayerParams(BatchnormLayerParams&& from) noexcept
    : BatchnormLayerParams() {
    *this = ::std::move(from);
  }

  inline BatchnormLayerParams& operator=(const BatchnormLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline BatchnormLayerParams& operator=(BatchnormLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const BatchnormLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const BatchnormLayerParams* internal_default_instance() {
    return reinterpret_cast<const BatchnormLayerParams*>(
               &_BatchnormLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    50;

  friend void swap(BatchnormLayerParams& a, BatchnormLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(BatchnormLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(BatchnormLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  BatchnormLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<BatchnormLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const BatchnormLayerParams& from);
  void MergeFrom(const BatchnormLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BatchnormLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.BatchnormLayerParams";
  }
  protected:
  explicit BatchnormLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kGammaFieldNumber = 15,
    kBetaFieldNumber = 16,
    kMeanFieldNumber = 17,
    kVarianceFieldNumber = 18,
    kChannelsFieldNumber = 1,
    kComputeMeanVarFieldNumber = 5,
    kInstanceNormalizationFieldNumber = 6,
    kEpsilonFieldNumber = 10,
  };
  // .CoreML.Specification.WeightParams gamma = 15;
  bool has_gamma() const;
  private:
  bool _internal_has_gamma() const;
  public:
  void clear_gamma();
  const ::CoreML::Specification::WeightParams& gamma() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_gamma();
  ::CoreML::Specification::WeightParams* mutable_gamma();
  void set_allocated_gamma(::CoreML::Specification::WeightParams* gamma);
  private:
  const ::CoreML::Specification::WeightParams& _internal_gamma() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_gamma();
  public:
  void unsafe_arena_set_allocated_gamma(
      ::CoreML::Specification::WeightParams* gamma);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_gamma();

  // .CoreML.Specification.WeightParams beta = 16;
  bool has_beta() const;
  private:
  bool _internal_has_beta() const;
  public:
  void clear_beta();
  const ::CoreML::Specification::WeightParams& beta() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_beta();
  ::CoreML::Specification::WeightParams* mutable_beta();
  void set_allocated_beta(::CoreML::Specification::WeightParams* beta);
  private:
  const ::CoreML::Specification::WeightParams& _internal_beta() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_beta();
  public:
  void unsafe_arena_set_allocated_beta(
      ::CoreML::Specification::WeightParams* beta);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_beta();

  // .CoreML.Specification.WeightParams mean = 17;
  bool has_mean() const;
  private:
  bool _internal_has_mean() const;
  public:
  void clear_mean();
  const ::CoreML::Specification::WeightParams& mean() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_mean();
  ::CoreML::Specification::WeightParams* mutable_mean();
  void set_allocated_mean(::CoreML::Specification::WeightParams* mean);
  private:
  const ::CoreML::Specification::WeightParams& _internal_mean() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_mean();
  public:
  void unsafe_arena_set_allocated_mean(
      ::CoreML::Specification::WeightParams* mean);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_mean();

  // .CoreML.Specification.WeightParams variance = 18;
  bool has_variance() const;
  private:
  bool _internal_has_variance() const;
  public:
  void clear_variance();
  const ::CoreML::Specification::WeightParams& variance() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_variance();
  ::CoreML::Specification::WeightParams* mutable_variance();
  void set_allocated_variance(::CoreML::Specification::WeightParams* variance);
  private:
  const ::CoreML::Specification::WeightParams& _internal_variance() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_variance();
  public:
  void unsafe_arena_set_allocated_variance(
      ::CoreML::Specification::WeightParams* variance);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_variance();

  // uint64 channels = 1;
  void clear_channels();
  uint64_t channels() const;
  void set_channels(uint64_t value);
  private:
  uint64_t _internal_channels() const;
  void _internal_set_channels(uint64_t value);
  public:

  // bool computeMeanVar = 5;
  void clear_computemeanvar();
  bool computemeanvar() const;
  void set_computemeanvar(bool value);
  private:
  bool _internal_computemeanvar() const;
  void _internal_set_computemeanvar(bool value);
  public:

  // bool instanceNormalization = 6;
  void clear_instancenormalization();
  bool instancenormalization() const;
  void set_instancenormalization(bool value);
  private:
  bool _internal_instancenormalization() const;
  void _internal_set_instancenormalization(bool value);
  public:

  // float epsilon = 10;
  void clear_epsilon();
  float epsilon() const;
  void set_epsilon(float value);
  private:
  float _internal_epsilon() const;
  void _internal_set_epsilon(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BatchnormLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::CoreML::Specification::WeightParams* gamma_;
  ::CoreML::Specification::WeightParams* beta_;
  ::CoreML::Specification::WeightParams* mean_;
  ::CoreML::Specification::WeightParams* variance_;
  uint64_t channels_;
  bool computemeanvar_;
  bool instancenormalization_;
  float epsilon_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class PoolingLayerParams_ValidCompletePadding final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PoolingLayerParams.ValidCompletePadding) */ {
 public:
  inline PoolingLayerParams_ValidCompletePadding() : PoolingLayerParams_ValidCompletePadding(nullptr) {}
  ~PoolingLayerParams_ValidCompletePadding() override;
  explicit constexpr PoolingLayerParams_ValidCompletePadding(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  PoolingLayerParams_ValidCompletePadding(const PoolingLayerParams_ValidCompletePadding& from);
  PoolingLayerParams_ValidCompletePadding(PoolingLayerParams_ValidCompletePadding&& from) noexcept
    : PoolingLayerParams_ValidCompletePadding() {
    *this = ::std::move(from);
  }

  inline PoolingLayerParams_ValidCompletePadding& operator=(const PoolingLayerParams_ValidCompletePadding& from) {
    CopyFrom(from);
    return *this;
  }
  inline PoolingLayerParams_ValidCompletePadding& operator=(PoolingLayerParams_ValidCompletePadding&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const PoolingLayerParams_ValidCompletePadding& default_instance() {
    return *internal_default_instance();
  }
  static inline const PoolingLayerParams_ValidCompletePadding* internal_default_instance() {
    return reinterpret_cast<const PoolingLayerParams_ValidCompletePadding*>(
               &_PoolingLayerParams_ValidCompletePadding_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    51;

  friend void swap(PoolingLayerParams_ValidCompletePadding& a, PoolingLayerParams_ValidCompletePadding& b) {
    a.Swap(&b);
  }
  inline void Swap(PoolingLayerParams_ValidCompletePadding* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(PoolingLayerParams_ValidCompletePadding* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  PoolingLayerParams_ValidCompletePadding* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<PoolingLayerParams_ValidCompletePadding>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const PoolingLayerParams_ValidCompletePadding& from);
  void MergeFrom(const PoolingLayerParams_ValidCompletePadding& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PoolingLayerParams_ValidCompletePadding* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.PoolingLayerParams.ValidCompletePadding";
  }
  protected:
  explicit PoolingLayerParams_ValidCompletePadding(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kPaddingAmountsFieldNumber = 10,
  };
  // repeated uint64 paddingAmounts = 10;
  int paddingamounts_size() const;
  private:
  int _internal_paddingamounts_size() const;
  public:
  void clear_paddingamounts();
  private:
  uint64_t _internal_paddingamounts(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_paddingamounts() const;
  void _internal_add_paddingamounts(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_paddingamounts();
  public:
  uint64_t paddingamounts(int index) const;
  void set_paddingamounts(int index, uint64_t value);
  void add_paddingamounts(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      paddingamounts() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_paddingamounts();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > paddingamounts_;
  mutable std::atomic<int> _paddingamounts_cached_byte_size_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class PoolingLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PoolingLayerParams) */ {
 public:
  inline PoolingLayerParams() : PoolingLayerParams(nullptr) {}
  ~PoolingLayerParams() override;
  explicit constexpr PoolingLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  PoolingLayerParams(const PoolingLayerParams& from);
  PoolingLayerParams(PoolingLayerParams&& from) noexcept
    : PoolingLayerParams() {
    *this = ::std::move(from);
  }

  inline PoolingLayerParams& operator=(const PoolingLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline PoolingLayerParams& operator=(PoolingLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const PoolingLayerParams& default_instance() {
    return *internal_default_instance();
  }
  enum PoolingPaddingTypeCase {
    kValid = 30,
    kSame = 31,
    kIncludeLastPixel = 32,
    POOLINGPADDINGTYPE_NOT_SET = 0,
  };

  static inline const PoolingLayerParams* internal_default_instance() {
    return reinterpret_cast<const PoolingLayerParams*>(
               &_PoolingLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    52;

  friend void swap(PoolingLayerParams& a, PoolingLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(PoolingLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(PoolingLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  PoolingLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<PoolingLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const PoolingLayerParams& from);
  void MergeFrom(const PoolingLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PoolingLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.PoolingLayerParams";
  }
  protected:
  explicit PoolingLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  typedef PoolingLayerParams_ValidCompletePadding ValidCompletePadding;

  typedef PoolingLayerParams_PoolingType PoolingType;
  static constexpr PoolingType MAX =
    PoolingLayerParams_PoolingType_MAX;
  static constexpr PoolingType AVERAGE =
    PoolingLayerParams_PoolingType_AVERAGE;
  static constexpr PoolingType L2 =
    PoolingLayerParams_PoolingType_L2;
  static inline bool PoolingType_IsValid(int value) {
    return PoolingLayerParams_PoolingType_IsValid(value);
  }
  static constexpr PoolingType PoolingType_MIN =
    PoolingLayerParams_PoolingType_PoolingType_MIN;
  static constexpr PoolingType PoolingType_MAX =
    PoolingLayerParams_PoolingType_PoolingType_MAX;
  static constexpr int PoolingType_ARRAYSIZE =
    PoolingLayerParams_PoolingType_PoolingType_ARRAYSIZE;
  template<typename T>
  static inline const std::string& PoolingType_Name(T enum_t_value) {
    static_assert(::std::is_same<T, PoolingType>::value ||
      ::std::is_integral<T>::value,
      "Incorrect type passed to function PoolingType_Name.");
    return PoolingLayerParams_PoolingType_Name(enum_t_value);
  }
  static inline bool PoolingType_Parse(::PROTOBUF_NAMESPACE_ID::ConstStringParam name,
      PoolingType* value) {
    return PoolingLayerParams_PoolingType_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  enum : int {
    kKernelSizeFieldNumber = 10,
    kStrideFieldNumber = 20,
    kTypeFieldNumber = 1,
    kAvgPoolExcludePaddingFieldNumber = 50,
    kGlobalPoolingFieldNumber = 60,
    kValidFieldNumber = 30,
    kSameFieldNumber = 31,
    kIncludeLastPixelFieldNumber = 32,
  };
  // repeated uint64 kernelSize = 10;
  int kernelsize_size() const;
  private:
  int _internal_kernelsize_size() const;
  public:
  void clear_kernelsize();
  private:
  uint64_t _internal_kernelsize(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_kernelsize() const;
  void _internal_add_kernelsize(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_kernelsize();
  public:
  uint64_t kernelsize(int index) const;
  void set_kernelsize(int index, uint64_t value);
  void add_kernelsize(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      kernelsize() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_kernelsize();

  // repeated uint64 stride = 20;
  int stride_size() const;
  private:
  int _internal_stride_size() const;
  public:
  void clear_stride();
  private:
  uint64_t _internal_stride(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_stride() const;
  void _internal_add_stride(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_stride();
  public:
  uint64_t stride(int index) const;
  void set_stride(int index, uint64_t value);
  void add_stride(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      stride() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_stride();

  // .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
  void clear_type();
  ::CoreML::Specification::PoolingLayerParams_PoolingType type() const;
  void set_type(::CoreML::Specification::PoolingLayerParams_PoolingType value);
  private:
  ::CoreML::Specification::PoolingLayerParams_PoolingType _internal_type() const;
  void _internal_set_type(::CoreML::Specification::PoolingLayerParams_PoolingType value);
  public:

  // bool avgPoolExcludePadding = 50;
  void clear_avgpoolexcludepadding();
  bool avgpoolexcludepadding() const;
  void set_avgpoolexcludepadding(bool value);
  private:
  bool _internal_avgpoolexcludepadding() const;
  void _internal_set_avgpoolexcludepadding(bool value);
  public:

  // bool globalPooling = 60;
  void clear_globalpooling();
  bool globalpooling() const;
  void set_globalpooling(bool value);
  private:
  bool _internal_globalpooling() const;
  void _internal_set_globalpooling(bool value);
  public:

  // .CoreML.Specification.ValidPadding valid = 30;
  bool has_valid() const;
  private:
  bool _internal_has_valid() const;
  public:
  void clear_valid();
  const ::CoreML::Specification::ValidPadding& valid() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ValidPadding* release_valid();
  ::CoreML::Specification::ValidPadding* mutable_valid();
  void set_allocated_valid(::CoreML::Specification::ValidPadding* valid);
  private:
  const ::CoreML::Specification::ValidPadding& _internal_valid() const;
  ::CoreML::Specification::ValidPadding* _internal_mutable_valid();
  public:
  void unsafe_arena_set_allocated_valid(
      ::CoreML::Specification::ValidPadding* valid);
  ::CoreML::Specification::ValidPadding* unsafe_arena_release_valid();

  // .CoreML.Specification.SamePadding same = 31;
  bool has_same() const;
  private:
  bool _internal_has_same() const;
  public:
  void clear_same();
  const ::CoreML::Specification::SamePadding& same() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::SamePadding* release_same();
  ::CoreML::Specification::SamePadding* mutable_same();
  void set_allocated_same(::CoreML::Specification::SamePadding* same);
  private:
  const ::CoreML::Specification::SamePadding& _internal_same() const;
  ::CoreML::Specification::SamePadding* _internal_mutable_same();
  public:
  void unsafe_arena_set_allocated_same(
      ::CoreML::Specification::SamePadding* same);
  ::CoreML::Specification::SamePadding* unsafe_arena_release_same();

  // .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
  bool has_includelastpixel() const;
  private:
  bool _internal_has_includelastpixel() const;
  public:
  void clear_includelastpixel();
  const ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding& includelastpixel() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* release_includelastpixel();
  ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* mutable_includelastpixel();
  void set_allocated_includelastpixel(::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* includelastpixel);
  private:
  const ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding& _internal_includelastpixel() const;
  ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* _internal_mutable_includelastpixel();
  public:
  void unsafe_arena_set_allocated_includelastpixel(
      ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* includelastpixel);
  ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* unsafe_arena_release_includelastpixel();

  void clear_PoolingPaddingType();
  PoolingPaddingTypeCase PoolingPaddingType_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.PoolingLayerParams)
 private:
  class _Internal;
  void set_has_valid();
  void set_has_same();
  void set_has_includelastpixel();

  inline bool has_PoolingPaddingType() const;
  inline void clear_has_PoolingPaddingType();

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > kernelsize_;
  mutable std::atomic<int> _kernelsize_cached_byte_size_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > stride_;
  mutable std::atomic<int> _stride_cached_byte_size_;
  int type_;
  bool avgpoolexcludepadding_;
  bool globalpooling_;
  union PoolingPaddingTypeUnion {
    constexpr PoolingPaddingTypeUnion() : _constinit_{} {}
      ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized _constinit_;
    ::CoreML::Specification::ValidPadding* valid_;
    ::CoreML::Specification::SamePadding* same_;
    ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* includelastpixel_;
  } PoolingPaddingType_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  uint32_t _oneof_case_[1];

  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class Pooling3DLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.Pooling3DLayerParams) */ {
 public:
  inline Pooling3DLayerParams() : Pooling3DLayerParams(nullptr) {}
  ~Pooling3DLayerParams() override;
  explicit constexpr Pooling3DLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  Pooling3DLayerParams(const Pooling3DLayerParams& from);
  Pooling3DLayerParams(Pooling3DLayerParams&& from) noexcept
    : Pooling3DLayerParams() {
    *this = ::std::move(from);
  }

  inline Pooling3DLayerParams& operator=(const Pooling3DLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline Pooling3DLayerParams& operator=(Pooling3DLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const Pooling3DLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const Pooling3DLayerParams* internal_default_instance() {
    return reinterpret_cast<const Pooling3DLayerParams*>(
               &_Pooling3DLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    53;

  friend void swap(Pooling3DLayerParams& a, Pooling3DLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(Pooling3DLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(Pooling3DLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  Pooling3DLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<Pooling3DLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const Pooling3DLayerParams& from);
  void MergeFrom(const Pooling3DLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(Pooling3DLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.Pooling3DLayerParams";
  }
  protected:
  explicit Pooling3DLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  typedef Pooling3DLayerParams_PoolingType3D PoolingType3D;
  static constexpr PoolingType3D MAX =
    Pooling3DLayerParams_PoolingType3D_MAX;
  static constexpr PoolingType3D AVERAGE =
    Pooling3DLayerParams_PoolingType3D_AVERAGE;
  static inline bool PoolingType3D_IsValid(int value) {
    return Pooling3DLayerParams_PoolingType3D_IsValid(value);
  }
  static constexpr PoolingType3D PoolingType3D_MIN =
    Pooling3DLayerParams_PoolingType3D_PoolingType3D_MIN;
  static constexpr PoolingType3D PoolingType3D_MAX =
    Pooling3DLayerParams_PoolingType3D_PoolingType3D_MAX;
  static constexpr int PoolingType3D_ARRAYSIZE =
    Pooling3DLayerParams_PoolingType3D_PoolingType3D_ARRAYSIZE;
  template<typename T>
  static inline const std::string& PoolingType3D_Name(T enum_t_value) {
    static_assert(::std::is_same<T, PoolingType3D>::value ||
      ::std::is_integral<T>::value,
      "Incorrect type passed to function PoolingType3D_Name.");
    return Pooling3DLayerParams_PoolingType3D_Name(enum_t_value);
  }
  static inline bool PoolingType3D_Parse(::PROTOBUF_NAMESPACE_ID::ConstStringParam name,
      PoolingType3D* value) {
    return Pooling3DLayerParams_PoolingType3D_Parse(name, value);
  }

  typedef Pooling3DLayerParams_Pooling3DPaddingType Pooling3DPaddingType;
  static constexpr Pooling3DPaddingType CUSTOM =
    Pooling3DLayerParams_Pooling3DPaddingType_CUSTOM;
  static constexpr Pooling3DPaddingType VALID =
    Pooling3DLayerParams_Pooling3DPaddingType_VALID;
  static constexpr Pooling3DPaddingType SAME =
    Pooling3DLayerParams_Pooling3DPaddingType_SAME;
  static inline bool Pooling3DPaddingType_IsValid(int value) {
    return Pooling3DLayerParams_Pooling3DPaddingType_IsValid(value);
  }
  static constexpr Pooling3DPaddingType Pooling3DPaddingType_MIN =
    Pooling3DLayerParams_Pooling3DPaddingType_Pooling3DPaddingType_MIN;
  static constexpr Pooling3DPaddingType Pooling3DPaddingType_MAX =
    Pooling3DLayerParams_Pooling3DPaddingType_Pooling3DPaddingType_MAX;
  static constexpr int Pooling3DPaddingType_ARRAYSIZE =
    Pooling3DLayerParams_Pooling3DPaddingType_Pooling3DPaddingType_ARRAYSIZE;
  template<typename T>
  static inline const std::string& Pooling3DPaddingType_Name(T enum_t_value) {
    static_assert(::std::is_same<T, Pooling3DPaddingType>::value ||
      ::std::is_integral<T>::value,
      "Incorrect type passed to function Pooling3DPaddingType_Name.");
    return Pooling3DLayerParams_Pooling3DPaddingType_Name(enum_t_value);
  }
  static inline bool Pooling3DPaddingType_Parse(::PROTOBUF_NAMESPACE_ID::ConstStringParam name,
      Pooling3DPaddingType* value) {
    return Pooling3DLayerParams_Pooling3DPaddingType_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  enum : int {
    kTypeFieldNumber = 1,
    kKernelDepthFieldNumber = 2,
    kKernelHeightFieldNumber = 3,
    kKernelWidthFieldNumber = 4,
    kStrideDepthFieldNumber = 5,
    kStrideHeightFieldNumber = 6,
    kStrideWidthFieldNumber = 7,
    kCustomPaddingFrontFieldNumber = 8,
    kCustomPaddingBackFieldNumber = 9,
    kCustomPaddingTopFieldNumber = 10,
    kCustomPaddingBottomFieldNumber = 11,
    kCustomPaddingLeftFieldNumber = 12,
    kCustomPaddingRightFieldNumber = 13,
    kCountExcludePaddingFieldNumber = 14,
    kPaddingTypeFieldNumber = 15,
  };
  // .CoreML.Specification.Pooling3DLayerParams.PoolingType3D type = 1;
  void clear_type();
  ::CoreML::Specification::Pooling3DLayerParams_PoolingType3D type() const;
  void set_type(::CoreML::Specification::Pooling3DLayerParams_PoolingType3D value);
  private:
  ::CoreML::Specification::Pooling3DLayerParams_PoolingType3D _internal_type() const;
  void _internal_set_type(::CoreML::Specification::Pooling3DLayerParams_PoolingType3D value);
  public:

  // int32 kernelDepth = 2;
  void clear_kerneldepth();
  int32_t kerneldepth() const;
  void set_kerneldepth(int32_t value);
  private:
  int32_t _internal_kerneldepth() const;
  void _internal_set_kerneldepth(int32_t value);
  public:

  // int32 kernelHeight = 3;
  void clear_kernelheight();
  int32_t kernelheight() const;
  void set_kernelheight(int32_t value);
  private:
  int32_t _internal_kernelheight() const;
  void _internal_set_kernelheight(int32_t value);
  public:

  // int32 kernelWidth = 4;
  void clear_kernelwidth();
  int32_t kernelwidth() const;
  void set_kernelwidth(int32_t value);
  private:
  int32_t _internal_kernelwidth() const;
  void _internal_set_kernelwidth(int32_t value);
  public:

  // int32 strideDepth = 5;
  void clear_stridedepth();
  int32_t stridedepth() const;
  void set_stridedepth(int32_t value);
  private:
  int32_t _internal_stridedepth() const;
  void _internal_set_stridedepth(int32_t value);
  public:

  // int32 strideHeight = 6;
  void clear_strideheight();
  int32_t strideheight() const;
  void set_strideheight(int32_t value);
  private:
  int32_t _internal_strideheight() const;
  void _internal_set_strideheight(int32_t value);
  public:

  // int32 strideWidth = 7;
  void clear_stridewidth();
  int32_t stridewidth() const;
  void set_stridewidth(int32_t value);
  private:
  int32_t _internal_stridewidth() const;
  void _internal_set_stridewidth(int32_t value);
  public:

  // int32 customPaddingFront = 8;
  void clear_custompaddingfront();
  int32_t custompaddingfront() const;
  void set_custompaddingfront(int32_t value);
  private:
  int32_t _internal_custompaddingfront() const;
  void _internal_set_custompaddingfront(int32_t value);
  public:

  // int32 customPaddingBack = 9;
  void clear_custompaddingback();
  int32_t custompaddingback() const;
  void set_custompaddingback(int32_t value);
  private:
  int32_t _internal_custompaddingback() const;
  void _internal_set_custompaddingback(int32_t value);
  public:

  // int32 customPaddingTop = 10;
  void clear_custompaddingtop();
  int32_t custompaddingtop() const;
  void set_custompaddingtop(int32_t value);
  private:
  int32_t _internal_custompaddingtop() const;
  void _internal_set_custompaddingtop(int32_t value);
  public:

  // int32 customPaddingBottom = 11;
  void clear_custompaddingbottom();
  int32_t custompaddingbottom() const;
  void set_custompaddingbottom(int32_t value);
  private:
  int32_t _internal_custompaddingbottom() const;
  void _internal_set_custompaddingbottom(int32_t value);
  public:

  // int32 customPaddingLeft = 12;
  void clear_custompaddingleft();
  int32_t custompaddingleft() const;
  void set_custompaddingleft(int32_t value);
  private:
  int32_t _internal_custompaddingleft() const;
  void _internal_set_custompaddingleft(int32_t value);
  public:

  // int32 customPaddingRight = 13;
  void clear_custompaddingright();
  int32_t custompaddingright() const;
  void set_custompaddingright(int32_t value);
  private:
  int32_t _internal_custompaddingright() const;
  void _internal_set_custompaddingright(int32_t value);
  public:

  // bool countExcludePadding = 14;
  void clear_countexcludepadding();
  bool countexcludepadding() const;
  void set_countexcludepadding(bool value);
  private:
  bool _internal_countexcludepadding() const;
  void _internal_set_countexcludepadding(bool value);
  public:

  // .CoreML.Specification.Pooling3DLayerParams.Pooling3DPaddingType paddingType = 15;
  void clear_paddingtype();
  ::CoreML::Specification::Pooling3DLayerParams_Pooling3DPaddingType paddingtype() const;
  void set_paddingtype(::CoreML::Specification::Pooling3DLayerParams_Pooling3DPaddingType value);
  private:
  ::CoreML::Specification::Pooling3DLayerParams_Pooling3DPaddingType _internal_paddingtype() const;
  void _internal_set_paddingtype(::CoreML::Specification::Pooling3DLayerParams_Pooling3DPaddingType value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.Pooling3DLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int type_;
  int32_t kerneldepth_;
  int32_t kernelheight_;
  int32_t kernelwidth_;
  int32_t stridedepth_;
  int32_t strideheight_;
  int32_t stridewidth_;
  int32_t custompaddingfront_;
  int32_t custompaddingback_;
  int32_t custompaddingtop_;
  int32_t custompaddingbottom_;
  int32_t custompaddingleft_;
  int32_t custompaddingright_;
  bool countexcludepadding_;
  int paddingtype_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class GlobalPooling3DLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.GlobalPooling3DLayerParams) */ {
 public:
  inline GlobalPooling3DLayerParams() : GlobalPooling3DLayerParams(nullptr) {}
  ~GlobalPooling3DLayerParams() override;
  explicit constexpr GlobalPooling3DLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  GlobalPooling3DLayerParams(const GlobalPooling3DLayerParams& from);
  GlobalPooling3DLayerParams(GlobalPooling3DLayerParams&& from) noexcept
    : GlobalPooling3DLayerParams() {
    *this = ::std::move(from);
  }

  inline GlobalPooling3DLayerParams& operator=(const GlobalPooling3DLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline GlobalPooling3DLayerParams& operator=(GlobalPooling3DLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const GlobalPooling3DLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const GlobalPooling3DLayerParams* internal_default_instance() {
    return reinterpret_cast<const GlobalPooling3DLayerParams*>(
               &_GlobalPooling3DLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    54;

  friend void swap(GlobalPooling3DLayerParams& a, GlobalPooling3DLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(GlobalPooling3DLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(GlobalPooling3DLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  GlobalPooling3DLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<GlobalPooling3DLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const GlobalPooling3DLayerParams& from);
  void MergeFrom(const GlobalPooling3DLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(GlobalPooling3DLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.GlobalPooling3DLayerParams";
  }
  protected:
  explicit GlobalPooling3DLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  typedef GlobalPooling3DLayerParams_GlobalPoolingType3D GlobalPoolingType3D;
  static constexpr GlobalPoolingType3D MAX =
    GlobalPooling3DLayerParams_GlobalPoolingType3D_MAX;
  static constexpr GlobalPoolingType3D AVERAGE =
    GlobalPooling3DLayerParams_GlobalPoolingType3D_AVERAGE;
  static inline bool GlobalPoolingType3D_IsValid(int value) {
    return GlobalPooling3DLayerParams_GlobalPoolingType3D_IsValid(value);
  }
  static constexpr GlobalPoolingType3D GlobalPoolingType3D_MIN =
    GlobalPooling3DLayerParams_GlobalPoolingType3D_GlobalPoolingType3D_MIN;
  static constexpr GlobalPoolingType3D GlobalPoolingType3D_MAX =
    GlobalPooling3DLayerParams_GlobalPoolingType3D_GlobalPoolingType3D_MAX;
  static constexpr int GlobalPoolingType3D_ARRAYSIZE =
    GlobalPooling3DLayerParams_GlobalPoolingType3D_GlobalPoolingType3D_ARRAYSIZE;
  template<typename T>
  static inline const std::string& GlobalPoolingType3D_Name(T enum_t_value) {
    static_assert(::std::is_same<T, GlobalPoolingType3D>::value ||
      ::std::is_integral<T>::value,
      "Incorrect type passed to function GlobalPoolingType3D_Name.");
    return GlobalPooling3DLayerParams_GlobalPoolingType3D_Name(enum_t_value);
  }
  static inline bool GlobalPoolingType3D_Parse(::PROTOBUF_NAMESPACE_ID::ConstStringParam name,
      GlobalPoolingType3D* value) {
    return GlobalPooling3DLayerParams_GlobalPoolingType3D_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  enum : int {
    kTypeFieldNumber = 1,
  };
  // .CoreML.Specification.GlobalPooling3DLayerParams.GlobalPoolingType3D type = 1;
  void clear_type();
  ::CoreML::Specification::GlobalPooling3DLayerParams_GlobalPoolingType3D type() const;
  void set_type(::CoreML::Specification::GlobalPooling3DLayerParams_GlobalPoolingType3D value);
  private:
  ::CoreML::Specification::GlobalPooling3DLayerParams_GlobalPoolingType3D _internal_type() const;
  void _internal_set_type(::CoreML::Specification::GlobalPooling3DLayerParams_GlobalPoolingType3D value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.GlobalPooling3DLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int type_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class PaddingLayerParams_PaddingConstant final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PaddingLayerParams.PaddingConstant) */ {
 public:
  inline PaddingLayerParams_PaddingConstant() : PaddingLayerParams_PaddingConstant(nullptr) {}
  ~PaddingLayerParams_PaddingConstant() override;
  explicit constexpr PaddingLayerParams_PaddingConstant(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  PaddingLayerParams_PaddingConstant(const PaddingLayerParams_PaddingConstant& from);
  PaddingLayerParams_PaddingConstant(PaddingLayerParams_PaddingConstant&& from) noexcept
    : PaddingLayerParams_PaddingConstant() {
    *this = ::std::move(from);
  }

  inline PaddingLayerParams_PaddingConstant& operator=(const PaddingLayerParams_PaddingConstant& from) {
    CopyFrom(from);
    return *this;
  }
  inline PaddingLayerParams_PaddingConstant& operator=(PaddingLayerParams_PaddingConstant&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const PaddingLayerParams_PaddingConstant& default_instance() {
    return *internal_default_instance();
  }
  static inline const PaddingLayerParams_PaddingConstant* internal_default_instance() {
    return reinterpret_cast<const PaddingLayerParams_PaddingConstant*>(
               &_PaddingLayerParams_PaddingConstant_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    55;

  friend void swap(PaddingLayerParams_PaddingConstant& a, PaddingLayerParams_PaddingConstant& b) {
    a.Swap(&b);
  }
  inline void Swap(PaddingLayerParams_PaddingConstant* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(PaddingLayerParams_PaddingConstant* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  PaddingLayerParams_PaddingConstant* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<PaddingLayerParams_PaddingConstant>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const PaddingLayerParams_PaddingConstant& from);
  void MergeFrom(const PaddingLayerParams_PaddingConstant& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PaddingLayerParams_PaddingConstant* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.PaddingLayerParams.PaddingConstant";
  }
  protected:
  explicit PaddingLayerParams_PaddingConstant(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kValueFieldNumber = 1,
  };
  // float value = 1;
  void clear_value();
  float value() const;
  void set_value(float value);
  private:
  float _internal_value() const;
  void _internal_set_value(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.PaddingLayerParams.PaddingConstant)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float value_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class PaddingLayerParams_PaddingReflection final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PaddingLayerParams.PaddingReflection) */ {
 public:
  inline PaddingLayerParams_PaddingReflection() : PaddingLayerParams_PaddingReflection(nullptr) {}
  ~PaddingLayerParams_PaddingReflection() override;
  explicit constexpr PaddingLayerParams_PaddingReflection(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  PaddingLayerParams_PaddingReflection(const PaddingLayerParams_PaddingReflection& from);
  PaddingLayerParams_PaddingReflection(PaddingLayerParams_PaddingReflection&& from) noexcept
    : PaddingLayerParams_PaddingReflection() {
    *this = ::std::move(from);
  }

  inline PaddingLayerParams_PaddingReflection& operator=(const PaddingLayerParams_PaddingReflection& from) {
    CopyFrom(from);
    return *this;
  }
  inline PaddingLayerParams_PaddingReflection& operator=(PaddingLayerParams_PaddingReflection&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const PaddingLayerParams_PaddingReflection& default_instance() {
    return *internal_default_instance();
  }
  static inline const PaddingLayerParams_PaddingReflection* internal_default_instance() {
    return reinterpret_cast<const PaddingLayerParams_PaddingReflection*>(
               &_PaddingLayerParams_PaddingReflection_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    56;

  friend void swap(PaddingLayerParams_PaddingReflection& a, PaddingLayerParams_PaddingReflection& b) {
    a.Swap(&b);
  }
  inline void Swap(PaddingLayerParams_PaddingReflection* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(PaddingLayerParams_PaddingReflection* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  PaddingLayerParams_PaddingReflection* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<PaddingLayerParams_PaddingReflection>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const PaddingLayerParams_PaddingReflection& from);
  void MergeFrom(const PaddingLayerParams_PaddingReflection& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PaddingLayerParams_PaddingReflection* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.PaddingLayerParams.PaddingReflection";
  }
  protected:
  explicit PaddingLayerParams_PaddingReflection(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.PaddingLayerParams.PaddingReflection)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class PaddingLayerParams_PaddingReplication final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PaddingLayerParams.PaddingReplication) */ {
 public:
  inline PaddingLayerParams_PaddingReplication() : PaddingLayerParams_PaddingReplication(nullptr) {}
  ~PaddingLayerParams_PaddingReplication() override;
  explicit constexpr PaddingLayerParams_PaddingReplication(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  PaddingLayerParams_PaddingReplication(const PaddingLayerParams_PaddingReplication& from);
  PaddingLayerParams_PaddingReplication(PaddingLayerParams_PaddingReplication&& from) noexcept
    : PaddingLayerParams_PaddingReplication() {
    *this = ::std::move(from);
  }

  inline PaddingLayerParams_PaddingReplication& operator=(const PaddingLayerParams_PaddingReplication& from) {
    CopyFrom(from);
    return *this;
  }
  inline PaddingLayerParams_PaddingReplication& operator=(PaddingLayerParams_PaddingReplication&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const PaddingLayerParams_PaddingReplication& default_instance() {
    return *internal_default_instance();
  }
  static inline const PaddingLayerParams_PaddingReplication* internal_default_instance() {
    return reinterpret_cast<const PaddingLayerParams_PaddingReplication*>(
               &_PaddingLayerParams_PaddingReplication_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    57;

  friend void swap(PaddingLayerParams_PaddingReplication& a, PaddingLayerParams_PaddingReplication& b) {
    a.Swap(&b);
  }
  inline void Swap(PaddingLayerParams_PaddingReplication* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(PaddingLayerParams_PaddingReplication* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  PaddingLayerParams_PaddingReplication* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<PaddingLayerParams_PaddingReplication>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const PaddingLayerParams_PaddingReplication& from);
  void MergeFrom(const PaddingLayerParams_PaddingReplication& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PaddingLayerParams_PaddingReplication* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.PaddingLayerParams.PaddingReplication";
  }
  protected:
  explicit PaddingLayerParams_PaddingReplication(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.PaddingLayerParams.PaddingReplication)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class PaddingLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PaddingLayerParams) */ {
 public:
  inline PaddingLayerParams() : PaddingLayerParams(nullptr) {}
  ~PaddingLayerParams() override;
  explicit constexpr PaddingLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  PaddingLayerParams(const PaddingLayerParams& from);
  PaddingLayerParams(PaddingLayerParams&& from) noexcept
    : PaddingLayerParams() {
    *this = ::std::move(from);
  }

  inline PaddingLayerParams& operator=(const PaddingLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline PaddingLayerParams& operator=(PaddingLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const PaddingLayerParams& default_instance() {
    return *internal_default_instance();
  }
  enum PaddingTypeCase {
    kConstant = 1,
    kReflection = 2,
    kReplication = 3,
    PADDINGTYPE_NOT_SET = 0,
  };

  static inline const PaddingLayerParams* internal_default_instance() {
    return reinterpret_cast<const PaddingLayerParams*>(
               &_PaddingLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    58;

  friend void swap(PaddingLayerParams& a, PaddingLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(PaddingLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(PaddingLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  PaddingLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<PaddingLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const PaddingLayerParams& from);
  void MergeFrom(const PaddingLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PaddingLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.PaddingLayerParams";
  }
  protected:
  explicit PaddingLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  typedef PaddingLayerParams_PaddingConstant PaddingConstant;
  typedef PaddingLayerParams_PaddingReflection PaddingReflection;
  typedef PaddingLayerParams_PaddingReplication PaddingReplication;

  // accessors -------------------------------------------------------

  enum : int {
    kPaddingAmountsFieldNumber = 10,
    kConstantFieldNumber = 1,
    kReflectionFieldNumber = 2,
    kReplicationFieldNumber = 3,
  };
  // .CoreML.Specification.BorderAmounts paddingAmounts = 10;
  bool has_paddingamounts() const;
  private:
  bool _internal_has_paddingamounts() const;
  public:
  void clear_paddingamounts();
  const ::CoreML::Specification::BorderAmounts& paddingamounts() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::BorderAmounts* release_paddingamounts();
  ::CoreML::Specification::BorderAmounts* mutable_paddingamounts();
  void set_allocated_paddingamounts(::CoreML::Specification::BorderAmounts* paddingamounts);
  private:
  const ::CoreML::Specification::BorderAmounts& _internal_paddingamounts() const;
  ::CoreML::Specification::BorderAmounts* _internal_mutable_paddingamounts();
  public:
  void unsafe_arena_set_allocated_paddingamounts(
      ::CoreML::Specification::BorderAmounts* paddingamounts);
  ::CoreML::Specification::BorderAmounts* unsafe_arena_release_paddingamounts();

  // .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
  bool has_constant() const;
  private:
  bool _internal_has_constant() const;
  public:
  void clear_constant();
  const ::CoreML::Specification::PaddingLayerParams_PaddingConstant& constant() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::PaddingLayerParams_PaddingConstant* release_constant();
  ::CoreML::Specification::PaddingLayerParams_PaddingConstant* mutable_constant();
  void set_allocated_constant(::CoreML::Specification::PaddingLayerParams_PaddingConstant* constant);
  private:
  const ::CoreML::Specification::PaddingLayerParams_PaddingConstant& _internal_constant() const;
  ::CoreML::Specification::PaddingLayerParams_PaddingConstant* _internal_mutable_constant();
  public:
  void unsafe_arena_set_allocated_constant(
      ::CoreML::Specification::PaddingLayerParams_PaddingConstant* constant);
  ::CoreML::Specification::PaddingLayerParams_PaddingConstant* unsafe_arena_release_constant();

  // .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
  bool has_reflection() const;
  private:
  bool _internal_has_reflection() const;
  public:
  void clear_reflection();
  const ::CoreML::Specification::PaddingLayerParams_PaddingReflection& reflection() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::PaddingLayerParams_PaddingReflection* release_reflection();
  ::CoreML::Specification::PaddingLayerParams_PaddingReflection* mutable_reflection();
  void set_allocated_reflection(::CoreML::Specification::PaddingLayerParams_PaddingReflection* reflection);
  private:
  const ::CoreML::Specification::PaddingLayerParams_PaddingReflection& _internal_reflection() const;
  ::CoreML::Specification::PaddingLayerParams_PaddingReflection* _internal_mutable_reflection();
  public:
  void unsafe_arena_set_allocated_reflection(
      ::CoreML::Specification::PaddingLayerParams_PaddingReflection* reflection);
  ::CoreML::Specification::PaddingLayerParams_PaddingReflection* unsafe_arena_release_reflection();

  // .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
  bool has_replication() const;
  private:
  bool _internal_has_replication() const;
  public:
  void clear_replication();
  const ::CoreML::Specification::PaddingLayerParams_PaddingReplication& replication() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::PaddingLayerParams_PaddingReplication* release_replication();
  ::CoreML::Specification::PaddingLayerParams_PaddingReplication* mutable_replication();
  void set_allocated_replication(::CoreML::Specification::PaddingLayerParams_PaddingReplication* replication);
  private:
  const ::CoreML::Specification::PaddingLayerParams_PaddingReplication& _internal_replication() const;
  ::CoreML::Specification::PaddingLayerParams_PaddingReplication* _internal_mutable_replication();
  public:
  void unsafe_arena_set_allocated_replication(
      ::CoreML::Specification::PaddingLayerParams_PaddingReplication* replication);
  ::CoreML::Specification::PaddingLayerParams_PaddingReplication* unsafe_arena_release_replication();

  void clear_PaddingType();
  PaddingTypeCase PaddingType_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.PaddingLayerParams)
 private:
  class _Internal;
  void set_has_constant();
  void set_has_reflection();
  void set_has_replication();

  inline bool has_PaddingType() const;
  inline void clear_has_PaddingType();

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::CoreML::Specification::BorderAmounts* paddingamounts_;
  union PaddingTypeUnion {
    constexpr PaddingTypeUnion() : _constinit_{} {}
      ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized _constinit_;
    ::CoreML::Specification::PaddingLayerParams_PaddingConstant* constant_;
    ::CoreML::Specification::PaddingLayerParams_PaddingReflection* reflection_;
    ::CoreML::Specification::PaddingLayerParams_PaddingReplication* replication_;
  } PaddingType_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  uint32_t _oneof_case_[1];

  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ConcatLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ConcatLayerParams) */ {
 public:
  inline ConcatLayerParams() : ConcatLayerParams(nullptr) {}
  ~ConcatLayerParams() override;
  explicit constexpr ConcatLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ConcatLayerParams(const ConcatLayerParams& from);
  ConcatLayerParams(ConcatLayerParams&& from) noexcept
    : ConcatLayerParams() {
    *this = ::std::move(from);
  }

  inline ConcatLayerParams& operator=(const ConcatLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ConcatLayerParams& operator=(ConcatLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ConcatLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ConcatLayerParams* internal_default_instance() {
    return reinterpret_cast<const ConcatLayerParams*>(
               &_ConcatLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    59;

  friend void swap(ConcatLayerParams& a, ConcatLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ConcatLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ConcatLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ConcatLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ConcatLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ConcatLayerParams& from);
  void MergeFrom(const ConcatLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ConcatLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ConcatLayerParams";
  }
  protected:
  explicit ConcatLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kSequenceConcatFieldNumber = 100,
  };
  // bool sequenceConcat = 100;
  void clear_sequenceconcat();
  bool sequenceconcat() const;
  void set_sequenceconcat(bool value);
  private:
  bool _internal_sequenceconcat() const;
  void _internal_set_sequenceconcat(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ConcatLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  bool sequenceconcat_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class LRNLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LRNLayerParams) */ {
 public:
  inline LRNLayerParams() : LRNLayerParams(nullptr) {}
  ~LRNLayerParams() override;
  explicit constexpr LRNLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  LRNLayerParams(const LRNLayerParams& from);
  LRNLayerParams(LRNLayerParams&& from) noexcept
    : LRNLayerParams() {
    *this = ::std::move(from);
  }

  inline LRNLayerParams& operator=(const LRNLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline LRNLayerParams& operator=(LRNLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const LRNLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const LRNLayerParams* internal_default_instance() {
    return reinterpret_cast<const LRNLayerParams*>(
               &_LRNLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    60;

  friend void swap(LRNLayerParams& a, LRNLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(LRNLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(LRNLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  LRNLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<LRNLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const LRNLayerParams& from);
  void MergeFrom(const LRNLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LRNLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.LRNLayerParams";
  }
  protected:
  explicit LRNLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAlphaFieldNumber = 1,
    kBetaFieldNumber = 2,
    kLocalSizeFieldNumber = 3,
    kKFieldNumber = 4,
  };
  // float alpha = 1;
  void clear_alpha();
  float alpha() const;
  void set_alpha(float value);
  private:
  float _internal_alpha() const;
  void _internal_set_alpha(float value);
  public:

  // float beta = 2;
  void clear_beta();
  float beta() const;
  void set_beta(float value);
  private:
  float _internal_beta() const;
  void _internal_set_beta(float value);
  public:

  // uint64 localSize = 3;
  void clear_localsize();
  uint64_t localsize() const;
  void set_localsize(uint64_t value);
  private:
  uint64_t _internal_localsize() const;
  void _internal_set_localsize(uint64_t value);
  public:

  // float k = 4;
  void clear_k();
  float k() const;
  void set_k(float value);
  private:
  float _internal_k() const;
  void _internal_set_k(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LRNLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float alpha_;
  float beta_;
  uint64_t localsize_;
  float k_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class SoftmaxLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SoftmaxLayerParams) */ {
 public:
  inline SoftmaxLayerParams() : SoftmaxLayerParams(nullptr) {}
  ~SoftmaxLayerParams() override;
  explicit constexpr SoftmaxLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SoftmaxLayerParams(const SoftmaxLayerParams& from);
  SoftmaxLayerParams(SoftmaxLayerParams&& from) noexcept
    : SoftmaxLayerParams() {
    *this = ::std::move(from);
  }

  inline SoftmaxLayerParams& operator=(const SoftmaxLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline SoftmaxLayerParams& operator=(SoftmaxLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const SoftmaxLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const SoftmaxLayerParams* internal_default_instance() {
    return reinterpret_cast<const SoftmaxLayerParams*>(
               &_SoftmaxLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    61;

  friend void swap(SoftmaxLayerParams& a, SoftmaxLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(SoftmaxLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SoftmaxLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SoftmaxLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SoftmaxLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const SoftmaxLayerParams& from);
  void MergeFrom(const SoftmaxLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SoftmaxLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.SoftmaxLayerParams";
  }
  protected:
  explicit SoftmaxLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SoftmaxLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class SplitLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SplitLayerParams) */ {
 public:
  inline SplitLayerParams() : SplitLayerParams(nullptr) {}
  ~SplitLayerParams() override;
  explicit constexpr SplitLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SplitLayerParams(const SplitLayerParams& from);
  SplitLayerParams(SplitLayerParams&& from) noexcept
    : SplitLayerParams() {
    *this = ::std::move(from);
  }

  inline SplitLayerParams& operator=(const SplitLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline SplitLayerParams& operator=(SplitLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const SplitLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const SplitLayerParams* internal_default_instance() {
    return reinterpret_cast<const SplitLayerParams*>(
               &_SplitLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    62;

  friend void swap(SplitLayerParams& a, SplitLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(SplitLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SplitLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SplitLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SplitLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const SplitLayerParams& from);
  void MergeFrom(const SplitLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SplitLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.SplitLayerParams";
  }
  protected:
  explicit SplitLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kNOutputsFieldNumber = 1,
  };
  // uint64 nOutputs = 1;
  void clear_noutputs();
  uint64_t noutputs() const;
  void set_noutputs(uint64_t value);
  private:
  uint64_t _internal_noutputs() const;
  void _internal_set_noutputs(uint64_t value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SplitLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  uint64_t noutputs_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class AddLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AddLayerParams) */ {
 public:
  inline AddLayerParams() : AddLayerParams(nullptr) {}
  ~AddLayerParams() override;
  explicit constexpr AddLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  AddLayerParams(const AddLayerParams& from);
  AddLayerParams(AddLayerParams&& from) noexcept
    : AddLayerParams() {
    *this = ::std::move(from);
  }

  inline AddLayerParams& operator=(const AddLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline AddLayerParams& operator=(AddLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const AddLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const AddLayerParams* internal_default_instance() {
    return reinterpret_cast<const AddLayerParams*>(
               &_AddLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    63;

  friend void swap(AddLayerParams& a, AddLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(AddLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(AddLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  AddLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<AddLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const AddLayerParams& from);
  void MergeFrom(const AddLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AddLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.AddLayerParams";
  }
  protected:
  explicit AddLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAlphaFieldNumber = 1,
  };
  // float alpha = 1;
  void clear_alpha();
  float alpha() const;
  void set_alpha(float value);
  private:
  float _internal_alpha() const;
  void _internal_set_alpha(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AddLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float alpha_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class MultiplyLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MultiplyLayerParams) */ {
 public:
  inline MultiplyLayerParams() : MultiplyLayerParams(nullptr) {}
  ~MultiplyLayerParams() override;
  explicit constexpr MultiplyLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  MultiplyLayerParams(const MultiplyLayerParams& from);
  MultiplyLayerParams(MultiplyLayerParams&& from) noexcept
    : MultiplyLayerParams() {
    *this = ::std::move(from);
  }

  inline MultiplyLayerParams& operator=(const MultiplyLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline MultiplyLayerParams& operator=(MultiplyLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const MultiplyLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const MultiplyLayerParams* internal_default_instance() {
    return reinterpret_cast<const MultiplyLayerParams*>(
               &_MultiplyLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    64;

  friend void swap(MultiplyLayerParams& a, MultiplyLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(MultiplyLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(MultiplyLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  MultiplyLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<MultiplyLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const MultiplyLayerParams& from);
  void MergeFrom(const MultiplyLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MultiplyLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.MultiplyLayerParams";
  }
  protected:
  explicit MultiplyLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAlphaFieldNumber = 1,
  };
  // float alpha = 1;
  void clear_alpha();
  float alpha() const;
  void set_alpha(float value);
  private:
  float _internal_alpha() const;
  void _internal_set_alpha(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MultiplyLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float alpha_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class UnaryFunctionLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.UnaryFunctionLayerParams) */ {
 public:
  inline UnaryFunctionLayerParams() : UnaryFunctionLayerParams(nullptr) {}
  ~UnaryFunctionLayerParams() override;
  explicit constexpr UnaryFunctionLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  UnaryFunctionLayerParams(const UnaryFunctionLayerParams& from);
  UnaryFunctionLayerParams(UnaryFunctionLayerParams&& from) noexcept
    : UnaryFunctionLayerParams() {
    *this = ::std::move(from);
  }

  inline UnaryFunctionLayerParams& operator=(const UnaryFunctionLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline UnaryFunctionLayerParams& operator=(UnaryFunctionLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const UnaryFunctionLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const UnaryFunctionLayerParams* internal_default_instance() {
    return reinterpret_cast<const UnaryFunctionLayerParams*>(
               &_UnaryFunctionLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    65;

  friend void swap(UnaryFunctionLayerParams& a, UnaryFunctionLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(UnaryFunctionLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(UnaryFunctionLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  UnaryFunctionLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<UnaryFunctionLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const UnaryFunctionLayerParams& from);
  void MergeFrom(const UnaryFunctionLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(UnaryFunctionLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.UnaryFunctionLayerParams";
  }
  protected:
  explicit UnaryFunctionLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  typedef UnaryFunctionLayerParams_Operation Operation;
  static constexpr Operation SQRT =
    UnaryFunctionLayerParams_Operation_SQRT;
  static constexpr Operation RSQRT =
    UnaryFunctionLayerParams_Operation_RSQRT;
  static constexpr Operation INVERSE =
    UnaryFunctionLayerParams_Operation_INVERSE;
  static constexpr Operation POWER =
    UnaryFunctionLayerParams_Operation_POWER;
  static constexpr Operation EXP =
    UnaryFunctionLayerParams_Operation_EXP;
  static constexpr Operation LOG =
    UnaryFunctionLayerParams_Operation_LOG;
  static constexpr Operation ABS =
    UnaryFunctionLayerParams_Operation_ABS;
  static constexpr Operation THRESHOLD =
    UnaryFunctionLayerParams_Operation_THRESHOLD;
  static inline bool Operation_IsValid(int value) {
    return UnaryFunctionLayerParams_Operation_IsValid(value);
  }
  static constexpr Operation Operation_MIN =
    UnaryFunctionLayerParams_Operation_Operation_MIN;
  static constexpr Operation Operation_MAX =
    UnaryFunctionLayerParams_Operation_Operation_MAX;
  static constexpr int Operation_ARRAYSIZE =
    UnaryFunctionLayerParams_Operation_Operation_ARRAYSIZE;
  template<typename T>
  static inline const std::string& Operation_Name(T enum_t_value) {
    static_assert(::std::is_same<T, Operation>::value ||
      ::std::is_integral<T>::value,
      "Incorrect type passed to function Operation_Name.");
    return UnaryFunctionLayerParams_Operation_Name(enum_t_value);
  }
  static inline bool Operation_Parse(::PROTOBUF_NAMESPACE_ID::ConstStringParam name,
      Operation* value) {
    return UnaryFunctionLayerParams_Operation_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  enum : int {
    kTypeFieldNumber = 1,
    kAlphaFieldNumber = 2,
    kEpsilonFieldNumber = 3,
    kShiftFieldNumber = 4,
    kScaleFieldNumber = 5,
  };
  // .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
  void clear_type();
  ::CoreML::Specification::UnaryFunctionLayerParams_Operation type() const;
  void set_type(::CoreML::Specification::UnaryFunctionLayerParams_Operation value);
  private:
  ::CoreML::Specification::UnaryFunctionLayerParams_Operation _internal_type() const;
  void _internal_set_type(::CoreML::Specification::UnaryFunctionLayerParams_Operation value);
  public:

  // float alpha = 2;
  void clear_alpha();
  float alpha() const;
  void set_alpha(float value);
  private:
  float _internal_alpha() const;
  void _internal_set_alpha(float value);
  public:

  // float epsilon = 3;
  void clear_epsilon();
  float epsilon() const;
  void set_epsilon(float value);
  private:
  float _internal_epsilon() const;
  void _internal_set_epsilon(float value);
  public:

  // float shift = 4;
  void clear_shift();
  float shift() const;
  void set_shift(float value);
  private:
  float _internal_shift() const;
  void _internal_set_shift(float value);
  public:

  // float scale = 5;
  void clear_scale();
  float scale() const;
  void set_scale(float value);
  private:
  float _internal_scale() const;
  void _internal_set_scale(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.UnaryFunctionLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int type_;
  float alpha_;
  float epsilon_;
  float shift_;
  float scale_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class UpsampleLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.UpsampleLayerParams) */ {
 public:
  inline UpsampleLayerParams() : UpsampleLayerParams(nullptr) {}
  ~UpsampleLayerParams() override;
  explicit constexpr UpsampleLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  UpsampleLayerParams(const UpsampleLayerParams& from);
  UpsampleLayerParams(UpsampleLayerParams&& from) noexcept
    : UpsampleLayerParams() {
    *this = ::std::move(from);
  }

  inline UpsampleLayerParams& operator=(const UpsampleLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline UpsampleLayerParams& operator=(UpsampleLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const UpsampleLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const UpsampleLayerParams* internal_default_instance() {
    return reinterpret_cast<const UpsampleLayerParams*>(
               &_UpsampleLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    66;

  friend void swap(UpsampleLayerParams& a, UpsampleLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(UpsampleLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(UpsampleLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  UpsampleLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<UpsampleLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const UpsampleLayerParams& from);
  void MergeFrom(const UpsampleLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(UpsampleLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.UpsampleLayerParams";
  }
  protected:
  explicit UpsampleLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  typedef UpsampleLayerParams_InterpolationMode InterpolationMode;
  static constexpr InterpolationMode NN =
    UpsampleLayerParams_InterpolationMode_NN;
  static constexpr InterpolationMode BILINEAR =
    UpsampleLayerParams_InterpolationMode_BILINEAR;
  static inline bool InterpolationMode_IsValid(int value) {
    return UpsampleLayerParams_InterpolationMode_IsValid(value);
  }
  static constexpr InterpolationMode InterpolationMode_MIN =
    UpsampleLayerParams_InterpolationMode_InterpolationMode_MIN;
  static constexpr InterpolationMode InterpolationMode_MAX =
    UpsampleLayerParams_InterpolationMode_InterpolationMode_MAX;
  static constexpr int InterpolationMode_ARRAYSIZE =
    UpsampleLayerParams_InterpolationMode_InterpolationMode_ARRAYSIZE;
  template<typename T>
  static inline const std::string& InterpolationMode_Name(T enum_t_value) {
    static_assert(::std::is_same<T, InterpolationMode>::value ||
      ::std::is_integral<T>::value,
      "Incorrect type passed to function InterpolationMode_Name.");
    return UpsampleLayerParams_InterpolationMode_Name(enum_t_value);
  }
  static inline bool InterpolationMode_Parse(::PROTOBUF_NAMESPACE_ID::ConstStringParam name,
      InterpolationMode* value) {
    return UpsampleLayerParams_InterpolationMode_Parse(name, value);
  }

  typedef UpsampleLayerParams_LinearUpsampleMode LinearUpsampleMode;
  static constexpr LinearUpsampleMode DEFAULT =
    UpsampleLayerParams_LinearUpsampleMode_DEFAULT;
  static constexpr LinearUpsampleMode ALIGN_CORNERS_TRUE =
    UpsampleLayerParams_LinearUpsampleMode_ALIGN_CORNERS_TRUE;
  static constexpr LinearUpsampleMode ALIGN_CORNERS_FALSE =
    UpsampleLayerParams_LinearUpsampleMode_ALIGN_CORNERS_FALSE;
  static inline bool LinearUpsampleMode_IsValid(int value) {
    return UpsampleLayerParams_LinearUpsampleMode_IsValid(value);
  }
  static constexpr LinearUpsampleMode LinearUpsampleMode_MIN =
    UpsampleLayerParams_LinearUpsampleMode_LinearUpsampleMode_MIN;
  static constexpr LinearUpsampleMode LinearUpsampleMode_MAX =
    UpsampleLayerParams_LinearUpsampleMode_LinearUpsampleMode_MAX;
  static constexpr int LinearUpsampleMode_ARRAYSIZE =
    UpsampleLayerParams_LinearUpsampleMode_LinearUpsampleMode_ARRAYSIZE;
  template<typename T>
  static inline const std::string& LinearUpsampleMode_Name(T enum_t_value) {
    static_assert(::std::is_same<T, LinearUpsampleMode>::value ||
      ::std::is_integral<T>::value,
      "Incorrect type passed to function LinearUpsampleMode_Name.");
    return UpsampleLayerParams_LinearUpsampleMode_Name(enum_t_value);
  }
  static inline bool LinearUpsampleMode_Parse(::PROTOBUF_NAMESPACE_ID::ConstStringParam name,
      LinearUpsampleMode* value) {
    return UpsampleLayerParams_LinearUpsampleMode_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  enum : int {
    kScalingFactorFieldNumber = 1,
    kFractionalScalingFactorFieldNumber = 7,
    kModeFieldNumber = 5,
    kLinearUpsampleModeFieldNumber = 6,
  };
  // repeated uint64 scalingFactor = 1;
  int scalingfactor_size() const;
  private:
  int _internal_scalingfactor_size() const;
  public:
  void clear_scalingfactor();
  private:
  uint64_t _internal_scalingfactor(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_scalingfactor() const;
  void _internal_add_scalingfactor(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_scalingfactor();
  public:
  uint64_t scalingfactor(int index) const;
  void set_scalingfactor(int index, uint64_t value);
  void add_scalingfactor(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      scalingfactor() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_scalingfactor();

  // repeated float fractionalScalingFactor = 7;
  int fractionalscalingfactor_size() const;
  private:
  int _internal_fractionalscalingfactor_size() const;
  public:
  void clear_fractionalscalingfactor();
  private:
  float _internal_fractionalscalingfactor(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
      _internal_fractionalscalingfactor() const;
  void _internal_add_fractionalscalingfactor(float value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
      _internal_mutable_fractionalscalingfactor();
  public:
  float fractionalscalingfactor(int index) const;
  void set_fractionalscalingfactor(int index, float value);
  void add_fractionalscalingfactor(float value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
      fractionalscalingfactor() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
      mutable_fractionalscalingfactor();

  // .CoreML.Specification.UpsampleLayerParams.InterpolationMode mode = 5;
  void clear_mode();
  ::CoreML::Specification::UpsampleLayerParams_InterpolationMode mode() const;
  void set_mode(::CoreML::Specification::UpsampleLayerParams_InterpolationMode value);
  private:
  ::CoreML::Specification::UpsampleLayerParams_InterpolationMode _internal_mode() const;
  void _internal_set_mode(::CoreML::Specification::UpsampleLayerParams_InterpolationMode value);
  public:

  // .CoreML.Specification.UpsampleLayerParams.LinearUpsampleMode linearUpsampleMode = 6;
  void clear_linearupsamplemode();
  ::CoreML::Specification::UpsampleLayerParams_LinearUpsampleMode linearupsamplemode() const;
  void set_linearupsamplemode(::CoreML::Specification::UpsampleLayerParams_LinearUpsampleMode value);
  private:
  ::CoreML::Specification::UpsampleLayerParams_LinearUpsampleMode _internal_linearupsamplemode() const;
  void _internal_set_linearupsamplemode(::CoreML::Specification::UpsampleLayerParams_LinearUpsampleMode value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.UpsampleLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > scalingfactor_;
  mutable std::atomic<int> _scalingfactor_cached_byte_size_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< float > fractionalscalingfactor_;
  int mode_;
  int linearupsamplemode_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ResizeBilinearLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ResizeBilinearLayerParams) */ {
 public:
  inline ResizeBilinearLayerParams() : ResizeBilinearLayerParams(nullptr) {}
  ~ResizeBilinearLayerParams() override;
  explicit constexpr ResizeBilinearLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ResizeBilinearLayerParams(const ResizeBilinearLayerParams& from);
  ResizeBilinearLayerParams(ResizeBilinearLayerParams&& from) noexcept
    : ResizeBilinearLayerParams() {
    *this = ::std::move(from);
  }

  inline ResizeBilinearLayerParams& operator=(const ResizeBilinearLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ResizeBilinearLayerParams& operator=(ResizeBilinearLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ResizeBilinearLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ResizeBilinearLayerParams* internal_default_instance() {
    return reinterpret_cast<const ResizeBilinearLayerParams*>(
               &_ResizeBilinearLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    67;

  friend void swap(ResizeBilinearLayerParams& a, ResizeBilinearLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ResizeBilinearLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ResizeBilinearLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ResizeBilinearLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ResizeBilinearLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ResizeBilinearLayerParams& from);
  void MergeFrom(const ResizeBilinearLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ResizeBilinearLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ResizeBilinearLayerParams";
  }
  protected:
  explicit ResizeBilinearLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kTargetSizeFieldNumber = 1,
    kModeFieldNumber = 2,
  };
  // repeated uint64 targetSize = 1;
  int targetsize_size() const;
  private:
  int _internal_targetsize_size() const;
  public:
  void clear_targetsize();
  private:
  uint64_t _internal_targetsize(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_targetsize() const;
  void _internal_add_targetsize(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_targetsize();
  public:
  uint64_t targetsize(int index) const;
  void set_targetsize(int index, uint64_t value);
  void add_targetsize(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      targetsize() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_targetsize();

  // .CoreML.Specification.SamplingMode mode = 2;
  bool has_mode() const;
  private:
  bool _internal_has_mode() const;
  public:
  void clear_mode();
  const ::CoreML::Specification::SamplingMode& mode() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::SamplingMode* release_mode();
  ::CoreML::Specification::SamplingMode* mutable_mode();
  void set_allocated_mode(::CoreML::Specification::SamplingMode* mode);
  private:
  const ::CoreML::Specification::SamplingMode& _internal_mode() const;
  ::CoreML::Specification::SamplingMode* _internal_mutable_mode();
  public:
  void unsafe_arena_set_allocated_mode(
      ::CoreML::Specification::SamplingMode* mode);
  ::CoreML::Specification::SamplingMode* unsafe_arena_release_mode();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ResizeBilinearLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > targetsize_;
  mutable std::atomic<int> _targetsize_cached_byte_size_;
  ::CoreML::Specification::SamplingMode* mode_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class CropResizeLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CropResizeLayerParams) */ {
 public:
  inline CropResizeLayerParams() : CropResizeLayerParams(nullptr) {}
  ~CropResizeLayerParams() override;
  explicit constexpr CropResizeLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  CropResizeLayerParams(const CropResizeLayerParams& from);
  CropResizeLayerParams(CropResizeLayerParams&& from) noexcept
    : CropResizeLayerParams() {
    *this = ::std::move(from);
  }

  inline CropResizeLayerParams& operator=(const CropResizeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline CropResizeLayerParams& operator=(CropResizeLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const CropResizeLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const CropResizeLayerParams* internal_default_instance() {
    return reinterpret_cast<const CropResizeLayerParams*>(
               &_CropResizeLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    68;

  friend void swap(CropResizeLayerParams& a, CropResizeLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(CropResizeLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(CropResizeLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  CropResizeLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<CropResizeLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const CropResizeLayerParams& from);
  void MergeFrom(const CropResizeLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CropResizeLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.CropResizeLayerParams";
  }
  protected:
  explicit CropResizeLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kTargetSizeFieldNumber = 1,
    kModeFieldNumber = 3,
    kBoxIndicesModeFieldNumber = 4,
    kNormalizedCoordinatesFieldNumber = 2,
    kSpatialScaleFieldNumber = 5,
  };
  // repeated uint64 targetSize = 1;
  int targetsize_size() const;
  private:
  int _internal_targetsize_size() const;
  public:
  void clear_targetsize();
  private:
  uint64_t _internal_targetsize(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_targetsize() const;
  void _internal_add_targetsize(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_targetsize();
  public:
  uint64_t targetsize(int index) const;
  void set_targetsize(int index, uint64_t value);
  void add_targetsize(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      targetsize() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_targetsize();

  // .CoreML.Specification.SamplingMode mode = 3;
  bool has_mode() const;
  private:
  bool _internal_has_mode() const;
  public:
  void clear_mode();
  const ::CoreML::Specification::SamplingMode& mode() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::SamplingMode* release_mode();
  ::CoreML::Specification::SamplingMode* mutable_mode();
  void set_allocated_mode(::CoreML::Specification::SamplingMode* mode);
  private:
  const ::CoreML::Specification::SamplingMode& _internal_mode() const;
  ::CoreML::Specification::SamplingMode* _internal_mutable_mode();
  public:
  void unsafe_arena_set_allocated_mode(
      ::CoreML::Specification::SamplingMode* mode);
  ::CoreML::Specification::SamplingMode* unsafe_arena_release_mode();

  // .CoreML.Specification.BoxCoordinatesMode boxIndicesMode = 4;
  bool has_boxindicesmode() const;
  private:
  bool _internal_has_boxindicesmode() const;
  public:
  void clear_boxindicesmode();
  const ::CoreML::Specification::BoxCoordinatesMode& boxindicesmode() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::BoxCoordinatesMode* release_boxindicesmode();
  ::CoreML::Specification::BoxCoordinatesMode* mutable_boxindicesmode();
  void set_allocated_boxindicesmode(::CoreML::Specification::BoxCoordinatesMode* boxindicesmode);
  private:
  const ::CoreML::Specification::BoxCoordinatesMode& _internal_boxindicesmode() const;
  ::CoreML::Specification::BoxCoordinatesMode* _internal_mutable_boxindicesmode();
  public:
  void unsafe_arena_set_allocated_boxindicesmode(
      ::CoreML::Specification::BoxCoordinatesMode* boxindicesmode);
  ::CoreML::Specification::BoxCoordinatesMode* unsafe_arena_release_boxindicesmode();

  // bool normalizedCoordinates = 2;
  void clear_normalizedcoordinates();
  bool normalizedcoordinates() const;
  void set_normalizedcoordinates(bool value);
  private:
  bool _internal_normalizedcoordinates() const;
  void _internal_set_normalizedcoordinates(bool value);
  public:

  // float spatialScale = 5;
  void clear_spatialscale();
  float spatialscale() const;
  void set_spatialscale(float value);
  private:
  float _internal_spatialscale() const;
  void _internal_set_spatialscale(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.CropResizeLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > targetsize_;
  mutable std::atomic<int> _targetsize_cached_byte_size_;
  ::CoreML::Specification::SamplingMode* mode_;
  ::CoreML::Specification::BoxCoordinatesMode* boxindicesmode_;
  bool normalizedcoordinates_;
  float spatialscale_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class BiasLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BiasLayerParams) */ {
 public:
  inline BiasLayerParams() : BiasLayerParams(nullptr) {}
  ~BiasLayerParams() override;
  explicit constexpr BiasLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  BiasLayerParams(const BiasLayerParams& from);
  BiasLayerParams(BiasLayerParams&& from) noexcept
    : BiasLayerParams() {
    *this = ::std::move(from);
  }

  inline BiasLayerParams& operator=(const BiasLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline BiasLayerParams& operator=(BiasLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const BiasLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const BiasLayerParams* internal_default_instance() {
    return reinterpret_cast<const BiasLayerParams*>(
               &_BiasLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    69;

  friend void swap(BiasLayerParams& a, BiasLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(BiasLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(BiasLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  BiasLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<BiasLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const BiasLayerParams& from);
  void MergeFrom(const BiasLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BiasLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.BiasLayerParams";
  }
  protected:
  explicit BiasLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kShapeFieldNumber = 1,
    kBiasFieldNumber = 2,
  };
  // repeated uint64 shape = 1;
  int shape_size() const;
  private:
  int _internal_shape_size() const;
  public:
  void clear_shape();
  private:
  uint64_t _internal_shape(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_shape() const;
  void _internal_add_shape(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_shape();
  public:
  uint64_t shape(int index) const;
  void set_shape(int index, uint64_t value);
  void add_shape(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      shape() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_shape();

  // .CoreML.Specification.WeightParams bias = 2;
  bool has_bias() const;
  private:
  bool _internal_has_bias() const;
  public:
  void clear_bias();
  const ::CoreML::Specification::WeightParams& bias() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_bias();
  ::CoreML::Specification::WeightParams* mutable_bias();
  void set_allocated_bias(::CoreML::Specification::WeightParams* bias);
  private:
  const ::CoreML::Specification::WeightParams& _internal_bias() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_bias();
  public:
  void unsafe_arena_set_allocated_bias(
      ::CoreML::Specification::WeightParams* bias);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_bias();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BiasLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > shape_;
  mutable std::atomic<int> _shape_cached_byte_size_;
  ::CoreML::Specification::WeightParams* bias_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ScaleLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ScaleLayerParams) */ {
 public:
  inline ScaleLayerParams() : ScaleLayerParams(nullptr) {}
  ~ScaleLayerParams() override;
  explicit constexpr ScaleLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ScaleLayerParams(const ScaleLayerParams& from);
  ScaleLayerParams(ScaleLayerParams&& from) noexcept
    : ScaleLayerParams() {
    *this = ::std::move(from);
  }

  inline ScaleLayerParams& operator=(const ScaleLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ScaleLayerParams& operator=(ScaleLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ScaleLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ScaleLayerParams* internal_default_instance() {
    return reinterpret_cast<const ScaleLayerParams*>(
               &_ScaleLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    70;

  friend void swap(ScaleLayerParams& a, ScaleLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ScaleLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ScaleLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ScaleLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ScaleLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ScaleLayerParams& from);
  void MergeFrom(const ScaleLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ScaleLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ScaleLayerParams";
  }
  protected:
  explicit ScaleLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kShapeScaleFieldNumber = 1,
    kShapeBiasFieldNumber = 4,
    kScaleFieldNumber = 2,
    kBiasFieldNumber = 5,
    kHasBiasFieldNumber = 3,
  };
  // repeated uint64 shapeScale = 1;
  int shapescale_size() const;
  private:
  int _internal_shapescale_size() const;
  public:
  void clear_shapescale();
  private:
  uint64_t _internal_shapescale(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_shapescale() const;
  void _internal_add_shapescale(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_shapescale();
  public:
  uint64_t shapescale(int index) const;
  void set_shapescale(int index, uint64_t value);
  void add_shapescale(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      shapescale() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_shapescale();

  // repeated uint64 shapeBias = 4;
  int shapebias_size() const;
  private:
  int _internal_shapebias_size() const;
  public:
  void clear_shapebias();
  private:
  uint64_t _internal_shapebias(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_shapebias() const;
  void _internal_add_shapebias(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_shapebias();
  public:
  uint64_t shapebias(int index) const;
  void set_shapebias(int index, uint64_t value);
  void add_shapebias(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      shapebias() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_shapebias();

  // .CoreML.Specification.WeightParams scale = 2;
  bool has_scale() const;
  private:
  bool _internal_has_scale() const;
  public:
  void clear_scale();
  const ::CoreML::Specification::WeightParams& scale() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_scale();
  ::CoreML::Specification::WeightParams* mutable_scale();
  void set_allocated_scale(::CoreML::Specification::WeightParams* scale);
  private:
  const ::CoreML::Specification::WeightParams& _internal_scale() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_scale();
  public:
  void unsafe_arena_set_allocated_scale(
      ::CoreML::Specification::WeightParams* scale);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_scale();

  // .CoreML.Specification.WeightParams bias = 5;
  bool has_bias() const;
  private:
  bool _internal_has_bias() const;
  public:
  void clear_bias();
  const ::CoreML::Specification::WeightParams& bias() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_bias();
  ::CoreML::Specification::WeightParams* mutable_bias();
  void set_allocated_bias(::CoreML::Specification::WeightParams* bias);
  private:
  const ::CoreML::Specification::WeightParams& _internal_bias() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_bias();
  public:
  void unsafe_arena_set_allocated_bias(
      ::CoreML::Specification::WeightParams* bias);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_bias();

  // bool hasBias = 3;
  void clear_hasbias();
  bool hasbias() const;
  void set_hasbias(bool value);
  private:
  bool _internal_hasbias() const;
  void _internal_set_hasbias(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ScaleLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > shapescale_;
  mutable std::atomic<int> _shapescale_cached_byte_size_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > shapebias_;
  mutable std::atomic<int> _shapebias_cached_byte_size_;
  ::CoreML::Specification::WeightParams* scale_;
  ::CoreML::Specification::WeightParams* bias_;
  bool hasbias_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class LoadConstantLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LoadConstantLayerParams) */ {
 public:
  inline LoadConstantLayerParams() : LoadConstantLayerParams(nullptr) {}
  ~LoadConstantLayerParams() override;
  explicit constexpr LoadConstantLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  LoadConstantLayerParams(const LoadConstantLayerParams& from);
  LoadConstantLayerParams(LoadConstantLayerParams&& from) noexcept
    : LoadConstantLayerParams() {
    *this = ::std::move(from);
  }

  inline LoadConstantLayerParams& operator=(const LoadConstantLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline LoadConstantLayerParams& operator=(LoadConstantLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const LoadConstantLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const LoadConstantLayerParams* internal_default_instance() {
    return reinterpret_cast<const LoadConstantLayerParams*>(
               &_LoadConstantLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    71;

  friend void swap(LoadConstantLayerParams& a, LoadConstantLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(LoadConstantLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(LoadConstantLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  LoadConstantLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<LoadConstantLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const LoadConstantLayerParams& from);
  void MergeFrom(const LoadConstantLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LoadConstantLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.LoadConstantLayerParams";
  }
  protected:
  explicit LoadConstantLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kShapeFieldNumber = 1,
    kDataFieldNumber = 2,
  };
  // repeated uint64 shape = 1;
  int shape_size() const;
  private:
  int _internal_shape_size() const;
  public:
  void clear_shape();
  private:
  uint64_t _internal_shape(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_shape() const;
  void _internal_add_shape(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_shape();
  public:
  uint64_t shape(int index) const;
  void set_shape(int index, uint64_t value);
  void add_shape(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      shape() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_shape();

  // .CoreML.Specification.WeightParams data = 2;
  bool has_data() const;
  private:
  bool _internal_has_data() const;
  public:
  void clear_data();
  const ::CoreML::Specification::WeightParams& data() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_data();
  ::CoreML::Specification::WeightParams* mutable_data();
  void set_allocated_data(::CoreML::Specification::WeightParams* data);
  private:
  const ::CoreML::Specification::WeightParams& _internal_data() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_data();
  public:
  void unsafe_arena_set_allocated_data(
      ::CoreML::Specification::WeightParams* data);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_data();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LoadConstantLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > shape_;
  mutable std::atomic<int> _shape_cached_byte_size_;
  ::CoreML::Specification::WeightParams* data_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class L2NormalizeLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.L2NormalizeLayerParams) */ {
 public:
  inline L2NormalizeLayerParams() : L2NormalizeLayerParams(nullptr) {}
  ~L2NormalizeLayerParams() override;
  explicit constexpr L2NormalizeLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  L2NormalizeLayerParams(const L2NormalizeLayerParams& from);
  L2NormalizeLayerParams(L2NormalizeLayerParams&& from) noexcept
    : L2NormalizeLayerParams() {
    *this = ::std::move(from);
  }

  inline L2NormalizeLayerParams& operator=(const L2NormalizeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline L2NormalizeLayerParams& operator=(L2NormalizeLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const L2NormalizeLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const L2NormalizeLayerParams* internal_default_instance() {
    return reinterpret_cast<const L2NormalizeLayerParams*>(
               &_L2NormalizeLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    72;

  friend void swap(L2NormalizeLayerParams& a, L2NormalizeLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(L2NormalizeLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(L2NormalizeLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  L2NormalizeLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<L2NormalizeLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const L2NormalizeLayerParams& from);
  void MergeFrom(const L2NormalizeLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(L2NormalizeLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.L2NormalizeLayerParams";
  }
  protected:
  explicit L2NormalizeLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kEpsilonFieldNumber = 1,
  };
  // float epsilon = 1;
  void clear_epsilon();
  float epsilon() const;
  void set_epsilon(float value);
  private:
  float _internal_epsilon() const;
  void _internal_set_epsilon(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.L2NormalizeLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float epsilon_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class FlattenLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.FlattenLayerParams) */ {
 public:
  inline FlattenLayerParams() : FlattenLayerParams(nullptr) {}
  ~FlattenLayerParams() override;
  explicit constexpr FlattenLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  FlattenLayerParams(const FlattenLayerParams& from);
  FlattenLayerParams(FlattenLayerParams&& from) noexcept
    : FlattenLayerParams() {
    *this = ::std::move(from);
  }

  inline FlattenLayerParams& operator=(const FlattenLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline FlattenLayerParams& operator=(FlattenLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const FlattenLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const FlattenLayerParams* internal_default_instance() {
    return reinterpret_cast<const FlattenLayerParams*>(
               &_FlattenLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    73;

  friend void swap(FlattenLayerParams& a, FlattenLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(FlattenLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(FlattenLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  FlattenLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<FlattenLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const FlattenLayerParams& from);
  void MergeFrom(const FlattenLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(FlattenLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.FlattenLayerParams";
  }
  protected:
  explicit FlattenLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  typedef FlattenLayerParams_FlattenOrder FlattenOrder;
  static constexpr FlattenOrder CHANNEL_FIRST =
    FlattenLayerParams_FlattenOrder_CHANNEL_FIRST;
  static constexpr FlattenOrder CHANNEL_LAST =
    FlattenLayerParams_FlattenOrder_CHANNEL_LAST;
  static inline bool FlattenOrder_IsValid(int value) {
    return FlattenLayerParams_FlattenOrder_IsValid(value);
  }
  static constexpr FlattenOrder FlattenOrder_MIN =
    FlattenLayerParams_FlattenOrder_FlattenOrder_MIN;
  static constexpr FlattenOrder FlattenOrder_MAX =
    FlattenLayerParams_FlattenOrder_FlattenOrder_MAX;
  static constexpr int FlattenOrder_ARRAYSIZE =
    FlattenLayerParams_FlattenOrder_FlattenOrder_ARRAYSIZE;
  template<typename T>
  static inline const std::string& FlattenOrder_Name(T enum_t_value) {
    static_assert(::std::is_same<T, FlattenOrder>::value ||
      ::std::is_integral<T>::value,
      "Incorrect type passed to function FlattenOrder_Name.");
    return FlattenLayerParams_FlattenOrder_Name(enum_t_value);
  }
  static inline bool FlattenOrder_Parse(::PROTOBUF_NAMESPACE_ID::ConstStringParam name,
      FlattenOrder* value) {
    return FlattenLayerParams_FlattenOrder_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  enum : int {
    kModeFieldNumber = 1,
  };
  // .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
  void clear_mode();
  ::CoreML::Specification::FlattenLayerParams_FlattenOrder mode() const;
  void set_mode(::CoreML::Specification::FlattenLayerParams_FlattenOrder value);
  private:
  ::CoreML::Specification::FlattenLayerParams_FlattenOrder _internal_mode() const;
  void _internal_set_mode(::CoreML::Specification::FlattenLayerParams_FlattenOrder value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.FlattenLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int mode_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ReshapeLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReshapeLayerParams) */ {
 public:
  inline ReshapeLayerParams() : ReshapeLayerParams(nullptr) {}
  ~ReshapeLayerParams() override;
  explicit constexpr ReshapeLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ReshapeLayerParams(const ReshapeLayerParams& from);
  ReshapeLayerParams(ReshapeLayerParams&& from) noexcept
    : ReshapeLayerParams() {
    *this = ::std::move(from);
  }

  inline ReshapeLayerParams& operator=(const ReshapeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ReshapeLayerParams& operator=(ReshapeLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ReshapeLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ReshapeLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReshapeLayerParams*>(
               &_ReshapeLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    74;

  friend void swap(ReshapeLayerParams& a, ReshapeLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ReshapeLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ReshapeLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ReshapeLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ReshapeLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ReshapeLayerParams& from);
  void MergeFrom(const ReshapeLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReshapeLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ReshapeLayerParams";
  }
  protected:
  explicit ReshapeLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  typedef ReshapeLayerParams_ReshapeOrder ReshapeOrder;
  static constexpr ReshapeOrder CHANNEL_FIRST =
    ReshapeLayerParams_ReshapeOrder_CHANNEL_FIRST;
  static constexpr ReshapeOrder CHANNEL_LAST =
    ReshapeLayerParams_ReshapeOrder_CHANNEL_LAST;
  static inline bool ReshapeOrder_IsValid(int value) {
    return ReshapeLayerParams_ReshapeOrder_IsValid(value);
  }
  static constexpr ReshapeOrder ReshapeOrder_MIN =
    ReshapeLayerParams_ReshapeOrder_ReshapeOrder_MIN;
  static constexpr ReshapeOrder ReshapeOrder_MAX =
    ReshapeLayerParams_ReshapeOrder_ReshapeOrder_MAX;
  static constexpr int ReshapeOrder_ARRAYSIZE =
    ReshapeLayerParams_ReshapeOrder_ReshapeOrder_ARRAYSIZE;
  template<typename T>
  static inline const std::string& ReshapeOrder_Name(T enum_t_value) {
    static_assert(::std::is_same<T, ReshapeOrder>::value ||
      ::std::is_integral<T>::value,
      "Incorrect type passed to function ReshapeOrder_Name.");
    return ReshapeLayerParams_ReshapeOrder_Name(enum_t_value);
  }
  static inline bool ReshapeOrder_Parse(::PROTOBUF_NAMESPACE_ID::ConstStringParam name,
      ReshapeOrder* value) {
    return ReshapeLayerParams_ReshapeOrder_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  enum : int {
    kTargetShapeFieldNumber = 1,
    kModeFieldNumber = 2,
  };
  // repeated int64 targetShape = 1;
  int targetshape_size() const;
  private:
  int _internal_targetshape_size() const;
  public:
  void clear_targetshape();
  private:
  int64_t _internal_targetshape(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_targetshape() const;
  void _internal_add_targetshape(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_targetshape();
  public:
  int64_t targetshape(int index) const;
  void set_targetshape(int index, int64_t value);
  void add_targetshape(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      targetshape() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_targetshape();

  // .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
  void clear_mode();
  ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder mode() const;
  void set_mode(::CoreML::Specification::ReshapeLayerParams_ReshapeOrder value);
  private:
  ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder _internal_mode() const;
  void _internal_set_mode(::CoreML::Specification::ReshapeLayerParams_ReshapeOrder value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReshapeLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > targetshape_;
  mutable std::atomic<int> _targetshape_cached_byte_size_;
  int mode_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class PermuteLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PermuteLayerParams) */ {
 public:
  inline PermuteLayerParams() : PermuteLayerParams(nullptr) {}
  ~PermuteLayerParams() override;
  explicit constexpr PermuteLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  PermuteLayerParams(const PermuteLayerParams& from);
  PermuteLayerParams(PermuteLayerParams&& from) noexcept
    : PermuteLayerParams() {
    *this = ::std::move(from);
  }

  inline PermuteLayerParams& operator=(const PermuteLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline PermuteLayerParams& operator=(PermuteLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const PermuteLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const PermuteLayerParams* internal_default_instance() {
    return reinterpret_cast<const PermuteLayerParams*>(
               &_PermuteLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    75;

  friend void swap(PermuteLayerParams& a, PermuteLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(PermuteLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(PermuteLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  PermuteLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<PermuteLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const PermuteLayerParams& from);
  void MergeFrom(const PermuteLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PermuteLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.PermuteLayerParams";
  }
  protected:
  explicit PermuteLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxisFieldNumber = 1,
  };
  // repeated uint64 axis = 1;
  int axis_size() const;
  private:
  int _internal_axis_size() const;
  public:
  void clear_axis();
  private:
  uint64_t _internal_axis(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_axis() const;
  void _internal_add_axis(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_axis();
  public:
  uint64_t axis(int index) const;
  void set_axis(int index, uint64_t value);
  void add_axis(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      axis() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_axis();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.PermuteLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > axis_;
  mutable std::atomic<int> _axis_cached_byte_size_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ReorganizeDataLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReorganizeDataLayerParams) */ {
 public:
  inline ReorganizeDataLayerParams() : ReorganizeDataLayerParams(nullptr) {}
  ~ReorganizeDataLayerParams() override;
  explicit constexpr ReorganizeDataLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ReorganizeDataLayerParams(const ReorganizeDataLayerParams& from);
  ReorganizeDataLayerParams(ReorganizeDataLayerParams&& from) noexcept
    : ReorganizeDataLayerParams() {
    *this = ::std::move(from);
  }

  inline ReorganizeDataLayerParams& operator=(const ReorganizeDataLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ReorganizeDataLayerParams& operator=(ReorganizeDataLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ReorganizeDataLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ReorganizeDataLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReorganizeDataLayerParams*>(
               &_ReorganizeDataLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    76;

  friend void swap(ReorganizeDataLayerParams& a, ReorganizeDataLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ReorganizeDataLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ReorganizeDataLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ReorganizeDataLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ReorganizeDataLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ReorganizeDataLayerParams& from);
  void MergeFrom(const ReorganizeDataLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReorganizeDataLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ReorganizeDataLayerParams";
  }
  protected:
  explicit ReorganizeDataLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  typedef ReorganizeDataLayerParams_ReorganizationType ReorganizationType;
  static constexpr ReorganizationType SPACE_TO_DEPTH =
    ReorganizeDataLayerParams_ReorganizationType_SPACE_TO_DEPTH;
  static constexpr ReorganizationType DEPTH_TO_SPACE =
    ReorganizeDataLayerParams_ReorganizationType_DEPTH_TO_SPACE;
  static constexpr ReorganizationType PIXEL_SHUFFLE =
    ReorganizeDataLayerParams_ReorganizationType_PIXEL_SHUFFLE;
  static inline bool ReorganizationType_IsValid(int value) {
    return ReorganizeDataLayerParams_ReorganizationType_IsValid(value);
  }
  static constexpr ReorganizationType ReorganizationType_MIN =
    ReorganizeDataLayerParams_ReorganizationType_ReorganizationType_MIN;
  static constexpr ReorganizationType ReorganizationType_MAX =
    ReorganizeDataLayerParams_ReorganizationType_ReorganizationType_MAX;
  static constexpr int ReorganizationType_ARRAYSIZE =
    ReorganizeDataLayerParams_ReorganizationType_ReorganizationType_ARRAYSIZE;
  template<typename T>
  static inline const std::string& ReorganizationType_Name(T enum_t_value) {
    static_assert(::std::is_same<T, ReorganizationType>::value ||
      ::std::is_integral<T>::value,
      "Incorrect type passed to function ReorganizationType_Name.");
    return ReorganizeDataLayerParams_ReorganizationType_Name(enum_t_value);
  }
  static inline bool ReorganizationType_Parse(::PROTOBUF_NAMESPACE_ID::ConstStringParam name,
      ReorganizationType* value) {
    return ReorganizeDataLayerParams_ReorganizationType_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  enum : int {
    kBlockSizeFieldNumber = 2,
    kModeFieldNumber = 1,
  };
  // uint64 blockSize = 2;
  void clear_blocksize();
  uint64_t blocksize() const;
  void set_blocksize(uint64_t value);
  private:
  uint64_t _internal_blocksize() const;
  void _internal_set_blocksize(uint64_t value);
  public:

  // .CoreML.Specification.ReorganizeDataLayerParams.ReorganizationType mode = 1;
  void clear_mode();
  ::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType mode() const;
  void set_mode(::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType value);
  private:
  ::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType _internal_mode() const;
  void _internal_set_mode(::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReorganizeDataLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  uint64_t blocksize_;
  int mode_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class SliceLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SliceLayerParams) */ {
 public:
  inline SliceLayerParams() : SliceLayerParams(nullptr) {}
  ~SliceLayerParams() override;
  explicit constexpr SliceLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SliceLayerParams(const SliceLayerParams& from);
  SliceLayerParams(SliceLayerParams&& from) noexcept
    : SliceLayerParams() {
    *this = ::std::move(from);
  }

  inline SliceLayerParams& operator=(const SliceLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline SliceLayerParams& operator=(SliceLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const SliceLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const SliceLayerParams* internal_default_instance() {
    return reinterpret_cast<const SliceLayerParams*>(
               &_SliceLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    77;

  friend void swap(SliceLayerParams& a, SliceLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(SliceLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SliceLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SliceLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SliceLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const SliceLayerParams& from);
  void MergeFrom(const SliceLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SliceLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.SliceLayerParams";
  }
  protected:
  explicit SliceLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  typedef SliceLayerParams_SliceAxis SliceAxis;
  static constexpr SliceAxis CHANNEL_AXIS =
    SliceLayerParams_SliceAxis_CHANNEL_AXIS;
  static constexpr SliceAxis HEIGHT_AXIS =
    SliceLayerParams_SliceAxis_HEIGHT_AXIS;
  static constexpr SliceAxis WIDTH_AXIS =
    SliceLayerParams_SliceAxis_WIDTH_AXIS;
  static inline bool SliceAxis_IsValid(int value) {
    return SliceLayerParams_SliceAxis_IsValid(value);
  }
  static constexpr SliceAxis SliceAxis_MIN =
    SliceLayerParams_SliceAxis_SliceAxis_MIN;
  static constexpr SliceAxis SliceAxis_MAX =
    SliceLayerParams_SliceAxis_SliceAxis_MAX;
  static constexpr int SliceAxis_ARRAYSIZE =
    SliceLayerParams_SliceAxis_SliceAxis_ARRAYSIZE;
  template<typename T>
  static inline const std::string& SliceAxis_Name(T enum_t_value) {
    static_assert(::std::is_same<T, SliceAxis>::value ||
      ::std::is_integral<T>::value,
      "Incorrect type passed to function SliceAxis_Name.");
    return SliceLayerParams_SliceAxis_Name(enum_t_value);
  }
  static inline bool SliceAxis_Parse(::PROTOBUF_NAMESPACE_ID::ConstStringParam name,
      SliceAxis* value) {
    return SliceLayerParams_SliceAxis_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  enum : int {
    kStartIndexFieldNumber = 1,
    kEndIndexFieldNumber = 2,
    kStrideFieldNumber = 3,
    kAxisFieldNumber = 4,
  };
  // int64 startIndex = 1;
  void clear_startindex();
  int64_t startindex() const;
  void set_startindex(int64_t value);
  private:
  int64_t _internal_startindex() const;
  void _internal_set_startindex(int64_t value);
  public:

  // int64 endIndex = 2;
  void clear_endindex();
  int64_t endindex() const;
  void set_endindex(int64_t value);
  private:
  int64_t _internal_endindex() const;
  void _internal_set_endindex(int64_t value);
  public:

  // uint64 stride = 3;
  void clear_stride();
  uint64_t stride() const;
  void set_stride(uint64_t value);
  private:
  uint64_t _internal_stride() const;
  void _internal_set_stride(uint64_t value);
  public:

  // .CoreML.Specification.SliceLayerParams.SliceAxis axis = 4;
  void clear_axis();
  ::CoreML::Specification::SliceLayerParams_SliceAxis axis() const;
  void set_axis(::CoreML::Specification::SliceLayerParams_SliceAxis value);
  private:
  ::CoreML::Specification::SliceLayerParams_SliceAxis _internal_axis() const;
  void _internal_set_axis(::CoreML::Specification::SliceLayerParams_SliceAxis value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SliceLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t startindex_;
  int64_t endindex_;
  uint64_t stride_;
  int axis_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ReduceLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceLayerParams) */ {
 public:
  inline ReduceLayerParams() : ReduceLayerParams(nullptr) {}
  ~ReduceLayerParams() override;
  explicit constexpr ReduceLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ReduceLayerParams(const ReduceLayerParams& from);
  ReduceLayerParams(ReduceLayerParams&& from) noexcept
    : ReduceLayerParams() {
    *this = ::std::move(from);
  }

  inline ReduceLayerParams& operator=(const ReduceLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ReduceLayerParams& operator=(ReduceLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ReduceLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ReduceLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceLayerParams*>(
               &_ReduceLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    78;

  friend void swap(ReduceLayerParams& a, ReduceLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ReduceLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ReduceLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ReduceLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ReduceLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ReduceLayerParams& from);
  void MergeFrom(const ReduceLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ReduceLayerParams";
  }
  protected:
  explicit ReduceLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  typedef ReduceLayerParams_ReduceOperation ReduceOperation;
  static constexpr ReduceOperation SUM =
    ReduceLayerParams_ReduceOperation_SUM;
  static constexpr ReduceOperation AVG =
    ReduceLayerParams_ReduceOperation_AVG;
  static constexpr ReduceOperation PROD =
    ReduceLayerParams_ReduceOperation_PROD;
  static constexpr ReduceOperation LOGSUM =
    ReduceLayerParams_ReduceOperation_LOGSUM;
  static constexpr ReduceOperation SUMSQUARE =
    ReduceLayerParams_ReduceOperation_SUMSQUARE;
  static constexpr ReduceOperation L1 =
    ReduceLayerParams_ReduceOperation_L1;
  static constexpr ReduceOperation L2 =
    ReduceLayerParams_ReduceOperation_L2;
  static constexpr ReduceOperation MAX =
    ReduceLayerParams_ReduceOperation_MAX;
  static constexpr ReduceOperation MIN =
    ReduceLayerParams_ReduceOperation_MIN;
  static constexpr ReduceOperation ARGMAX =
    ReduceLayerParams_ReduceOperation_ARGMAX;
  static inline bool ReduceOperation_IsValid(int value) {
    return ReduceLayerParams_ReduceOperation_IsValid(value);
  }
  static constexpr ReduceOperation ReduceOperation_MIN =
    ReduceLayerParams_ReduceOperation_ReduceOperation_MIN;
  static constexpr ReduceOperation ReduceOperation_MAX =
    ReduceLayerParams_ReduceOperation_ReduceOperation_MAX;
  static constexpr int ReduceOperation_ARRAYSIZE =
    ReduceLayerParams_ReduceOperation_ReduceOperation_ARRAYSIZE;
  template<typename T>
  static inline const std::string& ReduceOperation_Name(T enum_t_value) {
    static_assert(::std::is_same<T, ReduceOperation>::value ||
      ::std::is_integral<T>::value,
      "Incorrect type passed to function ReduceOperation_Name.");
    return ReduceLayerParams_ReduceOperation_Name(enum_t_value);
  }
  static inline bool ReduceOperation_Parse(::PROTOBUF_NAMESPACE_ID::ConstStringParam name,
      ReduceOperation* value) {
    return ReduceLayerParams_ReduceOperation_Parse(name, value);
  }

  typedef ReduceLayerParams_ReduceAxis ReduceAxis;
  static constexpr ReduceAxis CHW =
    ReduceLayerParams_ReduceAxis_CHW;
  static constexpr ReduceAxis HW =
    ReduceLayerParams_ReduceAxis_HW;
  static constexpr ReduceAxis C =
    ReduceLayerParams_ReduceAxis_C;
  static constexpr ReduceAxis H =
    ReduceLayerParams_ReduceAxis_H;
  static constexpr ReduceAxis W =
    ReduceLayerParams_ReduceAxis_W;
  static inline bool ReduceAxis_IsValid(int value) {
    return ReduceLayerParams_ReduceAxis_IsValid(value);
  }
  static constexpr ReduceAxis ReduceAxis_MIN =
    ReduceLayerParams_ReduceAxis_ReduceAxis_MIN;
  static constexpr ReduceAxis ReduceAxis_MAX =
    ReduceLayerParams_ReduceAxis_ReduceAxis_MAX;
  static constexpr int ReduceAxis_ARRAYSIZE =
    ReduceLayerParams_ReduceAxis_ReduceAxis_ARRAYSIZE;
  template<typename T>
  static inline const std::string& ReduceAxis_Name(T enum_t_value) {
    static_assert(::std::is_same<T, ReduceAxis>::value ||
      ::std::is_integral<T>::value,
      "Incorrect type passed to function ReduceAxis_Name.");
    return ReduceLayerParams_ReduceAxis_Name(enum_t_value);
  }
  static inline bool ReduceAxis_Parse(::PROTOBUF_NAMESPACE_ID::ConstStringParam name,
      ReduceAxis* value) {
    return ReduceLayerParams_ReduceAxis_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  enum : int {
    kModeFieldNumber = 1,
    kEpsilonFieldNumber = 2,
    kAxisFieldNumber = 3,
  };
  // .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
  void clear_mode();
  ::CoreML::Specification::ReduceLayerParams_ReduceOperation mode() const;
  void set_mode(::CoreML::Specification::ReduceLayerParams_ReduceOperation value);
  private:
  ::CoreML::Specification::ReduceLayerParams_ReduceOperation _internal_mode() const;
  void _internal_set_mode(::CoreML::Specification::ReduceLayerParams_ReduceOperation value);
  public:

  // float epsilon = 2;
  void clear_epsilon();
  float epsilon() const;
  void set_epsilon(float value);
  private:
  float _internal_epsilon() const;
  void _internal_set_epsilon(float value);
  public:

  // .CoreML.Specification.ReduceLayerParams.ReduceAxis axis = 3;
  void clear_axis();
  ::CoreML::Specification::ReduceLayerParams_ReduceAxis axis() const;
  void set_axis(::CoreML::Specification::ReduceLayerParams_ReduceAxis value);
  private:
  ::CoreML::Specification::ReduceLayerParams_ReduceAxis _internal_axis() const;
  void _internal_set_axis(::CoreML::Specification::ReduceLayerParams_ReduceAxis value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int mode_;
  float epsilon_;
  int axis_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class CropLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CropLayerParams) */ {
 public:
  inline CropLayerParams() : CropLayerParams(nullptr) {}
  ~CropLayerParams() override;
  explicit constexpr CropLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  CropLayerParams(const CropLayerParams& from);
  CropLayerParams(CropLayerParams&& from) noexcept
    : CropLayerParams() {
    *this = ::std::move(from);
  }

  inline CropLayerParams& operator=(const CropLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline CropLayerParams& operator=(CropLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const CropLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const CropLayerParams* internal_default_instance() {
    return reinterpret_cast<const CropLayerParams*>(
               &_CropLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    79;

  friend void swap(CropLayerParams& a, CropLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(CropLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(CropLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  CropLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<CropLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const CropLayerParams& from);
  void MergeFrom(const CropLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CropLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.CropLayerParams";
  }
  protected:
  explicit CropLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kOffsetFieldNumber = 5,
    kCropAmountsFieldNumber = 1,
  };
  // repeated uint64 offset = 5;
  int offset_size() const;
  private:
  int _internal_offset_size() const;
  public:
  void clear_offset();
  private:
  uint64_t _internal_offset(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_offset() const;
  void _internal_add_offset(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_offset();
  public:
  uint64_t offset(int index) const;
  void set_offset(int index, uint64_t value);
  void add_offset(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      offset() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_offset();

  // .CoreML.Specification.BorderAmounts cropAmounts = 1;
  bool has_cropamounts() const;
  private:
  bool _internal_has_cropamounts() const;
  public:
  void clear_cropamounts();
  const ::CoreML::Specification::BorderAmounts& cropamounts() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::BorderAmounts* release_cropamounts();
  ::CoreML::Specification::BorderAmounts* mutable_cropamounts();
  void set_allocated_cropamounts(::CoreML::Specification::BorderAmounts* cropamounts);
  private:
  const ::CoreML::Specification::BorderAmounts& _internal_cropamounts() const;
  ::CoreML::Specification::BorderAmounts* _internal_mutable_cropamounts();
  public:
  void unsafe_arena_set_allocated_cropamounts(
      ::CoreML::Specification::BorderAmounts* cropamounts);
  ::CoreML::Specification::BorderAmounts* unsafe_arena_release_cropamounts();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.CropLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > offset_;
  mutable std::atomic<int> _offset_cached_byte_size_;
  ::CoreML::Specification::BorderAmounts* cropamounts_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class AverageLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AverageLayerParams) */ {
 public:
  inline AverageLayerParams() : AverageLayerParams(nullptr) {}
  ~AverageLayerParams() override;
  explicit constexpr AverageLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  AverageLayerParams(const AverageLayerParams& from);
  AverageLayerParams(AverageLayerParams&& from) noexcept
    : AverageLayerParams() {
    *this = ::std::move(from);
  }

  inline AverageLayerParams& operator=(const AverageLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline AverageLayerParams& operator=(AverageLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const AverageLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const AverageLayerParams* internal_default_instance() {
    return reinterpret_cast<const AverageLayerParams*>(
               &_AverageLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    80;

  friend void swap(AverageLayerParams& a, AverageLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(AverageLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(AverageLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  AverageLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<AverageLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const AverageLayerParams& from);
  void MergeFrom(const AverageLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AverageLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.AverageLayerParams";
  }
  protected:
  explicit AverageLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AverageLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class MaxLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MaxLayerParams) */ {
 public:
  inline MaxLayerParams() : MaxLayerParams(nullptr) {}
  ~MaxLayerParams() override;
  explicit constexpr MaxLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  MaxLayerParams(const MaxLayerParams& from);
  MaxLayerParams(MaxLayerParams&& from) noexcept
    : MaxLayerParams() {
    *this = ::std::move(from);
  }

  inline MaxLayerParams& operator=(const MaxLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline MaxLayerParams& operator=(MaxLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const MaxLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const MaxLayerParams* internal_default_instance() {
    return reinterpret_cast<const MaxLayerParams*>(
               &_MaxLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    81;

  friend void swap(MaxLayerParams& a, MaxLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(MaxLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(MaxLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  MaxLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<MaxLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const MaxLayerParams& from);
  void MergeFrom(const MaxLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MaxLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.MaxLayerParams";
  }
  protected:
  explicit MaxLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MaxLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class MinLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MinLayerParams) */ {
 public:
  inline MinLayerParams() : MinLayerParams(nullptr) {}
  ~MinLayerParams() override;
  explicit constexpr MinLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  MinLayerParams(const MinLayerParams& from);
  MinLayerParams(MinLayerParams&& from) noexcept
    : MinLayerParams() {
    *this = ::std::move(from);
  }

  inline MinLayerParams& operator=(const MinLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline MinLayerParams& operator=(MinLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const MinLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const MinLayerParams* internal_default_instance() {
    return reinterpret_cast<const MinLayerParams*>(
               &_MinLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    82;

  friend void swap(MinLayerParams& a, MinLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(MinLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(MinLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  MinLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<MinLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const MinLayerParams& from);
  void MergeFrom(const MinLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MinLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.MinLayerParams";
  }
  protected:
  explicit MinLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MinLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class DotProductLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.DotProductLayerParams) */ {
 public:
  inline DotProductLayerParams() : DotProductLayerParams(nullptr) {}
  ~DotProductLayerParams() override;
  explicit constexpr DotProductLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  DotProductLayerParams(const DotProductLayerParams& from);
  DotProductLayerParams(DotProductLayerParams&& from) noexcept
    : DotProductLayerParams() {
    *this = ::std::move(from);
  }

  inline DotProductLayerParams& operator=(const DotProductLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline DotProductLayerParams& operator=(DotProductLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const DotProductLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const DotProductLayerParams* internal_default_instance() {
    return reinterpret_cast<const DotProductLayerParams*>(
               &_DotProductLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    83;

  friend void swap(DotProductLayerParams& a, DotProductLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(DotProductLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(DotProductLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  DotProductLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<DotProductLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const DotProductLayerParams& from);
  void MergeFrom(const DotProductLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(DotProductLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.DotProductLayerParams";
  }
  protected:
  explicit DotProductLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kCosineSimilarityFieldNumber = 1,
  };
  // bool cosineSimilarity = 1;
  void clear_cosinesimilarity();
  bool cosinesimilarity() const;
  void set_cosinesimilarity(bool value);
  private:
  bool _internal_cosinesimilarity() const;
  void _internal_set_cosinesimilarity(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.DotProductLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  bool cosinesimilarity_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class MeanVarianceNormalizeLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MeanVarianceNormalizeLayerParams) */ {
 public:
  inline MeanVarianceNormalizeLayerParams() : MeanVarianceNormalizeLayerParams(nullptr) {}
  ~MeanVarianceNormalizeLayerParams() override;
  explicit constexpr MeanVarianceNormalizeLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  MeanVarianceNormalizeLayerParams(const MeanVarianceNormalizeLayerParams& from);
  MeanVarianceNormalizeLayerParams(MeanVarianceNormalizeLayerParams&& from) noexcept
    : MeanVarianceNormalizeLayerParams() {
    *this = ::std::move(from);
  }

  inline MeanVarianceNormalizeLayerParams& operator=(const MeanVarianceNormalizeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline MeanVarianceNormalizeLayerParams& operator=(MeanVarianceNormalizeLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const MeanVarianceNormalizeLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const MeanVarianceNormalizeLayerParams* internal_default_instance() {
    return reinterpret_cast<const MeanVarianceNormalizeLayerParams*>(
               &_MeanVarianceNormalizeLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    84;

  friend void swap(MeanVarianceNormalizeLayerParams& a, MeanVarianceNormalizeLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(MeanVarianceNormalizeLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(MeanVarianceNormalizeLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  MeanVarianceNormalizeLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<MeanVarianceNormalizeLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const MeanVarianceNormalizeLayerParams& from);
  void MergeFrom(const MeanVarianceNormalizeLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MeanVarianceNormalizeLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.MeanVarianceNormalizeLayerParams";
  }
  protected:
  explicit MeanVarianceNormalizeLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAcrossChannelsFieldNumber = 1,
    kNormalizeVarianceFieldNumber = 2,
    kEpsilonFieldNumber = 3,
  };
  // bool acrossChannels = 1;
  void clear_acrosschannels();
  bool acrosschannels() const;
  void set_acrosschannels(bool value);
  private:
  bool _internal_acrosschannels() const;
  void _internal_set_acrosschannels(bool value);
  public:

  // bool normalizeVariance = 2;
  void clear_normalizevariance();
  bool normalizevariance() const;
  void set_normalizevariance(bool value);
  private:
  bool _internal_normalizevariance() const;
  void _internal_set_normalizevariance(bool value);
  public:

  // float epsilon = 3;
  void clear_epsilon();
  float epsilon() const;
  void set_epsilon(float value);
  private:
  float _internal_epsilon() const;
  void _internal_set_epsilon(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MeanVarianceNormalizeLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  bool acrosschannels_;
  bool normalizevariance_;
  float epsilon_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class SequenceRepeatLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SequenceRepeatLayerParams) */ {
 public:
  inline SequenceRepeatLayerParams() : SequenceRepeatLayerParams(nullptr) {}
  ~SequenceRepeatLayerParams() override;
  explicit constexpr SequenceRepeatLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SequenceRepeatLayerParams(const SequenceRepeatLayerParams& from);
  SequenceRepeatLayerParams(SequenceRepeatLayerParams&& from) noexcept
    : SequenceRepeatLayerParams() {
    *this = ::std::move(from);
  }

  inline SequenceRepeatLayerParams& operator=(const SequenceRepeatLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline SequenceRepeatLayerParams& operator=(SequenceRepeatLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const SequenceRepeatLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const SequenceRepeatLayerParams* internal_default_instance() {
    return reinterpret_cast<const SequenceRepeatLayerParams*>(
               &_SequenceRepeatLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    85;

  friend void swap(SequenceRepeatLayerParams& a, SequenceRepeatLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(SequenceRepeatLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SequenceRepeatLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SequenceRepeatLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SequenceRepeatLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const SequenceRepeatLayerParams& from);
  void MergeFrom(const SequenceRepeatLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SequenceRepeatLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.SequenceRepeatLayerParams";
  }
  protected:
  explicit SequenceRepeatLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kNRepetitionsFieldNumber = 1,
  };
  // uint64 nRepetitions = 1;
  void clear_nrepetitions();
  uint64_t nrepetitions() const;
  void set_nrepetitions(uint64_t value);
  private:
  uint64_t _internal_nrepetitions() const;
  void _internal_set_nrepetitions(uint64_t value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SequenceRepeatLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  uint64_t nrepetitions_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class SimpleRecurrentLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SimpleRecurrentLayerParams) */ {
 public:
  inline SimpleRecurrentLayerParams() : SimpleRecurrentLayerParams(nullptr) {}
  ~SimpleRecurrentLayerParams() override;
  explicit constexpr SimpleRecurrentLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SimpleRecurrentLayerParams(const SimpleRecurrentLayerParams& from);
  SimpleRecurrentLayerParams(SimpleRecurrentLayerParams&& from) noexcept
    : SimpleRecurrentLayerParams() {
    *this = ::std::move(from);
  }

  inline SimpleRecurrentLayerParams& operator=(const SimpleRecurrentLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline SimpleRecurrentLayerParams& operator=(SimpleRecurrentLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const SimpleRecurrentLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const SimpleRecurrentLayerParams* internal_default_instance() {
    return reinterpret_cast<const SimpleRecurrentLayerParams*>(
               &_SimpleRecurrentLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    86;

  friend void swap(SimpleRecurrentLayerParams& a, SimpleRecurrentLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(SimpleRecurrentLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SimpleRecurrentLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SimpleRecurrentLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SimpleRecurrentLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const SimpleRecurrentLayerParams& from);
  void MergeFrom(const SimpleRecurrentLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SimpleRecurrentLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.SimpleRecurrentLayerParams";
  }
  protected:
  explicit SimpleRecurrentLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kActivationFieldNumber = 10,
    kWeightMatrixFieldNumber = 30,
    kRecursionMatrixFieldNumber = 31,
    kBiasVectorFieldNumber = 32,
    kInputVectorSizeFieldNumber = 1,
    kOutputVectorSizeFieldNumber = 2,
    kSequenceOutputFieldNumber = 15,
    kHasBiasVectorFieldNumber = 20,
    kReverseInputFieldNumber = 100,
  };
  // .CoreML.Specification.ActivationParams activation = 10;
  bool has_activation() const;
  private:
  bool _internal_has_activation() const;
  public:
  void clear_activation();
  const ::CoreML::Specification::ActivationParams& activation() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::ActivationParams* release_activation();
  ::CoreML::Specification::ActivationParams* mutable_activation();
  void set_allocated_activation(::CoreML::Specification::ActivationParams* activation);
  private:
  const ::CoreML::Specification::ActivationParams& _internal_activation() const;
  ::CoreML::Specification::ActivationParams* _internal_mutable_activation();
  public:
  void unsafe_arena_set_allocated_activation(
      ::CoreML::Specification::ActivationParams* activation);
  ::CoreML::Specification::ActivationParams* unsafe_arena_release_activation();

  // .CoreML.Specification.WeightParams weightMatrix = 30;
  bool has_weightmatrix() const;
  private:
  bool _internal_has_weightmatrix() const;
  public:
  void clear_weightmatrix();
  const ::CoreML::Specification::WeightParams& weightmatrix() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_weightmatrix();
  ::CoreML::Specification::WeightParams* mutable_weightmatrix();
  void set_allocated_weightmatrix(::CoreML::Specification::WeightParams* weightmatrix);
  private:
  const ::CoreML::Specification::WeightParams& _internal_weightmatrix() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_weightmatrix();
  public:
  void unsafe_arena_set_allocated_weightmatrix(
      ::CoreML::Specification::WeightParams* weightmatrix);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_weightmatrix();

  // .CoreML.Specification.WeightParams recursionMatrix = 31;
  bool has_recursionmatrix() const;
  private:
  bool _internal_has_recursionmatrix() const;
  public:
  void clear_recursionmatrix();
  const ::CoreML::Specification::WeightParams& recursionmatrix() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_recursionmatrix();
  ::CoreML::Specification::WeightParams* mutable_recursionmatrix();
  void set_allocated_recursionmatrix(::CoreML::Specification::WeightParams* recursionmatrix);
  private:
  const ::CoreML::Specification::WeightParams& _internal_recursionmatrix() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_recursionmatrix();
  public:
  void unsafe_arena_set_allocated_recursionmatrix(
      ::CoreML::Specification::WeightParams* recursionmatrix);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_recursionmatrix();

  // .CoreML.Specification.WeightParams biasVector = 32;
  bool has_biasvector() const;
  private:
  bool _internal_has_biasvector() const;
  public:
  void clear_biasvector();
  const ::CoreML::Specification::WeightParams& biasvector() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_biasvector();
  ::CoreML::Specification::WeightParams* mutable_biasvector();
  void set_allocated_biasvector(::CoreML::Specification::WeightParams* biasvector);
  private:
  const ::CoreML::Specification::WeightParams& _internal_biasvector() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_biasvector();
  public:
  void unsafe_arena_set_allocated_biasvector(
      ::CoreML::Specification::WeightParams* biasvector);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_biasvector();

  // uint64 inputVectorSize = 1;
  void clear_inputvectorsize();
  uint64_t inputvectorsize() const;
  void set_inputvectorsize(uint64_t value);
  private:
  uint64_t _internal_inputvectorsize() const;
  void _internal_set_inputvectorsize(uint64_t value);
  public:

  // uint64 outputVectorSize = 2;
  void clear_outputvectorsize();
  uint64_t outputvectorsize() const;
  void set_outputvectorsize(uint64_t value);
  private:
  uint64_t _internal_outputvectorsize() const;
  void _internal_set_outputvectorsize(uint64_t value);
  public:

  // bool sequenceOutput = 15;
  void clear_sequenceoutput();
  bool sequenceoutput() const;
  void set_sequenceoutput(bool value);
  private:
  bool _internal_sequenceoutput() const;
  void _internal_set_sequenceoutput(bool value);
  public:

  // bool hasBiasVector = 20;
  void clear_hasbiasvector();
  bool hasbiasvector() const;
  void set_hasbiasvector(bool value);
  private:
  bool _internal_hasbiasvector() const;
  void _internal_set_hasbiasvector(bool value);
  public:

  // bool reverseInput = 100;
  void clear_reverseinput();
  bool reverseinput() const;
  void set_reverseinput(bool value);
  private:
  bool _internal_reverseinput() const;
  void _internal_set_reverseinput(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SimpleRecurrentLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::CoreML::Specification::ActivationParams* activation_;
  ::CoreML::Specification::WeightParams* weightmatrix_;
  ::CoreML::Specification::WeightParams* recursionmatrix_;
  ::CoreML::Specification::WeightParams* biasvector_;
  uint64_t inputvectorsize_;
  uint64_t outputvectorsize_;
  bool sequenceoutput_;
  bool hasbiasvector_;
  bool reverseinput_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class GRULayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.GRULayerParams) */ {
 public:
  inline GRULayerParams() : GRULayerParams(nullptr) {}
  ~GRULayerParams() override;
  explicit constexpr GRULayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  GRULayerParams(const GRULayerParams& from);
  GRULayerParams(GRULayerParams&& from) noexcept
    : GRULayerParams() {
    *this = ::std::move(from);
  }

  inline GRULayerParams& operator=(const GRULayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline GRULayerParams& operator=(GRULayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const GRULayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const GRULayerParams* internal_default_instance() {
    return reinterpret_cast<const GRULayerParams*>(
               &_GRULayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    87;

  friend void swap(GRULayerParams& a, GRULayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(GRULayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(GRULayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  GRULayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<GRULayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const GRULayerParams& from);
  void MergeFrom(const GRULayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(GRULayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.GRULayerParams";
  }
  protected:
  explicit GRULayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kActivationsFieldNumber = 10,
    kUpdateGateWeightMatrixFieldNumber = 30,
    kResetGateWeightMatrixFieldNumber = 31,
    kOutputGateWeightMatrixFieldNumber = 32,
    kUpdateGateRecursionMatrixFieldNumber = 50,
    kResetGateRecursionMatrixFieldNumber = 51,
    kOutputGateRecursionMatrixFieldNumber = 52,
    kUpdateGateBiasVectorFieldNumber = 70,
    kResetGateBiasVectorFieldNumber = 71,
    kOutputGateBiasVectorFieldNumber = 72,
    kInputVectorSizeFieldNumber = 1,
    kOutputVectorSizeFieldNumber = 2,
    kSequenceOutputFieldNumber = 15,
    kHasBiasVectorsFieldNumber = 20,
    kReverseInputFieldNumber = 100,
  };
  // repeated .CoreML.Specification.ActivationParams activations = 10;
  int activations_size() const;
  private:
  int _internal_activations_size() const;
  public:
  void clear_activations();
  ::CoreML::Specification::ActivationParams* mutable_activations(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
      mutable_activations();
  private:
  const ::CoreML::Specification::ActivationParams& _internal_activations(int index) const;
  ::CoreML::Specification::ActivationParams* _internal_add_activations();
  public:
  const ::CoreML::Specification::ActivationParams& activations(int index) const;
  ::CoreML::Specification::ActivationParams* add_activations();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
      activations() const;

  // .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
  bool has_updategateweightmatrix() const;
  private:
  bool _internal_has_updategateweightmatrix() const;
  public:
  void clear_updategateweightmatrix();
  const ::CoreML::Specification::WeightParams& updategateweightmatrix() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_updategateweightmatrix();
  ::CoreML::Specification::WeightParams* mutable_updategateweightmatrix();
  void set_allocated_updategateweightmatrix(::CoreML::Specification::WeightParams* updategateweightmatrix);
  private:
  const ::CoreML::Specification::WeightParams& _internal_updategateweightmatrix() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_updategateweightmatrix();
  public:
  void unsafe_arena_set_allocated_updategateweightmatrix(
      ::CoreML::Specification::WeightParams* updategateweightmatrix);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_updategateweightmatrix();

  // .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
  bool has_resetgateweightmatrix() const;
  private:
  bool _internal_has_resetgateweightmatrix() const;
  public:
  void clear_resetgateweightmatrix();
  const ::CoreML::Specification::WeightParams& resetgateweightmatrix() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_resetgateweightmatrix();
  ::CoreML::Specification::WeightParams* mutable_resetgateweightmatrix();
  void set_allocated_resetgateweightmatrix(::CoreML::Specification::WeightParams* resetgateweightmatrix);
  private:
  const ::CoreML::Specification::WeightParams& _internal_resetgateweightmatrix() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_resetgateweightmatrix();
  public:
  void unsafe_arena_set_allocated_resetgateweightmatrix(
      ::CoreML::Specification::WeightParams* resetgateweightmatrix);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_resetgateweightmatrix();

  // .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
  bool has_outputgateweightmatrix() const;
  private:
  bool _internal_has_outputgateweightmatrix() const;
  public:
  void clear_outputgateweightmatrix();
  const ::CoreML::Specification::WeightParams& outputgateweightmatrix() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_outputgateweightmatrix();
  ::CoreML::Specification::WeightParams* mutable_outputgateweightmatrix();
  void set_allocated_outputgateweightmatrix(::CoreML::Specification::WeightParams* outputgateweightmatrix);
  private:
  const ::CoreML::Specification::WeightParams& _internal_outputgateweightmatrix() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_outputgateweightmatrix();
  public:
  void unsafe_arena_set_allocated_outputgateweightmatrix(
      ::CoreML::Specification::WeightParams* outputgateweightmatrix);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_outputgateweightmatrix();

  // .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
  bool has_updategaterecursionmatrix() const;
  private:
  bool _internal_has_updategaterecursionmatrix() const;
  public:
  void clear_updategaterecursionmatrix();
  const ::CoreML::Specification::WeightParams& updategaterecursionmatrix() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_updategaterecursionmatrix();
  ::CoreML::Specification::WeightParams* mutable_updategaterecursionmatrix();
  void set_allocated_updategaterecursionmatrix(::CoreML::Specification::WeightParams* updategaterecursionmatrix);
  private:
  const ::CoreML::Specification::WeightParams& _internal_updategaterecursionmatrix() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_updategaterecursionmatrix();
  public:
  void unsafe_arena_set_allocated_updategaterecursionmatrix(
      ::CoreML::Specification::WeightParams* updategaterecursionmatrix);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_updategaterecursionmatrix();

  // .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
  bool has_resetgaterecursionmatrix() const;
  private:
  bool _internal_has_resetgaterecursionmatrix() const;
  public:
  void clear_resetgaterecursionmatrix();
  const ::CoreML::Specification::WeightParams& resetgaterecursionmatrix() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_resetgaterecursionmatrix();
  ::CoreML::Specification::WeightParams* mutable_resetgaterecursionmatrix();
  void set_allocated_resetgaterecursionmatrix(::CoreML::Specification::WeightParams* resetgaterecursionmatrix);
  private:
  const ::CoreML::Specification::WeightParams& _internal_resetgaterecursionmatrix() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_resetgaterecursionmatrix();
  public:
  void unsafe_arena_set_allocated_resetgaterecursionmatrix(
      ::CoreML::Specification::WeightParams* resetgaterecursionmatrix);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_resetgaterecursionmatrix();

  // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
  bool has_outputgaterecursionmatrix() const;
  private:
  bool _internal_has_outputgaterecursionmatrix() const;
  public:
  void clear_outputgaterecursionmatrix();
  const ::CoreML::Specification::WeightParams& outputgaterecursionmatrix() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_outputgaterecursionmatrix();
  ::CoreML::Specification::WeightParams* mutable_outputgaterecursionmatrix();
  void set_allocated_outputgaterecursionmatrix(::CoreML::Specification::WeightParams* outputgaterecursionmatrix);
  private:
  const ::CoreML::Specification::WeightParams& _internal_outputgaterecursionmatrix() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_outputgaterecursionmatrix();
  public:
  void unsafe_arena_set_allocated_outputgaterecursionmatrix(
      ::CoreML::Specification::WeightParams* outputgaterecursionmatrix);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_outputgaterecursionmatrix();

  // .CoreML.Specification.WeightParams updateGateBiasVector = 70;
  bool has_updategatebiasvector() const;
  private:
  bool _internal_has_updategatebiasvector() const;
  public:
  void clear_updategatebiasvector();
  const ::CoreML::Specification::WeightParams& updategatebiasvector() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_updategatebiasvector();
  ::CoreML::Specification::WeightParams* mutable_updategatebiasvector();
  void set_allocated_updategatebiasvector(::CoreML::Specification::WeightParams* updategatebiasvector);
  private:
  const ::CoreML::Specification::WeightParams& _internal_updategatebiasvector() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_updategatebiasvector();
  public:
  void unsafe_arena_set_allocated_updategatebiasvector(
      ::CoreML::Specification::WeightParams* updategatebiasvector);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_updategatebiasvector();

  // .CoreML.Specification.WeightParams resetGateBiasVector = 71;
  bool has_resetgatebiasvector() const;
  private:
  bool _internal_has_resetgatebiasvector() const;
  public:
  void clear_resetgatebiasvector();
  const ::CoreML::Specification::WeightParams& resetgatebiasvector() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_resetgatebiasvector();
  ::CoreML::Specification::WeightParams* mutable_resetgatebiasvector();
  void set_allocated_resetgatebiasvector(::CoreML::Specification::WeightParams* resetgatebiasvector);
  private:
  const ::CoreML::Specification::WeightParams& _internal_resetgatebiasvector() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_resetgatebiasvector();
  public:
  void unsafe_arena_set_allocated_resetgatebiasvector(
      ::CoreML::Specification::WeightParams* resetgatebiasvector);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_resetgatebiasvector();

  // .CoreML.Specification.WeightParams outputGateBiasVector = 72;
  bool has_outputgatebiasvector() const;
  private:
  bool _internal_has_outputgatebiasvector() const;
  public:
  void clear_outputgatebiasvector();
  const ::CoreML::Specification::WeightParams& outputgatebiasvector() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_outputgatebiasvector();
  ::CoreML::Specification::WeightParams* mutable_outputgatebiasvector();
  void set_allocated_outputgatebiasvector(::CoreML::Specification::WeightParams* outputgatebiasvector);
  private:
  const ::CoreML::Specification::WeightParams& _internal_outputgatebiasvector() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_outputgatebiasvector();
  public:
  void unsafe_arena_set_allocated_outputgatebiasvector(
      ::CoreML::Specification::WeightParams* outputgatebiasvector);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_outputgatebiasvector();

  // uint64 inputVectorSize = 1;
  void clear_inputvectorsize();
  uint64_t inputvectorsize() const;
  void set_inputvectorsize(uint64_t value);
  private:
  uint64_t _internal_inputvectorsize() const;
  void _internal_set_inputvectorsize(uint64_t value);
  public:

  // uint64 outputVectorSize = 2;
  void clear_outputvectorsize();
  uint64_t outputvectorsize() const;
  void set_outputvectorsize(uint64_t value);
  private:
  uint64_t _internal_outputvectorsize() const;
  void _internal_set_outputvectorsize(uint64_t value);
  public:

  // bool sequenceOutput = 15;
  void clear_sequenceoutput();
  bool sequenceoutput() const;
  void set_sequenceoutput(bool value);
  private:
  bool _internal_sequenceoutput() const;
  void _internal_set_sequenceoutput(bool value);
  public:

  // bool hasBiasVectors = 20;
  void clear_hasbiasvectors();
  bool hasbiasvectors() const;
  void set_hasbiasvectors(bool value);
  private:
  bool _internal_hasbiasvectors() const;
  void _internal_set_hasbiasvectors(bool value);
  public:

  // bool reverseInput = 100;
  void clear_reverseinput();
  bool reverseinput() const;
  void set_reverseinput(bool value);
  private:
  bool _internal_reverseinput() const;
  void _internal_set_reverseinput(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.GRULayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::ActivationParams > activations_;
  ::CoreML::Specification::WeightParams* updategateweightmatrix_;
  ::CoreML::Specification::WeightParams* resetgateweightmatrix_;
  ::CoreML::Specification::WeightParams* outputgateweightmatrix_;
  ::CoreML::Specification::WeightParams* updategaterecursionmatrix_;
  ::CoreML::Specification::WeightParams* resetgaterecursionmatrix_;
  ::CoreML::Specification::WeightParams* outputgaterecursionmatrix_;
  ::CoreML::Specification::WeightParams* updategatebiasvector_;
  ::CoreML::Specification::WeightParams* resetgatebiasvector_;
  ::CoreML::Specification::WeightParams* outputgatebiasvector_;
  uint64_t inputvectorsize_;
  uint64_t outputvectorsize_;
  bool sequenceoutput_;
  bool hasbiasvectors_;
  bool reverseinput_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class LSTMParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LSTMParams) */ {
 public:
  inline LSTMParams() : LSTMParams(nullptr) {}
  ~LSTMParams() override;
  explicit constexpr LSTMParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  LSTMParams(const LSTMParams& from);
  LSTMParams(LSTMParams&& from) noexcept
    : LSTMParams() {
    *this = ::std::move(from);
  }

  inline LSTMParams& operator=(const LSTMParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline LSTMParams& operator=(LSTMParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const LSTMParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const LSTMParams* internal_default_instance() {
    return reinterpret_cast<const LSTMParams*>(
               &_LSTMParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    88;

  friend void swap(LSTMParams& a, LSTMParams& b) {
    a.Swap(&b);
  }
  inline void Swap(LSTMParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(LSTMParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  LSTMParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<LSTMParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const LSTMParams& from);
  void MergeFrom(const LSTMParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LSTMParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.LSTMParams";
  }
  protected:
  explicit LSTMParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kCellClipThresholdFieldNumber = 60,
    kSequenceOutputFieldNumber = 10,
    kHasBiasVectorsFieldNumber = 20,
    kForgetBiasFieldNumber = 30,
    kHasPeepholeVectorsFieldNumber = 40,
    kCoupledInputAndForgetGateFieldNumber = 50,
  };
  // float cellClipThreshold = 60;
  void clear_cellclipthreshold();
  float cellclipthreshold() const;
  void set_cellclipthreshold(float value);
  private:
  float _internal_cellclipthreshold() const;
  void _internal_set_cellclipthreshold(float value);
  public:

  // bool sequenceOutput = 10;
  void clear_sequenceoutput();
  bool sequenceoutput() const;
  void set_sequenceoutput(bool value);
  private:
  bool _internal_sequenceoutput() const;
  void _internal_set_sequenceoutput(bool value);
  public:

  // bool hasBiasVectors = 20;
  void clear_hasbiasvectors();
  bool hasbiasvectors() const;
  void set_hasbiasvectors(bool value);
  private:
  bool _internal_hasbiasvectors() const;
  void _internal_set_hasbiasvectors(bool value);
  public:

  // bool forgetBias = 30;
  void clear_forgetbias();
  bool forgetbias() const;
  void set_forgetbias(bool value);
  private:
  bool _internal_forgetbias() const;
  void _internal_set_forgetbias(bool value);
  public:

  // bool hasPeepholeVectors = 40;
  void clear_haspeepholevectors();
  bool haspeepholevectors() const;
  void set_haspeepholevectors(bool value);
  private:
  bool _internal_haspeepholevectors() const;
  void _internal_set_haspeepholevectors(bool value);
  public:

  // bool coupledInputAndForgetGate = 50;
  void clear_coupledinputandforgetgate();
  bool coupledinputandforgetgate() const;
  void set_coupledinputandforgetgate(bool value);
  private:
  bool _internal_coupledinputandforgetgate() const;
  void _internal_set_coupledinputandforgetgate(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LSTMParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float cellclipthreshold_;
  bool sequenceoutput_;
  bool hasbiasvectors_;
  bool forgetbias_;
  bool haspeepholevectors_;
  bool coupledinputandforgetgate_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class LSTMWeightParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LSTMWeightParams) */ {
 public:
  inline LSTMWeightParams() : LSTMWeightParams(nullptr) {}
  ~LSTMWeightParams() override;
  explicit constexpr LSTMWeightParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  LSTMWeightParams(const LSTMWeightParams& from);
  LSTMWeightParams(LSTMWeightParams&& from) noexcept
    : LSTMWeightParams() {
    *this = ::std::move(from);
  }

  inline LSTMWeightParams& operator=(const LSTMWeightParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline LSTMWeightParams& operator=(LSTMWeightParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const LSTMWeightParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const LSTMWeightParams* internal_default_instance() {
    return reinterpret_cast<const LSTMWeightParams*>(
               &_LSTMWeightParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    89;

  friend void swap(LSTMWeightParams& a, LSTMWeightParams& b) {
    a.Swap(&b);
  }
  inline void Swap(LSTMWeightParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(LSTMWeightParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  LSTMWeightParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<LSTMWeightParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const LSTMWeightParams& from);
  void MergeFrom(const LSTMWeightParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LSTMWeightParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.LSTMWeightParams";
  }
  protected:
  explicit LSTMWeightParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kInputGateWeightMatrixFieldNumber = 1,
    kForgetGateWeightMatrixFieldNumber = 2,
    kBlockInputWeightMatrixFieldNumber = 3,
    kOutputGateWeightMatrixFieldNumber = 4,
    kInputGateRecursionMatrixFieldNumber = 20,
    kForgetGateRecursionMatrixFieldNumber = 21,
    kBlockInputRecursionMatrixFieldNumber = 22,
    kOutputGateRecursionMatrixFieldNumber = 23,
    kInputGateBiasVectorFieldNumber = 40,
    kForgetGateBiasVectorFieldNumber = 41,
    kBlockInputBiasVectorFieldNumber = 42,
    kOutputGateBiasVectorFieldNumber = 43,
    kInputGatePeepholeVectorFieldNumber = 60,
    kForgetGatePeepholeVectorFieldNumber = 61,
    kOutputGatePeepholeVectorFieldNumber = 62,
  };
  // .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
  bool has_inputgateweightmatrix() const;
  private:
  bool _internal_has_inputgateweightmatrix() const;
  public:
  void clear_inputgateweightmatrix();
  const ::CoreML::Specification::WeightParams& inputgateweightmatrix() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_inputgateweightmatrix();
  ::CoreML::Specification::WeightParams* mutable_inputgateweightmatrix();
  void set_allocated_inputgateweightmatrix(::CoreML::Specification::WeightParams* inputgateweightmatrix);
  private:
  const ::CoreML::Specification::WeightParams& _internal_inputgateweightmatrix() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_inputgateweightmatrix();
  public:
  void unsafe_arena_set_allocated_inputgateweightmatrix(
      ::CoreML::Specification::WeightParams* inputgateweightmatrix);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_inputgateweightmatrix();

  // .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
  bool has_forgetgateweightmatrix() const;
  private:
  bool _internal_has_forgetgateweightmatrix() const;
  public:
  void clear_forgetgateweightmatrix();
  const ::CoreML::Specification::WeightParams& forgetgateweightmatrix() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_forgetgateweightmatrix();
  ::CoreML::Specification::WeightParams* mutable_forgetgateweightmatrix();
  void set_allocated_forgetgateweightmatrix(::CoreML::Specification::WeightParams* forgetgateweightmatrix);
  private:
  const ::CoreML::Specification::WeightParams& _internal_forgetgateweightmatrix() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_forgetgateweightmatrix();
  public:
  void unsafe_arena_set_allocated_forgetgateweightmatrix(
      ::CoreML::Specification::WeightParams* forgetgateweightmatrix);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_forgetgateweightmatrix();

  // .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
  bool has_blockinputweightmatrix() const;
  private:
  bool _internal_has_blockinputweightmatrix() const;
  public:
  void clear_blockinputweightmatrix();
  const ::CoreML::Specification::WeightParams& blockinputweightmatrix() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_blockinputweightmatrix();
  ::CoreML::Specification::WeightParams* mutable_blockinputweightmatrix();
  void set_allocated_blockinputweightmatrix(::CoreML::Specification::WeightParams* blockinputweightmatrix);
  private:
  const ::CoreML::Specification::WeightParams& _internal_blockinputweightmatrix() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_blockinputweightmatrix();
  public:
  void unsafe_arena_set_allocated_blockinputweightmatrix(
      ::CoreML::Specification::WeightParams* blockinputweightmatrix);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_blockinputweightmatrix();

  // .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
  bool has_outputgateweightmatrix() const;
  private:
  bool _internal_has_outputgateweightmatrix() const;
  public:
  void clear_outputgateweightmatrix();
  const ::CoreML::Specification::WeightParams& outputgateweightmatrix() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_outputgateweightmatrix();
  ::CoreML::Specification::WeightParams* mutable_outputgateweightmatrix();
  void set_allocated_outputgateweightmatrix(::CoreML::Specification::WeightParams* outputgateweightmatrix);
  private:
  const ::CoreML::Specification::WeightParams& _internal_outputgateweightmatrix() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_outputgateweightmatrix();
  public:
  void unsafe_arena_set_allocated_outputgateweightmatrix(
      ::CoreML::Specification::WeightParams* outputgateweightmatrix);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_outputgateweightmatrix();

  // .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
  bool has_inputgaterecursionmatrix() const;
  private:
  bool _internal_has_inputgaterecursionmatrix() const;
  public:
  void clear_inputgaterecursionmatrix();
  const ::CoreML::Specification::WeightParams& inputgaterecursionmatrix() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_inputgaterecursionmatrix();
  ::CoreML::Specification::WeightParams* mutable_inputgaterecursionmatrix();
  void set_allocated_inputgaterecursionmatrix(::CoreML::Specification::WeightParams* inputgaterecursionmatrix);
  private:
  const ::CoreML::Specification::WeightParams& _internal_inputgaterecursionmatrix() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_inputgaterecursionmatrix();
  public:
  void unsafe_arena_set_allocated_inputgaterecursionmatrix(
      ::CoreML::Specification::WeightParams* inputgaterecursionmatrix);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_inputgaterecursionmatrix();

  // .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
  bool has_forgetgaterecursionmatrix() const;
  private:
  bool _internal_has_forgetgaterecursionmatrix() const;
  public:
  void clear_forgetgaterecursionmatrix();
  const ::CoreML::Specification::WeightParams& forgetgaterecursionmatrix() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_forgetgaterecursionmatrix();
  ::CoreML::Specification::WeightParams* mutable_forgetgaterecursionmatrix();
  void set_allocated_forgetgaterecursionmatrix(::CoreML::Specification::WeightParams* forgetgaterecursionmatrix);
  private:
  const ::CoreML::Specification::WeightParams& _internal_forgetgaterecursionmatrix() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_forgetgaterecursionmatrix();
  public:
  void unsafe_arena_set_allocated_forgetgaterecursionmatrix(
      ::CoreML::Specification::WeightParams* forgetgaterecursionmatrix);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_forgetgaterecursionmatrix();

  // .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
  bool has_blockinputrecursionmatrix() const;
  private:
  bool _internal_has_blockinputrecursionmatrix() const;
  public:
  void clear_blockinputrecursionmatrix();
  const ::CoreML::Specification::WeightParams& blockinputrecursionmatrix() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_blockinputrecursionmatrix();
  ::CoreML::Specification::WeightParams* mutable_blockinputrecursionmatrix();
  void set_allocated_blockinputrecursionmatrix(::CoreML::Specification::WeightParams* blockinputrecursionmatrix);
  private:
  const ::CoreML::Specification::WeightParams& _internal_blockinputrecursionmatrix() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_blockinputrecursionmatrix();
  public:
  void unsafe_arena_set_allocated_blockinputrecursionmatrix(
      ::CoreML::Specification::WeightParams* blockinputrecursionmatrix);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_blockinputrecursionmatrix();

  // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
  bool has_outputgaterecursionmatrix() const;
  private:
  bool _internal_has_outputgaterecursionmatrix() const;
  public:
  void clear_outputgaterecursionmatrix();
  const ::CoreML::Specification::WeightParams& outputgaterecursionmatrix() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_outputgaterecursionmatrix();
  ::CoreML::Specification::WeightParams* mutable_outputgaterecursionmatrix();
  void set_allocated_outputgaterecursionmatrix(::CoreML::Specification::WeightParams* outputgaterecursionmatrix);
  private:
  const ::CoreML::Specification::WeightParams& _internal_outputgaterecursionmatrix() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_outputgaterecursionmatrix();
  public:
  void unsafe_arena_set_allocated_outputgaterecursionmatrix(
      ::CoreML::Specification::WeightParams* outputgaterecursionmatrix);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_outputgaterecursionmatrix();

  // .CoreML.Specification.WeightParams inputGateBiasVector = 40;
  bool has_inputgatebiasvector() const;
  private:
  bool _internal_has_inputgatebiasvector() const;
  public:
  void clear_inputgatebiasvector();
  const ::CoreML::Specification::WeightParams& inputgatebiasvector() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_inputgatebiasvector();
  ::CoreML::Specification::WeightParams* mutable_inputgatebiasvector();
  void set_allocated_inputgatebiasvector(::CoreML::Specification::WeightParams* inputgatebiasvector);
  private:
  const ::CoreML::Specification::WeightParams& _internal_inputgatebiasvector() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_inputgatebiasvector();
  public:
  void unsafe_arena_set_allocated_inputgatebiasvector(
      ::CoreML::Specification::WeightParams* inputgatebiasvector);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_inputgatebiasvector();

  // .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
  bool has_forgetgatebiasvector() const;
  private:
  bool _internal_has_forgetgatebiasvector() const;
  public:
  void clear_forgetgatebiasvector();
  const ::CoreML::Specification::WeightParams& forgetgatebiasvector() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_forgetgatebiasvector();
  ::CoreML::Specification::WeightParams* mutable_forgetgatebiasvector();
  void set_allocated_forgetgatebiasvector(::CoreML::Specification::WeightParams* forgetgatebiasvector);
  private:
  const ::CoreML::Specification::WeightParams& _internal_forgetgatebiasvector() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_forgetgatebiasvector();
  public:
  void unsafe_arena_set_allocated_forgetgatebiasvector(
      ::CoreML::Specification::WeightParams* forgetgatebiasvector);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_forgetgatebiasvector();

  // .CoreML.Specification.WeightParams blockInputBiasVector = 42;
  bool has_blockinputbiasvector() const;
  private:
  bool _internal_has_blockinputbiasvector() const;
  public:
  void clear_blockinputbiasvector();
  const ::CoreML::Specification::WeightParams& blockinputbiasvector() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_blockinputbiasvector();
  ::CoreML::Specification::WeightParams* mutable_blockinputbiasvector();
  void set_allocated_blockinputbiasvector(::CoreML::Specification::WeightParams* blockinputbiasvector);
  private:
  const ::CoreML::Specification::WeightParams& _internal_blockinputbiasvector() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_blockinputbiasvector();
  public:
  void unsafe_arena_set_allocated_blockinputbiasvector(
      ::CoreML::Specification::WeightParams* blockinputbiasvector);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_blockinputbiasvector();

  // .CoreML.Specification.WeightParams outputGateBiasVector = 43;
  bool has_outputgatebiasvector() const;
  private:
  bool _internal_has_outputgatebiasvector() const;
  public:
  void clear_outputgatebiasvector();
  const ::CoreML::Specification::WeightParams& outputgatebiasvector() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_outputgatebiasvector();
  ::CoreML::Specification::WeightParams* mutable_outputgatebiasvector();
  void set_allocated_outputgatebiasvector(::CoreML::Specification::WeightParams* outputgatebiasvector);
  private:
  const ::CoreML::Specification::WeightParams& _internal_outputgatebiasvector() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_outputgatebiasvector();
  public:
  void unsafe_arena_set_allocated_outputgatebiasvector(
      ::CoreML::Specification::WeightParams* outputgatebiasvector);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_outputgatebiasvector();

  // .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
  bool has_inputgatepeepholevector() const;
  private:
  bool _internal_has_inputgatepeepholevector() const;
  public:
  void clear_inputgatepeepholevector();
  const ::CoreML::Specification::WeightParams& inputgatepeepholevector() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_inputgatepeepholevector();
  ::CoreML::Specification::WeightParams* mutable_inputgatepeepholevector();
  void set_allocated_inputgatepeepholevector(::CoreML::Specification::WeightParams* inputgatepeepholevector);
  private:
  const ::CoreML::Specification::WeightParams& _internal_inputgatepeepholevector() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_inputgatepeepholevector();
  public:
  void unsafe_arena_set_allocated_inputgatepeepholevector(
      ::CoreML::Specification::WeightParams* inputgatepeepholevector);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_inputgatepeepholevector();

  // .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
  bool has_forgetgatepeepholevector() const;
  private:
  bool _internal_has_forgetgatepeepholevector() const;
  public:
  void clear_forgetgatepeepholevector();
  const ::CoreML::Specification::WeightParams& forgetgatepeepholevector() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_forgetgatepeepholevector();
  ::CoreML::Specification::WeightParams* mutable_forgetgatepeepholevector();
  void set_allocated_forgetgatepeepholevector(::CoreML::Specification::WeightParams* forgetgatepeepholevector);
  private:
  const ::CoreML::Specification::WeightParams& _internal_forgetgatepeepholevector() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_forgetgatepeepholevector();
  public:
  void unsafe_arena_set_allocated_forgetgatepeepholevector(
      ::CoreML::Specification::WeightParams* forgetgatepeepholevector);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_forgetgatepeepholevector();

  // .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
  bool has_outputgatepeepholevector() const;
  private:
  bool _internal_has_outputgatepeepholevector() const;
  public:
  void clear_outputgatepeepholevector();
  const ::CoreML::Specification::WeightParams& outputgatepeepholevector() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_outputgatepeepholevector();
  ::CoreML::Specification::WeightParams* mutable_outputgatepeepholevector();
  void set_allocated_outputgatepeepholevector(::CoreML::Specification::WeightParams* outputgatepeepholevector);
  private:
  const ::CoreML::Specification::WeightParams& _internal_outputgatepeepholevector() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_outputgatepeepholevector();
  public:
  void unsafe_arena_set_allocated_outputgatepeepholevector(
      ::CoreML::Specification::WeightParams* outputgatepeepholevector);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_outputgatepeepholevector();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LSTMWeightParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::CoreML::Specification::WeightParams* inputgateweightmatrix_;
  ::CoreML::Specification::WeightParams* forgetgateweightmatrix_;
  ::CoreML::Specification::WeightParams* blockinputweightmatrix_;
  ::CoreML::Specification::WeightParams* outputgateweightmatrix_;
  ::CoreML::Specification::WeightParams* inputgaterecursionmatrix_;
  ::CoreML::Specification::WeightParams* forgetgaterecursionmatrix_;
  ::CoreML::Specification::WeightParams* blockinputrecursionmatrix_;
  ::CoreML::Specification::WeightParams* outputgaterecursionmatrix_;
  ::CoreML::Specification::WeightParams* inputgatebiasvector_;
  ::CoreML::Specification::WeightParams* forgetgatebiasvector_;
  ::CoreML::Specification::WeightParams* blockinputbiasvector_;
  ::CoreML::Specification::WeightParams* outputgatebiasvector_;
  ::CoreML::Specification::WeightParams* inputgatepeepholevector_;
  ::CoreML::Specification::WeightParams* forgetgatepeepholevector_;
  ::CoreML::Specification::WeightParams* outputgatepeepholevector_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class UniDirectionalLSTMLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.UniDirectionalLSTMLayerParams) */ {
 public:
  inline UniDirectionalLSTMLayerParams() : UniDirectionalLSTMLayerParams(nullptr) {}
  ~UniDirectionalLSTMLayerParams() override;
  explicit constexpr UniDirectionalLSTMLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  UniDirectionalLSTMLayerParams(const UniDirectionalLSTMLayerParams& from);
  UniDirectionalLSTMLayerParams(UniDirectionalLSTMLayerParams&& from) noexcept
    : UniDirectionalLSTMLayerParams() {
    *this = ::std::move(from);
  }

  inline UniDirectionalLSTMLayerParams& operator=(const UniDirectionalLSTMLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline UniDirectionalLSTMLayerParams& operator=(UniDirectionalLSTMLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const UniDirectionalLSTMLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const UniDirectionalLSTMLayerParams* internal_default_instance() {
    return reinterpret_cast<const UniDirectionalLSTMLayerParams*>(
               &_UniDirectionalLSTMLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    90;

  friend void swap(UniDirectionalLSTMLayerParams& a, UniDirectionalLSTMLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(UniDirectionalLSTMLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(UniDirectionalLSTMLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  UniDirectionalLSTMLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<UniDirectionalLSTMLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const UniDirectionalLSTMLayerParams& from);
  void MergeFrom(const UniDirectionalLSTMLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(UniDirectionalLSTMLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.UniDirectionalLSTMLayerParams";
  }
  protected:
  explicit UniDirectionalLSTMLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kActivationsFieldNumber = 10,
    kParamsFieldNumber = 15,
    kWeightParamsFieldNumber = 20,
    kInputVectorSizeFieldNumber = 1,
    kOutputVectorSizeFieldNumber = 2,
    kReverseInputFieldNumber = 100,
  };
  // repeated .CoreML.Specification.ActivationParams activations = 10;
  int activations_size() const;
  private:
  int _internal_activations_size() const;
  public:
  void clear_activations();
  ::CoreML::Specification::ActivationParams* mutable_activations(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
      mutable_activations();
  private:
  const ::CoreML::Specification::ActivationParams& _internal_activations(int index) const;
  ::CoreML::Specification::ActivationParams* _internal_add_activations();
  public:
  const ::CoreML::Specification::ActivationParams& activations(int index) const;
  ::CoreML::Specification::ActivationParams* add_activations();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
      activations() const;

  // .CoreML.Specification.LSTMParams params = 15;
  bool has_params() const;
  private:
  bool _internal_has_params() const;
  public:
  void clear_params();
  const ::CoreML::Specification::LSTMParams& params() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::LSTMParams* release_params();
  ::CoreML::Specification::LSTMParams* mutable_params();
  void set_allocated_params(::CoreML::Specification::LSTMParams* params);
  private:
  const ::CoreML::Specification::LSTMParams& _internal_params() const;
  ::CoreML::Specification::LSTMParams* _internal_mutable_params();
  public:
  void unsafe_arena_set_allocated_params(
      ::CoreML::Specification::LSTMParams* params);
  ::CoreML::Specification::LSTMParams* unsafe_arena_release_params();

  // .CoreML.Specification.LSTMWeightParams weightParams = 20;
  bool has_weightparams() const;
  private:
  bool _internal_has_weightparams() const;
  public:
  void clear_weightparams();
  const ::CoreML::Specification::LSTMWeightParams& weightparams() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::LSTMWeightParams* release_weightparams();
  ::CoreML::Specification::LSTMWeightParams* mutable_weightparams();
  void set_allocated_weightparams(::CoreML::Specification::LSTMWeightParams* weightparams);
  private:
  const ::CoreML::Specification::LSTMWeightParams& _internal_weightparams() const;
  ::CoreML::Specification::LSTMWeightParams* _internal_mutable_weightparams();
  public:
  void unsafe_arena_set_allocated_weightparams(
      ::CoreML::Specification::LSTMWeightParams* weightparams);
  ::CoreML::Specification::LSTMWeightParams* unsafe_arena_release_weightparams();

  // uint64 inputVectorSize = 1;
  void clear_inputvectorsize();
  uint64_t inputvectorsize() const;
  void set_inputvectorsize(uint64_t value);
  private:
  uint64_t _internal_inputvectorsize() const;
  void _internal_set_inputvectorsize(uint64_t value);
  public:

  // uint64 outputVectorSize = 2;
  void clear_outputvectorsize();
  uint64_t outputvectorsize() const;
  void set_outputvectorsize(uint64_t value);
  private:
  uint64_t _internal_outputvectorsize() const;
  void _internal_set_outputvectorsize(uint64_t value);
  public:

  // bool reverseInput = 100;
  void clear_reverseinput();
  bool reverseinput() const;
  void set_reverseinput(bool value);
  private:
  bool _internal_reverseinput() const;
  void _internal_set_reverseinput(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.UniDirectionalLSTMLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::ActivationParams > activations_;
  ::CoreML::Specification::LSTMParams* params_;
  ::CoreML::Specification::LSTMWeightParams* weightparams_;
  uint64_t inputvectorsize_;
  uint64_t outputvectorsize_;
  bool reverseinput_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class BiDirectionalLSTMLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BiDirectionalLSTMLayerParams) */ {
 public:
  inline BiDirectionalLSTMLayerParams() : BiDirectionalLSTMLayerParams(nullptr) {}
  ~BiDirectionalLSTMLayerParams() override;
  explicit constexpr BiDirectionalLSTMLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  BiDirectionalLSTMLayerParams(const BiDirectionalLSTMLayerParams& from);
  BiDirectionalLSTMLayerParams(BiDirectionalLSTMLayerParams&& from) noexcept
    : BiDirectionalLSTMLayerParams() {
    *this = ::std::move(from);
  }

  inline BiDirectionalLSTMLayerParams& operator=(const BiDirectionalLSTMLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline BiDirectionalLSTMLayerParams& operator=(BiDirectionalLSTMLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const BiDirectionalLSTMLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const BiDirectionalLSTMLayerParams* internal_default_instance() {
    return reinterpret_cast<const BiDirectionalLSTMLayerParams*>(
               &_BiDirectionalLSTMLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    91;

  friend void swap(BiDirectionalLSTMLayerParams& a, BiDirectionalLSTMLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(BiDirectionalLSTMLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(BiDirectionalLSTMLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  BiDirectionalLSTMLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<BiDirectionalLSTMLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const BiDirectionalLSTMLayerParams& from);
  void MergeFrom(const BiDirectionalLSTMLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BiDirectionalLSTMLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.BiDirectionalLSTMLayerParams";
  }
  protected:
  explicit BiDirectionalLSTMLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kActivationsForwardLSTMFieldNumber = 10,
    kActivationsBackwardLSTMFieldNumber = 11,
    kWeightParamsFieldNumber = 20,
    kParamsFieldNumber = 15,
    kInputVectorSizeFieldNumber = 1,
    kOutputVectorSizeFieldNumber = 2,
  };
  // repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
  int activationsforwardlstm_size() const;
  private:
  int _internal_activationsforwardlstm_size() const;
  public:
  void clear_activationsforwardlstm();
  ::CoreML::Specification::ActivationParams* mutable_activationsforwardlstm(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
      mutable_activationsforwardlstm();
  private:
  const ::CoreML::Specification::ActivationParams& _internal_activationsforwardlstm(int index) const;
  ::CoreML::Specification::ActivationParams* _internal_add_activationsforwardlstm();
  public:
  const ::CoreML::Specification::ActivationParams& activationsforwardlstm(int index) const;
  ::CoreML::Specification::ActivationParams* add_activationsforwardlstm();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
      activationsforwardlstm() const;

  // repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
  int activationsbackwardlstm_size() const;
  private:
  int _internal_activationsbackwardlstm_size() const;
  public:
  void clear_activationsbackwardlstm();
  ::CoreML::Specification::ActivationParams* mutable_activationsbackwardlstm(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
      mutable_activationsbackwardlstm();
  private:
  const ::CoreML::Specification::ActivationParams& _internal_activationsbackwardlstm(int index) const;
  ::CoreML::Specification::ActivationParams* _internal_add_activationsbackwardlstm();
  public:
  const ::CoreML::Specification::ActivationParams& activationsbackwardlstm(int index) const;
  ::CoreML::Specification::ActivationParams* add_activationsbackwardlstm();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
      activationsbackwardlstm() const;

  // repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
  int weightparams_size() const;
  private:
  int _internal_weightparams_size() const;
  public:
  void clear_weightparams();
  ::CoreML::Specification::LSTMWeightParams* mutable_weightparams(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams >*
      mutable_weightparams();
  private:
  const ::CoreML::Specification::LSTMWeightParams& _internal_weightparams(int index) const;
  ::CoreML::Specification::LSTMWeightParams* _internal_add_weightparams();
  public:
  const ::CoreML::Specification::LSTMWeightParams& weightparams(int index) const;
  ::CoreML::Specification::LSTMWeightParams* add_weightparams();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams >&
      weightparams() const;

  // .CoreML.Specification.LSTMParams params = 15;
  bool has_params() const;
  private:
  bool _internal_has_params() const;
  public:
  void clear_params();
  const ::CoreML::Specification::LSTMParams& params() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::LSTMParams* release_params();
  ::CoreML::Specification::LSTMParams* mutable_params();
  void set_allocated_params(::CoreML::Specification::LSTMParams* params);
  private:
  const ::CoreML::Specification::LSTMParams& _internal_params() const;
  ::CoreML::Specification::LSTMParams* _internal_mutable_params();
  public:
  void unsafe_arena_set_allocated_params(
      ::CoreML::Specification::LSTMParams* params);
  ::CoreML::Specification::LSTMParams* unsafe_arena_release_params();

  // uint64 inputVectorSize = 1;
  void clear_inputvectorsize();
  uint64_t inputvectorsize() const;
  void set_inputvectorsize(uint64_t value);
  private:
  uint64_t _internal_inputvectorsize() const;
  void _internal_set_inputvectorsize(uint64_t value);
  public:

  // uint64 outputVectorSize = 2;
  void clear_outputvectorsize();
  uint64_t outputvectorsize() const;
  void set_outputvectorsize(uint64_t value);
  private:
  uint64_t _internal_outputvectorsize() const;
  void _internal_set_outputvectorsize(uint64_t value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BiDirectionalLSTMLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::ActivationParams > activationsforwardlstm_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::ActivationParams > activationsbackwardlstm_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams > weightparams_;
  ::CoreML::Specification::LSTMParams* params_;
  uint64_t inputvectorsize_;
  uint64_t outputvectorsize_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class CustomLayerParams_CustomLayerParamValue final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CustomLayerParams.CustomLayerParamValue) */ {
 public:
  inline CustomLayerParams_CustomLayerParamValue() : CustomLayerParams_CustomLayerParamValue(nullptr) {}
  ~CustomLayerParams_CustomLayerParamValue() override;
  explicit constexpr CustomLayerParams_CustomLayerParamValue(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  CustomLayerParams_CustomLayerParamValue(const CustomLayerParams_CustomLayerParamValue& from);
  CustomLayerParams_CustomLayerParamValue(CustomLayerParams_CustomLayerParamValue&& from) noexcept
    : CustomLayerParams_CustomLayerParamValue() {
    *this = ::std::move(from);
  }

  inline CustomLayerParams_CustomLayerParamValue& operator=(const CustomLayerParams_CustomLayerParamValue& from) {
    CopyFrom(from);
    return *this;
  }
  inline CustomLayerParams_CustomLayerParamValue& operator=(CustomLayerParams_CustomLayerParamValue&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const CustomLayerParams_CustomLayerParamValue& default_instance() {
    return *internal_default_instance();
  }
  enum ValueCase {
    kDoubleValue = 10,
    kStringValue = 20,
    kIntValue = 30,
    kLongValue = 40,
    kBoolValue = 50,
    VALUE_NOT_SET = 0,
  };

  static inline const CustomLayerParams_CustomLayerParamValue* internal_default_instance() {
    return reinterpret_cast<const CustomLayerParams_CustomLayerParamValue*>(
               &_CustomLayerParams_CustomLayerParamValue_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    92;

  friend void swap(CustomLayerParams_CustomLayerParamValue& a, CustomLayerParams_CustomLayerParamValue& b) {
    a.Swap(&b);
  }
  inline void Swap(CustomLayerParams_CustomLayerParamValue* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(CustomLayerParams_CustomLayerParamValue* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  CustomLayerParams_CustomLayerParamValue* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<CustomLayerParams_CustomLayerParamValue>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const CustomLayerParams_CustomLayerParamValue& from);
  void MergeFrom(const CustomLayerParams_CustomLayerParamValue& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CustomLayerParams_CustomLayerParamValue* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.CustomLayerParams.CustomLayerParamValue";
  }
  protected:
  explicit CustomLayerParams_CustomLayerParamValue(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kDoubleValueFieldNumber = 10,
    kStringValueFieldNumber = 20,
    kIntValueFieldNumber = 30,
    kLongValueFieldNumber = 40,
    kBoolValueFieldNumber = 50,
  };
  // double doubleValue = 10;
  bool has_doublevalue() const;
  private:
  bool _internal_has_doublevalue() const;
  public:
  void clear_doublevalue();
  double doublevalue() const;
  void set_doublevalue(double value);
  private:
  double _internal_doublevalue() const;
  void _internal_set_doublevalue(double value);
  public:

  // string stringValue = 20;
  bool has_stringvalue() const;
  private:
  bool _internal_has_stringvalue() const;
  public:
  void clear_stringvalue();
  const std::string& stringvalue() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_stringvalue(ArgT0&& arg0, ArgT... args);
  std::string* mutable_stringvalue();
  PROTOBUF_NODISCARD std::string* release_stringvalue();
  void set_allocated_stringvalue(std::string* stringvalue);
  private:
  const std::string& _internal_stringvalue() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_stringvalue(const std::string& value);
  std::string* _internal_mutable_stringvalue();
  public:

  // int32 intValue = 30;
  bool has_intvalue() const;
  private:
  bool _internal_has_intvalue() const;
  public:
  void clear_intvalue();
  int32_t intvalue() const;
  void set_intvalue(int32_t value);
  private:
  int32_t _internal_intvalue() const;
  void _internal_set_intvalue(int32_t value);
  public:

  // int64 longValue = 40;
  bool has_longvalue() const;
  private:
  bool _internal_has_longvalue() const;
  public:
  void clear_longvalue();
  int64_t longvalue() const;
  void set_longvalue(int64_t value);
  private:
  int64_t _internal_longvalue() const;
  void _internal_set_longvalue(int64_t value);
  public:

  // bool boolValue = 50;
  bool has_boolvalue() const;
  private:
  bool _internal_has_boolvalue() const;
  public:
  void clear_boolvalue();
  bool boolvalue() const;
  void set_boolvalue(bool value);
  private:
  bool _internal_boolvalue() const;
  void _internal_set_boolvalue(bool value);
  public:

  void clear_value();
  ValueCase value_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
 private:
  class _Internal;
  void set_has_doublevalue();
  void set_has_stringvalue();
  void set_has_intvalue();
  void set_has_longvalue();
  void set_has_boolvalue();

  inline bool has_value() const;
  inline void clear_has_value();

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  union ValueUnion {
    constexpr ValueUnion() : _constinit_{} {}
      ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized _constinit_;
    double doublevalue_;
    ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr stringvalue_;
    int32_t intvalue_;
    int64_t longvalue_;
    bool boolvalue_;
  } value_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  uint32_t _oneof_case_[1];

  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class CustomLayerParams_ParametersEntry_DoNotUse : public ::PROTOBUF_NAMESPACE_ID::internal::MapEntryLite<CustomLayerParams_ParametersEntry_DoNotUse, 
    std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue,
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_STRING,
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_MESSAGE> {
public:
  typedef ::PROTOBUF_NAMESPACE_ID::internal::MapEntryLite<CustomLayerParams_ParametersEntry_DoNotUse, 
    std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue,
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_STRING,
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_MESSAGE> SuperType;
  CustomLayerParams_ParametersEntry_DoNotUse();
  explicit constexpr CustomLayerParams_ParametersEntry_DoNotUse(
      ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);
  explicit CustomLayerParams_ParametersEntry_DoNotUse(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  void MergeFrom(const CustomLayerParams_ParametersEntry_DoNotUse& other);
  static const CustomLayerParams_ParametersEntry_DoNotUse* internal_default_instance() { return reinterpret_cast<const CustomLayerParams_ParametersEntry_DoNotUse*>(&_CustomLayerParams_ParametersEntry_DoNotUse_default_instance_); }
  static bool ValidateKey(std::string* s) {
    return ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(s->data(), static_cast<int>(s->size()), ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::PARSE, "CoreML.Specification.CustomLayerParams.ParametersEntry.key");
 }
  static bool ValidateValue(void*) { return true; }
};

// -------------------------------------------------------------------

class CustomLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CustomLayerParams) */ {
 public:
  inline CustomLayerParams() : CustomLayerParams(nullptr) {}
  ~CustomLayerParams() override;
  explicit constexpr CustomLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  CustomLayerParams(const CustomLayerParams& from);
  CustomLayerParams(CustomLayerParams&& from) noexcept
    : CustomLayerParams() {
    *this = ::std::move(from);
  }

  inline CustomLayerParams& operator=(const CustomLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline CustomLayerParams& operator=(CustomLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const CustomLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const CustomLayerParams* internal_default_instance() {
    return reinterpret_cast<const CustomLayerParams*>(
               &_CustomLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    94;

  friend void swap(CustomLayerParams& a, CustomLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(CustomLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(CustomLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  CustomLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<CustomLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const CustomLayerParams& from);
  void MergeFrom(const CustomLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CustomLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.CustomLayerParams";
  }
  protected:
  explicit CustomLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  typedef CustomLayerParams_CustomLayerParamValue CustomLayerParamValue;

  // accessors -------------------------------------------------------

  enum : int {
    kWeightsFieldNumber = 20,
    kParametersFieldNumber = 30,
    kClassNameFieldNumber = 10,
    kDescriptionFieldNumber = 40,
  };
  // repeated .CoreML.Specification.WeightParams weights = 20;
  int weights_size() const;
  private:
  int _internal_weights_size() const;
  public:
  void clear_weights();
  ::CoreML::Specification::WeightParams* mutable_weights(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::WeightParams >*
      mutable_weights();
  private:
  const ::CoreML::Specification::WeightParams& _internal_weights(int index) const;
  ::CoreML::Specification::WeightParams* _internal_add_weights();
  public:
  const ::CoreML::Specification::WeightParams& weights(int index) const;
  ::CoreML::Specification::WeightParams* add_weights();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::WeightParams >&
      weights() const;

  // map<string, .CoreML.Specification.CustomLayerParams.CustomLayerParamValue> parameters = 30;
  int parameters_size() const;
  private:
  int _internal_parameters_size() const;
  public:
  void clear_parameters();
  private:
  const ::PROTOBUF_NAMESPACE_ID::Map< std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >&
      _internal_parameters() const;
  ::PROTOBUF_NAMESPACE_ID::Map< std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >*
      _internal_mutable_parameters();
  public:
  const ::PROTOBUF_NAMESPACE_ID::Map< std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >&
      parameters() const;
  ::PROTOBUF_NAMESPACE_ID::Map< std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >*
      mutable_parameters();

  // string className = 10;
  void clear_classname();
  const std::string& classname() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_classname(ArgT0&& arg0, ArgT... args);
  std::string* mutable_classname();
  PROTOBUF_NODISCARD std::string* release_classname();
  void set_allocated_classname(std::string* classname);
  private:
  const std::string& _internal_classname() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_classname(const std::string& value);
  std::string* _internal_mutable_classname();
  public:

  // string description = 40;
  void clear_description();
  const std::string& description() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_description(ArgT0&& arg0, ArgT... args);
  std::string* mutable_description();
  PROTOBUF_NODISCARD std::string* release_description();
  void set_allocated_description(std::string* description);
  private:
  const std::string& _internal_description() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_description(const std::string& value);
  std::string* _internal_mutable_description();
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.CustomLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::WeightParams > weights_;
  ::PROTOBUF_NAMESPACE_ID::internal::MapFieldLite<
      CustomLayerParams_ParametersEntry_DoNotUse,
      std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue,
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_STRING,
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_MESSAGE> parameters_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr classname_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr description_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class TransposeLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.TransposeLayerParams) */ {
 public:
  inline TransposeLayerParams() : TransposeLayerParams(nullptr) {}
  ~TransposeLayerParams() override;
  explicit constexpr TransposeLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  TransposeLayerParams(const TransposeLayerParams& from);
  TransposeLayerParams(TransposeLayerParams&& from) noexcept
    : TransposeLayerParams() {
    *this = ::std::move(from);
  }

  inline TransposeLayerParams& operator=(const TransposeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline TransposeLayerParams& operator=(TransposeLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const TransposeLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const TransposeLayerParams* internal_default_instance() {
    return reinterpret_cast<const TransposeLayerParams*>(
               &_TransposeLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    95;

  friend void swap(TransposeLayerParams& a, TransposeLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(TransposeLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(TransposeLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  TransposeLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<TransposeLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const TransposeLayerParams& from);
  void MergeFrom(const TransposeLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(TransposeLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.TransposeLayerParams";
  }
  protected:
  explicit TransposeLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxesFieldNumber = 1,
  };
  // repeated uint64 axes = 1;
  int axes_size() const;
  private:
  int _internal_axes_size() const;
  public:
  void clear_axes();
  private:
  uint64_t _internal_axes(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_axes() const;
  void _internal_add_axes(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_axes();
  public:
  uint64_t axes(int index) const;
  void set_axes(int index, uint64_t value);
  void add_axes(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      axes() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_axes();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.TransposeLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > axes_;
  mutable std::atomic<int> _axes_cached_byte_size_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class BatchedMatMulLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BatchedMatMulLayerParams) */ {
 public:
  inline BatchedMatMulLayerParams() : BatchedMatMulLayerParams(nullptr) {}
  ~BatchedMatMulLayerParams() override;
  explicit constexpr BatchedMatMulLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  BatchedMatMulLayerParams(const BatchedMatMulLayerParams& from);
  BatchedMatMulLayerParams(BatchedMatMulLayerParams&& from) noexcept
    : BatchedMatMulLayerParams() {
    *this = ::std::move(from);
  }

  inline BatchedMatMulLayerParams& operator=(const BatchedMatMulLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline BatchedMatMulLayerParams& operator=(BatchedMatMulLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const BatchedMatMulLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const BatchedMatMulLayerParams* internal_default_instance() {
    return reinterpret_cast<const BatchedMatMulLayerParams*>(
               &_BatchedMatMulLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    96;

  friend void swap(BatchedMatMulLayerParams& a, BatchedMatMulLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(BatchedMatMulLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(BatchedMatMulLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  BatchedMatMulLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<BatchedMatMulLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const BatchedMatMulLayerParams& from);
  void MergeFrom(const BatchedMatMulLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BatchedMatMulLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.BatchedMatMulLayerParams";
  }
  protected:
  explicit BatchedMatMulLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kWeightsFieldNumber = 8,
    kBiasFieldNumber = 9,
    kWeightMatrixFirstDimensionFieldNumber = 5,
    kWeightMatrixSecondDimensionFieldNumber = 6,
    kTransposeAFieldNumber = 1,
    kTransposeBFieldNumber = 2,
    kHasBiasFieldNumber = 7,
    kInt8DynamicQuantizeFieldNumber = 10,
  };
  // .CoreML.Specification.WeightParams weights = 8;
  bool has_weights() const;
  private:
  bool _internal_has_weights() const;
  public:
  void clear_weights();
  const ::CoreML::Specification::WeightParams& weights() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_weights();
  ::CoreML::Specification::WeightParams* mutable_weights();
  void set_allocated_weights(::CoreML::Specification::WeightParams* weights);
  private:
  const ::CoreML::Specification::WeightParams& _internal_weights() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_weights();
  public:
  void unsafe_arena_set_allocated_weights(
      ::CoreML::Specification::WeightParams* weights);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_weights();

  // .CoreML.Specification.WeightParams bias = 9;
  bool has_bias() const;
  private:
  bool _internal_has_bias() const;
  public:
  void clear_bias();
  const ::CoreML::Specification::WeightParams& bias() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_bias();
  ::CoreML::Specification::WeightParams* mutable_bias();
  void set_allocated_bias(::CoreML::Specification::WeightParams* bias);
  private:
  const ::CoreML::Specification::WeightParams& _internal_bias() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_bias();
  public:
  void unsafe_arena_set_allocated_bias(
      ::CoreML::Specification::WeightParams* bias);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_bias();

  // uint64 weightMatrixFirstDimension = 5;
  void clear_weightmatrixfirstdimension();
  uint64_t weightmatrixfirstdimension() const;
  void set_weightmatrixfirstdimension(uint64_t value);
  private:
  uint64_t _internal_weightmatrixfirstdimension() const;
  void _internal_set_weightmatrixfirstdimension(uint64_t value);
  public:

  // uint64 weightMatrixSecondDimension = 6;
  void clear_weightmatrixseconddimension();
  uint64_t weightmatrixseconddimension() const;
  void set_weightmatrixseconddimension(uint64_t value);
  private:
  uint64_t _internal_weightmatrixseconddimension() const;
  void _internal_set_weightmatrixseconddimension(uint64_t value);
  public:

  // bool transposeA = 1;
  void clear_transposea();
  bool transposea() const;
  void set_transposea(bool value);
  private:
  bool _internal_transposea() const;
  void _internal_set_transposea(bool value);
  public:

  // bool transposeB = 2;
  void clear_transposeb();
  bool transposeb() const;
  void set_transposeb(bool value);
  private:
  bool _internal_transposeb() const;
  void _internal_set_transposeb(bool value);
  public:

  // bool hasBias = 7;
  void clear_hasbias();
  bool hasbias() const;
  void set_hasbias(bool value);
  private:
  bool _internal_hasbias() const;
  void _internal_set_hasbias(bool value);
  public:

  // bool int8DynamicQuantize = 10;
  void clear_int8dynamicquantize();
  bool int8dynamicquantize() const;
  void set_int8dynamicquantize(bool value);
  private:
  bool _internal_int8dynamicquantize() const;
  void _internal_set_int8dynamicquantize(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BatchedMatMulLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::CoreML::Specification::WeightParams* weights_;
  ::CoreML::Specification::WeightParams* bias_;
  uint64_t weightmatrixfirstdimension_;
  uint64_t weightmatrixseconddimension_;
  bool transposea_;
  bool transposeb_;
  bool hasbias_;
  bool int8dynamicquantize_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ConcatNDLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ConcatNDLayerParams) */ {
 public:
  inline ConcatNDLayerParams() : ConcatNDLayerParams(nullptr) {}
  ~ConcatNDLayerParams() override;
  explicit constexpr ConcatNDLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ConcatNDLayerParams(const ConcatNDLayerParams& from);
  ConcatNDLayerParams(ConcatNDLayerParams&& from) noexcept
    : ConcatNDLayerParams() {
    *this = ::std::move(from);
  }

  inline ConcatNDLayerParams& operator=(const ConcatNDLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ConcatNDLayerParams& operator=(ConcatNDLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ConcatNDLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ConcatNDLayerParams* internal_default_instance() {
    return reinterpret_cast<const ConcatNDLayerParams*>(
               &_ConcatNDLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    97;

  friend void swap(ConcatNDLayerParams& a, ConcatNDLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ConcatNDLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ConcatNDLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ConcatNDLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ConcatNDLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ConcatNDLayerParams& from);
  void MergeFrom(const ConcatNDLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ConcatNDLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ConcatNDLayerParams";
  }
  protected:
  explicit ConcatNDLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxisFieldNumber = 1,
    kInterleaveFieldNumber = 2,
  };
  // int64 axis = 1;
  void clear_axis();
  int64_t axis() const;
  void set_axis(int64_t value);
  private:
  int64_t _internal_axis() const;
  void _internal_set_axis(int64_t value);
  public:

  // bool interleave = 2;
  void clear_interleave();
  bool interleave() const;
  void set_interleave(bool value);
  private:
  bool _internal_interleave() const;
  void _internal_set_interleave(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ConcatNDLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t axis_;
  bool interleave_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class SoftmaxNDLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SoftmaxNDLayerParams) */ {
 public:
  inline SoftmaxNDLayerParams() : SoftmaxNDLayerParams(nullptr) {}
  ~SoftmaxNDLayerParams() override;
  explicit constexpr SoftmaxNDLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SoftmaxNDLayerParams(const SoftmaxNDLayerParams& from);
  SoftmaxNDLayerParams(SoftmaxNDLayerParams&& from) noexcept
    : SoftmaxNDLayerParams() {
    *this = ::std::move(from);
  }

  inline SoftmaxNDLayerParams& operator=(const SoftmaxNDLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline SoftmaxNDLayerParams& operator=(SoftmaxNDLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const SoftmaxNDLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const SoftmaxNDLayerParams* internal_default_instance() {
    return reinterpret_cast<const SoftmaxNDLayerParams*>(
               &_SoftmaxNDLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    98;

  friend void swap(SoftmaxNDLayerParams& a, SoftmaxNDLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(SoftmaxNDLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SoftmaxNDLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SoftmaxNDLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SoftmaxNDLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const SoftmaxNDLayerParams& from);
  void MergeFrom(const SoftmaxNDLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SoftmaxNDLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.SoftmaxNDLayerParams";
  }
  protected:
  explicit SoftmaxNDLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxisFieldNumber = 1,
  };
  // int64 axis = 1;
  void clear_axis();
  int64_t axis() const;
  void set_axis(int64_t value);
  private:
  int64_t _internal_axis() const;
  void _internal_set_axis(int64_t value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SoftmaxNDLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t axis_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ReverseLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReverseLayerParams) */ {
 public:
  inline ReverseLayerParams() : ReverseLayerParams(nullptr) {}
  ~ReverseLayerParams() override;
  explicit constexpr ReverseLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ReverseLayerParams(const ReverseLayerParams& from);
  ReverseLayerParams(ReverseLayerParams&& from) noexcept
    : ReverseLayerParams() {
    *this = ::std::move(from);
  }

  inline ReverseLayerParams& operator=(const ReverseLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ReverseLayerParams& operator=(ReverseLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ReverseLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ReverseLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReverseLayerParams*>(
               &_ReverseLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    99;

  friend void swap(ReverseLayerParams& a, ReverseLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ReverseLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ReverseLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ReverseLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ReverseLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ReverseLayerParams& from);
  void MergeFrom(const ReverseLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReverseLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ReverseLayerParams";
  }
  protected:
  explicit ReverseLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kReverseDimFieldNumber = 1,
  };
  // repeated bool reverseDim = 1;
  int reversedim_size() const;
  private:
  int _internal_reversedim_size() const;
  public:
  void clear_reversedim();
  private:
  bool _internal_reversedim(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
      _internal_reversedim() const;
  void _internal_add_reversedim(bool value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
      _internal_mutable_reversedim();
  public:
  bool reversedim(int index) const;
  void set_reversedim(int index, bool value);
  void add_reversedim(bool value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
      reversedim() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
      mutable_reversedim();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReverseLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool > reversedim_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ReverseSeqLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReverseSeqLayerParams) */ {
 public:
  inline ReverseSeqLayerParams() : ReverseSeqLayerParams(nullptr) {}
  ~ReverseSeqLayerParams() override;
  explicit constexpr ReverseSeqLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ReverseSeqLayerParams(const ReverseSeqLayerParams& from);
  ReverseSeqLayerParams(ReverseSeqLayerParams&& from) noexcept
    : ReverseSeqLayerParams() {
    *this = ::std::move(from);
  }

  inline ReverseSeqLayerParams& operator=(const ReverseSeqLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ReverseSeqLayerParams& operator=(ReverseSeqLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ReverseSeqLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ReverseSeqLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReverseSeqLayerParams*>(
               &_ReverseSeqLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    100;

  friend void swap(ReverseSeqLayerParams& a, ReverseSeqLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ReverseSeqLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ReverseSeqLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ReverseSeqLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ReverseSeqLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ReverseSeqLayerParams& from);
  void MergeFrom(const ReverseSeqLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReverseSeqLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ReverseSeqLayerParams";
  }
  protected:
  explicit ReverseSeqLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kBatchAxisFieldNumber = 1,
    kSequenceAxisFieldNumber = 2,
  };
  // int64 batchAxis = 1;
  void clear_batchaxis();
  int64_t batchaxis() const;
  void set_batchaxis(int64_t value);
  private:
  int64_t _internal_batchaxis() const;
  void _internal_set_batchaxis(int64_t value);
  public:

  // int64 sequenceAxis = 2;
  void clear_sequenceaxis();
  int64_t sequenceaxis() const;
  void set_sequenceaxis(int64_t value);
  private:
  int64_t _internal_sequenceaxis() const;
  void _internal_set_sequenceaxis(int64_t value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReverseSeqLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t batchaxis_;
  int64_t sequenceaxis_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class LoadConstantNDLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LoadConstantNDLayerParams) */ {
 public:
  inline LoadConstantNDLayerParams() : LoadConstantNDLayerParams(nullptr) {}
  ~LoadConstantNDLayerParams() override;
  explicit constexpr LoadConstantNDLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  LoadConstantNDLayerParams(const LoadConstantNDLayerParams& from);
  LoadConstantNDLayerParams(LoadConstantNDLayerParams&& from) noexcept
    : LoadConstantNDLayerParams() {
    *this = ::std::move(from);
  }

  inline LoadConstantNDLayerParams& operator=(const LoadConstantNDLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline LoadConstantNDLayerParams& operator=(LoadConstantNDLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const LoadConstantNDLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const LoadConstantNDLayerParams* internal_default_instance() {
    return reinterpret_cast<const LoadConstantNDLayerParams*>(
               &_LoadConstantNDLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    101;

  friend void swap(LoadConstantNDLayerParams& a, LoadConstantNDLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(LoadConstantNDLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(LoadConstantNDLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  LoadConstantNDLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<LoadConstantNDLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const LoadConstantNDLayerParams& from);
  void MergeFrom(const LoadConstantNDLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LoadConstantNDLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.LoadConstantNDLayerParams";
  }
  protected:
  explicit LoadConstantNDLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kShapeFieldNumber = 1,
    kDataFieldNumber = 2,
  };
  // repeated uint64 shape = 1;
  int shape_size() const;
  private:
  int _internal_shape_size() const;
  public:
  void clear_shape();
  private:
  uint64_t _internal_shape(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_shape() const;
  void _internal_add_shape(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_shape();
  public:
  uint64_t shape(int index) const;
  void set_shape(int index, uint64_t value);
  void add_shape(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      shape() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_shape();

  // .CoreML.Specification.WeightParams data = 2;
  bool has_data() const;
  private:
  bool _internal_has_data() const;
  public:
  void clear_data();
  const ::CoreML::Specification::WeightParams& data() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_data();
  ::CoreML::Specification::WeightParams* mutable_data();
  void set_allocated_data(::CoreML::Specification::WeightParams* data);
  private:
  const ::CoreML::Specification::WeightParams& _internal_data() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_data();
  public:
  void unsafe_arena_set_allocated_data(
      ::CoreML::Specification::WeightParams* data);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_data();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LoadConstantNDLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > shape_;
  mutable std::atomic<int> _shape_cached_byte_size_;
  ::CoreML::Specification::WeightParams* data_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class FillLikeLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.FillLikeLayerParams) */ {
 public:
  inline FillLikeLayerParams() : FillLikeLayerParams(nullptr) {}
  ~FillLikeLayerParams() override;
  explicit constexpr FillLikeLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  FillLikeLayerParams(const FillLikeLayerParams& from);
  FillLikeLayerParams(FillLikeLayerParams&& from) noexcept
    : FillLikeLayerParams() {
    *this = ::std::move(from);
  }

  inline FillLikeLayerParams& operator=(const FillLikeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline FillLikeLayerParams& operator=(FillLikeLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const FillLikeLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const FillLikeLayerParams* internal_default_instance() {
    return reinterpret_cast<const FillLikeLayerParams*>(
               &_FillLikeLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    102;

  friend void swap(FillLikeLayerParams& a, FillLikeLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(FillLikeLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(FillLikeLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  FillLikeLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<FillLikeLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const FillLikeLayerParams& from);
  void MergeFrom(const FillLikeLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(FillLikeLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.FillLikeLayerParams";
  }
  protected:
  explicit FillLikeLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kValueFieldNumber = 1,
  };
  // float value = 1;
  void clear_value();
  float value() const;
  void set_value(float value);
  private:
  float _internal_value() const;
  void _internal_set_value(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.FillLikeLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float value_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class FillStaticLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.FillStaticLayerParams) */ {
 public:
  inline FillStaticLayerParams() : FillStaticLayerParams(nullptr) {}
  ~FillStaticLayerParams() override;
  explicit constexpr FillStaticLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  FillStaticLayerParams(const FillStaticLayerParams& from);
  FillStaticLayerParams(FillStaticLayerParams&& from) noexcept
    : FillStaticLayerParams() {
    *this = ::std::move(from);
  }

  inline FillStaticLayerParams& operator=(const FillStaticLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline FillStaticLayerParams& operator=(FillStaticLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const FillStaticLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const FillStaticLayerParams* internal_default_instance() {
    return reinterpret_cast<const FillStaticLayerParams*>(
               &_FillStaticLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    103;

  friend void swap(FillStaticLayerParams& a, FillStaticLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(FillStaticLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(FillStaticLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  FillStaticLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<FillStaticLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const FillStaticLayerParams& from);
  void MergeFrom(const FillStaticLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(FillStaticLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.FillStaticLayerParams";
  }
  protected:
  explicit FillStaticLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kTargetShapeFieldNumber = 2,
    kValueFieldNumber = 1,
  };
  // repeated uint64 targetShape = 2;
  int targetshape_size() const;
  private:
  int _internal_targetshape_size() const;
  public:
  void clear_targetshape();
  private:
  uint64_t _internal_targetshape(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_targetshape() const;
  void _internal_add_targetshape(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_targetshape();
  public:
  uint64_t targetshape(int index) const;
  void set_targetshape(int index, uint64_t value);
  void add_targetshape(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      targetshape() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_targetshape();

  // float value = 1;
  void clear_value();
  float value() const;
  void set_value(float value);
  private:
  float _internal_value() const;
  void _internal_set_value(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.FillStaticLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > targetshape_;
  mutable std::atomic<int> _targetshape_cached_byte_size_;
  float value_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class FillDynamicLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.FillDynamicLayerParams) */ {
 public:
  inline FillDynamicLayerParams() : FillDynamicLayerParams(nullptr) {}
  ~FillDynamicLayerParams() override;
  explicit constexpr FillDynamicLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  FillDynamicLayerParams(const FillDynamicLayerParams& from);
  FillDynamicLayerParams(FillDynamicLayerParams&& from) noexcept
    : FillDynamicLayerParams() {
    *this = ::std::move(from);
  }

  inline FillDynamicLayerParams& operator=(const FillDynamicLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline FillDynamicLayerParams& operator=(FillDynamicLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const FillDynamicLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const FillDynamicLayerParams* internal_default_instance() {
    return reinterpret_cast<const FillDynamicLayerParams*>(
               &_FillDynamicLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    104;

  friend void swap(FillDynamicLayerParams& a, FillDynamicLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(FillDynamicLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(FillDynamicLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  FillDynamicLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<FillDynamicLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const FillDynamicLayerParams& from);
  void MergeFrom(const FillDynamicLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(FillDynamicLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.FillDynamicLayerParams";
  }
  protected:
  explicit FillDynamicLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kValueFieldNumber = 1,
  };
  // float value = 1;
  void clear_value();
  float value() const;
  void set_value(float value);
  private:
  float _internal_value() const;
  void _internal_set_value(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.FillDynamicLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float value_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class WhereBroadcastableLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.WhereBroadcastableLayerParams) */ {
 public:
  inline WhereBroadcastableLayerParams() : WhereBroadcastableLayerParams(nullptr) {}
  ~WhereBroadcastableLayerParams() override;
  explicit constexpr WhereBroadcastableLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  WhereBroadcastableLayerParams(const WhereBroadcastableLayerParams& from);
  WhereBroadcastableLayerParams(WhereBroadcastableLayerParams&& from) noexcept
    : WhereBroadcastableLayerParams() {
    *this = ::std::move(from);
  }

  inline WhereBroadcastableLayerParams& operator=(const WhereBroadcastableLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline WhereBroadcastableLayerParams& operator=(WhereBroadcastableLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const WhereBroadcastableLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const WhereBroadcastableLayerParams* internal_default_instance() {
    return reinterpret_cast<const WhereBroadcastableLayerParams*>(
               &_WhereBroadcastableLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    105;

  friend void swap(WhereBroadcastableLayerParams& a, WhereBroadcastableLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(WhereBroadcastableLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(WhereBroadcastableLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  WhereBroadcastableLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<WhereBroadcastableLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const WhereBroadcastableLayerParams& from);
  void MergeFrom(const WhereBroadcastableLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(WhereBroadcastableLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.WhereBroadcastableLayerParams";
  }
  protected:
  explicit WhereBroadcastableLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.WhereBroadcastableLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class SinLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SinLayerParams) */ {
 public:
  inline SinLayerParams() : SinLayerParams(nullptr) {}
  ~SinLayerParams() override;
  explicit constexpr SinLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SinLayerParams(const SinLayerParams& from);
  SinLayerParams(SinLayerParams&& from) noexcept
    : SinLayerParams() {
    *this = ::std::move(from);
  }

  inline SinLayerParams& operator=(const SinLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline SinLayerParams& operator=(SinLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const SinLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const SinLayerParams* internal_default_instance() {
    return reinterpret_cast<const SinLayerParams*>(
               &_SinLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    106;

  friend void swap(SinLayerParams& a, SinLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(SinLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SinLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SinLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SinLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const SinLayerParams& from);
  void MergeFrom(const SinLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SinLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.SinLayerParams";
  }
  protected:
  explicit SinLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SinLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class CosLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CosLayerParams) */ {
 public:
  inline CosLayerParams() : CosLayerParams(nullptr) {}
  ~CosLayerParams() override;
  explicit constexpr CosLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  CosLayerParams(const CosLayerParams& from);
  CosLayerParams(CosLayerParams&& from) noexcept
    : CosLayerParams() {
    *this = ::std::move(from);
  }

  inline CosLayerParams& operator=(const CosLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline CosLayerParams& operator=(CosLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const CosLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const CosLayerParams* internal_default_instance() {
    return reinterpret_cast<const CosLayerParams*>(
               &_CosLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    107;

  friend void swap(CosLayerParams& a, CosLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(CosLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(CosLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  CosLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<CosLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const CosLayerParams& from);
  void MergeFrom(const CosLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CosLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.CosLayerParams";
  }
  protected:
  explicit CosLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.CosLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class TanLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.TanLayerParams) */ {
 public:
  inline TanLayerParams() : TanLayerParams(nullptr) {}
  ~TanLayerParams() override;
  explicit constexpr TanLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  TanLayerParams(const TanLayerParams& from);
  TanLayerParams(TanLayerParams&& from) noexcept
    : TanLayerParams() {
    *this = ::std::move(from);
  }

  inline TanLayerParams& operator=(const TanLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline TanLayerParams& operator=(TanLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const TanLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const TanLayerParams* internal_default_instance() {
    return reinterpret_cast<const TanLayerParams*>(
               &_TanLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    108;

  friend void swap(TanLayerParams& a, TanLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(TanLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(TanLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  TanLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<TanLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const TanLayerParams& from);
  void MergeFrom(const TanLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(TanLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.TanLayerParams";
  }
  protected:
  explicit TanLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.TanLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class AsinLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AsinLayerParams) */ {
 public:
  inline AsinLayerParams() : AsinLayerParams(nullptr) {}
  ~AsinLayerParams() override;
  explicit constexpr AsinLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  AsinLayerParams(const AsinLayerParams& from);
  AsinLayerParams(AsinLayerParams&& from) noexcept
    : AsinLayerParams() {
    *this = ::std::move(from);
  }

  inline AsinLayerParams& operator=(const AsinLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline AsinLayerParams& operator=(AsinLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const AsinLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const AsinLayerParams* internal_default_instance() {
    return reinterpret_cast<const AsinLayerParams*>(
               &_AsinLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    109;

  friend void swap(AsinLayerParams& a, AsinLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(AsinLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(AsinLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  AsinLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<AsinLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const AsinLayerParams& from);
  void MergeFrom(const AsinLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AsinLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.AsinLayerParams";
  }
  protected:
  explicit AsinLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AsinLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class AcosLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AcosLayerParams) */ {
 public:
  inline AcosLayerParams() : AcosLayerParams(nullptr) {}
  ~AcosLayerParams() override;
  explicit constexpr AcosLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  AcosLayerParams(const AcosLayerParams& from);
  AcosLayerParams(AcosLayerParams&& from) noexcept
    : AcosLayerParams() {
    *this = ::std::move(from);
  }

  inline AcosLayerParams& operator=(const AcosLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline AcosLayerParams& operator=(AcosLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const AcosLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const AcosLayerParams* internal_default_instance() {
    return reinterpret_cast<const AcosLayerParams*>(
               &_AcosLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    110;

  friend void swap(AcosLayerParams& a, AcosLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(AcosLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(AcosLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  AcosLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<AcosLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const AcosLayerParams& from);
  void MergeFrom(const AcosLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AcosLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.AcosLayerParams";
  }
  protected:
  explicit AcosLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AcosLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class AtanLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AtanLayerParams) */ {
 public:
  inline AtanLayerParams() : AtanLayerParams(nullptr) {}
  ~AtanLayerParams() override;
  explicit constexpr AtanLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  AtanLayerParams(const AtanLayerParams& from);
  AtanLayerParams(AtanLayerParams&& from) noexcept
    : AtanLayerParams() {
    *this = ::std::move(from);
  }

  inline AtanLayerParams& operator=(const AtanLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline AtanLayerParams& operator=(AtanLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const AtanLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const AtanLayerParams* internal_default_instance() {
    return reinterpret_cast<const AtanLayerParams*>(
               &_AtanLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    111;

  friend void swap(AtanLayerParams& a, AtanLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(AtanLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(AtanLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  AtanLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<AtanLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const AtanLayerParams& from);
  void MergeFrom(const AtanLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AtanLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.AtanLayerParams";
  }
  protected:
  explicit AtanLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AtanLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class SinhLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SinhLayerParams) */ {
 public:
  inline SinhLayerParams() : SinhLayerParams(nullptr) {}
  ~SinhLayerParams() override;
  explicit constexpr SinhLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SinhLayerParams(const SinhLayerParams& from);
  SinhLayerParams(SinhLayerParams&& from) noexcept
    : SinhLayerParams() {
    *this = ::std::move(from);
  }

  inline SinhLayerParams& operator=(const SinhLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline SinhLayerParams& operator=(SinhLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const SinhLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const SinhLayerParams* internal_default_instance() {
    return reinterpret_cast<const SinhLayerParams*>(
               &_SinhLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    112;

  friend void swap(SinhLayerParams& a, SinhLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(SinhLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SinhLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SinhLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SinhLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const SinhLayerParams& from);
  void MergeFrom(const SinhLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SinhLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.SinhLayerParams";
  }
  protected:
  explicit SinhLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SinhLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class CoshLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CoshLayerParams) */ {
 public:
  inline CoshLayerParams() : CoshLayerParams(nullptr) {}
  ~CoshLayerParams() override;
  explicit constexpr CoshLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  CoshLayerParams(const CoshLayerParams& from);
  CoshLayerParams(CoshLayerParams&& from) noexcept
    : CoshLayerParams() {
    *this = ::std::move(from);
  }

  inline CoshLayerParams& operator=(const CoshLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline CoshLayerParams& operator=(CoshLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const CoshLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const CoshLayerParams* internal_default_instance() {
    return reinterpret_cast<const CoshLayerParams*>(
               &_CoshLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    113;

  friend void swap(CoshLayerParams& a, CoshLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(CoshLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(CoshLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  CoshLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<CoshLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const CoshLayerParams& from);
  void MergeFrom(const CoshLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CoshLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.CoshLayerParams";
  }
  protected:
  explicit CoshLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.CoshLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class TanhLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.TanhLayerParams) */ {
 public:
  inline TanhLayerParams() : TanhLayerParams(nullptr) {}
  ~TanhLayerParams() override;
  explicit constexpr TanhLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  TanhLayerParams(const TanhLayerParams& from);
  TanhLayerParams(TanhLayerParams&& from) noexcept
    : TanhLayerParams() {
    *this = ::std::move(from);
  }

  inline TanhLayerParams& operator=(const TanhLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline TanhLayerParams& operator=(TanhLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const TanhLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const TanhLayerParams* internal_default_instance() {
    return reinterpret_cast<const TanhLayerParams*>(
               &_TanhLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    114;

  friend void swap(TanhLayerParams& a, TanhLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(TanhLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(TanhLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  TanhLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<TanhLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const TanhLayerParams& from);
  void MergeFrom(const TanhLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(TanhLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.TanhLayerParams";
  }
  protected:
  explicit TanhLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.TanhLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class AsinhLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AsinhLayerParams) */ {
 public:
  inline AsinhLayerParams() : AsinhLayerParams(nullptr) {}
  ~AsinhLayerParams() override;
  explicit constexpr AsinhLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  AsinhLayerParams(const AsinhLayerParams& from);
  AsinhLayerParams(AsinhLayerParams&& from) noexcept
    : AsinhLayerParams() {
    *this = ::std::move(from);
  }

  inline AsinhLayerParams& operator=(const AsinhLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline AsinhLayerParams& operator=(AsinhLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const AsinhLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const AsinhLayerParams* internal_default_instance() {
    return reinterpret_cast<const AsinhLayerParams*>(
               &_AsinhLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    115;

  friend void swap(AsinhLayerParams& a, AsinhLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(AsinhLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(AsinhLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  AsinhLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<AsinhLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const AsinhLayerParams& from);
  void MergeFrom(const AsinhLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AsinhLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.AsinhLayerParams";
  }
  protected:
  explicit AsinhLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AsinhLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class AcoshLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AcoshLayerParams) */ {
 public:
  inline AcoshLayerParams() : AcoshLayerParams(nullptr) {}
  ~AcoshLayerParams() override;
  explicit constexpr AcoshLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  AcoshLayerParams(const AcoshLayerParams& from);
  AcoshLayerParams(AcoshLayerParams&& from) noexcept
    : AcoshLayerParams() {
    *this = ::std::move(from);
  }

  inline AcoshLayerParams& operator=(const AcoshLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline AcoshLayerParams& operator=(AcoshLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const AcoshLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const AcoshLayerParams* internal_default_instance() {
    return reinterpret_cast<const AcoshLayerParams*>(
               &_AcoshLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    116;

  friend void swap(AcoshLayerParams& a, AcoshLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(AcoshLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(AcoshLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  AcoshLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<AcoshLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const AcoshLayerParams& from);
  void MergeFrom(const AcoshLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AcoshLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.AcoshLayerParams";
  }
  protected:
  explicit AcoshLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AcoshLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class AtanhLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AtanhLayerParams) */ {
 public:
  inline AtanhLayerParams() : AtanhLayerParams(nullptr) {}
  ~AtanhLayerParams() override;
  explicit constexpr AtanhLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  AtanhLayerParams(const AtanhLayerParams& from);
  AtanhLayerParams(AtanhLayerParams&& from) noexcept
    : AtanhLayerParams() {
    *this = ::std::move(from);
  }

  inline AtanhLayerParams& operator=(const AtanhLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline AtanhLayerParams& operator=(AtanhLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const AtanhLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const AtanhLayerParams* internal_default_instance() {
    return reinterpret_cast<const AtanhLayerParams*>(
               &_AtanhLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    117;

  friend void swap(AtanhLayerParams& a, AtanhLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(AtanhLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(AtanhLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  AtanhLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<AtanhLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const AtanhLayerParams& from);
  void MergeFrom(const AtanhLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AtanhLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.AtanhLayerParams";
  }
  protected:
  explicit AtanhLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AtanhLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class PowBroadcastableLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PowBroadcastableLayerParams) */ {
 public:
  inline PowBroadcastableLayerParams() : PowBroadcastableLayerParams(nullptr) {}
  ~PowBroadcastableLayerParams() override;
  explicit constexpr PowBroadcastableLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  PowBroadcastableLayerParams(const PowBroadcastableLayerParams& from);
  PowBroadcastableLayerParams(PowBroadcastableLayerParams&& from) noexcept
    : PowBroadcastableLayerParams() {
    *this = ::std::move(from);
  }

  inline PowBroadcastableLayerParams& operator=(const PowBroadcastableLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline PowBroadcastableLayerParams& operator=(PowBroadcastableLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const PowBroadcastableLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const PowBroadcastableLayerParams* internal_default_instance() {
    return reinterpret_cast<const PowBroadcastableLayerParams*>(
               &_PowBroadcastableLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    118;

  friend void swap(PowBroadcastableLayerParams& a, PowBroadcastableLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(PowBroadcastableLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(PowBroadcastableLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  PowBroadcastableLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<PowBroadcastableLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const PowBroadcastableLayerParams& from);
  void MergeFrom(const PowBroadcastableLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PowBroadcastableLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.PowBroadcastableLayerParams";
  }
  protected:
  explicit PowBroadcastableLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.PowBroadcastableLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class Exp2LayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.Exp2LayerParams) */ {
 public:
  inline Exp2LayerParams() : Exp2LayerParams(nullptr) {}
  ~Exp2LayerParams() override;
  explicit constexpr Exp2LayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  Exp2LayerParams(const Exp2LayerParams& from);
  Exp2LayerParams(Exp2LayerParams&& from) noexcept
    : Exp2LayerParams() {
    *this = ::std::move(from);
  }

  inline Exp2LayerParams& operator=(const Exp2LayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline Exp2LayerParams& operator=(Exp2LayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const Exp2LayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const Exp2LayerParams* internal_default_instance() {
    return reinterpret_cast<const Exp2LayerParams*>(
               &_Exp2LayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    119;

  friend void swap(Exp2LayerParams& a, Exp2LayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(Exp2LayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(Exp2LayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  Exp2LayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<Exp2LayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const Exp2LayerParams& from);
  void MergeFrom(const Exp2LayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(Exp2LayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.Exp2LayerParams";
  }
  protected:
  explicit Exp2LayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.Exp2LayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class WhereNonZeroLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.WhereNonZeroLayerParams) */ {
 public:
  inline WhereNonZeroLayerParams() : WhereNonZeroLayerParams(nullptr) {}
  ~WhereNonZeroLayerParams() override;
  explicit constexpr WhereNonZeroLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  WhereNonZeroLayerParams(const WhereNonZeroLayerParams& from);
  WhereNonZeroLayerParams(WhereNonZeroLayerParams&& from) noexcept
    : WhereNonZeroLayerParams() {
    *this = ::std::move(from);
  }

  inline WhereNonZeroLayerParams& operator=(const WhereNonZeroLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline WhereNonZeroLayerParams& operator=(WhereNonZeroLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const WhereNonZeroLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const WhereNonZeroLayerParams* internal_default_instance() {
    return reinterpret_cast<const WhereNonZeroLayerParams*>(
               &_WhereNonZeroLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    120;

  friend void swap(WhereNonZeroLayerParams& a, WhereNonZeroLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(WhereNonZeroLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(WhereNonZeroLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  WhereNonZeroLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<WhereNonZeroLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const WhereNonZeroLayerParams& from);
  void MergeFrom(const WhereNonZeroLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(WhereNonZeroLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.WhereNonZeroLayerParams";
  }
  protected:
  explicit WhereNonZeroLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.WhereNonZeroLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class MatrixBandPartLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MatrixBandPartLayerParams) */ {
 public:
  inline MatrixBandPartLayerParams() : MatrixBandPartLayerParams(nullptr) {}
  ~MatrixBandPartLayerParams() override;
  explicit constexpr MatrixBandPartLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  MatrixBandPartLayerParams(const MatrixBandPartLayerParams& from);
  MatrixBandPartLayerParams(MatrixBandPartLayerParams&& from) noexcept
    : MatrixBandPartLayerParams() {
    *this = ::std::move(from);
  }

  inline MatrixBandPartLayerParams& operator=(const MatrixBandPartLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline MatrixBandPartLayerParams& operator=(MatrixBandPartLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const MatrixBandPartLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const MatrixBandPartLayerParams* internal_default_instance() {
    return reinterpret_cast<const MatrixBandPartLayerParams*>(
               &_MatrixBandPartLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    121;

  friend void swap(MatrixBandPartLayerParams& a, MatrixBandPartLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(MatrixBandPartLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(MatrixBandPartLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  MatrixBandPartLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<MatrixBandPartLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const MatrixBandPartLayerParams& from);
  void MergeFrom(const MatrixBandPartLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MatrixBandPartLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.MatrixBandPartLayerParams";
  }
  protected:
  explicit MatrixBandPartLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kNumLowerFieldNumber = 1,
    kNumUpperFieldNumber = 2,
  };
  // int64 numLower = 1;
  void clear_numlower();
  int64_t numlower() const;
  void set_numlower(int64_t value);
  private:
  int64_t _internal_numlower() const;
  void _internal_set_numlower(int64_t value);
  public:

  // int64 numUpper = 2;
  void clear_numupper();
  int64_t numupper() const;
  void set_numupper(int64_t value);
  private:
  int64_t _internal_numupper() const;
  void _internal_set_numupper(int64_t value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MatrixBandPartLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t numlower_;
  int64_t numupper_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class UpperTriangularLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.UpperTriangularLayerParams) */ {
 public:
  inline UpperTriangularLayerParams() : UpperTriangularLayerParams(nullptr) {}
  ~UpperTriangularLayerParams() override;
  explicit constexpr UpperTriangularLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  UpperTriangularLayerParams(const UpperTriangularLayerParams& from);
  UpperTriangularLayerParams(UpperTriangularLayerParams&& from) noexcept
    : UpperTriangularLayerParams() {
    *this = ::std::move(from);
  }

  inline UpperTriangularLayerParams& operator=(const UpperTriangularLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline UpperTriangularLayerParams& operator=(UpperTriangularLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const UpperTriangularLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const UpperTriangularLayerParams* internal_default_instance() {
    return reinterpret_cast<const UpperTriangularLayerParams*>(
               &_UpperTriangularLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    122;

  friend void swap(UpperTriangularLayerParams& a, UpperTriangularLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(UpperTriangularLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(UpperTriangularLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  UpperTriangularLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<UpperTriangularLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const UpperTriangularLayerParams& from);
  void MergeFrom(const UpperTriangularLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(UpperTriangularLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.UpperTriangularLayerParams";
  }
  protected:
  explicit UpperTriangularLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kKFieldNumber = 1,
  };
  // int64 k = 1;
  void clear_k();
  int64_t k() const;
  void set_k(int64_t value);
  private:
  int64_t _internal_k() const;
  void _internal_set_k(int64_t value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.UpperTriangularLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t k_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class LowerTriangularLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LowerTriangularLayerParams) */ {
 public:
  inline LowerTriangularLayerParams() : LowerTriangularLayerParams(nullptr) {}
  ~LowerTriangularLayerParams() override;
  explicit constexpr LowerTriangularLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  LowerTriangularLayerParams(const LowerTriangularLayerParams& from);
  LowerTriangularLayerParams(LowerTriangularLayerParams&& from) noexcept
    : LowerTriangularLayerParams() {
    *this = ::std::move(from);
  }

  inline LowerTriangularLayerParams& operator=(const LowerTriangularLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline LowerTriangularLayerParams& operator=(LowerTriangularLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const LowerTriangularLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const LowerTriangularLayerParams* internal_default_instance() {
    return reinterpret_cast<const LowerTriangularLayerParams*>(
               &_LowerTriangularLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    123;

  friend void swap(LowerTriangularLayerParams& a, LowerTriangularLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(LowerTriangularLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(LowerTriangularLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  LowerTriangularLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<LowerTriangularLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const LowerTriangularLayerParams& from);
  void MergeFrom(const LowerTriangularLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LowerTriangularLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.LowerTriangularLayerParams";
  }
  protected:
  explicit LowerTriangularLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kKFieldNumber = 1,
  };
  // int64 k = 1;
  void clear_k();
  int64_t k() const;
  void set_k(int64_t value);
  private:
  int64_t _internal_k() const;
  void _internal_set_k(int64_t value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LowerTriangularLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t k_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class BroadcastToLikeLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BroadcastToLikeLayerParams) */ {
 public:
  inline BroadcastToLikeLayerParams() : BroadcastToLikeLayerParams(nullptr) {}
  ~BroadcastToLikeLayerParams() override;
  explicit constexpr BroadcastToLikeLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  BroadcastToLikeLayerParams(const BroadcastToLikeLayerParams& from);
  BroadcastToLikeLayerParams(BroadcastToLikeLayerParams&& from) noexcept
    : BroadcastToLikeLayerParams() {
    *this = ::std::move(from);
  }

  inline BroadcastToLikeLayerParams& operator=(const BroadcastToLikeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline BroadcastToLikeLayerParams& operator=(BroadcastToLikeLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const BroadcastToLikeLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const BroadcastToLikeLayerParams* internal_default_instance() {
    return reinterpret_cast<const BroadcastToLikeLayerParams*>(
               &_BroadcastToLikeLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    124;

  friend void swap(BroadcastToLikeLayerParams& a, BroadcastToLikeLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(BroadcastToLikeLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(BroadcastToLikeLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  BroadcastToLikeLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<BroadcastToLikeLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const BroadcastToLikeLayerParams& from);
  void MergeFrom(const BroadcastToLikeLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BroadcastToLikeLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.BroadcastToLikeLayerParams";
  }
  protected:
  explicit BroadcastToLikeLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BroadcastToLikeLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class BroadcastToStaticLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BroadcastToStaticLayerParams) */ {
 public:
  inline BroadcastToStaticLayerParams() : BroadcastToStaticLayerParams(nullptr) {}
  ~BroadcastToStaticLayerParams() override;
  explicit constexpr BroadcastToStaticLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  BroadcastToStaticLayerParams(const BroadcastToStaticLayerParams& from);
  BroadcastToStaticLayerParams(BroadcastToStaticLayerParams&& from) noexcept
    : BroadcastToStaticLayerParams() {
    *this = ::std::move(from);
  }

  inline BroadcastToStaticLayerParams& operator=(const BroadcastToStaticLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline BroadcastToStaticLayerParams& operator=(BroadcastToStaticLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const BroadcastToStaticLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const BroadcastToStaticLayerParams* internal_default_instance() {
    return reinterpret_cast<const BroadcastToStaticLayerParams*>(
               &_BroadcastToStaticLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    125;

  friend void swap(BroadcastToStaticLayerParams& a, BroadcastToStaticLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(BroadcastToStaticLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(BroadcastToStaticLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  BroadcastToStaticLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<BroadcastToStaticLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const BroadcastToStaticLayerParams& from);
  void MergeFrom(const BroadcastToStaticLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BroadcastToStaticLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.BroadcastToStaticLayerParams";
  }
  protected:
  explicit BroadcastToStaticLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kTargetShapeFieldNumber = 1,
  };
  // repeated uint64 targetShape = 1;
  int targetshape_size() const;
  private:
  int _internal_targetshape_size() const;
  public:
  void clear_targetshape();
  private:
  uint64_t _internal_targetshape(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_targetshape() const;
  void _internal_add_targetshape(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_targetshape();
  public:
  uint64_t targetshape(int index) const;
  void set_targetshape(int index, uint64_t value);
  void add_targetshape(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      targetshape() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_targetshape();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BroadcastToStaticLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > targetshape_;
  mutable std::atomic<int> _targetshape_cached_byte_size_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class BroadcastToDynamicLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BroadcastToDynamicLayerParams) */ {
 public:
  inline BroadcastToDynamicLayerParams() : BroadcastToDynamicLayerParams(nullptr) {}
  ~BroadcastToDynamicLayerParams() override;
  explicit constexpr BroadcastToDynamicLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  BroadcastToDynamicLayerParams(const BroadcastToDynamicLayerParams& from);
  BroadcastToDynamicLayerParams(BroadcastToDynamicLayerParams&& from) noexcept
    : BroadcastToDynamicLayerParams() {
    *this = ::std::move(from);
  }

  inline BroadcastToDynamicLayerParams& operator=(const BroadcastToDynamicLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline BroadcastToDynamicLayerParams& operator=(BroadcastToDynamicLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const BroadcastToDynamicLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const BroadcastToDynamicLayerParams* internal_default_instance() {
    return reinterpret_cast<const BroadcastToDynamicLayerParams*>(
               &_BroadcastToDynamicLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    126;

  friend void swap(BroadcastToDynamicLayerParams& a, BroadcastToDynamicLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(BroadcastToDynamicLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(BroadcastToDynamicLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  BroadcastToDynamicLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<BroadcastToDynamicLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const BroadcastToDynamicLayerParams& from);
  void MergeFrom(const BroadcastToDynamicLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BroadcastToDynamicLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.BroadcastToDynamicLayerParams";
  }
  protected:
  explicit BroadcastToDynamicLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BroadcastToDynamicLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class AddBroadcastableLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AddBroadcastableLayerParams) */ {
 public:
  inline AddBroadcastableLayerParams() : AddBroadcastableLayerParams(nullptr) {}
  ~AddBroadcastableLayerParams() override;
  explicit constexpr AddBroadcastableLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  AddBroadcastableLayerParams(const AddBroadcastableLayerParams& from);
  AddBroadcastableLayerParams(AddBroadcastableLayerParams&& from) noexcept
    : AddBroadcastableLayerParams() {
    *this = ::std::move(from);
  }

  inline AddBroadcastableLayerParams& operator=(const AddBroadcastableLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline AddBroadcastableLayerParams& operator=(AddBroadcastableLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const AddBroadcastableLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const AddBroadcastableLayerParams* internal_default_instance() {
    return reinterpret_cast<const AddBroadcastableLayerParams*>(
               &_AddBroadcastableLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    127;

  friend void swap(AddBroadcastableLayerParams& a, AddBroadcastableLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(AddBroadcastableLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(AddBroadcastableLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  AddBroadcastableLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<AddBroadcastableLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const AddBroadcastableLayerParams& from);
  void MergeFrom(const AddBroadcastableLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AddBroadcastableLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.AddBroadcastableLayerParams";
  }
  protected:
  explicit AddBroadcastableLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AddBroadcastableLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class MaxBroadcastableLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MaxBroadcastableLayerParams) */ {
 public:
  inline MaxBroadcastableLayerParams() : MaxBroadcastableLayerParams(nullptr) {}
  ~MaxBroadcastableLayerParams() override;
  explicit constexpr MaxBroadcastableLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  MaxBroadcastableLayerParams(const MaxBroadcastableLayerParams& from);
  MaxBroadcastableLayerParams(MaxBroadcastableLayerParams&& from) noexcept
    : MaxBroadcastableLayerParams() {
    *this = ::std::move(from);
  }

  inline MaxBroadcastableLayerParams& operator=(const MaxBroadcastableLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline MaxBroadcastableLayerParams& operator=(MaxBroadcastableLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const MaxBroadcastableLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const MaxBroadcastableLayerParams* internal_default_instance() {
    return reinterpret_cast<const MaxBroadcastableLayerParams*>(
               &_MaxBroadcastableLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    128;

  friend void swap(MaxBroadcastableLayerParams& a, MaxBroadcastableLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(MaxBroadcastableLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(MaxBroadcastableLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  MaxBroadcastableLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<MaxBroadcastableLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const MaxBroadcastableLayerParams& from);
  void MergeFrom(const MaxBroadcastableLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MaxBroadcastableLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.MaxBroadcastableLayerParams";
  }
  protected:
  explicit MaxBroadcastableLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MaxBroadcastableLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class MinBroadcastableLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MinBroadcastableLayerParams) */ {
 public:
  inline MinBroadcastableLayerParams() : MinBroadcastableLayerParams(nullptr) {}
  ~MinBroadcastableLayerParams() override;
  explicit constexpr MinBroadcastableLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  MinBroadcastableLayerParams(const MinBroadcastableLayerParams& from);
  MinBroadcastableLayerParams(MinBroadcastableLayerParams&& from) noexcept
    : MinBroadcastableLayerParams() {
    *this = ::std::move(from);
  }

  inline MinBroadcastableLayerParams& operator=(const MinBroadcastableLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline MinBroadcastableLayerParams& operator=(MinBroadcastableLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const MinBroadcastableLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const MinBroadcastableLayerParams* internal_default_instance() {
    return reinterpret_cast<const MinBroadcastableLayerParams*>(
               &_MinBroadcastableLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    129;

  friend void swap(MinBroadcastableLayerParams& a, MinBroadcastableLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(MinBroadcastableLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(MinBroadcastableLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  MinBroadcastableLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<MinBroadcastableLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const MinBroadcastableLayerParams& from);
  void MergeFrom(const MinBroadcastableLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MinBroadcastableLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.MinBroadcastableLayerParams";
  }
  protected:
  explicit MinBroadcastableLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MinBroadcastableLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ModBroadcastableLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ModBroadcastableLayerParams) */ {
 public:
  inline ModBroadcastableLayerParams() : ModBroadcastableLayerParams(nullptr) {}
  ~ModBroadcastableLayerParams() override;
  explicit constexpr ModBroadcastableLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ModBroadcastableLayerParams(const ModBroadcastableLayerParams& from);
  ModBroadcastableLayerParams(ModBroadcastableLayerParams&& from) noexcept
    : ModBroadcastableLayerParams() {
    *this = ::std::move(from);
  }

  inline ModBroadcastableLayerParams& operator=(const ModBroadcastableLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ModBroadcastableLayerParams& operator=(ModBroadcastableLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ModBroadcastableLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ModBroadcastableLayerParams* internal_default_instance() {
    return reinterpret_cast<const ModBroadcastableLayerParams*>(
               &_ModBroadcastableLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    130;

  friend void swap(ModBroadcastableLayerParams& a, ModBroadcastableLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ModBroadcastableLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ModBroadcastableLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ModBroadcastableLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ModBroadcastableLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ModBroadcastableLayerParams& from);
  void MergeFrom(const ModBroadcastableLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ModBroadcastableLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ModBroadcastableLayerParams";
  }
  protected:
  explicit ModBroadcastableLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ModBroadcastableLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class FloorDivBroadcastableLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.FloorDivBroadcastableLayerParams) */ {
 public:
  inline FloorDivBroadcastableLayerParams() : FloorDivBroadcastableLayerParams(nullptr) {}
  ~FloorDivBroadcastableLayerParams() override;
  explicit constexpr FloorDivBroadcastableLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  FloorDivBroadcastableLayerParams(const FloorDivBroadcastableLayerParams& from);
  FloorDivBroadcastableLayerParams(FloorDivBroadcastableLayerParams&& from) noexcept
    : FloorDivBroadcastableLayerParams() {
    *this = ::std::move(from);
  }

  inline FloorDivBroadcastableLayerParams& operator=(const FloorDivBroadcastableLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline FloorDivBroadcastableLayerParams& operator=(FloorDivBroadcastableLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const FloorDivBroadcastableLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const FloorDivBroadcastableLayerParams* internal_default_instance() {
    return reinterpret_cast<const FloorDivBroadcastableLayerParams*>(
               &_FloorDivBroadcastableLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    131;

  friend void swap(FloorDivBroadcastableLayerParams& a, FloorDivBroadcastableLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(FloorDivBroadcastableLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(FloorDivBroadcastableLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  FloorDivBroadcastableLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<FloorDivBroadcastableLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const FloorDivBroadcastableLayerParams& from);
  void MergeFrom(const FloorDivBroadcastableLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(FloorDivBroadcastableLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.FloorDivBroadcastableLayerParams";
  }
  protected:
  explicit FloorDivBroadcastableLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.FloorDivBroadcastableLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class SubtractBroadcastableLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SubtractBroadcastableLayerParams) */ {
 public:
  inline SubtractBroadcastableLayerParams() : SubtractBroadcastableLayerParams(nullptr) {}
  ~SubtractBroadcastableLayerParams() override;
  explicit constexpr SubtractBroadcastableLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SubtractBroadcastableLayerParams(const SubtractBroadcastableLayerParams& from);
  SubtractBroadcastableLayerParams(SubtractBroadcastableLayerParams&& from) noexcept
    : SubtractBroadcastableLayerParams() {
    *this = ::std::move(from);
  }

  inline SubtractBroadcastableLayerParams& operator=(const SubtractBroadcastableLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline SubtractBroadcastableLayerParams& operator=(SubtractBroadcastableLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const SubtractBroadcastableLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const SubtractBroadcastableLayerParams* internal_default_instance() {
    return reinterpret_cast<const SubtractBroadcastableLayerParams*>(
               &_SubtractBroadcastableLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    132;

  friend void swap(SubtractBroadcastableLayerParams& a, SubtractBroadcastableLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(SubtractBroadcastableLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SubtractBroadcastableLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SubtractBroadcastableLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SubtractBroadcastableLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const SubtractBroadcastableLayerParams& from);
  void MergeFrom(const SubtractBroadcastableLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SubtractBroadcastableLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.SubtractBroadcastableLayerParams";
  }
  protected:
  explicit SubtractBroadcastableLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SubtractBroadcastableLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class MultiplyBroadcastableLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MultiplyBroadcastableLayerParams) */ {
 public:
  inline MultiplyBroadcastableLayerParams() : MultiplyBroadcastableLayerParams(nullptr) {}
  ~MultiplyBroadcastableLayerParams() override;
  explicit constexpr MultiplyBroadcastableLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  MultiplyBroadcastableLayerParams(const MultiplyBroadcastableLayerParams& from);
  MultiplyBroadcastableLayerParams(MultiplyBroadcastableLayerParams&& from) noexcept
    : MultiplyBroadcastableLayerParams() {
    *this = ::std::move(from);
  }

  inline MultiplyBroadcastableLayerParams& operator=(const MultiplyBroadcastableLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline MultiplyBroadcastableLayerParams& operator=(MultiplyBroadcastableLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const MultiplyBroadcastableLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const MultiplyBroadcastableLayerParams* internal_default_instance() {
    return reinterpret_cast<const MultiplyBroadcastableLayerParams*>(
               &_MultiplyBroadcastableLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    133;

  friend void swap(MultiplyBroadcastableLayerParams& a, MultiplyBroadcastableLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(MultiplyBroadcastableLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(MultiplyBroadcastableLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  MultiplyBroadcastableLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<MultiplyBroadcastableLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const MultiplyBroadcastableLayerParams& from);
  void MergeFrom(const MultiplyBroadcastableLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MultiplyBroadcastableLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.MultiplyBroadcastableLayerParams";
  }
  protected:
  explicit MultiplyBroadcastableLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MultiplyBroadcastableLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class DivideBroadcastableLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.DivideBroadcastableLayerParams) */ {
 public:
  inline DivideBroadcastableLayerParams() : DivideBroadcastableLayerParams(nullptr) {}
  ~DivideBroadcastableLayerParams() override;
  explicit constexpr DivideBroadcastableLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  DivideBroadcastableLayerParams(const DivideBroadcastableLayerParams& from);
  DivideBroadcastableLayerParams(DivideBroadcastableLayerParams&& from) noexcept
    : DivideBroadcastableLayerParams() {
    *this = ::std::move(from);
  }

  inline DivideBroadcastableLayerParams& operator=(const DivideBroadcastableLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline DivideBroadcastableLayerParams& operator=(DivideBroadcastableLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const DivideBroadcastableLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const DivideBroadcastableLayerParams* internal_default_instance() {
    return reinterpret_cast<const DivideBroadcastableLayerParams*>(
               &_DivideBroadcastableLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    134;

  friend void swap(DivideBroadcastableLayerParams& a, DivideBroadcastableLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(DivideBroadcastableLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(DivideBroadcastableLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  DivideBroadcastableLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<DivideBroadcastableLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const DivideBroadcastableLayerParams& from);
  void MergeFrom(const DivideBroadcastableLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(DivideBroadcastableLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.DivideBroadcastableLayerParams";
  }
  protected:
  explicit DivideBroadcastableLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.DivideBroadcastableLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class GatherLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.GatherLayerParams) */ {
 public:
  inline GatherLayerParams() : GatherLayerParams(nullptr) {}
  ~GatherLayerParams() override;
  explicit constexpr GatherLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  GatherLayerParams(const GatherLayerParams& from);
  GatherLayerParams(GatherLayerParams&& from) noexcept
    : GatherLayerParams() {
    *this = ::std::move(from);
  }

  inline GatherLayerParams& operator=(const GatherLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline GatherLayerParams& operator=(GatherLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const GatherLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const GatherLayerParams* internal_default_instance() {
    return reinterpret_cast<const GatherLayerParams*>(
               &_GatherLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    135;

  friend void swap(GatherLayerParams& a, GatherLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(GatherLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(GatherLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  GatherLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<GatherLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const GatherLayerParams& from);
  void MergeFrom(const GatherLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(GatherLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.GatherLayerParams";
  }
  protected:
  explicit GatherLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxisFieldNumber = 1,
  };
  // int64 axis = 1;
  void clear_axis();
  int64_t axis() const;
  void set_axis(int64_t value);
  private:
  int64_t _internal_axis() const;
  void _internal_set_axis(int64_t value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.GatherLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t axis_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ScatterLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ScatterLayerParams) */ {
 public:
  inline ScatterLayerParams() : ScatterLayerParams(nullptr) {}
  ~ScatterLayerParams() override;
  explicit constexpr ScatterLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ScatterLayerParams(const ScatterLayerParams& from);
  ScatterLayerParams(ScatterLayerParams&& from) noexcept
    : ScatterLayerParams() {
    *this = ::std::move(from);
  }

  inline ScatterLayerParams& operator=(const ScatterLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ScatterLayerParams& operator=(ScatterLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ScatterLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ScatterLayerParams* internal_default_instance() {
    return reinterpret_cast<const ScatterLayerParams*>(
               &_ScatterLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    136;

  friend void swap(ScatterLayerParams& a, ScatterLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ScatterLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ScatterLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ScatterLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ScatterLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ScatterLayerParams& from);
  void MergeFrom(const ScatterLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ScatterLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ScatterLayerParams";
  }
  protected:
  explicit ScatterLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxisFieldNumber = 1,
    kModeFieldNumber = 2,
  };
  // int64 axis = 1;
  void clear_axis();
  int64_t axis() const;
  void set_axis(int64_t value);
  private:
  int64_t _internal_axis() const;
  void _internal_set_axis(int64_t value);
  public:

  // .CoreML.Specification.ScatterMode mode = 2;
  void clear_mode();
  ::CoreML::Specification::ScatterMode mode() const;
  void set_mode(::CoreML::Specification::ScatterMode value);
  private:
  ::CoreML::Specification::ScatterMode _internal_mode() const;
  void _internal_set_mode(::CoreML::Specification::ScatterMode value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ScatterLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t axis_;
  int mode_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class GatherNDLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.GatherNDLayerParams) */ {
 public:
  inline GatherNDLayerParams() : GatherNDLayerParams(nullptr) {}
  ~GatherNDLayerParams() override;
  explicit constexpr GatherNDLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  GatherNDLayerParams(const GatherNDLayerParams& from);
  GatherNDLayerParams(GatherNDLayerParams&& from) noexcept
    : GatherNDLayerParams() {
    *this = ::std::move(from);
  }

  inline GatherNDLayerParams& operator=(const GatherNDLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline GatherNDLayerParams& operator=(GatherNDLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const GatherNDLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const GatherNDLayerParams* internal_default_instance() {
    return reinterpret_cast<const GatherNDLayerParams*>(
               &_GatherNDLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    137;

  friend void swap(GatherNDLayerParams& a, GatherNDLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(GatherNDLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(GatherNDLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  GatherNDLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<GatherNDLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const GatherNDLayerParams& from);
  void MergeFrom(const GatherNDLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(GatherNDLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.GatherNDLayerParams";
  }
  protected:
  explicit GatherNDLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.GatherNDLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ScatterNDLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ScatterNDLayerParams) */ {
 public:
  inline ScatterNDLayerParams() : ScatterNDLayerParams(nullptr) {}
  ~ScatterNDLayerParams() override;
  explicit constexpr ScatterNDLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ScatterNDLayerParams(const ScatterNDLayerParams& from);
  ScatterNDLayerParams(ScatterNDLayerParams&& from) noexcept
    : ScatterNDLayerParams() {
    *this = ::std::move(from);
  }

  inline ScatterNDLayerParams& operator=(const ScatterNDLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ScatterNDLayerParams& operator=(ScatterNDLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ScatterNDLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ScatterNDLayerParams* internal_default_instance() {
    return reinterpret_cast<const ScatterNDLayerParams*>(
               &_ScatterNDLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    138;

  friend void swap(ScatterNDLayerParams& a, ScatterNDLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ScatterNDLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ScatterNDLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ScatterNDLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ScatterNDLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ScatterNDLayerParams& from);
  void MergeFrom(const ScatterNDLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ScatterNDLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ScatterNDLayerParams";
  }
  protected:
  explicit ScatterNDLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kModeFieldNumber = 1,
  };
  // .CoreML.Specification.ScatterMode mode = 1;
  void clear_mode();
  ::CoreML::Specification::ScatterMode mode() const;
  void set_mode(::CoreML::Specification::ScatterMode value);
  private:
  ::CoreML::Specification::ScatterMode _internal_mode() const;
  void _internal_set_mode(::CoreML::Specification::ScatterMode value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ScatterNDLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int mode_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class GatherAlongAxisLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.GatherAlongAxisLayerParams) */ {
 public:
  inline GatherAlongAxisLayerParams() : GatherAlongAxisLayerParams(nullptr) {}
  ~GatherAlongAxisLayerParams() override;
  explicit constexpr GatherAlongAxisLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  GatherAlongAxisLayerParams(const GatherAlongAxisLayerParams& from);
  GatherAlongAxisLayerParams(GatherAlongAxisLayerParams&& from) noexcept
    : GatherAlongAxisLayerParams() {
    *this = ::std::move(from);
  }

  inline GatherAlongAxisLayerParams& operator=(const GatherAlongAxisLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline GatherAlongAxisLayerParams& operator=(GatherAlongAxisLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const GatherAlongAxisLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const GatherAlongAxisLayerParams* internal_default_instance() {
    return reinterpret_cast<const GatherAlongAxisLayerParams*>(
               &_GatherAlongAxisLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    139;

  friend void swap(GatherAlongAxisLayerParams& a, GatherAlongAxisLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(GatherAlongAxisLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(GatherAlongAxisLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  GatherAlongAxisLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<GatherAlongAxisLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const GatherAlongAxisLayerParams& from);
  void MergeFrom(const GatherAlongAxisLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(GatherAlongAxisLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.GatherAlongAxisLayerParams";
  }
  protected:
  explicit GatherAlongAxisLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxisFieldNumber = 1,
  };
  // int64 axis = 1;
  void clear_axis();
  int64_t axis() const;
  void set_axis(int64_t value);
  private:
  int64_t _internal_axis() const;
  void _internal_set_axis(int64_t value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.GatherAlongAxisLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t axis_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ScatterAlongAxisLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ScatterAlongAxisLayerParams) */ {
 public:
  inline ScatterAlongAxisLayerParams() : ScatterAlongAxisLayerParams(nullptr) {}
  ~ScatterAlongAxisLayerParams() override;
  explicit constexpr ScatterAlongAxisLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ScatterAlongAxisLayerParams(const ScatterAlongAxisLayerParams& from);
  ScatterAlongAxisLayerParams(ScatterAlongAxisLayerParams&& from) noexcept
    : ScatterAlongAxisLayerParams() {
    *this = ::std::move(from);
  }

  inline ScatterAlongAxisLayerParams& operator=(const ScatterAlongAxisLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ScatterAlongAxisLayerParams& operator=(ScatterAlongAxisLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ScatterAlongAxisLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ScatterAlongAxisLayerParams* internal_default_instance() {
    return reinterpret_cast<const ScatterAlongAxisLayerParams*>(
               &_ScatterAlongAxisLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    140;

  friend void swap(ScatterAlongAxisLayerParams& a, ScatterAlongAxisLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ScatterAlongAxisLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ScatterAlongAxisLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ScatterAlongAxisLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ScatterAlongAxisLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ScatterAlongAxisLayerParams& from);
  void MergeFrom(const ScatterAlongAxisLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ScatterAlongAxisLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ScatterAlongAxisLayerParams";
  }
  protected:
  explicit ScatterAlongAxisLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxisFieldNumber = 1,
    kModeFieldNumber = 2,
  };
  // int64 axis = 1;
  void clear_axis();
  int64_t axis() const;
  void set_axis(int64_t value);
  private:
  int64_t _internal_axis() const;
  void _internal_set_axis(int64_t value);
  public:

  // .CoreML.Specification.ScatterMode mode = 2;
  void clear_mode();
  ::CoreML::Specification::ScatterMode mode() const;
  void set_mode(::CoreML::Specification::ScatterMode value);
  private:
  ::CoreML::Specification::ScatterMode _internal_mode() const;
  void _internal_set_mode(::CoreML::Specification::ScatterMode value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ScatterAlongAxisLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t axis_;
  int mode_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class StackLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.StackLayerParams) */ {
 public:
  inline StackLayerParams() : StackLayerParams(nullptr) {}
  ~StackLayerParams() override;
  explicit constexpr StackLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  StackLayerParams(const StackLayerParams& from);
  StackLayerParams(StackLayerParams&& from) noexcept
    : StackLayerParams() {
    *this = ::std::move(from);
  }

  inline StackLayerParams& operator=(const StackLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline StackLayerParams& operator=(StackLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const StackLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const StackLayerParams* internal_default_instance() {
    return reinterpret_cast<const StackLayerParams*>(
               &_StackLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    141;

  friend void swap(StackLayerParams& a, StackLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(StackLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(StackLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  StackLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<StackLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const StackLayerParams& from);
  void MergeFrom(const StackLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(StackLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.StackLayerParams";
  }
  protected:
  explicit StackLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxisFieldNumber = 1,
  };
  // int64 axis = 1;
  void clear_axis();
  int64_t axis() const;
  void set_axis(int64_t value);
  private:
  int64_t _internal_axis() const;
  void _internal_set_axis(int64_t value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.StackLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t axis_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class RankPreservingReshapeLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RankPreservingReshapeLayerParams) */ {
 public:
  inline RankPreservingReshapeLayerParams() : RankPreservingReshapeLayerParams(nullptr) {}
  ~RankPreservingReshapeLayerParams() override;
  explicit constexpr RankPreservingReshapeLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  RankPreservingReshapeLayerParams(const RankPreservingReshapeLayerParams& from);
  RankPreservingReshapeLayerParams(RankPreservingReshapeLayerParams&& from) noexcept
    : RankPreservingReshapeLayerParams() {
    *this = ::std::move(from);
  }

  inline RankPreservingReshapeLayerParams& operator=(const RankPreservingReshapeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline RankPreservingReshapeLayerParams& operator=(RankPreservingReshapeLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const RankPreservingReshapeLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const RankPreservingReshapeLayerParams* internal_default_instance() {
    return reinterpret_cast<const RankPreservingReshapeLayerParams*>(
               &_RankPreservingReshapeLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    142;

  friend void swap(RankPreservingReshapeLayerParams& a, RankPreservingReshapeLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(RankPreservingReshapeLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(RankPreservingReshapeLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  RankPreservingReshapeLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<RankPreservingReshapeLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const RankPreservingReshapeLayerParams& from);
  void MergeFrom(const RankPreservingReshapeLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RankPreservingReshapeLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.RankPreservingReshapeLayerParams";
  }
  protected:
  explicit RankPreservingReshapeLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kTargetShapeFieldNumber = 1,
  };
  // repeated int64 targetShape = 1;
  int targetshape_size() const;
  private:
  int _internal_targetshape_size() const;
  public:
  void clear_targetshape();
  private:
  int64_t _internal_targetshape(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_targetshape() const;
  void _internal_add_targetshape(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_targetshape();
  public:
  int64_t targetshape(int index) const;
  void set_targetshape(int index, int64_t value);
  void add_targetshape(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      targetshape() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_targetshape();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RankPreservingReshapeLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > targetshape_;
  mutable std::atomic<int> _targetshape_cached_byte_size_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ConstantPaddingLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ConstantPaddingLayerParams) */ {
 public:
  inline ConstantPaddingLayerParams() : ConstantPaddingLayerParams(nullptr) {}
  ~ConstantPaddingLayerParams() override;
  explicit constexpr ConstantPaddingLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ConstantPaddingLayerParams(const ConstantPaddingLayerParams& from);
  ConstantPaddingLayerParams(ConstantPaddingLayerParams&& from) noexcept
    : ConstantPaddingLayerParams() {
    *this = ::std::move(from);
  }

  inline ConstantPaddingLayerParams& operator=(const ConstantPaddingLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ConstantPaddingLayerParams& operator=(ConstantPaddingLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ConstantPaddingLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ConstantPaddingLayerParams* internal_default_instance() {
    return reinterpret_cast<const ConstantPaddingLayerParams*>(
               &_ConstantPaddingLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    143;

  friend void swap(ConstantPaddingLayerParams& a, ConstantPaddingLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ConstantPaddingLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ConstantPaddingLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ConstantPaddingLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ConstantPaddingLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ConstantPaddingLayerParams& from);
  void MergeFrom(const ConstantPaddingLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ConstantPaddingLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ConstantPaddingLayerParams";
  }
  protected:
  explicit ConstantPaddingLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kPadAmountsFieldNumber = 2,
    kValueFieldNumber = 1,
    kPadToGivenOutputSizeModeFieldNumber = 3,
  };
  // repeated uint64 padAmounts = 2;
  int padamounts_size() const;
  private:
  int _internal_padamounts_size() const;
  public:
  void clear_padamounts();
  private:
  uint64_t _internal_padamounts(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_padamounts() const;
  void _internal_add_padamounts(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_padamounts();
  public:
  uint64_t padamounts(int index) const;
  void set_padamounts(int index, uint64_t value);
  void add_padamounts(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      padamounts() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_padamounts();

  // float value = 1;
  void clear_value();
  float value() const;
  void set_value(float value);
  private:
  float _internal_value() const;
  void _internal_set_value(float value);
  public:

  // bool padToGivenOutputSizeMode = 3;
  void clear_padtogivenoutputsizemode();
  bool padtogivenoutputsizemode() const;
  void set_padtogivenoutputsizemode(bool value);
  private:
  bool _internal_padtogivenoutputsizemode() const;
  void _internal_set_padtogivenoutputsizemode(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ConstantPaddingLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > padamounts_;
  mutable std::atomic<int> _padamounts_cached_byte_size_;
  float value_;
  bool padtogivenoutputsizemode_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class RandomNormalLikeLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RandomNormalLikeLayerParams) */ {
 public:
  inline RandomNormalLikeLayerParams() : RandomNormalLikeLayerParams(nullptr) {}
  ~RandomNormalLikeLayerParams() override;
  explicit constexpr RandomNormalLikeLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  RandomNormalLikeLayerParams(const RandomNormalLikeLayerParams& from);
  RandomNormalLikeLayerParams(RandomNormalLikeLayerParams&& from) noexcept
    : RandomNormalLikeLayerParams() {
    *this = ::std::move(from);
  }

  inline RandomNormalLikeLayerParams& operator=(const RandomNormalLikeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline RandomNormalLikeLayerParams& operator=(RandomNormalLikeLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const RandomNormalLikeLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const RandomNormalLikeLayerParams* internal_default_instance() {
    return reinterpret_cast<const RandomNormalLikeLayerParams*>(
               &_RandomNormalLikeLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    144;

  friend void swap(RandomNormalLikeLayerParams& a, RandomNormalLikeLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(RandomNormalLikeLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(RandomNormalLikeLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  RandomNormalLikeLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<RandomNormalLikeLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const RandomNormalLikeLayerParams& from);
  void MergeFrom(const RandomNormalLikeLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RandomNormalLikeLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.RandomNormalLikeLayerParams";
  }
  protected:
  explicit RandomNormalLikeLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kSeedFieldNumber = 1,
    kMeanFieldNumber = 2,
    kStdDevFieldNumber = 3,
  };
  // int64 seed = 1;
  void clear_seed();
  int64_t seed() const;
  void set_seed(int64_t value);
  private:
  int64_t _internal_seed() const;
  void _internal_set_seed(int64_t value);
  public:

  // float mean = 2;
  void clear_mean();
  float mean() const;
  void set_mean(float value);
  private:
  float _internal_mean() const;
  void _internal_set_mean(float value);
  public:

  // float stdDev = 3;
  void clear_stddev();
  float stddev() const;
  void set_stddev(float value);
  private:
  float _internal_stddev() const;
  void _internal_set_stddev(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RandomNormalLikeLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t seed_;
  float mean_;
  float stddev_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class RandomNormalStaticLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RandomNormalStaticLayerParams) */ {
 public:
  inline RandomNormalStaticLayerParams() : RandomNormalStaticLayerParams(nullptr) {}
  ~RandomNormalStaticLayerParams() override;
  explicit constexpr RandomNormalStaticLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  RandomNormalStaticLayerParams(const RandomNormalStaticLayerParams& from);
  RandomNormalStaticLayerParams(RandomNormalStaticLayerParams&& from) noexcept
    : RandomNormalStaticLayerParams() {
    *this = ::std::move(from);
  }

  inline RandomNormalStaticLayerParams& operator=(const RandomNormalStaticLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline RandomNormalStaticLayerParams& operator=(RandomNormalStaticLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const RandomNormalStaticLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const RandomNormalStaticLayerParams* internal_default_instance() {
    return reinterpret_cast<const RandomNormalStaticLayerParams*>(
               &_RandomNormalStaticLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    145;

  friend void swap(RandomNormalStaticLayerParams& a, RandomNormalStaticLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(RandomNormalStaticLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(RandomNormalStaticLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  RandomNormalStaticLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<RandomNormalStaticLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const RandomNormalStaticLayerParams& from);
  void MergeFrom(const RandomNormalStaticLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RandomNormalStaticLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.RandomNormalStaticLayerParams";
  }
  protected:
  explicit RandomNormalStaticLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kOutputShapeFieldNumber = 4,
    kSeedFieldNumber = 1,
    kMeanFieldNumber = 2,
    kStdDevFieldNumber = 3,
  };
  // repeated uint64 outputShape = 4;
  int outputshape_size() const;
  private:
  int _internal_outputshape_size() const;
  public:
  void clear_outputshape();
  private:
  uint64_t _internal_outputshape(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_outputshape() const;
  void _internal_add_outputshape(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_outputshape();
  public:
  uint64_t outputshape(int index) const;
  void set_outputshape(int index, uint64_t value);
  void add_outputshape(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      outputshape() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_outputshape();

  // int64 seed = 1;
  void clear_seed();
  int64_t seed() const;
  void set_seed(int64_t value);
  private:
  int64_t _internal_seed() const;
  void _internal_set_seed(int64_t value);
  public:

  // float mean = 2;
  void clear_mean();
  float mean() const;
  void set_mean(float value);
  private:
  float _internal_mean() const;
  void _internal_set_mean(float value);
  public:

  // float stdDev = 3;
  void clear_stddev();
  float stddev() const;
  void set_stddev(float value);
  private:
  float _internal_stddev() const;
  void _internal_set_stddev(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RandomNormalStaticLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > outputshape_;
  mutable std::atomic<int> _outputshape_cached_byte_size_;
  int64_t seed_;
  float mean_;
  float stddev_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class RandomNormalDynamicLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RandomNormalDynamicLayerParams) */ {
 public:
  inline RandomNormalDynamicLayerParams() : RandomNormalDynamicLayerParams(nullptr) {}
  ~RandomNormalDynamicLayerParams() override;
  explicit constexpr RandomNormalDynamicLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  RandomNormalDynamicLayerParams(const RandomNormalDynamicLayerParams& from);
  RandomNormalDynamicLayerParams(RandomNormalDynamicLayerParams&& from) noexcept
    : RandomNormalDynamicLayerParams() {
    *this = ::std::move(from);
  }

  inline RandomNormalDynamicLayerParams& operator=(const RandomNormalDynamicLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline RandomNormalDynamicLayerParams& operator=(RandomNormalDynamicLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const RandomNormalDynamicLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const RandomNormalDynamicLayerParams* internal_default_instance() {
    return reinterpret_cast<const RandomNormalDynamicLayerParams*>(
               &_RandomNormalDynamicLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    146;

  friend void swap(RandomNormalDynamicLayerParams& a, RandomNormalDynamicLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(RandomNormalDynamicLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(RandomNormalDynamicLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  RandomNormalDynamicLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<RandomNormalDynamicLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const RandomNormalDynamicLayerParams& from);
  void MergeFrom(const RandomNormalDynamicLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RandomNormalDynamicLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.RandomNormalDynamicLayerParams";
  }
  protected:
  explicit RandomNormalDynamicLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kSeedFieldNumber = 1,
    kMeanFieldNumber = 2,
    kStdDevFieldNumber = 3,
  };
  // int64 seed = 1;
  void clear_seed();
  int64_t seed() const;
  void set_seed(int64_t value);
  private:
  int64_t _internal_seed() const;
  void _internal_set_seed(int64_t value);
  public:

  // float mean = 2;
  void clear_mean();
  float mean() const;
  void set_mean(float value);
  private:
  float _internal_mean() const;
  void _internal_set_mean(float value);
  public:

  // float stdDev = 3;
  void clear_stddev();
  float stddev() const;
  void set_stddev(float value);
  private:
  float _internal_stddev() const;
  void _internal_set_stddev(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RandomNormalDynamicLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t seed_;
  float mean_;
  float stddev_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class RandomUniformLikeLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RandomUniformLikeLayerParams) */ {
 public:
  inline RandomUniformLikeLayerParams() : RandomUniformLikeLayerParams(nullptr) {}
  ~RandomUniformLikeLayerParams() override;
  explicit constexpr RandomUniformLikeLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  RandomUniformLikeLayerParams(const RandomUniformLikeLayerParams& from);
  RandomUniformLikeLayerParams(RandomUniformLikeLayerParams&& from) noexcept
    : RandomUniformLikeLayerParams() {
    *this = ::std::move(from);
  }

  inline RandomUniformLikeLayerParams& operator=(const RandomUniformLikeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline RandomUniformLikeLayerParams& operator=(RandomUniformLikeLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const RandomUniformLikeLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const RandomUniformLikeLayerParams* internal_default_instance() {
    return reinterpret_cast<const RandomUniformLikeLayerParams*>(
               &_RandomUniformLikeLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    147;

  friend void swap(RandomUniformLikeLayerParams& a, RandomUniformLikeLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(RandomUniformLikeLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(RandomUniformLikeLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  RandomUniformLikeLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<RandomUniformLikeLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const RandomUniformLikeLayerParams& from);
  void MergeFrom(const RandomUniformLikeLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RandomUniformLikeLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.RandomUniformLikeLayerParams";
  }
  protected:
  explicit RandomUniformLikeLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kSeedFieldNumber = 1,
    kMinValFieldNumber = 2,
    kMaxValFieldNumber = 3,
  };
  // int64 seed = 1;
  void clear_seed();
  int64_t seed() const;
  void set_seed(int64_t value);
  private:
  int64_t _internal_seed() const;
  void _internal_set_seed(int64_t value);
  public:

  // float minVal = 2;
  void clear_minval();
  float minval() const;
  void set_minval(float value);
  private:
  float _internal_minval() const;
  void _internal_set_minval(float value);
  public:

  // float maxVal = 3;
  void clear_maxval();
  float maxval() const;
  void set_maxval(float value);
  private:
  float _internal_maxval() const;
  void _internal_set_maxval(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RandomUniformLikeLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t seed_;
  float minval_;
  float maxval_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class RandomUniformStaticLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RandomUniformStaticLayerParams) */ {
 public:
  inline RandomUniformStaticLayerParams() : RandomUniformStaticLayerParams(nullptr) {}
  ~RandomUniformStaticLayerParams() override;
  explicit constexpr RandomUniformStaticLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  RandomUniformStaticLayerParams(const RandomUniformStaticLayerParams& from);
  RandomUniformStaticLayerParams(RandomUniformStaticLayerParams&& from) noexcept
    : RandomUniformStaticLayerParams() {
    *this = ::std::move(from);
  }

  inline RandomUniformStaticLayerParams& operator=(const RandomUniformStaticLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline RandomUniformStaticLayerParams& operator=(RandomUniformStaticLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const RandomUniformStaticLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const RandomUniformStaticLayerParams* internal_default_instance() {
    return reinterpret_cast<const RandomUniformStaticLayerParams*>(
               &_RandomUniformStaticLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    148;

  friend void swap(RandomUniformStaticLayerParams& a, RandomUniformStaticLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(RandomUniformStaticLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(RandomUniformStaticLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  RandomUniformStaticLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<RandomUniformStaticLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const RandomUniformStaticLayerParams& from);
  void MergeFrom(const RandomUniformStaticLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RandomUniformStaticLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.RandomUniformStaticLayerParams";
  }
  protected:
  explicit RandomUniformStaticLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kOutputShapeFieldNumber = 4,
    kSeedFieldNumber = 1,
    kMinValFieldNumber = 2,
    kMaxValFieldNumber = 3,
  };
  // repeated uint64 outputShape = 4;
  int outputshape_size() const;
  private:
  int _internal_outputshape_size() const;
  public:
  void clear_outputshape();
  private:
  uint64_t _internal_outputshape(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_outputshape() const;
  void _internal_add_outputshape(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_outputshape();
  public:
  uint64_t outputshape(int index) const;
  void set_outputshape(int index, uint64_t value);
  void add_outputshape(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      outputshape() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_outputshape();

  // int64 seed = 1;
  void clear_seed();
  int64_t seed() const;
  void set_seed(int64_t value);
  private:
  int64_t _internal_seed() const;
  void _internal_set_seed(int64_t value);
  public:

  // float minVal = 2;
  void clear_minval();
  float minval() const;
  void set_minval(float value);
  private:
  float _internal_minval() const;
  void _internal_set_minval(float value);
  public:

  // float maxVal = 3;
  void clear_maxval();
  float maxval() const;
  void set_maxval(float value);
  private:
  float _internal_maxval() const;
  void _internal_set_maxval(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RandomUniformStaticLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > outputshape_;
  mutable std::atomic<int> _outputshape_cached_byte_size_;
  int64_t seed_;
  float minval_;
  float maxval_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class RandomUniformDynamicLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RandomUniformDynamicLayerParams) */ {
 public:
  inline RandomUniformDynamicLayerParams() : RandomUniformDynamicLayerParams(nullptr) {}
  ~RandomUniformDynamicLayerParams() override;
  explicit constexpr RandomUniformDynamicLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  RandomUniformDynamicLayerParams(const RandomUniformDynamicLayerParams& from);
  RandomUniformDynamicLayerParams(RandomUniformDynamicLayerParams&& from) noexcept
    : RandomUniformDynamicLayerParams() {
    *this = ::std::move(from);
  }

  inline RandomUniformDynamicLayerParams& operator=(const RandomUniformDynamicLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline RandomUniformDynamicLayerParams& operator=(RandomUniformDynamicLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const RandomUniformDynamicLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const RandomUniformDynamicLayerParams* internal_default_instance() {
    return reinterpret_cast<const RandomUniformDynamicLayerParams*>(
               &_RandomUniformDynamicLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    149;

  friend void swap(RandomUniformDynamicLayerParams& a, RandomUniformDynamicLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(RandomUniformDynamicLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(RandomUniformDynamicLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  RandomUniformDynamicLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<RandomUniformDynamicLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const RandomUniformDynamicLayerParams& from);
  void MergeFrom(const RandomUniformDynamicLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RandomUniformDynamicLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.RandomUniformDynamicLayerParams";
  }
  protected:
  explicit RandomUniformDynamicLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kSeedFieldNumber = 1,
    kMinValFieldNumber = 2,
    kMaxValFieldNumber = 3,
  };
  // int64 seed = 1;
  void clear_seed();
  int64_t seed() const;
  void set_seed(int64_t value);
  private:
  int64_t _internal_seed() const;
  void _internal_set_seed(int64_t value);
  public:

  // float minVal = 2;
  void clear_minval();
  float minval() const;
  void set_minval(float value);
  private:
  float _internal_minval() const;
  void _internal_set_minval(float value);
  public:

  // float maxVal = 3;
  void clear_maxval();
  float maxval() const;
  void set_maxval(float value);
  private:
  float _internal_maxval() const;
  void _internal_set_maxval(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RandomUniformDynamicLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t seed_;
  float minval_;
  float maxval_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class RandomBernoulliLikeLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RandomBernoulliLikeLayerParams) */ {
 public:
  inline RandomBernoulliLikeLayerParams() : RandomBernoulliLikeLayerParams(nullptr) {}
  ~RandomBernoulliLikeLayerParams() override;
  explicit constexpr RandomBernoulliLikeLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  RandomBernoulliLikeLayerParams(const RandomBernoulliLikeLayerParams& from);
  RandomBernoulliLikeLayerParams(RandomBernoulliLikeLayerParams&& from) noexcept
    : RandomBernoulliLikeLayerParams() {
    *this = ::std::move(from);
  }

  inline RandomBernoulliLikeLayerParams& operator=(const RandomBernoulliLikeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline RandomBernoulliLikeLayerParams& operator=(RandomBernoulliLikeLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const RandomBernoulliLikeLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const RandomBernoulliLikeLayerParams* internal_default_instance() {
    return reinterpret_cast<const RandomBernoulliLikeLayerParams*>(
               &_RandomBernoulliLikeLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    150;

  friend void swap(RandomBernoulliLikeLayerParams& a, RandomBernoulliLikeLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(RandomBernoulliLikeLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(RandomBernoulliLikeLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  RandomBernoulliLikeLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<RandomBernoulliLikeLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const RandomBernoulliLikeLayerParams& from);
  void MergeFrom(const RandomBernoulliLikeLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RandomBernoulliLikeLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.RandomBernoulliLikeLayerParams";
  }
  protected:
  explicit RandomBernoulliLikeLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kSeedFieldNumber = 1,
    kProbFieldNumber = 2,
  };
  // int64 seed = 1;
  void clear_seed();
  int64_t seed() const;
  void set_seed(int64_t value);
  private:
  int64_t _internal_seed() const;
  void _internal_set_seed(int64_t value);
  public:

  // float prob = 2;
  void clear_prob();
  float prob() const;
  void set_prob(float value);
  private:
  float _internal_prob() const;
  void _internal_set_prob(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RandomBernoulliLikeLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t seed_;
  float prob_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class RandomBernoulliStaticLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RandomBernoulliStaticLayerParams) */ {
 public:
  inline RandomBernoulliStaticLayerParams() : RandomBernoulliStaticLayerParams(nullptr) {}
  ~RandomBernoulliStaticLayerParams() override;
  explicit constexpr RandomBernoulliStaticLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  RandomBernoulliStaticLayerParams(const RandomBernoulliStaticLayerParams& from);
  RandomBernoulliStaticLayerParams(RandomBernoulliStaticLayerParams&& from) noexcept
    : RandomBernoulliStaticLayerParams() {
    *this = ::std::move(from);
  }

  inline RandomBernoulliStaticLayerParams& operator=(const RandomBernoulliStaticLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline RandomBernoulliStaticLayerParams& operator=(RandomBernoulliStaticLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const RandomBernoulliStaticLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const RandomBernoulliStaticLayerParams* internal_default_instance() {
    return reinterpret_cast<const RandomBernoulliStaticLayerParams*>(
               &_RandomBernoulliStaticLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    151;

  friend void swap(RandomBernoulliStaticLayerParams& a, RandomBernoulliStaticLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(RandomBernoulliStaticLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(RandomBernoulliStaticLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  RandomBernoulliStaticLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<RandomBernoulliStaticLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const RandomBernoulliStaticLayerParams& from);
  void MergeFrom(const RandomBernoulliStaticLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RandomBernoulliStaticLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.RandomBernoulliStaticLayerParams";
  }
  protected:
  explicit RandomBernoulliStaticLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kOutputShapeFieldNumber = 3,
    kSeedFieldNumber = 1,
    kProbFieldNumber = 2,
  };
  // repeated uint64 outputShape = 3;
  int outputshape_size() const;
  private:
  int _internal_outputshape_size() const;
  public:
  void clear_outputshape();
  private:
  uint64_t _internal_outputshape(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_outputshape() const;
  void _internal_add_outputshape(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_outputshape();
  public:
  uint64_t outputshape(int index) const;
  void set_outputshape(int index, uint64_t value);
  void add_outputshape(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      outputshape() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_outputshape();

  // int64 seed = 1;
  void clear_seed();
  int64_t seed() const;
  void set_seed(int64_t value);
  private:
  int64_t _internal_seed() const;
  void _internal_set_seed(int64_t value);
  public:

  // float prob = 2;
  void clear_prob();
  float prob() const;
  void set_prob(float value);
  private:
  float _internal_prob() const;
  void _internal_set_prob(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RandomBernoulliStaticLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > outputshape_;
  mutable std::atomic<int> _outputshape_cached_byte_size_;
  int64_t seed_;
  float prob_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class RandomBernoulliDynamicLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RandomBernoulliDynamicLayerParams) */ {
 public:
  inline RandomBernoulliDynamicLayerParams() : RandomBernoulliDynamicLayerParams(nullptr) {}
  ~RandomBernoulliDynamicLayerParams() override;
  explicit constexpr RandomBernoulliDynamicLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  RandomBernoulliDynamicLayerParams(const RandomBernoulliDynamicLayerParams& from);
  RandomBernoulliDynamicLayerParams(RandomBernoulliDynamicLayerParams&& from) noexcept
    : RandomBernoulliDynamicLayerParams() {
    *this = ::std::move(from);
  }

  inline RandomBernoulliDynamicLayerParams& operator=(const RandomBernoulliDynamicLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline RandomBernoulliDynamicLayerParams& operator=(RandomBernoulliDynamicLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const RandomBernoulliDynamicLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const RandomBernoulliDynamicLayerParams* internal_default_instance() {
    return reinterpret_cast<const RandomBernoulliDynamicLayerParams*>(
               &_RandomBernoulliDynamicLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    152;

  friend void swap(RandomBernoulliDynamicLayerParams& a, RandomBernoulliDynamicLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(RandomBernoulliDynamicLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(RandomBernoulliDynamicLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  RandomBernoulliDynamicLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<RandomBernoulliDynamicLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const RandomBernoulliDynamicLayerParams& from);
  void MergeFrom(const RandomBernoulliDynamicLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RandomBernoulliDynamicLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.RandomBernoulliDynamicLayerParams";
  }
  protected:
  explicit RandomBernoulliDynamicLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kSeedFieldNumber = 1,
    kProbFieldNumber = 2,
  };
  // int64 seed = 1;
  void clear_seed();
  int64_t seed() const;
  void set_seed(int64_t value);
  private:
  int64_t _internal_seed() const;
  void _internal_set_seed(int64_t value);
  public:

  // float prob = 2;
  void clear_prob();
  float prob() const;
  void set_prob(float value);
  private:
  float _internal_prob() const;
  void _internal_set_prob(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RandomBernoulliDynamicLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t seed_;
  float prob_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class CategoricalDistributionLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CategoricalDistributionLayerParams) */ {
 public:
  inline CategoricalDistributionLayerParams() : CategoricalDistributionLayerParams(nullptr) {}
  ~CategoricalDistributionLayerParams() override;
  explicit constexpr CategoricalDistributionLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  CategoricalDistributionLayerParams(const CategoricalDistributionLayerParams& from);
  CategoricalDistributionLayerParams(CategoricalDistributionLayerParams&& from) noexcept
    : CategoricalDistributionLayerParams() {
    *this = ::std::move(from);
  }

  inline CategoricalDistributionLayerParams& operator=(const CategoricalDistributionLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline CategoricalDistributionLayerParams& operator=(CategoricalDistributionLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const CategoricalDistributionLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const CategoricalDistributionLayerParams* internal_default_instance() {
    return reinterpret_cast<const CategoricalDistributionLayerParams*>(
               &_CategoricalDistributionLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    153;

  friend void swap(CategoricalDistributionLayerParams& a, CategoricalDistributionLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(CategoricalDistributionLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(CategoricalDistributionLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  CategoricalDistributionLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<CategoricalDistributionLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const CategoricalDistributionLayerParams& from);
  void MergeFrom(const CategoricalDistributionLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CategoricalDistributionLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.CategoricalDistributionLayerParams";
  }
  protected:
  explicit CategoricalDistributionLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kSeedFieldNumber = 1,
    kNumSamplesFieldNumber = 2,
    kIsLogitsFieldNumber = 3,
    kEpsFieldNumber = 4,
    kTemperatureFieldNumber = 5,
  };
  // int64 seed = 1;
  void clear_seed();
  int64_t seed() const;
  void set_seed(int64_t value);
  private:
  int64_t _internal_seed() const;
  void _internal_set_seed(int64_t value);
  public:

  // int64 numSamples = 2;
  void clear_numsamples();
  int64_t numsamples() const;
  void set_numsamples(int64_t value);
  private:
  int64_t _internal_numsamples() const;
  void _internal_set_numsamples(int64_t value);
  public:

  // bool isLogits = 3;
  void clear_islogits();
  bool islogits() const;
  void set_islogits(bool value);
  private:
  bool _internal_islogits() const;
  void _internal_set_islogits(bool value);
  public:

  // float eps = 4;
  void clear_eps();
  float eps() const;
  void set_eps(float value);
  private:
  float _internal_eps() const;
  void _internal_set_eps(float value);
  public:

  // float temperature = 5;
  void clear_temperature();
  float temperature() const;
  void set_temperature(float value);
  private:
  float _internal_temperature() const;
  void _internal_set_temperature(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.CategoricalDistributionLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t seed_;
  int64_t numsamples_;
  bool islogits_;
  float eps_;
  float temperature_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ReduceL1LayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceL1LayerParams) */ {
 public:
  inline ReduceL1LayerParams() : ReduceL1LayerParams(nullptr) {}
  ~ReduceL1LayerParams() override;
  explicit constexpr ReduceL1LayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ReduceL1LayerParams(const ReduceL1LayerParams& from);
  ReduceL1LayerParams(ReduceL1LayerParams&& from) noexcept
    : ReduceL1LayerParams() {
    *this = ::std::move(from);
  }

  inline ReduceL1LayerParams& operator=(const ReduceL1LayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ReduceL1LayerParams& operator=(ReduceL1LayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ReduceL1LayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ReduceL1LayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceL1LayerParams*>(
               &_ReduceL1LayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    154;

  friend void swap(ReduceL1LayerParams& a, ReduceL1LayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ReduceL1LayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ReduceL1LayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ReduceL1LayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ReduceL1LayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ReduceL1LayerParams& from);
  void MergeFrom(const ReduceL1LayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceL1LayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ReduceL1LayerParams";
  }
  protected:
  explicit ReduceL1LayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxesFieldNumber = 1,
    kKeepDimsFieldNumber = 2,
    kReduceAllFieldNumber = 3,
  };
  // repeated int64 axes = 1;
  int axes_size() const;
  private:
  int _internal_axes_size() const;
  public:
  void clear_axes();
  private:
  int64_t _internal_axes(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_axes() const;
  void _internal_add_axes(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_axes();
  public:
  int64_t axes(int index) const;
  void set_axes(int index, int64_t value);
  void add_axes(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      axes() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_axes();

  // bool keepDims = 2;
  void clear_keepdims();
  bool keepdims() const;
  void set_keepdims(bool value);
  private:
  bool _internal_keepdims() const;
  void _internal_set_keepdims(bool value);
  public:

  // bool reduceAll = 3;
  void clear_reduceall();
  bool reduceall() const;
  void set_reduceall(bool value);
  private:
  bool _internal_reduceall() const;
  void _internal_set_reduceall(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceL1LayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > axes_;
  mutable std::atomic<int> _axes_cached_byte_size_;
  bool keepdims_;
  bool reduceall_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ReduceL2LayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceL2LayerParams) */ {
 public:
  inline ReduceL2LayerParams() : ReduceL2LayerParams(nullptr) {}
  ~ReduceL2LayerParams() override;
  explicit constexpr ReduceL2LayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ReduceL2LayerParams(const ReduceL2LayerParams& from);
  ReduceL2LayerParams(ReduceL2LayerParams&& from) noexcept
    : ReduceL2LayerParams() {
    *this = ::std::move(from);
  }

  inline ReduceL2LayerParams& operator=(const ReduceL2LayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ReduceL2LayerParams& operator=(ReduceL2LayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ReduceL2LayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ReduceL2LayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceL2LayerParams*>(
               &_ReduceL2LayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    155;

  friend void swap(ReduceL2LayerParams& a, ReduceL2LayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ReduceL2LayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ReduceL2LayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ReduceL2LayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ReduceL2LayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ReduceL2LayerParams& from);
  void MergeFrom(const ReduceL2LayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceL2LayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ReduceL2LayerParams";
  }
  protected:
  explicit ReduceL2LayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxesFieldNumber = 1,
    kKeepDimsFieldNumber = 2,
    kReduceAllFieldNumber = 3,
  };
  // repeated int64 axes = 1;
  int axes_size() const;
  private:
  int _internal_axes_size() const;
  public:
  void clear_axes();
  private:
  int64_t _internal_axes(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_axes() const;
  void _internal_add_axes(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_axes();
  public:
  int64_t axes(int index) const;
  void set_axes(int index, int64_t value);
  void add_axes(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      axes() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_axes();

  // bool keepDims = 2;
  void clear_keepdims();
  bool keepdims() const;
  void set_keepdims(bool value);
  private:
  bool _internal_keepdims() const;
  void _internal_set_keepdims(bool value);
  public:

  // bool reduceAll = 3;
  void clear_reduceall();
  bool reduceall() const;
  void set_reduceall(bool value);
  private:
  bool _internal_reduceall() const;
  void _internal_set_reduceall(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceL2LayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > axes_;
  mutable std::atomic<int> _axes_cached_byte_size_;
  bool keepdims_;
  bool reduceall_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ReduceMaxLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceMaxLayerParams) */ {
 public:
  inline ReduceMaxLayerParams() : ReduceMaxLayerParams(nullptr) {}
  ~ReduceMaxLayerParams() override;
  explicit constexpr ReduceMaxLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ReduceMaxLayerParams(const ReduceMaxLayerParams& from);
  ReduceMaxLayerParams(ReduceMaxLayerParams&& from) noexcept
    : ReduceMaxLayerParams() {
    *this = ::std::move(from);
  }

  inline ReduceMaxLayerParams& operator=(const ReduceMaxLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ReduceMaxLayerParams& operator=(ReduceMaxLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ReduceMaxLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ReduceMaxLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceMaxLayerParams*>(
               &_ReduceMaxLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    156;

  friend void swap(ReduceMaxLayerParams& a, ReduceMaxLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ReduceMaxLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ReduceMaxLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ReduceMaxLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ReduceMaxLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ReduceMaxLayerParams& from);
  void MergeFrom(const ReduceMaxLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceMaxLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ReduceMaxLayerParams";
  }
  protected:
  explicit ReduceMaxLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxesFieldNumber = 1,
    kKeepDimsFieldNumber = 2,
    kReduceAllFieldNumber = 3,
  };
  // repeated int64 axes = 1;
  int axes_size() const;
  private:
  int _internal_axes_size() const;
  public:
  void clear_axes();
  private:
  int64_t _internal_axes(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_axes() const;
  void _internal_add_axes(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_axes();
  public:
  int64_t axes(int index) const;
  void set_axes(int index, int64_t value);
  void add_axes(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      axes() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_axes();

  // bool keepDims = 2;
  void clear_keepdims();
  bool keepdims() const;
  void set_keepdims(bool value);
  private:
  bool _internal_keepdims() const;
  void _internal_set_keepdims(bool value);
  public:

  // bool reduceAll = 3;
  void clear_reduceall();
  bool reduceall() const;
  void set_reduceall(bool value);
  private:
  bool _internal_reduceall() const;
  void _internal_set_reduceall(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceMaxLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > axes_;
  mutable std::atomic<int> _axes_cached_byte_size_;
  bool keepdims_;
  bool reduceall_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ReduceMinLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceMinLayerParams) */ {
 public:
  inline ReduceMinLayerParams() : ReduceMinLayerParams(nullptr) {}
  ~ReduceMinLayerParams() override;
  explicit constexpr ReduceMinLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ReduceMinLayerParams(const ReduceMinLayerParams& from);
  ReduceMinLayerParams(ReduceMinLayerParams&& from) noexcept
    : ReduceMinLayerParams() {
    *this = ::std::move(from);
  }

  inline ReduceMinLayerParams& operator=(const ReduceMinLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ReduceMinLayerParams& operator=(ReduceMinLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ReduceMinLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ReduceMinLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceMinLayerParams*>(
               &_ReduceMinLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    157;

  friend void swap(ReduceMinLayerParams& a, ReduceMinLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ReduceMinLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ReduceMinLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ReduceMinLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ReduceMinLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ReduceMinLayerParams& from);
  void MergeFrom(const ReduceMinLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceMinLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ReduceMinLayerParams";
  }
  protected:
  explicit ReduceMinLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxesFieldNumber = 1,
    kKeepDimsFieldNumber = 2,
    kReduceAllFieldNumber = 3,
  };
  // repeated int64 axes = 1;
  int axes_size() const;
  private:
  int _internal_axes_size() const;
  public:
  void clear_axes();
  private:
  int64_t _internal_axes(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_axes() const;
  void _internal_add_axes(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_axes();
  public:
  int64_t axes(int index) const;
  void set_axes(int index, int64_t value);
  void add_axes(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      axes() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_axes();

  // bool keepDims = 2;
  void clear_keepdims();
  bool keepdims() const;
  void set_keepdims(bool value);
  private:
  bool _internal_keepdims() const;
  void _internal_set_keepdims(bool value);
  public:

  // bool reduceAll = 3;
  void clear_reduceall();
  bool reduceall() const;
  void set_reduceall(bool value);
  private:
  bool _internal_reduceall() const;
  void _internal_set_reduceall(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceMinLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > axes_;
  mutable std::atomic<int> _axes_cached_byte_size_;
  bool keepdims_;
  bool reduceall_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ReduceSumLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceSumLayerParams) */ {
 public:
  inline ReduceSumLayerParams() : ReduceSumLayerParams(nullptr) {}
  ~ReduceSumLayerParams() override;
  explicit constexpr ReduceSumLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ReduceSumLayerParams(const ReduceSumLayerParams& from);
  ReduceSumLayerParams(ReduceSumLayerParams&& from) noexcept
    : ReduceSumLayerParams() {
    *this = ::std::move(from);
  }

  inline ReduceSumLayerParams& operator=(const ReduceSumLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ReduceSumLayerParams& operator=(ReduceSumLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ReduceSumLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ReduceSumLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceSumLayerParams*>(
               &_ReduceSumLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    158;

  friend void swap(ReduceSumLayerParams& a, ReduceSumLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ReduceSumLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ReduceSumLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ReduceSumLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ReduceSumLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ReduceSumLayerParams& from);
  void MergeFrom(const ReduceSumLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceSumLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ReduceSumLayerParams";
  }
  protected:
  explicit ReduceSumLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxesFieldNumber = 1,
    kKeepDimsFieldNumber = 2,
    kReduceAllFieldNumber = 3,
  };
  // repeated int64 axes = 1;
  int axes_size() const;
  private:
  int _internal_axes_size() const;
  public:
  void clear_axes();
  private:
  int64_t _internal_axes(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_axes() const;
  void _internal_add_axes(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_axes();
  public:
  int64_t axes(int index) const;
  void set_axes(int index, int64_t value);
  void add_axes(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      axes() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_axes();

  // bool keepDims = 2;
  void clear_keepdims();
  bool keepdims() const;
  void set_keepdims(bool value);
  private:
  bool _internal_keepdims() const;
  void _internal_set_keepdims(bool value);
  public:

  // bool reduceAll = 3;
  void clear_reduceall();
  bool reduceall() const;
  void set_reduceall(bool value);
  private:
  bool _internal_reduceall() const;
  void _internal_set_reduceall(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceSumLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > axes_;
  mutable std::atomic<int> _axes_cached_byte_size_;
  bool keepdims_;
  bool reduceall_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ReduceProdLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceProdLayerParams) */ {
 public:
  inline ReduceProdLayerParams() : ReduceProdLayerParams(nullptr) {}
  ~ReduceProdLayerParams() override;
  explicit constexpr ReduceProdLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ReduceProdLayerParams(const ReduceProdLayerParams& from);
  ReduceProdLayerParams(ReduceProdLayerParams&& from) noexcept
    : ReduceProdLayerParams() {
    *this = ::std::move(from);
  }

  inline ReduceProdLayerParams& operator=(const ReduceProdLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ReduceProdLayerParams& operator=(ReduceProdLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ReduceProdLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ReduceProdLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceProdLayerParams*>(
               &_ReduceProdLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    159;

  friend void swap(ReduceProdLayerParams& a, ReduceProdLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ReduceProdLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ReduceProdLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ReduceProdLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ReduceProdLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ReduceProdLayerParams& from);
  void MergeFrom(const ReduceProdLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceProdLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ReduceProdLayerParams";
  }
  protected:
  explicit ReduceProdLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxesFieldNumber = 1,
    kKeepDimsFieldNumber = 2,
    kReduceAllFieldNumber = 3,
  };
  // repeated int64 axes = 1;
  int axes_size() const;
  private:
  int _internal_axes_size() const;
  public:
  void clear_axes();
  private:
  int64_t _internal_axes(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_axes() const;
  void _internal_add_axes(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_axes();
  public:
  int64_t axes(int index) const;
  void set_axes(int index, int64_t value);
  void add_axes(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      axes() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_axes();

  // bool keepDims = 2;
  void clear_keepdims();
  bool keepdims() const;
  void set_keepdims(bool value);
  private:
  bool _internal_keepdims() const;
  void _internal_set_keepdims(bool value);
  public:

  // bool reduceAll = 3;
  void clear_reduceall();
  bool reduceall() const;
  void set_reduceall(bool value);
  private:
  bool _internal_reduceall() const;
  void _internal_set_reduceall(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceProdLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > axes_;
  mutable std::atomic<int> _axes_cached_byte_size_;
  bool keepdims_;
  bool reduceall_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ReduceMeanLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceMeanLayerParams) */ {
 public:
  inline ReduceMeanLayerParams() : ReduceMeanLayerParams(nullptr) {}
  ~ReduceMeanLayerParams() override;
  explicit constexpr ReduceMeanLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ReduceMeanLayerParams(const ReduceMeanLayerParams& from);
  ReduceMeanLayerParams(ReduceMeanLayerParams&& from) noexcept
    : ReduceMeanLayerParams() {
    *this = ::std::move(from);
  }

  inline ReduceMeanLayerParams& operator=(const ReduceMeanLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ReduceMeanLayerParams& operator=(ReduceMeanLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ReduceMeanLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ReduceMeanLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceMeanLayerParams*>(
               &_ReduceMeanLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    160;

  friend void swap(ReduceMeanLayerParams& a, ReduceMeanLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ReduceMeanLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ReduceMeanLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ReduceMeanLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ReduceMeanLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ReduceMeanLayerParams& from);
  void MergeFrom(const ReduceMeanLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceMeanLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ReduceMeanLayerParams";
  }
  protected:
  explicit ReduceMeanLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxesFieldNumber = 1,
    kKeepDimsFieldNumber = 2,
    kReduceAllFieldNumber = 3,
  };
  // repeated int64 axes = 1;
  int axes_size() const;
  private:
  int _internal_axes_size() const;
  public:
  void clear_axes();
  private:
  int64_t _internal_axes(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_axes() const;
  void _internal_add_axes(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_axes();
  public:
  int64_t axes(int index) const;
  void set_axes(int index, int64_t value);
  void add_axes(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      axes() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_axes();

  // bool keepDims = 2;
  void clear_keepdims();
  bool keepdims() const;
  void set_keepdims(bool value);
  private:
  bool _internal_keepdims() const;
  void _internal_set_keepdims(bool value);
  public:

  // bool reduceAll = 3;
  void clear_reduceall();
  bool reduceall() const;
  void set_reduceall(bool value);
  private:
  bool _internal_reduceall() const;
  void _internal_set_reduceall(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceMeanLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > axes_;
  mutable std::atomic<int> _axes_cached_byte_size_;
  bool keepdims_;
  bool reduceall_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ReduceLogSumLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceLogSumLayerParams) */ {
 public:
  inline ReduceLogSumLayerParams() : ReduceLogSumLayerParams(nullptr) {}
  ~ReduceLogSumLayerParams() override;
  explicit constexpr ReduceLogSumLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ReduceLogSumLayerParams(const ReduceLogSumLayerParams& from);
  ReduceLogSumLayerParams(ReduceLogSumLayerParams&& from) noexcept
    : ReduceLogSumLayerParams() {
    *this = ::std::move(from);
  }

  inline ReduceLogSumLayerParams& operator=(const ReduceLogSumLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ReduceLogSumLayerParams& operator=(ReduceLogSumLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ReduceLogSumLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ReduceLogSumLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceLogSumLayerParams*>(
               &_ReduceLogSumLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    161;

  friend void swap(ReduceLogSumLayerParams& a, ReduceLogSumLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ReduceLogSumLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ReduceLogSumLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ReduceLogSumLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ReduceLogSumLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ReduceLogSumLayerParams& from);
  void MergeFrom(const ReduceLogSumLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceLogSumLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ReduceLogSumLayerParams";
  }
  protected:
  explicit ReduceLogSumLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxesFieldNumber = 1,
    kKeepDimsFieldNumber = 2,
    kReduceAllFieldNumber = 3,
  };
  // repeated int64 axes = 1;
  int axes_size() const;
  private:
  int _internal_axes_size() const;
  public:
  void clear_axes();
  private:
  int64_t _internal_axes(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_axes() const;
  void _internal_add_axes(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_axes();
  public:
  int64_t axes(int index) const;
  void set_axes(int index, int64_t value);
  void add_axes(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      axes() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_axes();

  // bool keepDims = 2;
  void clear_keepdims();
  bool keepdims() const;
  void set_keepdims(bool value);
  private:
  bool _internal_keepdims() const;
  void _internal_set_keepdims(bool value);
  public:

  // bool reduceAll = 3;
  void clear_reduceall();
  bool reduceall() const;
  void set_reduceall(bool value);
  private:
  bool _internal_reduceall() const;
  void _internal_set_reduceall(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceLogSumLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > axes_;
  mutable std::atomic<int> _axes_cached_byte_size_;
  bool keepdims_;
  bool reduceall_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ReduceSumSquareLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceSumSquareLayerParams) */ {
 public:
  inline ReduceSumSquareLayerParams() : ReduceSumSquareLayerParams(nullptr) {}
  ~ReduceSumSquareLayerParams() override;
  explicit constexpr ReduceSumSquareLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ReduceSumSquareLayerParams(const ReduceSumSquareLayerParams& from);
  ReduceSumSquareLayerParams(ReduceSumSquareLayerParams&& from) noexcept
    : ReduceSumSquareLayerParams() {
    *this = ::std::move(from);
  }

  inline ReduceSumSquareLayerParams& operator=(const ReduceSumSquareLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ReduceSumSquareLayerParams& operator=(ReduceSumSquareLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ReduceSumSquareLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ReduceSumSquareLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceSumSquareLayerParams*>(
               &_ReduceSumSquareLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    162;

  friend void swap(ReduceSumSquareLayerParams& a, ReduceSumSquareLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ReduceSumSquareLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ReduceSumSquareLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ReduceSumSquareLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ReduceSumSquareLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ReduceSumSquareLayerParams& from);
  void MergeFrom(const ReduceSumSquareLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceSumSquareLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ReduceSumSquareLayerParams";
  }
  protected:
  explicit ReduceSumSquareLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxesFieldNumber = 1,
    kKeepDimsFieldNumber = 2,
    kReduceAllFieldNumber = 3,
  };
  // repeated int64 axes = 1;
  int axes_size() const;
  private:
  int _internal_axes_size() const;
  public:
  void clear_axes();
  private:
  int64_t _internal_axes(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_axes() const;
  void _internal_add_axes(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_axes();
  public:
  int64_t axes(int index) const;
  void set_axes(int index, int64_t value);
  void add_axes(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      axes() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_axes();

  // bool keepDims = 2;
  void clear_keepdims();
  bool keepdims() const;
  void set_keepdims(bool value);
  private:
  bool _internal_keepdims() const;
  void _internal_set_keepdims(bool value);
  public:

  // bool reduceAll = 3;
  void clear_reduceall();
  bool reduceall() const;
  void set_reduceall(bool value);
  private:
  bool _internal_reduceall() const;
  void _internal_set_reduceall(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceSumSquareLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > axes_;
  mutable std::atomic<int> _axes_cached_byte_size_;
  bool keepdims_;
  bool reduceall_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ReduceLogSumExpLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceLogSumExpLayerParams) */ {
 public:
  inline ReduceLogSumExpLayerParams() : ReduceLogSumExpLayerParams(nullptr) {}
  ~ReduceLogSumExpLayerParams() override;
  explicit constexpr ReduceLogSumExpLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ReduceLogSumExpLayerParams(const ReduceLogSumExpLayerParams& from);
  ReduceLogSumExpLayerParams(ReduceLogSumExpLayerParams&& from) noexcept
    : ReduceLogSumExpLayerParams() {
    *this = ::std::move(from);
  }

  inline ReduceLogSumExpLayerParams& operator=(const ReduceLogSumExpLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ReduceLogSumExpLayerParams& operator=(ReduceLogSumExpLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ReduceLogSumExpLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ReduceLogSumExpLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceLogSumExpLayerParams*>(
               &_ReduceLogSumExpLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    163;

  friend void swap(ReduceLogSumExpLayerParams& a, ReduceLogSumExpLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ReduceLogSumExpLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ReduceLogSumExpLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ReduceLogSumExpLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ReduceLogSumExpLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ReduceLogSumExpLayerParams& from);
  void MergeFrom(const ReduceLogSumExpLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceLogSumExpLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ReduceLogSumExpLayerParams";
  }
  protected:
  explicit ReduceLogSumExpLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxesFieldNumber = 1,
    kKeepDimsFieldNumber = 2,
    kReduceAllFieldNumber = 3,
  };
  // repeated int64 axes = 1;
  int axes_size() const;
  private:
  int _internal_axes_size() const;
  public:
  void clear_axes();
  private:
  int64_t _internal_axes(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_axes() const;
  void _internal_add_axes(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_axes();
  public:
  int64_t axes(int index) const;
  void set_axes(int index, int64_t value);
  void add_axes(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      axes() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_axes();

  // bool keepDims = 2;
  void clear_keepdims();
  bool keepdims() const;
  void set_keepdims(bool value);
  private:
  bool _internal_keepdims() const;
  void _internal_set_keepdims(bool value);
  public:

  // bool reduceAll = 3;
  void clear_reduceall();
  bool reduceall() const;
  void set_reduceall(bool value);
  private:
  bool _internal_reduceall() const;
  void _internal_set_reduceall(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceLogSumExpLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > axes_;
  mutable std::atomic<int> _axes_cached_byte_size_;
  bool keepdims_;
  bool reduceall_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ExpandDimsLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ExpandDimsLayerParams) */ {
 public:
  inline ExpandDimsLayerParams() : ExpandDimsLayerParams(nullptr) {}
  ~ExpandDimsLayerParams() override;
  explicit constexpr ExpandDimsLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ExpandDimsLayerParams(const ExpandDimsLayerParams& from);
  ExpandDimsLayerParams(ExpandDimsLayerParams&& from) noexcept
    : ExpandDimsLayerParams() {
    *this = ::std::move(from);
  }

  inline ExpandDimsLayerParams& operator=(const ExpandDimsLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ExpandDimsLayerParams& operator=(ExpandDimsLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ExpandDimsLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ExpandDimsLayerParams* internal_default_instance() {
    return reinterpret_cast<const ExpandDimsLayerParams*>(
               &_ExpandDimsLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    164;

  friend void swap(ExpandDimsLayerParams& a, ExpandDimsLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ExpandDimsLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ExpandDimsLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ExpandDimsLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ExpandDimsLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ExpandDimsLayerParams& from);
  void MergeFrom(const ExpandDimsLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ExpandDimsLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ExpandDimsLayerParams";
  }
  protected:
  explicit ExpandDimsLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxesFieldNumber = 1,
  };
  // repeated int64 axes = 1;
  int axes_size() const;
  private:
  int _internal_axes_size() const;
  public:
  void clear_axes();
  private:
  int64_t _internal_axes(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_axes() const;
  void _internal_add_axes(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_axes();
  public:
  int64_t axes(int index) const;
  void set_axes(int index, int64_t value);
  void add_axes(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      axes() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_axes();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ExpandDimsLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > axes_;
  mutable std::atomic<int> _axes_cached_byte_size_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class FlattenTo2DLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.FlattenTo2DLayerParams) */ {
 public:
  inline FlattenTo2DLayerParams() : FlattenTo2DLayerParams(nullptr) {}
  ~FlattenTo2DLayerParams() override;
  explicit constexpr FlattenTo2DLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  FlattenTo2DLayerParams(const FlattenTo2DLayerParams& from);
  FlattenTo2DLayerParams(FlattenTo2DLayerParams&& from) noexcept
    : FlattenTo2DLayerParams() {
    *this = ::std::move(from);
  }

  inline FlattenTo2DLayerParams& operator=(const FlattenTo2DLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline FlattenTo2DLayerParams& operator=(FlattenTo2DLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const FlattenTo2DLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const FlattenTo2DLayerParams* internal_default_instance() {
    return reinterpret_cast<const FlattenTo2DLayerParams*>(
               &_FlattenTo2DLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    165;

  friend void swap(FlattenTo2DLayerParams& a, FlattenTo2DLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(FlattenTo2DLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(FlattenTo2DLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  FlattenTo2DLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<FlattenTo2DLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const FlattenTo2DLayerParams& from);
  void MergeFrom(const FlattenTo2DLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(FlattenTo2DLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.FlattenTo2DLayerParams";
  }
  protected:
  explicit FlattenTo2DLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxisFieldNumber = 1,
  };
  // int64 axis = 1;
  void clear_axis();
  int64_t axis() const;
  void set_axis(int64_t value);
  private:
  int64_t _internal_axis() const;
  void _internal_set_axis(int64_t value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.FlattenTo2DLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t axis_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ReshapeStaticLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReshapeStaticLayerParams) */ {
 public:
  inline ReshapeStaticLayerParams() : ReshapeStaticLayerParams(nullptr) {}
  ~ReshapeStaticLayerParams() override;
  explicit constexpr ReshapeStaticLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ReshapeStaticLayerParams(const ReshapeStaticLayerParams& from);
  ReshapeStaticLayerParams(ReshapeStaticLayerParams&& from) noexcept
    : ReshapeStaticLayerParams() {
    *this = ::std::move(from);
  }

  inline ReshapeStaticLayerParams& operator=(const ReshapeStaticLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ReshapeStaticLayerParams& operator=(ReshapeStaticLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ReshapeStaticLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ReshapeStaticLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReshapeStaticLayerParams*>(
               &_ReshapeStaticLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    166;

  friend void swap(ReshapeStaticLayerParams& a, ReshapeStaticLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ReshapeStaticLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ReshapeStaticLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ReshapeStaticLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ReshapeStaticLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ReshapeStaticLayerParams& from);
  void MergeFrom(const ReshapeStaticLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReshapeStaticLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ReshapeStaticLayerParams";
  }
  protected:
  explicit ReshapeStaticLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kTargetShapeFieldNumber = 1,
  };
  // repeated int64 targetShape = 1;
  int targetshape_size() const;
  private:
  int _internal_targetshape_size() const;
  public:
  void clear_targetshape();
  private:
  int64_t _internal_targetshape(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_targetshape() const;
  void _internal_add_targetshape(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_targetshape();
  public:
  int64_t targetshape(int index) const;
  void set_targetshape(int index, int64_t value);
  void add_targetshape(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      targetshape() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_targetshape();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReshapeStaticLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > targetshape_;
  mutable std::atomic<int> _targetshape_cached_byte_size_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ReshapeLikeLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReshapeLikeLayerParams) */ {
 public:
  inline ReshapeLikeLayerParams() : ReshapeLikeLayerParams(nullptr) {}
  ~ReshapeLikeLayerParams() override;
  explicit constexpr ReshapeLikeLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ReshapeLikeLayerParams(const ReshapeLikeLayerParams& from);
  ReshapeLikeLayerParams(ReshapeLikeLayerParams&& from) noexcept
    : ReshapeLikeLayerParams() {
    *this = ::std::move(from);
  }

  inline ReshapeLikeLayerParams& operator=(const ReshapeLikeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ReshapeLikeLayerParams& operator=(ReshapeLikeLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ReshapeLikeLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ReshapeLikeLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReshapeLikeLayerParams*>(
               &_ReshapeLikeLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    167;

  friend void swap(ReshapeLikeLayerParams& a, ReshapeLikeLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ReshapeLikeLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ReshapeLikeLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ReshapeLikeLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ReshapeLikeLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ReshapeLikeLayerParams& from);
  void MergeFrom(const ReshapeLikeLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReshapeLikeLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ReshapeLikeLayerParams";
  }
  protected:
  explicit ReshapeLikeLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReshapeLikeLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ReshapeDynamicLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReshapeDynamicLayerParams) */ {
 public:
  inline ReshapeDynamicLayerParams() : ReshapeDynamicLayerParams(nullptr) {}
  ~ReshapeDynamicLayerParams() override;
  explicit constexpr ReshapeDynamicLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ReshapeDynamicLayerParams(const ReshapeDynamicLayerParams& from);
  ReshapeDynamicLayerParams(ReshapeDynamicLayerParams&& from) noexcept
    : ReshapeDynamicLayerParams() {
    *this = ::std::move(from);
  }

  inline ReshapeDynamicLayerParams& operator=(const ReshapeDynamicLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ReshapeDynamicLayerParams& operator=(ReshapeDynamicLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ReshapeDynamicLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ReshapeDynamicLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReshapeDynamicLayerParams*>(
               &_ReshapeDynamicLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    168;

  friend void swap(ReshapeDynamicLayerParams& a, ReshapeDynamicLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ReshapeDynamicLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ReshapeDynamicLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ReshapeDynamicLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ReshapeDynamicLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ReshapeDynamicLayerParams& from);
  void MergeFrom(const ReshapeDynamicLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReshapeDynamicLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ReshapeDynamicLayerParams";
  }
  protected:
  explicit ReshapeDynamicLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReshapeDynamicLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class SqueezeLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SqueezeLayerParams) */ {
 public:
  inline SqueezeLayerParams() : SqueezeLayerParams(nullptr) {}
  ~SqueezeLayerParams() override;
  explicit constexpr SqueezeLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SqueezeLayerParams(const SqueezeLayerParams& from);
  SqueezeLayerParams(SqueezeLayerParams&& from) noexcept
    : SqueezeLayerParams() {
    *this = ::std::move(from);
  }

  inline SqueezeLayerParams& operator=(const SqueezeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline SqueezeLayerParams& operator=(SqueezeLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const SqueezeLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const SqueezeLayerParams* internal_default_instance() {
    return reinterpret_cast<const SqueezeLayerParams*>(
               &_SqueezeLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    169;

  friend void swap(SqueezeLayerParams& a, SqueezeLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(SqueezeLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SqueezeLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SqueezeLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SqueezeLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const SqueezeLayerParams& from);
  void MergeFrom(const SqueezeLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SqueezeLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.SqueezeLayerParams";
  }
  protected:
  explicit SqueezeLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxesFieldNumber = 1,
    kSqueezeAllFieldNumber = 2,
  };
  // repeated int64 axes = 1;
  int axes_size() const;
  private:
  int _internal_axes_size() const;
  public:
  void clear_axes();
  private:
  int64_t _internal_axes(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_axes() const;
  void _internal_add_axes(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_axes();
  public:
  int64_t axes(int index) const;
  void set_axes(int index, int64_t value);
  void add_axes(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      axes() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_axes();

  // bool squeezeAll = 2;
  void clear_squeezeall();
  bool squeezeall() const;
  void set_squeezeall(bool value);
  private:
  bool _internal_squeezeall() const;
  void _internal_set_squeezeall(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SqueezeLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > axes_;
  mutable std::atomic<int> _axes_cached_byte_size_;
  bool squeezeall_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class TopKLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.TopKLayerParams) */ {
 public:
  inline TopKLayerParams() : TopKLayerParams(nullptr) {}
  ~TopKLayerParams() override;
  explicit constexpr TopKLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  TopKLayerParams(const TopKLayerParams& from);
  TopKLayerParams(TopKLayerParams&& from) noexcept
    : TopKLayerParams() {
    *this = ::std::move(from);
  }

  inline TopKLayerParams& operator=(const TopKLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline TopKLayerParams& operator=(TopKLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const TopKLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const TopKLayerParams* internal_default_instance() {
    return reinterpret_cast<const TopKLayerParams*>(
               &_TopKLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    170;

  friend void swap(TopKLayerParams& a, TopKLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(TopKLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(TopKLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  TopKLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<TopKLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const TopKLayerParams& from);
  void MergeFrom(const TopKLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(TopKLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.TopKLayerParams";
  }
  protected:
  explicit TopKLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxisFieldNumber = 1,
    kKFieldNumber = 2,
    kUseBottomKFieldNumber = 3,
  };
  // int64 axis = 1;
  void clear_axis();
  int64_t axis() const;
  void set_axis(int64_t value);
  private:
  int64_t _internal_axis() const;
  void _internal_set_axis(int64_t value);
  public:

  // uint64 K = 2;
  void clear_k();
  uint64_t k() const;
  void set_k(uint64_t value);
  private:
  uint64_t _internal_k() const;
  void _internal_set_k(uint64_t value);
  public:

  // bool useBottomK = 3;
  void clear_usebottomk();
  bool usebottomk() const;
  void set_usebottomk(bool value);
  private:
  bool _internal_usebottomk() const;
  void _internal_set_usebottomk(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.TopKLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t axis_;
  uint64_t k_;
  bool usebottomk_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ArgMaxLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ArgMaxLayerParams) */ {
 public:
  inline ArgMaxLayerParams() : ArgMaxLayerParams(nullptr) {}
  ~ArgMaxLayerParams() override;
  explicit constexpr ArgMaxLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ArgMaxLayerParams(const ArgMaxLayerParams& from);
  ArgMaxLayerParams(ArgMaxLayerParams&& from) noexcept
    : ArgMaxLayerParams() {
    *this = ::std::move(from);
  }

  inline ArgMaxLayerParams& operator=(const ArgMaxLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ArgMaxLayerParams& operator=(ArgMaxLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ArgMaxLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ArgMaxLayerParams* internal_default_instance() {
    return reinterpret_cast<const ArgMaxLayerParams*>(
               &_ArgMaxLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    171;

  friend void swap(ArgMaxLayerParams& a, ArgMaxLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ArgMaxLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ArgMaxLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ArgMaxLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ArgMaxLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ArgMaxLayerParams& from);
  void MergeFrom(const ArgMaxLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ArgMaxLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ArgMaxLayerParams";
  }
  protected:
  explicit ArgMaxLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxisFieldNumber = 1,
    kRemoveDimFieldNumber = 2,
  };
  // int64 axis = 1;
  void clear_axis();
  int64_t axis() const;
  void set_axis(int64_t value);
  private:
  int64_t _internal_axis() const;
  void _internal_set_axis(int64_t value);
  public:

  // bool removeDim = 2;
  void clear_removedim();
  bool removedim() const;
  void set_removedim(bool value);
  private:
  bool _internal_removedim() const;
  void _internal_set_removedim(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ArgMaxLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t axis_;
  bool removedim_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ArgMinLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ArgMinLayerParams) */ {
 public:
  inline ArgMinLayerParams() : ArgMinLayerParams(nullptr) {}
  ~ArgMinLayerParams() override;
  explicit constexpr ArgMinLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ArgMinLayerParams(const ArgMinLayerParams& from);
  ArgMinLayerParams(ArgMinLayerParams&& from) noexcept
    : ArgMinLayerParams() {
    *this = ::std::move(from);
  }

  inline ArgMinLayerParams& operator=(const ArgMinLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ArgMinLayerParams& operator=(ArgMinLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ArgMinLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ArgMinLayerParams* internal_default_instance() {
    return reinterpret_cast<const ArgMinLayerParams*>(
               &_ArgMinLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    172;

  friend void swap(ArgMinLayerParams& a, ArgMinLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ArgMinLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ArgMinLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ArgMinLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ArgMinLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ArgMinLayerParams& from);
  void MergeFrom(const ArgMinLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ArgMinLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ArgMinLayerParams";
  }
  protected:
  explicit ArgMinLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxisFieldNumber = 1,
    kRemoveDimFieldNumber = 2,
  };
  // int64 axis = 1;
  void clear_axis();
  int64_t axis() const;
  void set_axis(int64_t value);
  private:
  int64_t _internal_axis() const;
  void _internal_set_axis(int64_t value);
  public:

  // bool removeDim = 2;
  void clear_removedim();
  bool removedim() const;
  void set_removedim(bool value);
  private:
  bool _internal_removedim() const;
  void _internal_set_removedim(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ArgMinLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t axis_;
  bool removedim_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class SplitNDLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SplitNDLayerParams) */ {
 public:
  inline SplitNDLayerParams() : SplitNDLayerParams(nullptr) {}
  ~SplitNDLayerParams() override;
  explicit constexpr SplitNDLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SplitNDLayerParams(const SplitNDLayerParams& from);
  SplitNDLayerParams(SplitNDLayerParams&& from) noexcept
    : SplitNDLayerParams() {
    *this = ::std::move(from);
  }

  inline SplitNDLayerParams& operator=(const SplitNDLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline SplitNDLayerParams& operator=(SplitNDLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const SplitNDLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const SplitNDLayerParams* internal_default_instance() {
    return reinterpret_cast<const SplitNDLayerParams*>(
               &_SplitNDLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    173;

  friend void swap(SplitNDLayerParams& a, SplitNDLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(SplitNDLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SplitNDLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SplitNDLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SplitNDLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const SplitNDLayerParams& from);
  void MergeFrom(const SplitNDLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SplitNDLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.SplitNDLayerParams";
  }
  protected:
  explicit SplitNDLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kSplitSizesFieldNumber = 3,
    kAxisFieldNumber = 1,
    kNumSplitsFieldNumber = 2,
  };
  // repeated uint64 splitSizes = 3;
  int splitsizes_size() const;
  private:
  int _internal_splitsizes_size() const;
  public:
  void clear_splitsizes();
  private:
  uint64_t _internal_splitsizes(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_splitsizes() const;
  void _internal_add_splitsizes(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_splitsizes();
  public:
  uint64_t splitsizes(int index) const;
  void set_splitsizes(int index, uint64_t value);
  void add_splitsizes(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      splitsizes() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_splitsizes();

  // int64 axis = 1;
  void clear_axis();
  int64_t axis() const;
  void set_axis(int64_t value);
  private:
  int64_t _internal_axis() const;
  void _internal_set_axis(int64_t value);
  public:

  // uint64 numSplits = 2;
  void clear_numsplits();
  uint64_t numsplits() const;
  void set_numsplits(uint64_t value);
  private:
  uint64_t _internal_numsplits() const;
  void _internal_set_numsplits(uint64_t value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SplitNDLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > splitsizes_;
  mutable std::atomic<int> _splitsizes_cached_byte_size_;
  int64_t axis_;
  uint64_t numsplits_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class CeilLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CeilLayerParams) */ {
 public:
  inline CeilLayerParams() : CeilLayerParams(nullptr) {}
  ~CeilLayerParams() override;
  explicit constexpr CeilLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  CeilLayerParams(const CeilLayerParams& from);
  CeilLayerParams(CeilLayerParams&& from) noexcept
    : CeilLayerParams() {
    *this = ::std::move(from);
  }

  inline CeilLayerParams& operator=(const CeilLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline CeilLayerParams& operator=(CeilLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const CeilLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const CeilLayerParams* internal_default_instance() {
    return reinterpret_cast<const CeilLayerParams*>(
               &_CeilLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    174;

  friend void swap(CeilLayerParams& a, CeilLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(CeilLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(CeilLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  CeilLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<CeilLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const CeilLayerParams& from);
  void MergeFrom(const CeilLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CeilLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.CeilLayerParams";
  }
  protected:
  explicit CeilLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.CeilLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class RoundLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RoundLayerParams) */ {
 public:
  inline RoundLayerParams() : RoundLayerParams(nullptr) {}
  ~RoundLayerParams() override;
  explicit constexpr RoundLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  RoundLayerParams(const RoundLayerParams& from);
  RoundLayerParams(RoundLayerParams&& from) noexcept
    : RoundLayerParams() {
    *this = ::std::move(from);
  }

  inline RoundLayerParams& operator=(const RoundLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline RoundLayerParams& operator=(RoundLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const RoundLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const RoundLayerParams* internal_default_instance() {
    return reinterpret_cast<const RoundLayerParams*>(
               &_RoundLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    175;

  friend void swap(RoundLayerParams& a, RoundLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(RoundLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(RoundLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  RoundLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<RoundLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const RoundLayerParams& from);
  void MergeFrom(const RoundLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RoundLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.RoundLayerParams";
  }
  protected:
  explicit RoundLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RoundLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class FloorLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.FloorLayerParams) */ {
 public:
  inline FloorLayerParams() : FloorLayerParams(nullptr) {}
  ~FloorLayerParams() override;
  explicit constexpr FloorLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  FloorLayerParams(const FloorLayerParams& from);
  FloorLayerParams(FloorLayerParams&& from) noexcept
    : FloorLayerParams() {
    *this = ::std::move(from);
  }

  inline FloorLayerParams& operator=(const FloorLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline FloorLayerParams& operator=(FloorLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const FloorLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const FloorLayerParams* internal_default_instance() {
    return reinterpret_cast<const FloorLayerParams*>(
               &_FloorLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    176;

  friend void swap(FloorLayerParams& a, FloorLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(FloorLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(FloorLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  FloorLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<FloorLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const FloorLayerParams& from);
  void MergeFrom(const FloorLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(FloorLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.FloorLayerParams";
  }
  protected:
  explicit FloorLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.FloorLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class SignLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SignLayerParams) */ {
 public:
  inline SignLayerParams() : SignLayerParams(nullptr) {}
  ~SignLayerParams() override;
  explicit constexpr SignLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SignLayerParams(const SignLayerParams& from);
  SignLayerParams(SignLayerParams&& from) noexcept
    : SignLayerParams() {
    *this = ::std::move(from);
  }

  inline SignLayerParams& operator=(const SignLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline SignLayerParams& operator=(SignLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const SignLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const SignLayerParams* internal_default_instance() {
    return reinterpret_cast<const SignLayerParams*>(
               &_SignLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    177;

  friend void swap(SignLayerParams& a, SignLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(SignLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SignLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SignLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SignLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const SignLayerParams& from);
  void MergeFrom(const SignLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SignLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.SignLayerParams";
  }
  protected:
  explicit SignLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SignLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ClipLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ClipLayerParams) */ {
 public:
  inline ClipLayerParams() : ClipLayerParams(nullptr) {}
  ~ClipLayerParams() override;
  explicit constexpr ClipLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ClipLayerParams(const ClipLayerParams& from);
  ClipLayerParams(ClipLayerParams&& from) noexcept
    : ClipLayerParams() {
    *this = ::std::move(from);
  }

  inline ClipLayerParams& operator=(const ClipLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ClipLayerParams& operator=(ClipLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ClipLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ClipLayerParams* internal_default_instance() {
    return reinterpret_cast<const ClipLayerParams*>(
               &_ClipLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    178;

  friend void swap(ClipLayerParams& a, ClipLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ClipLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ClipLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ClipLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ClipLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ClipLayerParams& from);
  void MergeFrom(const ClipLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ClipLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ClipLayerParams";
  }
  protected:
  explicit ClipLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kMinValFieldNumber = 1,
    kMaxValFieldNumber = 2,
  };
  // float minVal = 1;
  void clear_minval();
  float minval() const;
  void set_minval(float value);
  private:
  float _internal_minval() const;
  void _internal_set_minval(float value);
  public:

  // float maxVal = 2;
  void clear_maxval();
  float maxval() const;
  void set_maxval(float value);
  private:
  float _internal_maxval() const;
  void _internal_set_maxval(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ClipLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float minval_;
  float maxval_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class SliceStaticLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SliceStaticLayerParams) */ {
 public:
  inline SliceStaticLayerParams() : SliceStaticLayerParams(nullptr) {}
  ~SliceStaticLayerParams() override;
  explicit constexpr SliceStaticLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SliceStaticLayerParams(const SliceStaticLayerParams& from);
  SliceStaticLayerParams(SliceStaticLayerParams&& from) noexcept
    : SliceStaticLayerParams() {
    *this = ::std::move(from);
  }

  inline SliceStaticLayerParams& operator=(const SliceStaticLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline SliceStaticLayerParams& operator=(SliceStaticLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const SliceStaticLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const SliceStaticLayerParams* internal_default_instance() {
    return reinterpret_cast<const SliceStaticLayerParams*>(
               &_SliceStaticLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    179;

  friend void swap(SliceStaticLayerParams& a, SliceStaticLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(SliceStaticLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SliceStaticLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SliceStaticLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SliceStaticLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const SliceStaticLayerParams& from);
  void MergeFrom(const SliceStaticLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SliceStaticLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.SliceStaticLayerParams";
  }
  protected:
  explicit SliceStaticLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kBeginIdsFieldNumber = 1,
    kBeginMasksFieldNumber = 2,
    kEndIdsFieldNumber = 3,
    kEndMasksFieldNumber = 4,
    kStridesFieldNumber = 5,
    kSqueezeMasksFieldNumber = 6,
  };
  // repeated int64 beginIds = 1;
  int beginids_size() const;
  private:
  int _internal_beginids_size() const;
  public:
  void clear_beginids();
  private:
  int64_t _internal_beginids(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_beginids() const;
  void _internal_add_beginids(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_beginids();
  public:
  int64_t beginids(int index) const;
  void set_beginids(int index, int64_t value);
  void add_beginids(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      beginids() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_beginids();

  // repeated bool beginMasks = 2;
  int beginmasks_size() const;
  private:
  int _internal_beginmasks_size() const;
  public:
  void clear_beginmasks();
  private:
  bool _internal_beginmasks(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
      _internal_beginmasks() const;
  void _internal_add_beginmasks(bool value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
      _internal_mutable_beginmasks();
  public:
  bool beginmasks(int index) const;
  void set_beginmasks(int index, bool value);
  void add_beginmasks(bool value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
      beginmasks() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
      mutable_beginmasks();

  // repeated int64 endIds = 3;
  int endids_size() const;
  private:
  int _internal_endids_size() const;
  public:
  void clear_endids();
  private:
  int64_t _internal_endids(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_endids() const;
  void _internal_add_endids(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_endids();
  public:
  int64_t endids(int index) const;
  void set_endids(int index, int64_t value);
  void add_endids(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      endids() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_endids();

  // repeated bool endMasks = 4;
  int endmasks_size() const;
  private:
  int _internal_endmasks_size() const;
  public:
  void clear_endmasks();
  private:
  bool _internal_endmasks(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
      _internal_endmasks() const;
  void _internal_add_endmasks(bool value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
      _internal_mutable_endmasks();
  public:
  bool endmasks(int index) const;
  void set_endmasks(int index, bool value);
  void add_endmasks(bool value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
      endmasks() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
      mutable_endmasks();

  // repeated int64 strides = 5;
  int strides_size() const;
  private:
  int _internal_strides_size() const;
  public:
  void clear_strides();
  private:
  int64_t _internal_strides(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_strides() const;
  void _internal_add_strides(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_strides();
  public:
  int64_t strides(int index) const;
  void set_strides(int index, int64_t value);
  void add_strides(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      strides() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_strides();

  // repeated bool squeezeMasks = 6;
  int squeezemasks_size() const;
  private:
  int _internal_squeezemasks_size() const;
  public:
  void clear_squeezemasks();
  private:
  bool _internal_squeezemasks(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
      _internal_squeezemasks() const;
  void _internal_add_squeezemasks(bool value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
      _internal_mutable_squeezemasks();
  public:
  bool squeezemasks(int index) const;
  void set_squeezemasks(int index, bool value);
  void add_squeezemasks(bool value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
      squeezemasks() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
      mutable_squeezemasks();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SliceStaticLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > beginids_;
  mutable std::atomic<int> _beginids_cached_byte_size_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool > beginmasks_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > endids_;
  mutable std::atomic<int> _endids_cached_byte_size_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool > endmasks_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > strides_;
  mutable std::atomic<int> _strides_cached_byte_size_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool > squeezemasks_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class SliceDynamicLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SliceDynamicLayerParams) */ {
 public:
  inline SliceDynamicLayerParams() : SliceDynamicLayerParams(nullptr) {}
  ~SliceDynamicLayerParams() override;
  explicit constexpr SliceDynamicLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SliceDynamicLayerParams(const SliceDynamicLayerParams& from);
  SliceDynamicLayerParams(SliceDynamicLayerParams&& from) noexcept
    : SliceDynamicLayerParams() {
    *this = ::std::move(from);
  }

  inline SliceDynamicLayerParams& operator=(const SliceDynamicLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline SliceDynamicLayerParams& operator=(SliceDynamicLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const SliceDynamicLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const SliceDynamicLayerParams* internal_default_instance() {
    return reinterpret_cast<const SliceDynamicLayerParams*>(
               &_SliceDynamicLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    180;

  friend void swap(SliceDynamicLayerParams& a, SliceDynamicLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(SliceDynamicLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SliceDynamicLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SliceDynamicLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SliceDynamicLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const SliceDynamicLayerParams& from);
  void MergeFrom(const SliceDynamicLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SliceDynamicLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.SliceDynamicLayerParams";
  }
  protected:
  explicit SliceDynamicLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kBeginMasksFieldNumber = 2,
    kEndIdsFieldNumber = 3,
    kEndMasksFieldNumber = 4,
    kStridesFieldNumber = 5,
    kSqueezeMasksFieldNumber = 6,
  };
  // repeated bool beginMasks = 2;
  int beginmasks_size() const;
  private:
  int _internal_beginmasks_size() const;
  public:
  void clear_beginmasks();
  private:
  bool _internal_beginmasks(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
      _internal_beginmasks() const;
  void _internal_add_beginmasks(bool value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
      _internal_mutable_beginmasks();
  public:
  bool beginmasks(int index) const;
  void set_beginmasks(int index, bool value);
  void add_beginmasks(bool value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
      beginmasks() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
      mutable_beginmasks();

  // repeated int64 endIds = 3;
  int endids_size() const;
  private:
  int _internal_endids_size() const;
  public:
  void clear_endids();
  private:
  int64_t _internal_endids(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_endids() const;
  void _internal_add_endids(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_endids();
  public:
  int64_t endids(int index) const;
  void set_endids(int index, int64_t value);
  void add_endids(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      endids() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_endids();

  // repeated bool endMasks = 4;
  int endmasks_size() const;
  private:
  int _internal_endmasks_size() const;
  public:
  void clear_endmasks();
  private:
  bool _internal_endmasks(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
      _internal_endmasks() const;
  void _internal_add_endmasks(bool value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
      _internal_mutable_endmasks();
  public:
  bool endmasks(int index) const;
  void set_endmasks(int index, bool value);
  void add_endmasks(bool value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
      endmasks() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
      mutable_endmasks();

  // repeated int64 strides = 5;
  int strides_size() const;
  private:
  int _internal_strides_size() const;
  public:
  void clear_strides();
  private:
  int64_t _internal_strides(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_strides() const;
  void _internal_add_strides(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_strides();
  public:
  int64_t strides(int index) const;
  void set_strides(int index, int64_t value);
  void add_strides(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      strides() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_strides();

  // repeated bool squeezeMasks = 6;
  int squeezemasks_size() const;
  private:
  int _internal_squeezemasks_size() const;
  public:
  void clear_squeezemasks();
  private:
  bool _internal_squeezemasks(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
      _internal_squeezemasks() const;
  void _internal_add_squeezemasks(bool value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
      _internal_mutable_squeezemasks();
  public:
  bool squeezemasks(int index) const;
  void set_squeezemasks(int index, bool value);
  void add_squeezemasks(bool value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
      squeezemasks() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
      mutable_squeezemasks();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SliceDynamicLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool > beginmasks_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > endids_;
  mutable std::atomic<int> _endids_cached_byte_size_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool > endmasks_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > strides_;
  mutable std::atomic<int> _strides_cached_byte_size_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool > squeezemasks_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class TileLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.TileLayerParams) */ {
 public:
  inline TileLayerParams() : TileLayerParams(nullptr) {}
  ~TileLayerParams() override;
  explicit constexpr TileLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  TileLayerParams(const TileLayerParams& from);
  TileLayerParams(TileLayerParams&& from) noexcept
    : TileLayerParams() {
    *this = ::std::move(from);
  }

  inline TileLayerParams& operator=(const TileLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline TileLayerParams& operator=(TileLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const TileLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const TileLayerParams* internal_default_instance() {
    return reinterpret_cast<const TileLayerParams*>(
               &_TileLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    181;

  friend void swap(TileLayerParams& a, TileLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(TileLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(TileLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  TileLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<TileLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const TileLayerParams& from);
  void MergeFrom(const TileLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(TileLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.TileLayerParams";
  }
  protected:
  explicit TileLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kRepsFieldNumber = 1,
  };
  // repeated uint64 reps = 1;
  int reps_size() const;
  private:
  int _internal_reps_size() const;
  public:
  void clear_reps();
  private:
  uint64_t _internal_reps(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      _internal_reps() const;
  void _internal_add_reps(uint64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      _internal_mutable_reps();
  public:
  uint64_t reps(int index) const;
  void set_reps(int index, uint64_t value);
  void add_reps(uint64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
      reps() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
      mutable_reps();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.TileLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t > reps_;
  mutable std::atomic<int> _reps_cached_byte_size_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class GetShapeLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.GetShapeLayerParams) */ {
 public:
  inline GetShapeLayerParams() : GetShapeLayerParams(nullptr) {}
  ~GetShapeLayerParams() override;
  explicit constexpr GetShapeLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  GetShapeLayerParams(const GetShapeLayerParams& from);
  GetShapeLayerParams(GetShapeLayerParams&& from) noexcept
    : GetShapeLayerParams() {
    *this = ::std::move(from);
  }

  inline GetShapeLayerParams& operator=(const GetShapeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline GetShapeLayerParams& operator=(GetShapeLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const GetShapeLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const GetShapeLayerParams* internal_default_instance() {
    return reinterpret_cast<const GetShapeLayerParams*>(
               &_GetShapeLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    182;

  friend void swap(GetShapeLayerParams& a, GetShapeLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(GetShapeLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(GetShapeLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  GetShapeLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<GetShapeLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const GetShapeLayerParams& from);
  void MergeFrom(const GetShapeLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(GetShapeLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.GetShapeLayerParams";
  }
  protected:
  explicit GetShapeLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.GetShapeLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ErfLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ErfLayerParams) */ {
 public:
  inline ErfLayerParams() : ErfLayerParams(nullptr) {}
  ~ErfLayerParams() override;
  explicit constexpr ErfLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ErfLayerParams(const ErfLayerParams& from);
  ErfLayerParams(ErfLayerParams&& from) noexcept
    : ErfLayerParams() {
    *this = ::std::move(from);
  }

  inline ErfLayerParams& operator=(const ErfLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ErfLayerParams& operator=(ErfLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ErfLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ErfLayerParams* internal_default_instance() {
    return reinterpret_cast<const ErfLayerParams*>(
               &_ErfLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    183;

  friend void swap(ErfLayerParams& a, ErfLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ErfLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ErfLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ErfLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ErfLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ErfLayerParams& from);
  void MergeFrom(const ErfLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ErfLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ErfLayerParams";
  }
  protected:
  explicit ErfLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ErfLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class GeluLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.GeluLayerParams) */ {
 public:
  inline GeluLayerParams() : GeluLayerParams(nullptr) {}
  ~GeluLayerParams() override;
  explicit constexpr GeluLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  GeluLayerParams(const GeluLayerParams& from);
  GeluLayerParams(GeluLayerParams&& from) noexcept
    : GeluLayerParams() {
    *this = ::std::move(from);
  }

  inline GeluLayerParams& operator=(const GeluLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline GeluLayerParams& operator=(GeluLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const GeluLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const GeluLayerParams* internal_default_instance() {
    return reinterpret_cast<const GeluLayerParams*>(
               &_GeluLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    184;

  friend void swap(GeluLayerParams& a, GeluLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(GeluLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(GeluLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  GeluLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<GeluLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const GeluLayerParams& from);
  void MergeFrom(const GeluLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(GeluLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.GeluLayerParams";
  }
  protected:
  explicit GeluLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  typedef GeluLayerParams_GeluMode GeluMode;
  static constexpr GeluMode EXACT =
    GeluLayerParams_GeluMode_EXACT;
  static constexpr GeluMode TANH_APPROXIMATION =
    GeluLayerParams_GeluMode_TANH_APPROXIMATION;
  static constexpr GeluMode SIGMOID_APPROXIMATION =
    GeluLayerParams_GeluMode_SIGMOID_APPROXIMATION;
  static inline bool GeluMode_IsValid(int value) {
    return GeluLayerParams_GeluMode_IsValid(value);
  }
  static constexpr GeluMode GeluMode_MIN =
    GeluLayerParams_GeluMode_GeluMode_MIN;
  static constexpr GeluMode GeluMode_MAX =
    GeluLayerParams_GeluMode_GeluMode_MAX;
  static constexpr int GeluMode_ARRAYSIZE =
    GeluLayerParams_GeluMode_GeluMode_ARRAYSIZE;
  template<typename T>
  static inline const std::string& GeluMode_Name(T enum_t_value) {
    static_assert(::std::is_same<T, GeluMode>::value ||
      ::std::is_integral<T>::value,
      "Incorrect type passed to function GeluMode_Name.");
    return GeluLayerParams_GeluMode_Name(enum_t_value);
  }
  static inline bool GeluMode_Parse(::PROTOBUF_NAMESPACE_ID::ConstStringParam name,
      GeluMode* value) {
    return GeluLayerParams_GeluMode_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  enum : int {
    kModeFieldNumber = 1,
  };
  // .CoreML.Specification.GeluLayerParams.GeluMode mode = 1;
  void clear_mode();
  ::CoreML::Specification::GeluLayerParams_GeluMode mode() const;
  void set_mode(::CoreML::Specification::GeluLayerParams_GeluMode value);
  private:
  ::CoreML::Specification::GeluLayerParams_GeluMode _internal_mode() const;
  void _internal_set_mode(::CoreML::Specification::GeluLayerParams_GeluMode value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.GeluLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int mode_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class RangeStaticLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RangeStaticLayerParams) */ {
 public:
  inline RangeStaticLayerParams() : RangeStaticLayerParams(nullptr) {}
  ~RangeStaticLayerParams() override;
  explicit constexpr RangeStaticLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  RangeStaticLayerParams(const RangeStaticLayerParams& from);
  RangeStaticLayerParams(RangeStaticLayerParams&& from) noexcept
    : RangeStaticLayerParams() {
    *this = ::std::move(from);
  }

  inline RangeStaticLayerParams& operator=(const RangeStaticLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline RangeStaticLayerParams& operator=(RangeStaticLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const RangeStaticLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const RangeStaticLayerParams* internal_default_instance() {
    return reinterpret_cast<const RangeStaticLayerParams*>(
               &_RangeStaticLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    185;

  friend void swap(RangeStaticLayerParams& a, RangeStaticLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(RangeStaticLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(RangeStaticLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  RangeStaticLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<RangeStaticLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const RangeStaticLayerParams& from);
  void MergeFrom(const RangeStaticLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RangeStaticLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.RangeStaticLayerParams";
  }
  protected:
  explicit RangeStaticLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kEndValueFieldNumber = 1,
    kStartValueFieldNumber = 2,
    kStepSizeValueFieldNumber = 3,
  };
  // float endValue = 1;
  void clear_endvalue();
  float endvalue() const;
  void set_endvalue(float value);
  private:
  float _internal_endvalue() const;
  void _internal_set_endvalue(float value);
  public:

  // float startValue = 2;
  void clear_startvalue();
  float startvalue() const;
  void set_startvalue(float value);
  private:
  float _internal_startvalue() const;
  void _internal_set_startvalue(float value);
  public:

  // float stepSizeValue = 3;
  void clear_stepsizevalue();
  float stepsizevalue() const;
  void set_stepsizevalue(float value);
  private:
  float _internal_stepsizevalue() const;
  void _internal_set_stepsizevalue(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RangeStaticLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float endvalue_;
  float startvalue_;
  float stepsizevalue_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class RangeDynamicLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RangeDynamicLayerParams) */ {
 public:
  inline RangeDynamicLayerParams() : RangeDynamicLayerParams(nullptr) {}
  ~RangeDynamicLayerParams() override;
  explicit constexpr RangeDynamicLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  RangeDynamicLayerParams(const RangeDynamicLayerParams& from);
  RangeDynamicLayerParams(RangeDynamicLayerParams&& from) noexcept
    : RangeDynamicLayerParams() {
    *this = ::std::move(from);
  }

  inline RangeDynamicLayerParams& operator=(const RangeDynamicLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline RangeDynamicLayerParams& operator=(RangeDynamicLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const RangeDynamicLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const RangeDynamicLayerParams* internal_default_instance() {
    return reinterpret_cast<const RangeDynamicLayerParams*>(
               &_RangeDynamicLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    186;

  friend void swap(RangeDynamicLayerParams& a, RangeDynamicLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(RangeDynamicLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(RangeDynamicLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  RangeDynamicLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<RangeDynamicLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const RangeDynamicLayerParams& from);
  void MergeFrom(const RangeDynamicLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RangeDynamicLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.RangeDynamicLayerParams";
  }
  protected:
  explicit RangeDynamicLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kStartValueFieldNumber = 2,
    kStepSizeValueFieldNumber = 3,
  };
  // float startValue = 2;
  void clear_startvalue();
  float startvalue() const;
  void set_startvalue(float value);
  private:
  float _internal_startvalue() const;
  void _internal_set_startvalue(float value);
  public:

  // float stepSizeValue = 3;
  void clear_stepsizevalue();
  float stepsizevalue() const;
  void set_stepsizevalue(float value);
  private:
  float _internal_stepsizevalue() const;
  void _internal_set_stepsizevalue(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RangeDynamicLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float startvalue_;
  float stepsizevalue_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class SlidingWindowsLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SlidingWindowsLayerParams) */ {
 public:
  inline SlidingWindowsLayerParams() : SlidingWindowsLayerParams(nullptr) {}
  ~SlidingWindowsLayerParams() override;
  explicit constexpr SlidingWindowsLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SlidingWindowsLayerParams(const SlidingWindowsLayerParams& from);
  SlidingWindowsLayerParams(SlidingWindowsLayerParams&& from) noexcept
    : SlidingWindowsLayerParams() {
    *this = ::std::move(from);
  }

  inline SlidingWindowsLayerParams& operator=(const SlidingWindowsLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline SlidingWindowsLayerParams& operator=(SlidingWindowsLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const SlidingWindowsLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const SlidingWindowsLayerParams* internal_default_instance() {
    return reinterpret_cast<const SlidingWindowsLayerParams*>(
               &_SlidingWindowsLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    187;

  friend void swap(SlidingWindowsLayerParams& a, SlidingWindowsLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(SlidingWindowsLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SlidingWindowsLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SlidingWindowsLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SlidingWindowsLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const SlidingWindowsLayerParams& from);
  void MergeFrom(const SlidingWindowsLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SlidingWindowsLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.SlidingWindowsLayerParams";
  }
  protected:
  explicit SlidingWindowsLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxisFieldNumber = 1,
    kWindowSizeFieldNumber = 2,
    kStepFieldNumber = 3,
  };
  // int64 axis = 1;
  void clear_axis();
  int64_t axis() const;
  void set_axis(int64_t value);
  private:
  int64_t _internal_axis() const;
  void _internal_set_axis(int64_t value);
  public:

  // uint64 windowSize = 2;
  void clear_windowsize();
  uint64_t windowsize() const;
  void set_windowsize(uint64_t value);
  private:
  uint64_t _internal_windowsize() const;
  void _internal_set_windowsize(uint64_t value);
  public:

  // uint64 step = 3;
  void clear_step();
  uint64_t step() const;
  void set_step(uint64_t value);
  private:
  uint64_t _internal_step() const;
  void _internal_set_step(uint64_t value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SlidingWindowsLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t axis_;
  uint64_t windowsize_;
  uint64_t step_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class LayerNormalizationLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LayerNormalizationLayerParams) */ {
 public:
  inline LayerNormalizationLayerParams() : LayerNormalizationLayerParams(nullptr) {}
  ~LayerNormalizationLayerParams() override;
  explicit constexpr LayerNormalizationLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  LayerNormalizationLayerParams(const LayerNormalizationLayerParams& from);
  LayerNormalizationLayerParams(LayerNormalizationLayerParams&& from) noexcept
    : LayerNormalizationLayerParams() {
    *this = ::std::move(from);
  }

  inline LayerNormalizationLayerParams& operator=(const LayerNormalizationLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline LayerNormalizationLayerParams& operator=(LayerNormalizationLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const LayerNormalizationLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const LayerNormalizationLayerParams* internal_default_instance() {
    return reinterpret_cast<const LayerNormalizationLayerParams*>(
               &_LayerNormalizationLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    188;

  friend void swap(LayerNormalizationLayerParams& a, LayerNormalizationLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(LayerNormalizationLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(LayerNormalizationLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  LayerNormalizationLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<LayerNormalizationLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const LayerNormalizationLayerParams& from);
  void MergeFrom(const LayerNormalizationLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LayerNormalizationLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.LayerNormalizationLayerParams";
  }
  protected:
  explicit LayerNormalizationLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kNormalizedShapeFieldNumber = 1,
    kGammaFieldNumber = 3,
    kBetaFieldNumber = 4,
    kEpsFieldNumber = 2,
  };
  // repeated int64 normalizedShape = 1;
  int normalizedshape_size() const;
  private:
  int _internal_normalizedshape_size() const;
  public:
  void clear_normalizedshape();
  private:
  int64_t _internal_normalizedshape(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_normalizedshape() const;
  void _internal_add_normalizedshape(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_normalizedshape();
  public:
  int64_t normalizedshape(int index) const;
  void set_normalizedshape(int index, int64_t value);
  void add_normalizedshape(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      normalizedshape() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_normalizedshape();

  // .CoreML.Specification.WeightParams gamma = 3;
  bool has_gamma() const;
  private:
  bool _internal_has_gamma() const;
  public:
  void clear_gamma();
  const ::CoreML::Specification::WeightParams& gamma() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_gamma();
  ::CoreML::Specification::WeightParams* mutable_gamma();
  void set_allocated_gamma(::CoreML::Specification::WeightParams* gamma);
  private:
  const ::CoreML::Specification::WeightParams& _internal_gamma() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_gamma();
  public:
  void unsafe_arena_set_allocated_gamma(
      ::CoreML::Specification::WeightParams* gamma);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_gamma();

  // .CoreML.Specification.WeightParams beta = 4;
  bool has_beta() const;
  private:
  bool _internal_has_beta() const;
  public:
  void clear_beta();
  const ::CoreML::Specification::WeightParams& beta() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::WeightParams* release_beta();
  ::CoreML::Specification::WeightParams* mutable_beta();
  void set_allocated_beta(::CoreML::Specification::WeightParams* beta);
  private:
  const ::CoreML::Specification::WeightParams& _internal_beta() const;
  ::CoreML::Specification::WeightParams* _internal_mutable_beta();
  public:
  void unsafe_arena_set_allocated_beta(
      ::CoreML::Specification::WeightParams* beta);
  ::CoreML::Specification::WeightParams* unsafe_arena_release_beta();

  // float eps = 2;
  void clear_eps();
  float eps() const;
  void set_eps(float value);
  private:
  float _internal_eps() const;
  void _internal_set_eps(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LayerNormalizationLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > normalizedshape_;
  mutable std::atomic<int> _normalizedshape_cached_byte_size_;
  ::CoreML::Specification::WeightParams* gamma_;
  ::CoreML::Specification::WeightParams* beta_;
  float eps_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class NonMaximumSuppressionLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NonMaximumSuppressionLayerParams) */ {
 public:
  inline NonMaximumSuppressionLayerParams() : NonMaximumSuppressionLayerParams(nullptr) {}
  ~NonMaximumSuppressionLayerParams() override;
  explicit constexpr NonMaximumSuppressionLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  NonMaximumSuppressionLayerParams(const NonMaximumSuppressionLayerParams& from);
  NonMaximumSuppressionLayerParams(NonMaximumSuppressionLayerParams&& from) noexcept
    : NonMaximumSuppressionLayerParams() {
    *this = ::std::move(from);
  }

  inline NonMaximumSuppressionLayerParams& operator=(const NonMaximumSuppressionLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline NonMaximumSuppressionLayerParams& operator=(NonMaximumSuppressionLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const NonMaximumSuppressionLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const NonMaximumSuppressionLayerParams* internal_default_instance() {
    return reinterpret_cast<const NonMaximumSuppressionLayerParams*>(
               &_NonMaximumSuppressionLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    189;

  friend void swap(NonMaximumSuppressionLayerParams& a, NonMaximumSuppressionLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(NonMaximumSuppressionLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(NonMaximumSuppressionLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  NonMaximumSuppressionLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<NonMaximumSuppressionLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const NonMaximumSuppressionLayerParams& from);
  void MergeFrom(const NonMaximumSuppressionLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NonMaximumSuppressionLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.NonMaximumSuppressionLayerParams";
  }
  protected:
  explicit NonMaximumSuppressionLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kIouThresholdFieldNumber = 1,
    kScoreThresholdFieldNumber = 2,
    kMaxBoxesFieldNumber = 3,
    kPerClassSuppressionFieldNumber = 4,
  };
  // float iouThreshold = 1;
  void clear_iouthreshold();
  float iouthreshold() const;
  void set_iouthreshold(float value);
  private:
  float _internal_iouthreshold() const;
  void _internal_set_iouthreshold(float value);
  public:

  // float scoreThreshold = 2;
  void clear_scorethreshold();
  float scorethreshold() const;
  void set_scorethreshold(float value);
  private:
  float _internal_scorethreshold() const;
  void _internal_set_scorethreshold(float value);
  public:

  // uint64 maxBoxes = 3;
  void clear_maxboxes();
  uint64_t maxboxes() const;
  void set_maxboxes(uint64_t value);
  private:
  uint64_t _internal_maxboxes() const;
  void _internal_set_maxboxes(uint64_t value);
  public:

  // bool perClassSuppression = 4;
  void clear_perclasssuppression();
  bool perclasssuppression() const;
  void set_perclasssuppression(bool value);
  private:
  bool _internal_perclasssuppression() const;
  void _internal_set_perclasssuppression(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.NonMaximumSuppressionLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float iouthreshold_;
  float scorethreshold_;
  uint64_t maxboxes_;
  bool perclasssuppression_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ClampedReLULayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ClampedReLULayerParams) */ {
 public:
  inline ClampedReLULayerParams() : ClampedReLULayerParams(nullptr) {}
  ~ClampedReLULayerParams() override;
  explicit constexpr ClampedReLULayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ClampedReLULayerParams(const ClampedReLULayerParams& from);
  ClampedReLULayerParams(ClampedReLULayerParams&& from) noexcept
    : ClampedReLULayerParams() {
    *this = ::std::move(from);
  }

  inline ClampedReLULayerParams& operator=(const ClampedReLULayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ClampedReLULayerParams& operator=(ClampedReLULayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ClampedReLULayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ClampedReLULayerParams* internal_default_instance() {
    return reinterpret_cast<const ClampedReLULayerParams*>(
               &_ClampedReLULayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    190;

  friend void swap(ClampedReLULayerParams& a, ClampedReLULayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ClampedReLULayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ClampedReLULayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ClampedReLULayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ClampedReLULayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ClampedReLULayerParams& from);
  void MergeFrom(const ClampedReLULayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ClampedReLULayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ClampedReLULayerParams";
  }
  protected:
  explicit ClampedReLULayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAlphaFieldNumber = 1,
    kBetaFieldNumber = 2,
  };
  // float alpha = 1;
  void clear_alpha();
  float alpha() const;
  void set_alpha(float value);
  private:
  float _internal_alpha() const;
  void _internal_set_alpha(float value);
  public:

  // float beta = 2;
  void clear_beta();
  float beta() const;
  void set_beta(float value);
  private:
  float _internal_beta() const;
  void _internal_set_beta(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ClampedReLULayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  float alpha_;
  float beta_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class ArgSortLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ArgSortLayerParams) */ {
 public:
  inline ArgSortLayerParams() : ArgSortLayerParams(nullptr) {}
  ~ArgSortLayerParams() override;
  explicit constexpr ArgSortLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ArgSortLayerParams(const ArgSortLayerParams& from);
  ArgSortLayerParams(ArgSortLayerParams&& from) noexcept
    : ArgSortLayerParams() {
    *this = ::std::move(from);
  }

  inline ArgSortLayerParams& operator=(const ArgSortLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline ArgSortLayerParams& operator=(ArgSortLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ArgSortLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const ArgSortLayerParams* internal_default_instance() {
    return reinterpret_cast<const ArgSortLayerParams*>(
               &_ArgSortLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    191;

  friend void swap(ArgSortLayerParams& a, ArgSortLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(ArgSortLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ArgSortLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ArgSortLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ArgSortLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const ArgSortLayerParams& from);
  void MergeFrom(const ArgSortLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ArgSortLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.ArgSortLayerParams";
  }
  protected:
  explicit ArgSortLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxisFieldNumber = 1,
    kDescendingFieldNumber = 2,
  };
  // int64 axis = 1;
  void clear_axis();
  int64_t axis() const;
  void set_axis(int64_t value);
  private:
  int64_t _internal_axis() const;
  void _internal_set_axis(int64_t value);
  public:

  // bool descending = 2;
  void clear_descending();
  bool descending() const;
  void set_descending(bool value);
  private:
  bool _internal_descending() const;
  void _internal_set_descending(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ArgSortLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t axis_;
  bool descending_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class SliceBySizeLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SliceBySizeLayerParams) */ {
 public:
  inline SliceBySizeLayerParams() : SliceBySizeLayerParams(nullptr) {}
  ~SliceBySizeLayerParams() override;
  explicit constexpr SliceBySizeLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SliceBySizeLayerParams(const SliceBySizeLayerParams& from);
  SliceBySizeLayerParams(SliceBySizeLayerParams&& from) noexcept
    : SliceBySizeLayerParams() {
    *this = ::std::move(from);
  }

  inline SliceBySizeLayerParams& operator=(const SliceBySizeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline SliceBySizeLayerParams& operator=(SliceBySizeLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const SliceBySizeLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const SliceBySizeLayerParams* internal_default_instance() {
    return reinterpret_cast<const SliceBySizeLayerParams*>(
               &_SliceBySizeLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    192;

  friend void swap(SliceBySizeLayerParams& a, SliceBySizeLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(SliceBySizeLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SliceBySizeLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SliceBySizeLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SliceBySizeLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const SliceBySizeLayerParams& from);
  void MergeFrom(const SliceBySizeLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SliceBySizeLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.SliceBySizeLayerParams";
  }
  protected:
  explicit SliceBySizeLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kSizeFieldNumber = 2,
    kAxisFieldNumber = 3,
  };
  // int64 size = 2;
  void clear_size();
  int64_t size() const;
  void set_size(int64_t value);
  private:
  int64_t _internal_size() const;
  void _internal_set_size(int64_t value);
  public:

  // int64 axis = 3;
  void clear_axis();
  int64_t axis() const;
  void set_axis(int64_t value);
  private:
  int64_t _internal_axis() const;
  void _internal_set_axis(int64_t value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SliceBySizeLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t size_;
  int64_t axis_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class NeuralNetworkClassifier final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NeuralNetworkClassifier) */ {
 public:
  inline NeuralNetworkClassifier() : NeuralNetworkClassifier(nullptr) {}
  ~NeuralNetworkClassifier() override;
  explicit constexpr NeuralNetworkClassifier(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  NeuralNetworkClassifier(const NeuralNetworkClassifier& from);
  NeuralNetworkClassifier(NeuralNetworkClassifier&& from) noexcept
    : NeuralNetworkClassifier() {
    *this = ::std::move(from);
  }

  inline NeuralNetworkClassifier& operator=(const NeuralNetworkClassifier& from) {
    CopyFrom(from);
    return *this;
  }
  inline NeuralNetworkClassifier& operator=(NeuralNetworkClassifier&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const NeuralNetworkClassifier& default_instance() {
    return *internal_default_instance();
  }
  enum ClassLabelsCase {
    kStringClassLabels = 100,
    kInt64ClassLabels = 101,
    CLASSLABELS_NOT_SET = 0,
  };

  static inline const NeuralNetworkClassifier* internal_default_instance() {
    return reinterpret_cast<const NeuralNetworkClassifier*>(
               &_NeuralNetworkClassifier_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    193;

  friend void swap(NeuralNetworkClassifier& a, NeuralNetworkClassifier& b) {
    a.Swap(&b);
  }
  inline void Swap(NeuralNetworkClassifier* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(NeuralNetworkClassifier* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  NeuralNetworkClassifier* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<NeuralNetworkClassifier>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const NeuralNetworkClassifier& from);
  void MergeFrom(const NeuralNetworkClassifier& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NeuralNetworkClassifier* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.NeuralNetworkClassifier";
  }
  protected:
  explicit NeuralNetworkClassifier(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kLayersFieldNumber = 1,
    kPreprocessingFieldNumber = 2,
    kLabelProbabilityLayerNameFieldNumber = 200,
    kUpdateParamsFieldNumber = 10,
    kArrayInputShapeMappingFieldNumber = 5,
    kImageInputShapeMappingFieldNumber = 6,
    kStringClassLabelsFieldNumber = 100,
    kInt64ClassLabelsFieldNumber = 101,
  };
  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  int layers_size() const;
  private:
  int _internal_layers_size() const;
  public:
  void clear_layers();
  ::CoreML::Specification::NeuralNetworkLayer* mutable_layers(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
      mutable_layers();
  private:
  const ::CoreML::Specification::NeuralNetworkLayer& _internal_layers(int index) const;
  ::CoreML::Specification::NeuralNetworkLayer* _internal_add_layers();
  public:
  const ::CoreML::Specification::NeuralNetworkLayer& layers(int index) const;
  ::CoreML::Specification::NeuralNetworkLayer* add_layers();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
      layers() const;

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  int preprocessing_size() const;
  private:
  int _internal_preprocessing_size() const;
  public:
  void clear_preprocessing();
  ::CoreML::Specification::NeuralNetworkPreprocessing* mutable_preprocessing(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
      mutable_preprocessing();
  private:
  const ::CoreML::Specification::NeuralNetworkPreprocessing& _internal_preprocessing(int index) const;
  ::CoreML::Specification::NeuralNetworkPreprocessing* _internal_add_preprocessing();
  public:
  const ::CoreML::Specification::NeuralNetworkPreprocessing& preprocessing(int index) const;
  ::CoreML::Specification::NeuralNetworkPreprocessing* add_preprocessing();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
      preprocessing() const;

  // string labelProbabilityLayerName = 200;
  void clear_labelprobabilitylayername();
  const std::string& labelprobabilitylayername() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_labelprobabilitylayername(ArgT0&& arg0, ArgT... args);
  std::string* mutable_labelprobabilitylayername();
  PROTOBUF_NODISCARD std::string* release_labelprobabilitylayername();
  void set_allocated_labelprobabilitylayername(std::string* labelprobabilitylayername);
  private:
  const std::string& _internal_labelprobabilitylayername() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_labelprobabilitylayername(const std::string& value);
  std::string* _internal_mutable_labelprobabilitylayername();
  public:

  // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
  bool has_updateparams() const;
  private:
  bool _internal_has_updateparams() const;
  public:
  void clear_updateparams();
  const ::CoreML::Specification::NetworkUpdateParameters& updateparams() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::NetworkUpdateParameters* release_updateparams();
  ::CoreML::Specification::NetworkUpdateParameters* mutable_updateparams();
  void set_allocated_updateparams(::CoreML::Specification::NetworkUpdateParameters* updateparams);
  private:
  const ::CoreML::Specification::NetworkUpdateParameters& _internal_updateparams() const;
  ::CoreML::Specification::NetworkUpdateParameters* _internal_mutable_updateparams();
  public:
  void unsafe_arena_set_allocated_updateparams(
      ::CoreML::Specification::NetworkUpdateParameters* updateparams);
  ::CoreML::Specification::NetworkUpdateParameters* unsafe_arena_release_updateparams();

  // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
  void clear_arrayinputshapemapping();
  ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping arrayinputshapemapping() const;
  void set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value);
  private:
  ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping _internal_arrayinputshapemapping() const;
  void _internal_set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value);
  public:

  // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
  void clear_imageinputshapemapping();
  ::CoreML::Specification::NeuralNetworkImageShapeMapping imageinputshapemapping() const;
  void set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value);
  private:
  ::CoreML::Specification::NeuralNetworkImageShapeMapping _internal_imageinputshapemapping() const;
  void _internal_set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value);
  public:

  // .CoreML.Specification.StringVector stringClassLabels = 100;
  bool has_stringclasslabels() const;
  private:
  bool _internal_has_stringclasslabels() const;
  public:
  void clear_stringclasslabels();
  const ::CoreML::Specification::StringVector& stringclasslabels() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::StringVector* release_stringclasslabels();
  ::CoreML::Specification::StringVector* mutable_stringclasslabels();
  void set_allocated_stringclasslabels(::CoreML::Specification::StringVector* stringclasslabels);
  private:
  const ::CoreML::Specification::StringVector& _internal_stringclasslabels() const;
  ::CoreML::Specification::StringVector* _internal_mutable_stringclasslabels();
  public:
  void unsafe_arena_set_allocated_stringclasslabels(
      ::CoreML::Specification::StringVector* stringclasslabels);
  ::CoreML::Specification::StringVector* unsafe_arena_release_stringclasslabels();

  // .CoreML.Specification.Int64Vector int64ClassLabels = 101;
  bool has_int64classlabels() const;
  private:
  bool _internal_has_int64classlabels() const;
  public:
  void clear_int64classlabels();
  const ::CoreML::Specification::Int64Vector& int64classlabels() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::Int64Vector* release_int64classlabels();
  ::CoreML::Specification::Int64Vector* mutable_int64classlabels();
  void set_allocated_int64classlabels(::CoreML::Specification::Int64Vector* int64classlabels);
  private:
  const ::CoreML::Specification::Int64Vector& _internal_int64classlabels() const;
  ::CoreML::Specification::Int64Vector* _internal_mutable_int64classlabels();
  public:
  void unsafe_arena_set_allocated_int64classlabels(
      ::CoreML::Specification::Int64Vector* int64classlabels);
  ::CoreML::Specification::Int64Vector* unsafe_arena_release_int64classlabels();

  void clear_ClassLabels();
  ClassLabelsCase ClassLabels_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.NeuralNetworkClassifier)
 private:
  class _Internal;
  void set_has_stringclasslabels();
  void set_has_int64classlabels();

  inline bool has_ClassLabels() const;
  inline void clear_has_ClassLabels();

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer > layers_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing > preprocessing_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr labelprobabilitylayername_;
  ::CoreML::Specification::NetworkUpdateParameters* updateparams_;
  int arrayinputshapemapping_;
  int imageinputshapemapping_;
  union ClassLabelsUnion {
    constexpr ClassLabelsUnion() : _constinit_{} {}
      ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized _constinit_;
    ::CoreML::Specification::StringVector* stringclasslabels_;
    ::CoreML::Specification::Int64Vector* int64classlabels_;
  } ClassLabels_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  uint32_t _oneof_case_[1];

  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class OneHotLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.OneHotLayerParams) */ {
 public:
  inline OneHotLayerParams() : OneHotLayerParams(nullptr) {}
  ~OneHotLayerParams() override;
  explicit constexpr OneHotLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  OneHotLayerParams(const OneHotLayerParams& from);
  OneHotLayerParams(OneHotLayerParams&& from) noexcept
    : OneHotLayerParams() {
    *this = ::std::move(from);
  }

  inline OneHotLayerParams& operator=(const OneHotLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline OneHotLayerParams& operator=(OneHotLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const OneHotLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const OneHotLayerParams* internal_default_instance() {
    return reinterpret_cast<const OneHotLayerParams*>(
               &_OneHotLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    194;

  friend void swap(OneHotLayerParams& a, OneHotLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(OneHotLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(OneHotLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  OneHotLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<OneHotLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const OneHotLayerParams& from);
  void MergeFrom(const OneHotLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(OneHotLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.OneHotLayerParams";
  }
  protected:
  explicit OneHotLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kOneHotVectorSizeFieldNumber = 1,
    kAxisFieldNumber = 2,
    kOnValueFieldNumber = 3,
    kOffValueFieldNumber = 4,
  };
  // uint64 oneHotVectorSize = 1;
  void clear_onehotvectorsize();
  uint64_t onehotvectorsize() const;
  void set_onehotvectorsize(uint64_t value);
  private:
  uint64_t _internal_onehotvectorsize() const;
  void _internal_set_onehotvectorsize(uint64_t value);
  public:

  // int64 axis = 2;
  void clear_axis();
  int64_t axis() const;
  void set_axis(int64_t value);
  private:
  int64_t _internal_axis() const;
  void _internal_set_axis(int64_t value);
  public:

  // float onValue = 3;
  void clear_onvalue();
  float onvalue() const;
  void set_onvalue(float value);
  private:
  float _internal_onvalue() const;
  void _internal_set_onvalue(float value);
  public:

  // float offValue = 4;
  void clear_offvalue();
  float offvalue() const;
  void set_offvalue(float value);
  private:
  float _internal_offvalue() const;
  void _internal_set_offvalue(float value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.OneHotLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  uint64_t onehotvectorsize_;
  int64_t axis_;
  float onvalue_;
  float offvalue_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class CumSumLayerParams final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CumSumLayerParams) */ {
 public:
  inline CumSumLayerParams() : CumSumLayerParams(nullptr) {}
  ~CumSumLayerParams() override;
  explicit constexpr CumSumLayerParams(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  CumSumLayerParams(const CumSumLayerParams& from);
  CumSumLayerParams(CumSumLayerParams&& from) noexcept
    : CumSumLayerParams() {
    *this = ::std::move(from);
  }

  inline CumSumLayerParams& operator=(const CumSumLayerParams& from) {
    CopyFrom(from);
    return *this;
  }
  inline CumSumLayerParams& operator=(CumSumLayerParams&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const CumSumLayerParams& default_instance() {
    return *internal_default_instance();
  }
  static inline const CumSumLayerParams* internal_default_instance() {
    return reinterpret_cast<const CumSumLayerParams*>(
               &_CumSumLayerParams_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    195;

  friend void swap(CumSumLayerParams& a, CumSumLayerParams& b) {
    a.Swap(&b);
  }
  inline void Swap(CumSumLayerParams* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(CumSumLayerParams* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  CumSumLayerParams* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<CumSumLayerParams>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const CumSumLayerParams& from);
  void MergeFrom(const CumSumLayerParams& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CumSumLayerParams* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.CumSumLayerParams";
  }
  protected:
  explicit CumSumLayerParams(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAxisFieldNumber = 1,
    kExcludeFinalSumFieldNumber = 2,
    kReverseFieldNumber = 3,
  };
  // int64 axis = 1;
  void clear_axis();
  int64_t axis() const;
  void set_axis(int64_t value);
  private:
  int64_t _internal_axis() const;
  void _internal_set_axis(int64_t value);
  public:

  // bool excludeFinalSum = 2;
  void clear_excludefinalsum();
  bool excludefinalsum() const;
  void set_excludefinalsum(bool value);
  private:
  bool _internal_excludefinalsum() const;
  void _internal_set_excludefinalsum(bool value);
  public:

  // bool reverse = 3;
  void clear_reverse();
  bool reverse() const;
  void set_reverse(bool value);
  private:
  bool _internal_reverse() const;
  void _internal_set_reverse(bool value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.CumSumLayerParams)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int64_t axis_;
  bool excludefinalsum_;
  bool reverse_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class NeuralNetworkRegressor final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NeuralNetworkRegressor) */ {
 public:
  inline NeuralNetworkRegressor() : NeuralNetworkRegressor(nullptr) {}
  ~NeuralNetworkRegressor() override;
  explicit constexpr NeuralNetworkRegressor(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  NeuralNetworkRegressor(const NeuralNetworkRegressor& from);
  NeuralNetworkRegressor(NeuralNetworkRegressor&& from) noexcept
    : NeuralNetworkRegressor() {
    *this = ::std::move(from);
  }

  inline NeuralNetworkRegressor& operator=(const NeuralNetworkRegressor& from) {
    CopyFrom(from);
    return *this;
  }
  inline NeuralNetworkRegressor& operator=(NeuralNetworkRegressor&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const NeuralNetworkRegressor& default_instance() {
    return *internal_default_instance();
  }
  static inline const NeuralNetworkRegressor* internal_default_instance() {
    return reinterpret_cast<const NeuralNetworkRegressor*>(
               &_NeuralNetworkRegressor_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    196;

  friend void swap(NeuralNetworkRegressor& a, NeuralNetworkRegressor& b) {
    a.Swap(&b);
  }
  inline void Swap(NeuralNetworkRegressor* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(NeuralNetworkRegressor* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  NeuralNetworkRegressor* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<NeuralNetworkRegressor>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const NeuralNetworkRegressor& from);
  void MergeFrom(const NeuralNetworkRegressor& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NeuralNetworkRegressor* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.NeuralNetworkRegressor";
  }
  protected:
  explicit NeuralNetworkRegressor(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kLayersFieldNumber = 1,
    kPreprocessingFieldNumber = 2,
    kUpdateParamsFieldNumber = 10,
    kArrayInputShapeMappingFieldNumber = 5,
    kImageInputShapeMappingFieldNumber = 6,
  };
  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  int layers_size() const;
  private:
  int _internal_layers_size() const;
  public:
  void clear_layers();
  ::CoreML::Specification::NeuralNetworkLayer* mutable_layers(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
      mutable_layers();
  private:
  const ::CoreML::Specification::NeuralNetworkLayer& _internal_layers(int index) const;
  ::CoreML::Specification::NeuralNetworkLayer* _internal_add_layers();
  public:
  const ::CoreML::Specification::NeuralNetworkLayer& layers(int index) const;
  ::CoreML::Specification::NeuralNetworkLayer* add_layers();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
      layers() const;

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  int preprocessing_size() const;
  private:
  int _internal_preprocessing_size() const;
  public:
  void clear_preprocessing();
  ::CoreML::Specification::NeuralNetworkPreprocessing* mutable_preprocessing(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
      mutable_preprocessing();
  private:
  const ::CoreML::Specification::NeuralNetworkPreprocessing& _internal_preprocessing(int index) const;
  ::CoreML::Specification::NeuralNetworkPreprocessing* _internal_add_preprocessing();
  public:
  const ::CoreML::Specification::NeuralNetworkPreprocessing& preprocessing(int index) const;
  ::CoreML::Specification::NeuralNetworkPreprocessing* add_preprocessing();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
      preprocessing() const;

  // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
  bool has_updateparams() const;
  private:
  bool _internal_has_updateparams() const;
  public:
  void clear_updateparams();
  const ::CoreML::Specification::NetworkUpdateParameters& updateparams() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::NetworkUpdateParameters* release_updateparams();
  ::CoreML::Specification::NetworkUpdateParameters* mutable_updateparams();
  void set_allocated_updateparams(::CoreML::Specification::NetworkUpdateParameters* updateparams);
  private:
  const ::CoreML::Specification::NetworkUpdateParameters& _internal_updateparams() const;
  ::CoreML::Specification::NetworkUpdateParameters* _internal_mutable_updateparams();
  public:
  void unsafe_arena_set_allocated_updateparams(
      ::CoreML::Specification::NetworkUpdateParameters* updateparams);
  ::CoreML::Specification::NetworkUpdateParameters* unsafe_arena_release_updateparams();

  // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
  void clear_arrayinputshapemapping();
  ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping arrayinputshapemapping() const;
  void set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value);
  private:
  ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping _internal_arrayinputshapemapping() const;
  void _internal_set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value);
  public:

  // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
  void clear_imageinputshapemapping();
  ::CoreML::Specification::NeuralNetworkImageShapeMapping imageinputshapemapping() const;
  void set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value);
  private:
  ::CoreML::Specification::NeuralNetworkImageShapeMapping _internal_imageinputshapemapping() const;
  void _internal_set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value);
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.NeuralNetworkRegressor)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer > layers_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing > preprocessing_;
  ::CoreML::Specification::NetworkUpdateParameters* updateparams_;
  int arrayinputshapemapping_;
  int imageinputshapemapping_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class NetworkUpdateParameters final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NetworkUpdateParameters) */ {
 public:
  inline NetworkUpdateParameters() : NetworkUpdateParameters(nullptr) {}
  ~NetworkUpdateParameters() override;
  explicit constexpr NetworkUpdateParameters(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  NetworkUpdateParameters(const NetworkUpdateParameters& from);
  NetworkUpdateParameters(NetworkUpdateParameters&& from) noexcept
    : NetworkUpdateParameters() {
    *this = ::std::move(from);
  }

  inline NetworkUpdateParameters& operator=(const NetworkUpdateParameters& from) {
    CopyFrom(from);
    return *this;
  }
  inline NetworkUpdateParameters& operator=(NetworkUpdateParameters&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const NetworkUpdateParameters& default_instance() {
    return *internal_default_instance();
  }
  static inline const NetworkUpdateParameters* internal_default_instance() {
    return reinterpret_cast<const NetworkUpdateParameters*>(
               &_NetworkUpdateParameters_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    197;

  friend void swap(NetworkUpdateParameters& a, NetworkUpdateParameters& b) {
    a.Swap(&b);
  }
  inline void Swap(NetworkUpdateParameters* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(NetworkUpdateParameters* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  NetworkUpdateParameters* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<NetworkUpdateParameters>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const NetworkUpdateParameters& from);
  void MergeFrom(const NetworkUpdateParameters& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NetworkUpdateParameters* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.NetworkUpdateParameters";
  }
  protected:
  explicit NetworkUpdateParameters(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kLossLayersFieldNumber = 1,
    kOptimizerFieldNumber = 2,
    kEpochsFieldNumber = 3,
    kShuffleFieldNumber = 10,
    kSeedFieldNumber = 20,
  };
  // repeated .CoreML.Specification.LossLayer lossLayers = 1;
  int losslayers_size() const;
  private:
  int _internal_losslayers_size() const;
  public:
  void clear_losslayers();
  ::CoreML::Specification::LossLayer* mutable_losslayers(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::LossLayer >*
      mutable_losslayers();
  private:
  const ::CoreML::Specification::LossLayer& _internal_losslayers(int index) const;
  ::CoreML::Specification::LossLayer* _internal_add_losslayers();
  public:
  const ::CoreML::Specification::LossLayer& losslayers(int index) const;
  ::CoreML::Specification::LossLayer* add_losslayers();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::LossLayer >&
      losslayers() const;

  // .CoreML.Specification.Optimizer optimizer = 2;
  bool has_optimizer() const;
  private:
  bool _internal_has_optimizer() const;
  public:
  void clear_optimizer();
  const ::CoreML::Specification::Optimizer& optimizer() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::Optimizer* release_optimizer();
  ::CoreML::Specification::Optimizer* mutable_optimizer();
  void set_allocated_optimizer(::CoreML::Specification::Optimizer* optimizer);
  private:
  const ::CoreML::Specification::Optimizer& _internal_optimizer() const;
  ::CoreML::Specification::Optimizer* _internal_mutable_optimizer();
  public:
  void unsafe_arena_set_allocated_optimizer(
      ::CoreML::Specification::Optimizer* optimizer);
  ::CoreML::Specification::Optimizer* unsafe_arena_release_optimizer();

  // .CoreML.Specification.Int64Parameter epochs = 3;
  bool has_epochs() const;
  private:
  bool _internal_has_epochs() const;
  public:
  void clear_epochs();
  const ::CoreML::Specification::Int64Parameter& epochs() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::Int64Parameter* release_epochs();
  ::CoreML::Specification::Int64Parameter* mutable_epochs();
  void set_allocated_epochs(::CoreML::Specification::Int64Parameter* epochs);
  private:
  const ::CoreML::Specification::Int64Parameter& _internal_epochs() const;
  ::CoreML::Specification::Int64Parameter* _internal_mutable_epochs();
  public:
  void unsafe_arena_set_allocated_epochs(
      ::CoreML::Specification::Int64Parameter* epochs);
  ::CoreML::Specification::Int64Parameter* unsafe_arena_release_epochs();

  // .CoreML.Specification.BoolParameter shuffle = 10;
  bool has_shuffle() const;
  private:
  bool _internal_has_shuffle() const;
  public:
  void clear_shuffle();
  const ::CoreML::Specification::BoolParameter& shuffle() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::BoolParameter* release_shuffle();
  ::CoreML::Specification::BoolParameter* mutable_shuffle();
  void set_allocated_shuffle(::CoreML::Specification::BoolParameter* shuffle);
  private:
  const ::CoreML::Specification::BoolParameter& _internal_shuffle() const;
  ::CoreML::Specification::BoolParameter* _internal_mutable_shuffle();
  public:
  void unsafe_arena_set_allocated_shuffle(
      ::CoreML::Specification::BoolParameter* shuffle);
  ::CoreML::Specification::BoolParameter* unsafe_arena_release_shuffle();

  // .CoreML.Specification.Int64Parameter seed = 20;
  bool has_seed() const;
  private:
  bool _internal_has_seed() const;
  public:
  void clear_seed();
  const ::CoreML::Specification::Int64Parameter& seed() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::Int64Parameter* release_seed();
  ::CoreML::Specification::Int64Parameter* mutable_seed();
  void set_allocated_seed(::CoreML::Specification::Int64Parameter* seed);
  private:
  const ::CoreML::Specification::Int64Parameter& _internal_seed() const;
  ::CoreML::Specification::Int64Parameter* _internal_mutable_seed();
  public:
  void unsafe_arena_set_allocated_seed(
      ::CoreML::Specification::Int64Parameter* seed);
  ::CoreML::Specification::Int64Parameter* unsafe_arena_release_seed();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.NetworkUpdateParameters)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::LossLayer > losslayers_;
  ::CoreML::Specification::Optimizer* optimizer_;
  ::CoreML::Specification::Int64Parameter* epochs_;
  ::CoreML::Specification::BoolParameter* shuffle_;
  ::CoreML::Specification::Int64Parameter* seed_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class LossLayer final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LossLayer) */ {
 public:
  inline LossLayer() : LossLayer(nullptr) {}
  ~LossLayer() override;
  explicit constexpr LossLayer(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  LossLayer(const LossLayer& from);
  LossLayer(LossLayer&& from) noexcept
    : LossLayer() {
    *this = ::std::move(from);
  }

  inline LossLayer& operator=(const LossLayer& from) {
    CopyFrom(from);
    return *this;
  }
  inline LossLayer& operator=(LossLayer&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const LossLayer& default_instance() {
    return *internal_default_instance();
  }
  enum LossLayerTypeCase {
    kCategoricalCrossEntropyLossLayer = 10,
    kMeanSquaredErrorLossLayer = 11,
    LOSSLAYERTYPE_NOT_SET = 0,
  };

  static inline const LossLayer* internal_default_instance() {
    return reinterpret_cast<const LossLayer*>(
               &_LossLayer_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    198;

  friend void swap(LossLayer& a, LossLayer& b) {
    a.Swap(&b);
  }
  inline void Swap(LossLayer* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(LossLayer* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  LossLayer* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<LossLayer>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const LossLayer& from);
  void MergeFrom(const LossLayer& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LossLayer* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.LossLayer";
  }
  protected:
  explicit LossLayer(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kNameFieldNumber = 1,
    kCategoricalCrossEntropyLossLayerFieldNumber = 10,
    kMeanSquaredErrorLossLayerFieldNumber = 11,
  };
  // string name = 1;
  void clear_name();
  const std::string& name() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_name(ArgT0&& arg0, ArgT... args);
  std::string* mutable_name();
  PROTOBUF_NODISCARD std::string* release_name();
  void set_allocated_name(std::string* name);
  private:
  const std::string& _internal_name() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_name(const std::string& value);
  std::string* _internal_mutable_name();
  public:

  // .CoreML.Specification.CategoricalCrossEntropyLossLayer categoricalCrossEntropyLossLayer = 10;
  bool has_categoricalcrossentropylosslayer() const;
  private:
  bool _internal_has_categoricalcrossentropylosslayer() const;
  public:
  void clear_categoricalcrossentropylosslayer();
  const ::CoreML::Specification::CategoricalCrossEntropyLossLayer& categoricalcrossentropylosslayer() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::CategoricalCrossEntropyLossLayer* release_categoricalcrossentropylosslayer();
  ::CoreML::Specification::CategoricalCrossEntropyLossLayer* mutable_categoricalcrossentropylosslayer();
  void set_allocated_categoricalcrossentropylosslayer(::CoreML::Specification::CategoricalCrossEntropyLossLayer* categoricalcrossentropylosslayer);
  private:
  const ::CoreML::Specification::CategoricalCrossEntropyLossLayer& _internal_categoricalcrossentropylosslayer() const;
  ::CoreML::Specification::CategoricalCrossEntropyLossLayer* _internal_mutable_categoricalcrossentropylosslayer();
  public:
  void unsafe_arena_set_allocated_categoricalcrossentropylosslayer(
      ::CoreML::Specification::CategoricalCrossEntropyLossLayer* categoricalcrossentropylosslayer);
  ::CoreML::Specification::CategoricalCrossEntropyLossLayer* unsafe_arena_release_categoricalcrossentropylosslayer();

  // .CoreML.Specification.MeanSquaredErrorLossLayer meanSquaredErrorLossLayer = 11;
  bool has_meansquarederrorlosslayer() const;
  private:
  bool _internal_has_meansquarederrorlosslayer() const;
  public:
  void clear_meansquarederrorlosslayer();
  const ::CoreML::Specification::MeanSquaredErrorLossLayer& meansquarederrorlosslayer() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::MeanSquaredErrorLossLayer* release_meansquarederrorlosslayer();
  ::CoreML::Specification::MeanSquaredErrorLossLayer* mutable_meansquarederrorlosslayer();
  void set_allocated_meansquarederrorlosslayer(::CoreML::Specification::MeanSquaredErrorLossLayer* meansquarederrorlosslayer);
  private:
  const ::CoreML::Specification::MeanSquaredErrorLossLayer& _internal_meansquarederrorlosslayer() const;
  ::CoreML::Specification::MeanSquaredErrorLossLayer* _internal_mutable_meansquarederrorlosslayer();
  public:
  void unsafe_arena_set_allocated_meansquarederrorlosslayer(
      ::CoreML::Specification::MeanSquaredErrorLossLayer* meansquarederrorlosslayer);
  ::CoreML::Specification::MeanSquaredErrorLossLayer* unsafe_arena_release_meansquarederrorlosslayer();

  void clear_LossLayerType();
  LossLayerTypeCase LossLayerType_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.LossLayer)
 private:
  class _Internal;
  void set_has_categoricalcrossentropylosslayer();
  void set_has_meansquarederrorlosslayer();

  inline bool has_LossLayerType() const;
  inline void clear_has_LossLayerType();

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr name_;
  union LossLayerTypeUnion {
    constexpr LossLayerTypeUnion() : _constinit_{} {}
      ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized _constinit_;
    ::CoreML::Specification::CategoricalCrossEntropyLossLayer* categoricalcrossentropylosslayer_;
    ::CoreML::Specification::MeanSquaredErrorLossLayer* meansquarederrorlosslayer_;
  } LossLayerType_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  uint32_t _oneof_case_[1];

  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class CategoricalCrossEntropyLossLayer final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CategoricalCrossEntropyLossLayer) */ {
 public:
  inline CategoricalCrossEntropyLossLayer() : CategoricalCrossEntropyLossLayer(nullptr) {}
  ~CategoricalCrossEntropyLossLayer() override;
  explicit constexpr CategoricalCrossEntropyLossLayer(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  CategoricalCrossEntropyLossLayer(const CategoricalCrossEntropyLossLayer& from);
  CategoricalCrossEntropyLossLayer(CategoricalCrossEntropyLossLayer&& from) noexcept
    : CategoricalCrossEntropyLossLayer() {
    *this = ::std::move(from);
  }

  inline CategoricalCrossEntropyLossLayer& operator=(const CategoricalCrossEntropyLossLayer& from) {
    CopyFrom(from);
    return *this;
  }
  inline CategoricalCrossEntropyLossLayer& operator=(CategoricalCrossEntropyLossLayer&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const CategoricalCrossEntropyLossLayer& default_instance() {
    return *internal_default_instance();
  }
  static inline const CategoricalCrossEntropyLossLayer* internal_default_instance() {
    return reinterpret_cast<const CategoricalCrossEntropyLossLayer*>(
               &_CategoricalCrossEntropyLossLayer_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    199;

  friend void swap(CategoricalCrossEntropyLossLayer& a, CategoricalCrossEntropyLossLayer& b) {
    a.Swap(&b);
  }
  inline void Swap(CategoricalCrossEntropyLossLayer* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(CategoricalCrossEntropyLossLayer* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  CategoricalCrossEntropyLossLayer* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<CategoricalCrossEntropyLossLayer>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const CategoricalCrossEntropyLossLayer& from);
  void MergeFrom(const CategoricalCrossEntropyLossLayer& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CategoricalCrossEntropyLossLayer* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.CategoricalCrossEntropyLossLayer";
  }
  protected:
  explicit CategoricalCrossEntropyLossLayer(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kInputFieldNumber = 1,
    kTargetFieldNumber = 2,
  };
  // string input = 1;
  void clear_input();
  const std::string& input() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_input(ArgT0&& arg0, ArgT... args);
  std::string* mutable_input();
  PROTOBUF_NODISCARD std::string* release_input();
  void set_allocated_input(std::string* input);
  private:
  const std::string& _internal_input() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_input(const std::string& value);
  std::string* _internal_mutable_input();
  public:

  // string target = 2;
  void clear_target();
  const std::string& target() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_target(ArgT0&& arg0, ArgT... args);
  std::string* mutable_target();
  PROTOBUF_NODISCARD std::string* release_target();
  void set_allocated_target(std::string* target);
  private:
  const std::string& _internal_target() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_target(const std::string& value);
  std::string* _internal_mutable_target();
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.CategoricalCrossEntropyLossLayer)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr input_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr target_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class MeanSquaredErrorLossLayer final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MeanSquaredErrorLossLayer) */ {
 public:
  inline MeanSquaredErrorLossLayer() : MeanSquaredErrorLossLayer(nullptr) {}
  ~MeanSquaredErrorLossLayer() override;
  explicit constexpr MeanSquaredErrorLossLayer(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  MeanSquaredErrorLossLayer(const MeanSquaredErrorLossLayer& from);
  MeanSquaredErrorLossLayer(MeanSquaredErrorLossLayer&& from) noexcept
    : MeanSquaredErrorLossLayer() {
    *this = ::std::move(from);
  }

  inline MeanSquaredErrorLossLayer& operator=(const MeanSquaredErrorLossLayer& from) {
    CopyFrom(from);
    return *this;
  }
  inline MeanSquaredErrorLossLayer& operator=(MeanSquaredErrorLossLayer&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const MeanSquaredErrorLossLayer& default_instance() {
    return *internal_default_instance();
  }
  static inline const MeanSquaredErrorLossLayer* internal_default_instance() {
    return reinterpret_cast<const MeanSquaredErrorLossLayer*>(
               &_MeanSquaredErrorLossLayer_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    200;

  friend void swap(MeanSquaredErrorLossLayer& a, MeanSquaredErrorLossLayer& b) {
    a.Swap(&b);
  }
  inline void Swap(MeanSquaredErrorLossLayer* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(MeanSquaredErrorLossLayer* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  MeanSquaredErrorLossLayer* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<MeanSquaredErrorLossLayer>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const MeanSquaredErrorLossLayer& from);
  void MergeFrom(const MeanSquaredErrorLossLayer& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MeanSquaredErrorLossLayer* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.MeanSquaredErrorLossLayer";
  }
  protected:
  explicit MeanSquaredErrorLossLayer(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kInputFieldNumber = 1,
    kTargetFieldNumber = 2,
  };
  // string input = 1;
  void clear_input();
  const std::string& input() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_input(ArgT0&& arg0, ArgT... args);
  std::string* mutable_input();
  PROTOBUF_NODISCARD std::string* release_input();
  void set_allocated_input(std::string* input);
  private:
  const std::string& _internal_input() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_input(const std::string& value);
  std::string* _internal_mutable_input();
  public:

  // string target = 2;
  void clear_target();
  const std::string& target() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_target(ArgT0&& arg0, ArgT... args);
  std::string* mutable_target();
  PROTOBUF_NODISCARD std::string* release_target();
  void set_allocated_target(std::string* target);
  private:
  const std::string& _internal_target() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_target(const std::string& value);
  std::string* _internal_mutable_target();
  public:

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MeanSquaredErrorLossLayer)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr input_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr target_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class Optimizer final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.Optimizer) */ {
 public:
  inline Optimizer() : Optimizer(nullptr) {}
  ~Optimizer() override;
  explicit constexpr Optimizer(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  Optimizer(const Optimizer& from);
  Optimizer(Optimizer&& from) noexcept
    : Optimizer() {
    *this = ::std::move(from);
  }

  inline Optimizer& operator=(const Optimizer& from) {
    CopyFrom(from);
    return *this;
  }
  inline Optimizer& operator=(Optimizer&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const Optimizer& default_instance() {
    return *internal_default_instance();
  }
  enum OptimizerTypeCase {
    kSgdOptimizer = 10,
    kAdamOptimizer = 11,
    OPTIMIZERTYPE_NOT_SET = 0,
  };

  static inline const Optimizer* internal_default_instance() {
    return reinterpret_cast<const Optimizer*>(
               &_Optimizer_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    201;

  friend void swap(Optimizer& a, Optimizer& b) {
    a.Swap(&b);
  }
  inline void Swap(Optimizer* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(Optimizer* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  Optimizer* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<Optimizer>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const Optimizer& from);
  void MergeFrom(const Optimizer& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(Optimizer* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.Optimizer";
  }
  protected:
  explicit Optimizer(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kSgdOptimizerFieldNumber = 10,
    kAdamOptimizerFieldNumber = 11,
  };
  // .CoreML.Specification.SGDOptimizer sgdOptimizer = 10;
  bool has_sgdoptimizer() const;
  private:
  bool _internal_has_sgdoptimizer() const;
  public:
  void clear_sgdoptimizer();
  const ::CoreML::Specification::SGDOptimizer& sgdoptimizer() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::SGDOptimizer* release_sgdoptimizer();
  ::CoreML::Specification::SGDOptimizer* mutable_sgdoptimizer();
  void set_allocated_sgdoptimizer(::CoreML::Specification::SGDOptimizer* sgdoptimizer);
  private:
  const ::CoreML::Specification::SGDOptimizer& _internal_sgdoptimizer() const;
  ::CoreML::Specification::SGDOptimizer* _internal_mutable_sgdoptimizer();
  public:
  void unsafe_arena_set_allocated_sgdoptimizer(
      ::CoreML::Specification::SGDOptimizer* sgdoptimizer);
  ::CoreML::Specification::SGDOptimizer* unsafe_arena_release_sgdoptimizer();

  // .CoreML.Specification.AdamOptimizer adamOptimizer = 11;
  bool has_adamoptimizer() const;
  private:
  bool _internal_has_adamoptimizer() const;
  public:
  void clear_adamoptimizer();
  const ::CoreML::Specification::AdamOptimizer& adamoptimizer() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::AdamOptimizer* release_adamoptimizer();
  ::CoreML::Specification::AdamOptimizer* mutable_adamoptimizer();
  void set_allocated_adamoptimizer(::CoreML::Specification::AdamOptimizer* adamoptimizer);
  private:
  const ::CoreML::Specification::AdamOptimizer& _internal_adamoptimizer() const;
  ::CoreML::Specification::AdamOptimizer* _internal_mutable_adamoptimizer();
  public:
  void unsafe_arena_set_allocated_adamoptimizer(
      ::CoreML::Specification::AdamOptimizer* adamoptimizer);
  ::CoreML::Specification::AdamOptimizer* unsafe_arena_release_adamoptimizer();

  void clear_OptimizerType();
  OptimizerTypeCase OptimizerType_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.Optimizer)
 private:
  class _Internal;
  void set_has_sgdoptimizer();
  void set_has_adamoptimizer();

  inline bool has_OptimizerType() const;
  inline void clear_has_OptimizerType();

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  union OptimizerTypeUnion {
    constexpr OptimizerTypeUnion() : _constinit_{} {}
      ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized _constinit_;
    ::CoreML::Specification::SGDOptimizer* sgdoptimizer_;
    ::CoreML::Specification::AdamOptimizer* adamoptimizer_;
  } OptimizerType_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  uint32_t _oneof_case_[1];

  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class SGDOptimizer final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SGDOptimizer) */ {
 public:
  inline SGDOptimizer() : SGDOptimizer(nullptr) {}
  ~SGDOptimizer() override;
  explicit constexpr SGDOptimizer(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SGDOptimizer(const SGDOptimizer& from);
  SGDOptimizer(SGDOptimizer&& from) noexcept
    : SGDOptimizer() {
    *this = ::std::move(from);
  }

  inline SGDOptimizer& operator=(const SGDOptimizer& from) {
    CopyFrom(from);
    return *this;
  }
  inline SGDOptimizer& operator=(SGDOptimizer&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const SGDOptimizer& default_instance() {
    return *internal_default_instance();
  }
  static inline const SGDOptimizer* internal_default_instance() {
    return reinterpret_cast<const SGDOptimizer*>(
               &_SGDOptimizer_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    202;

  friend void swap(SGDOptimizer& a, SGDOptimizer& b) {
    a.Swap(&b);
  }
  inline void Swap(SGDOptimizer* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SGDOptimizer* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SGDOptimizer* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SGDOptimizer>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const SGDOptimizer& from);
  void MergeFrom(const SGDOptimizer& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SGDOptimizer* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.SGDOptimizer";
  }
  protected:
  explicit SGDOptimizer(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kLearningRateFieldNumber = 1,
    kMiniBatchSizeFieldNumber = 2,
    kMomentumFieldNumber = 3,
  };
  // .CoreML.Specification.DoubleParameter learningRate = 1;
  bool has_learningrate() const;
  private:
  bool _internal_has_learningrate() const;
  public:
  void clear_learningrate();
  const ::CoreML::Specification::DoubleParameter& learningrate() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::DoubleParameter* release_learningrate();
  ::CoreML::Specification::DoubleParameter* mutable_learningrate();
  void set_allocated_learningrate(::CoreML::Specification::DoubleParameter* learningrate);
  private:
  const ::CoreML::Specification::DoubleParameter& _internal_learningrate() const;
  ::CoreML::Specification::DoubleParameter* _internal_mutable_learningrate();
  public:
  void unsafe_arena_set_allocated_learningrate(
      ::CoreML::Specification::DoubleParameter* learningrate);
  ::CoreML::Specification::DoubleParameter* unsafe_arena_release_learningrate();

  // .CoreML.Specification.Int64Parameter miniBatchSize = 2;
  bool has_minibatchsize() const;
  private:
  bool _internal_has_minibatchsize() const;
  public:
  void clear_minibatchsize();
  const ::CoreML::Specification::Int64Parameter& minibatchsize() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::Int64Parameter* release_minibatchsize();
  ::CoreML::Specification::Int64Parameter* mutable_minibatchsize();
  void set_allocated_minibatchsize(::CoreML::Specification::Int64Parameter* minibatchsize);
  private:
  const ::CoreML::Specification::Int64Parameter& _internal_minibatchsize() const;
  ::CoreML::Specification::Int64Parameter* _internal_mutable_minibatchsize();
  public:
  void unsafe_arena_set_allocated_minibatchsize(
      ::CoreML::Specification::Int64Parameter* minibatchsize);
  ::CoreML::Specification::Int64Parameter* unsafe_arena_release_minibatchsize();

  // .CoreML.Specification.DoubleParameter momentum = 3;
  bool has_momentum() const;
  private:
  bool _internal_has_momentum() const;
  public:
  void clear_momentum();
  const ::CoreML::Specification::DoubleParameter& momentum() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::DoubleParameter* release_momentum();
  ::CoreML::Specification::DoubleParameter* mutable_momentum();
  void set_allocated_momentum(::CoreML::Specification::DoubleParameter* momentum);
  private:
  const ::CoreML::Specification::DoubleParameter& _internal_momentum() const;
  ::CoreML::Specification::DoubleParameter* _internal_mutable_momentum();
  public:
  void unsafe_arena_set_allocated_momentum(
      ::CoreML::Specification::DoubleParameter* momentum);
  ::CoreML::Specification::DoubleParameter* unsafe_arena_release_momentum();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SGDOptimizer)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::CoreML::Specification::DoubleParameter* learningrate_;
  ::CoreML::Specification::Int64Parameter* minibatchsize_;
  ::CoreML::Specification::DoubleParameter* momentum_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// -------------------------------------------------------------------

class AdamOptimizer final :
    public ::PROTOBUF_NAMESPACE_ID::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AdamOptimizer) */ {
 public:
  inline AdamOptimizer() : AdamOptimizer(nullptr) {}
  ~AdamOptimizer() override;
  explicit constexpr AdamOptimizer(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  AdamOptimizer(const AdamOptimizer& from);
  AdamOptimizer(AdamOptimizer&& from) noexcept
    : AdamOptimizer() {
    *this = ::std::move(from);
  }

  inline AdamOptimizer& operator=(const AdamOptimizer& from) {
    CopyFrom(from);
    return *this;
  }
  inline AdamOptimizer& operator=(AdamOptimizer&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const AdamOptimizer& default_instance() {
    return *internal_default_instance();
  }
  static inline const AdamOptimizer* internal_default_instance() {
    return reinterpret_cast<const AdamOptimizer*>(
               &_AdamOptimizer_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    203;

  friend void swap(AdamOptimizer& a, AdamOptimizer& b) {
    a.Swap(&b);
  }
  inline void Swap(AdamOptimizer* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(AdamOptimizer* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  AdamOptimizer* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<AdamOptimizer>(arena);
  }
  void CheckTypeAndMergeFrom(const ::PROTOBUF_NAMESPACE_ID::MessageLite& from)  final;
  void CopyFrom(const AdamOptimizer& from);
  void MergeFrom(const AdamOptimizer& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AdamOptimizer* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "CoreML.Specification.AdamOptimizer";
  }
  protected:
  explicit AdamOptimizer(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  std::string GetTypeName() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kLearningRateFieldNumber = 1,
    kMiniBatchSizeFieldNumber = 2,
    kBeta1FieldNumber = 3,
    kBeta2FieldNumber = 4,
    kEpsFieldNumber = 5,
  };
  // .CoreML.Specification.DoubleParameter learningRate = 1;
  bool has_learningrate() const;
  private:
  bool _internal_has_learningrate() const;
  public:
  void clear_learningrate();
  const ::CoreML::Specification::DoubleParameter& learningrate() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::DoubleParameter* release_learningrate();
  ::CoreML::Specification::DoubleParameter* mutable_learningrate();
  void set_allocated_learningrate(::CoreML::Specification::DoubleParameter* learningrate);
  private:
  const ::CoreML::Specification::DoubleParameter& _internal_learningrate() const;
  ::CoreML::Specification::DoubleParameter* _internal_mutable_learningrate();
  public:
  void unsafe_arena_set_allocated_learningrate(
      ::CoreML::Specification::DoubleParameter* learningrate);
  ::CoreML::Specification::DoubleParameter* unsafe_arena_release_learningrate();

  // .CoreML.Specification.Int64Parameter miniBatchSize = 2;
  bool has_minibatchsize() const;
  private:
  bool _internal_has_minibatchsize() const;
  public:
  void clear_minibatchsize();
  const ::CoreML::Specification::Int64Parameter& minibatchsize() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::Int64Parameter* release_minibatchsize();
  ::CoreML::Specification::Int64Parameter* mutable_minibatchsize();
  void set_allocated_minibatchsize(::CoreML::Specification::Int64Parameter* minibatchsize);
  private:
  const ::CoreML::Specification::Int64Parameter& _internal_minibatchsize() const;
  ::CoreML::Specification::Int64Parameter* _internal_mutable_minibatchsize();
  public:
  void unsafe_arena_set_allocated_minibatchsize(
      ::CoreML::Specification::Int64Parameter* minibatchsize);
  ::CoreML::Specification::Int64Parameter* unsafe_arena_release_minibatchsize();

  // .CoreML.Specification.DoubleParameter beta1 = 3;
  bool has_beta1() const;
  private:
  bool _internal_has_beta1() const;
  public:
  void clear_beta1();
  const ::CoreML::Specification::DoubleParameter& beta1() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::DoubleParameter* release_beta1();
  ::CoreML::Specification::DoubleParameter* mutable_beta1();
  void set_allocated_beta1(::CoreML::Specification::DoubleParameter* beta1);
  private:
  const ::CoreML::Specification::DoubleParameter& _internal_beta1() const;
  ::CoreML::Specification::DoubleParameter* _internal_mutable_beta1();
  public:
  void unsafe_arena_set_allocated_beta1(
      ::CoreML::Specification::DoubleParameter* beta1);
  ::CoreML::Specification::DoubleParameter* unsafe_arena_release_beta1();

  // .CoreML.Specification.DoubleParameter beta2 = 4;
  bool has_beta2() const;
  private:
  bool _internal_has_beta2() const;
  public:
  void clear_beta2();
  const ::CoreML::Specification::DoubleParameter& beta2() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::DoubleParameter* release_beta2();
  ::CoreML::Specification::DoubleParameter* mutable_beta2();
  void set_allocated_beta2(::CoreML::Specification::DoubleParameter* beta2);
  private:
  const ::CoreML::Specification::DoubleParameter& _internal_beta2() const;
  ::CoreML::Specification::DoubleParameter* _internal_mutable_beta2();
  public:
  void unsafe_arena_set_allocated_beta2(
      ::CoreML::Specification::DoubleParameter* beta2);
  ::CoreML::Specification::DoubleParameter* unsafe_arena_release_beta2();

  // .CoreML.Specification.DoubleParameter eps = 5;
  bool has_eps() const;
  private:
  bool _internal_has_eps() const;
  public:
  void clear_eps();
  const ::CoreML::Specification::DoubleParameter& eps() const;
  PROTOBUF_NODISCARD ::CoreML::Specification::DoubleParameter* release_eps();
  ::CoreML::Specification::DoubleParameter* mutable_eps();
  void set_allocated_eps(::CoreML::Specification::DoubleParameter* eps);
  private:
  const ::CoreML::Specification::DoubleParameter& _internal_eps() const;
  ::CoreML::Specification::DoubleParameter* _internal_mutable_eps();
  public:
  void unsafe_arena_set_allocated_eps(
      ::CoreML::Specification::DoubleParameter* eps);
  ::CoreML::Specification::DoubleParameter* unsafe_arena_release_eps();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AdamOptimizer)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::CoreML::Specification::DoubleParameter* learningrate_;
  ::CoreML::Specification::Int64Parameter* minibatchsize_;
  ::CoreML::Specification::DoubleParameter* beta1_;
  ::CoreML::Specification::DoubleParameter* beta2_;
  ::CoreML::Specification::DoubleParameter* eps_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_NeuralNetwork_2eproto;
};
// ===================================================================


// ===================================================================

#ifdef __GNUC__
  #pragma GCC diagnostic push
  #pragma GCC diagnostic ignored "-Wstrict-aliasing"
#endif  // __GNUC__
// NeuralNetwork

// repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
inline int NeuralNetwork::_internal_layers_size() const {
  return layers_.size();
}
inline int NeuralNetwork::layers_size() const {
  return _internal_layers_size();
}
inline void NeuralNetwork::clear_layers() {
  layers_.Clear();
}
inline ::CoreML::Specification::NeuralNetworkLayer* NeuralNetwork::mutable_layers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetwork.layers)
  return layers_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
NeuralNetwork::mutable_layers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetwork.layers)
  return &layers_;
}
inline const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetwork::_internal_layers(int index) const {
  return layers_.Get(index);
}
inline const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetwork::layers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.layers)
  return _internal_layers(index);
}
inline ::CoreML::Specification::NeuralNetworkLayer* NeuralNetwork::_internal_add_layers() {
  return layers_.Add();
}
inline ::CoreML::Specification::NeuralNetworkLayer* NeuralNetwork::add_layers() {
  ::CoreML::Specification::NeuralNetworkLayer* _add = _internal_add_layers();
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetwork.layers)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
NeuralNetwork::layers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetwork.layers)
  return layers_;
}

// repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
inline int NeuralNetwork::_internal_preprocessing_size() const {
  return preprocessing_.size();
}
inline int NeuralNetwork::preprocessing_size() const {
  return _internal_preprocessing_size();
}
inline void NeuralNetwork::clear_preprocessing() {
  preprocessing_.Clear();
}
inline ::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetwork::mutable_preprocessing(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
NeuralNetwork::mutable_preprocessing() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetwork.preprocessing)
  return &preprocessing_;
}
inline const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetwork::_internal_preprocessing(int index) const {
  return preprocessing_.Get(index);
}
inline const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetwork::preprocessing(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.preprocessing)
  return _internal_preprocessing(index);
}
inline ::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetwork::_internal_add_preprocessing() {
  return preprocessing_.Add();
}
inline ::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetwork::add_preprocessing() {
  ::CoreML::Specification::NeuralNetworkPreprocessing* _add = _internal_add_preprocessing();
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetwork.preprocessing)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
NeuralNetwork::preprocessing() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_;
}

// .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
inline void NeuralNetwork::clear_arrayinputshapemapping() {
  arrayinputshapemapping_ = 0;
}
inline ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping NeuralNetwork::_internal_arrayinputshapemapping() const {
  return static_cast< ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping >(arrayinputshapemapping_);
}
inline ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping NeuralNetwork::arrayinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.arrayInputShapeMapping)
  return _internal_arrayinputshapemapping();
}
inline void NeuralNetwork::_internal_set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value) {
  
  arrayinputshapemapping_ = value;
}
inline void NeuralNetwork::set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value) {
  _internal_set_arrayinputshapemapping(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetwork.arrayInputShapeMapping)
}

// .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
inline void NeuralNetwork::clear_imageinputshapemapping() {
  imageinputshapemapping_ = 0;
}
inline ::CoreML::Specification::NeuralNetworkImageShapeMapping NeuralNetwork::_internal_imageinputshapemapping() const {
  return static_cast< ::CoreML::Specification::NeuralNetworkImageShapeMapping >(imageinputshapemapping_);
}
inline ::CoreML::Specification::NeuralNetworkImageShapeMapping NeuralNetwork::imageinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.imageInputShapeMapping)
  return _internal_imageinputshapemapping();
}
inline void NeuralNetwork::_internal_set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value) {
  
  imageinputshapemapping_ = value;
}
inline void NeuralNetwork::set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value) {
  _internal_set_imageinputshapemapping(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetwork.imageInputShapeMapping)
}

// .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
inline bool NeuralNetwork::_internal_has_updateparams() const {
  return this != internal_default_instance() && updateparams_ != nullptr;
}
inline bool NeuralNetwork::has_updateparams() const {
  return _internal_has_updateparams();
}
inline void NeuralNetwork::clear_updateparams() {
  if (GetArenaForAllocation() == nullptr && updateparams_ != nullptr) {
    delete updateparams_;
  }
  updateparams_ = nullptr;
}
inline const ::CoreML::Specification::NetworkUpdateParameters& NeuralNetwork::_internal_updateparams() const {
  const ::CoreML::Specification::NetworkUpdateParameters* p = updateparams_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::NetworkUpdateParameters&>(
      ::CoreML::Specification::_NetworkUpdateParameters_default_instance_);
}
inline const ::CoreML::Specification::NetworkUpdateParameters& NeuralNetwork::updateparams() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.updateParams)
  return _internal_updateparams();
}
inline void NeuralNetwork::unsafe_arena_set_allocated_updateparams(
    ::CoreML::Specification::NetworkUpdateParameters* updateparams) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(updateparams_);
  }
  updateparams_ = updateparams;
  if (updateparams) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetwork.updateParams)
}
inline ::CoreML::Specification::NetworkUpdateParameters* NeuralNetwork::release_updateparams() {
  
  ::CoreML::Specification::NetworkUpdateParameters* temp = updateparams_;
  updateparams_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::NetworkUpdateParameters* NeuralNetwork::unsafe_arena_release_updateparams() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetwork.updateParams)
  
  ::CoreML::Specification::NetworkUpdateParameters* temp = updateparams_;
  updateparams_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::NetworkUpdateParameters* NeuralNetwork::_internal_mutable_updateparams() {
  
  if (updateparams_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::NetworkUpdateParameters>(GetArenaForAllocation());
    updateparams_ = p;
  }
  return updateparams_;
}
inline ::CoreML::Specification::NetworkUpdateParameters* NeuralNetwork::mutable_updateparams() {
  ::CoreML::Specification::NetworkUpdateParameters* _msg = _internal_mutable_updateparams();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetwork.updateParams)
  return _msg;
}
inline void NeuralNetwork::set_allocated_updateparams(::CoreML::Specification::NetworkUpdateParameters* updateparams) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete updateparams_;
  }
  if (updateparams) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::NetworkUpdateParameters>::GetOwningArena(updateparams);
    if (message_arena != submessage_arena) {
      updateparams = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, updateparams, submessage_arena);
    }
    
  } else {
    
  }
  updateparams_ = updateparams;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetwork.updateParams)
}

// -------------------------------------------------------------------

// NeuralNetworkImageScaler

// float channelScale = 10;
inline void NeuralNetworkImageScaler::clear_channelscale() {
  channelscale_ = 0;
}
inline float NeuralNetworkImageScaler::_internal_channelscale() const {
  return channelscale_;
}
inline float NeuralNetworkImageScaler::channelscale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.channelScale)
  return _internal_channelscale();
}
inline void NeuralNetworkImageScaler::_internal_set_channelscale(float value) {
  
  channelscale_ = value;
}
inline void NeuralNetworkImageScaler::set_channelscale(float value) {
  _internal_set_channelscale(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.channelScale)
}

// float blueBias = 20;
inline void NeuralNetworkImageScaler::clear_bluebias() {
  bluebias_ = 0;
}
inline float NeuralNetworkImageScaler::_internal_bluebias() const {
  return bluebias_;
}
inline float NeuralNetworkImageScaler::bluebias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.blueBias)
  return _internal_bluebias();
}
inline void NeuralNetworkImageScaler::_internal_set_bluebias(float value) {
  
  bluebias_ = value;
}
inline void NeuralNetworkImageScaler::set_bluebias(float value) {
  _internal_set_bluebias(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.blueBias)
}

// float greenBias = 21;
inline void NeuralNetworkImageScaler::clear_greenbias() {
  greenbias_ = 0;
}
inline float NeuralNetworkImageScaler::_internal_greenbias() const {
  return greenbias_;
}
inline float NeuralNetworkImageScaler::greenbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.greenBias)
  return _internal_greenbias();
}
inline void NeuralNetworkImageScaler::_internal_set_greenbias(float value) {
  
  greenbias_ = value;
}
inline void NeuralNetworkImageScaler::set_greenbias(float value) {
  _internal_set_greenbias(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.greenBias)
}

// float redBias = 22;
inline void NeuralNetworkImageScaler::clear_redbias() {
  redbias_ = 0;
}
inline float NeuralNetworkImageScaler::_internal_redbias() const {
  return redbias_;
}
inline float NeuralNetworkImageScaler::redbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.redBias)
  return _internal_redbias();
}
inline void NeuralNetworkImageScaler::_internal_set_redbias(float value) {
  
  redbias_ = value;
}
inline void NeuralNetworkImageScaler::set_redbias(float value) {
  _internal_set_redbias(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.redBias)
}

// float grayBias = 30;
inline void NeuralNetworkImageScaler::clear_graybias() {
  graybias_ = 0;
}
inline float NeuralNetworkImageScaler::_internal_graybias() const {
  return graybias_;
}
inline float NeuralNetworkImageScaler::graybias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.grayBias)
  return _internal_graybias();
}
inline void NeuralNetworkImageScaler::_internal_set_graybias(float value) {
  
  graybias_ = value;
}
inline void NeuralNetworkImageScaler::set_graybias(float value) {
  _internal_set_graybias(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.grayBias)
}

// -------------------------------------------------------------------

// NeuralNetworkMeanImage

// repeated float meanImage = 1;
inline int NeuralNetworkMeanImage::_internal_meanimage_size() const {
  return meanimage_.size();
}
inline int NeuralNetworkMeanImage::meanimage_size() const {
  return _internal_meanimage_size();
}
inline void NeuralNetworkMeanImage::clear_meanimage() {
  meanimage_.Clear();
}
inline float NeuralNetworkMeanImage::_internal_meanimage(int index) const {
  return meanimage_.Get(index);
}
inline float NeuralNetworkMeanImage::meanimage(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
  return _internal_meanimage(index);
}
inline void NeuralNetworkMeanImage::set_meanimage(int index, float value) {
  meanimage_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
}
inline void NeuralNetworkMeanImage::_internal_add_meanimage(float value) {
  meanimage_.Add(value);
}
inline void NeuralNetworkMeanImage::add_meanimage(float value) {
  _internal_add_meanimage(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
NeuralNetworkMeanImage::_internal_meanimage() const {
  return meanimage_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
NeuralNetworkMeanImage::meanimage() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
  return _internal_meanimage();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
NeuralNetworkMeanImage::_internal_mutable_meanimage() {
  return &meanimage_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
NeuralNetworkMeanImage::mutable_meanimage() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
  return _internal_mutable_meanimage();
}

// -------------------------------------------------------------------

// NeuralNetworkPreprocessing

// string featureName = 1;
inline void NeuralNetworkPreprocessing::clear_featurename() {
  featurename_.ClearToEmpty();
}
inline const std::string& NeuralNetworkPreprocessing::featurename() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
  return _internal_featurename();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void NeuralNetworkPreprocessing::set_featurename(ArgT0&& arg0, ArgT... args) {
 
 featurename_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
inline std::string* NeuralNetworkPreprocessing::mutable_featurename() {
  std::string* _s = _internal_mutable_featurename();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
  return _s;
}
inline const std::string& NeuralNetworkPreprocessing::_internal_featurename() const {
  return featurename_.Get();
}
inline void NeuralNetworkPreprocessing::_internal_set_featurename(const std::string& value) {
  
  featurename_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* NeuralNetworkPreprocessing::_internal_mutable_featurename() {
  
  return featurename_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* NeuralNetworkPreprocessing::release_featurename() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
  return featurename_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void NeuralNetworkPreprocessing::set_allocated_featurename(std::string* featurename) {
  if (featurename != nullptr) {
    
  } else {
    
  }
  featurename_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), featurename,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (featurename_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    featurename_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}

// .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
inline bool NeuralNetworkPreprocessing::_internal_has_scaler() const {
  return preprocessor_case() == kScaler;
}
inline bool NeuralNetworkPreprocessing::has_scaler() const {
  return _internal_has_scaler();
}
inline void NeuralNetworkPreprocessing::set_has_scaler() {
  _oneof_case_[0] = kScaler;
}
inline void NeuralNetworkPreprocessing::clear_scaler() {
  if (_internal_has_scaler()) {
    if (GetArenaForAllocation() == nullptr) {
      delete preprocessor_.scaler_;
    }
    clear_has_preprocessor();
  }
}
inline ::CoreML::Specification::NeuralNetworkImageScaler* NeuralNetworkPreprocessing::release_scaler() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  if (_internal_has_scaler()) {
    clear_has_preprocessor();
      ::CoreML::Specification::NeuralNetworkImageScaler* temp = preprocessor_.scaler_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    preprocessor_.scaler_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::NeuralNetworkImageScaler& NeuralNetworkPreprocessing::_internal_scaler() const {
  return _internal_has_scaler()
      ? *preprocessor_.scaler_
      : reinterpret_cast< ::CoreML::Specification::NeuralNetworkImageScaler&>(::CoreML::Specification::_NeuralNetworkImageScaler_default_instance_);
}
inline const ::CoreML::Specification::NeuralNetworkImageScaler& NeuralNetworkPreprocessing::scaler() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  return _internal_scaler();
}
inline ::CoreML::Specification::NeuralNetworkImageScaler* NeuralNetworkPreprocessing::unsafe_arena_release_scaler() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  if (_internal_has_scaler()) {
    clear_has_preprocessor();
    ::CoreML::Specification::NeuralNetworkImageScaler* temp = preprocessor_.scaler_;
    preprocessor_.scaler_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkPreprocessing::unsafe_arena_set_allocated_scaler(::CoreML::Specification::NeuralNetworkImageScaler* scaler) {
  clear_preprocessor();
  if (scaler) {
    set_has_scaler();
    preprocessor_.scaler_ = scaler;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
}
inline ::CoreML::Specification::NeuralNetworkImageScaler* NeuralNetworkPreprocessing::_internal_mutable_scaler() {
  if (!_internal_has_scaler()) {
    clear_preprocessor();
    set_has_scaler();
    preprocessor_.scaler_ = CreateMaybeMessage< ::CoreML::Specification::NeuralNetworkImageScaler >(GetArenaForAllocation());
  }
  return preprocessor_.scaler_;
}
inline ::CoreML::Specification::NeuralNetworkImageScaler* NeuralNetworkPreprocessing::mutable_scaler() {
  ::CoreML::Specification::NeuralNetworkImageScaler* _msg = _internal_mutable_scaler();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  return _msg;
}

// .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
inline bool NeuralNetworkPreprocessing::_internal_has_meanimage() const {
  return preprocessor_case() == kMeanImage;
}
inline bool NeuralNetworkPreprocessing::has_meanimage() const {
  return _internal_has_meanimage();
}
inline void NeuralNetworkPreprocessing::set_has_meanimage() {
  _oneof_case_[0] = kMeanImage;
}
inline void NeuralNetworkPreprocessing::clear_meanimage() {
  if (_internal_has_meanimage()) {
    if (GetArenaForAllocation() == nullptr) {
      delete preprocessor_.meanimage_;
    }
    clear_has_preprocessor();
  }
}
inline ::CoreML::Specification::NeuralNetworkMeanImage* NeuralNetworkPreprocessing::release_meanimage() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  if (_internal_has_meanimage()) {
    clear_has_preprocessor();
      ::CoreML::Specification::NeuralNetworkMeanImage* temp = preprocessor_.meanimage_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    preprocessor_.meanimage_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::NeuralNetworkMeanImage& NeuralNetworkPreprocessing::_internal_meanimage() const {
  return _internal_has_meanimage()
      ? *preprocessor_.meanimage_
      : reinterpret_cast< ::CoreML::Specification::NeuralNetworkMeanImage&>(::CoreML::Specification::_NeuralNetworkMeanImage_default_instance_);
}
inline const ::CoreML::Specification::NeuralNetworkMeanImage& NeuralNetworkPreprocessing::meanimage() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  return _internal_meanimage();
}
inline ::CoreML::Specification::NeuralNetworkMeanImage* NeuralNetworkPreprocessing::unsafe_arena_release_meanimage() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  if (_internal_has_meanimage()) {
    clear_has_preprocessor();
    ::CoreML::Specification::NeuralNetworkMeanImage* temp = preprocessor_.meanimage_;
    preprocessor_.meanimage_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkPreprocessing::unsafe_arena_set_allocated_meanimage(::CoreML::Specification::NeuralNetworkMeanImage* meanimage) {
  clear_preprocessor();
  if (meanimage) {
    set_has_meanimage();
    preprocessor_.meanimage_ = meanimage;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
}
inline ::CoreML::Specification::NeuralNetworkMeanImage* NeuralNetworkPreprocessing::_internal_mutable_meanimage() {
  if (!_internal_has_meanimage()) {
    clear_preprocessor();
    set_has_meanimage();
    preprocessor_.meanimage_ = CreateMaybeMessage< ::CoreML::Specification::NeuralNetworkMeanImage >(GetArenaForAllocation());
  }
  return preprocessor_.meanimage_;
}
inline ::CoreML::Specification::NeuralNetworkMeanImage* NeuralNetworkPreprocessing::mutable_meanimage() {
  ::CoreML::Specification::NeuralNetworkMeanImage* _msg = _internal_mutable_meanimage();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  return _msg;
}

inline bool NeuralNetworkPreprocessing::has_preprocessor() const {
  return preprocessor_case() != PREPROCESSOR_NOT_SET;
}
inline void NeuralNetworkPreprocessing::clear_has_preprocessor() {
  _oneof_case_[0] = PREPROCESSOR_NOT_SET;
}
inline NeuralNetworkPreprocessing::PreprocessorCase NeuralNetworkPreprocessing::preprocessor_case() const {
  return NeuralNetworkPreprocessing::PreprocessorCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// ActivationReLU

// -------------------------------------------------------------------

// ActivationLeakyReLU

// float alpha = 1;
inline void ActivationLeakyReLU::clear_alpha() {
  alpha_ = 0;
}
inline float ActivationLeakyReLU::_internal_alpha() const {
  return alpha_;
}
inline float ActivationLeakyReLU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationLeakyReLU.alpha)
  return _internal_alpha();
}
inline void ActivationLeakyReLU::_internal_set_alpha(float value) {
  
  alpha_ = value;
}
inline void ActivationLeakyReLU::set_alpha(float value) {
  _internal_set_alpha(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationLeakyReLU.alpha)
}

// -------------------------------------------------------------------

// ActivationTanh

// -------------------------------------------------------------------

// ActivationScaledTanh

// float alpha = 1;
inline void ActivationScaledTanh::clear_alpha() {
  alpha_ = 0;
}
inline float ActivationScaledTanh::_internal_alpha() const {
  return alpha_;
}
inline float ActivationScaledTanh::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationScaledTanh.alpha)
  return _internal_alpha();
}
inline void ActivationScaledTanh::_internal_set_alpha(float value) {
  
  alpha_ = value;
}
inline void ActivationScaledTanh::set_alpha(float value) {
  _internal_set_alpha(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationScaledTanh.alpha)
}

// float beta = 2;
inline void ActivationScaledTanh::clear_beta() {
  beta_ = 0;
}
inline float ActivationScaledTanh::_internal_beta() const {
  return beta_;
}
inline float ActivationScaledTanh::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationScaledTanh.beta)
  return _internal_beta();
}
inline void ActivationScaledTanh::_internal_set_beta(float value) {
  
  beta_ = value;
}
inline void ActivationScaledTanh::set_beta(float value) {
  _internal_set_beta(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationScaledTanh.beta)
}

// -------------------------------------------------------------------

// ActivationSigmoid

// -------------------------------------------------------------------

// ActivationLinear

// float alpha = 1;
inline void ActivationLinear::clear_alpha() {
  alpha_ = 0;
}
inline float ActivationLinear::_internal_alpha() const {
  return alpha_;
}
inline float ActivationLinear::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationLinear.alpha)
  return _internal_alpha();
}
inline void ActivationLinear::_internal_set_alpha(float value) {
  
  alpha_ = value;
}
inline void ActivationLinear::set_alpha(float value) {
  _internal_set_alpha(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationLinear.alpha)
}

// float beta = 2;
inline void ActivationLinear::clear_beta() {
  beta_ = 0;
}
inline float ActivationLinear::_internal_beta() const {
  return beta_;
}
inline float ActivationLinear::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationLinear.beta)
  return _internal_beta();
}
inline void ActivationLinear::_internal_set_beta(float value) {
  
  beta_ = value;
}
inline void ActivationLinear::set_beta(float value) {
  _internal_set_beta(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationLinear.beta)
}

// -------------------------------------------------------------------

// ActivationSigmoidHard

// float alpha = 1;
inline void ActivationSigmoidHard::clear_alpha() {
  alpha_ = 0;
}
inline float ActivationSigmoidHard::_internal_alpha() const {
  return alpha_;
}
inline float ActivationSigmoidHard::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationSigmoidHard.alpha)
  return _internal_alpha();
}
inline void ActivationSigmoidHard::_internal_set_alpha(float value) {
  
  alpha_ = value;
}
inline void ActivationSigmoidHard::set_alpha(float value) {
  _internal_set_alpha(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationSigmoidHard.alpha)
}

// float beta = 2;
inline void ActivationSigmoidHard::clear_beta() {
  beta_ = 0;
}
inline float ActivationSigmoidHard::_internal_beta() const {
  return beta_;
}
inline float ActivationSigmoidHard::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationSigmoidHard.beta)
  return _internal_beta();
}
inline void ActivationSigmoidHard::_internal_set_beta(float value) {
  
  beta_ = value;
}
inline void ActivationSigmoidHard::set_beta(float value) {
  _internal_set_beta(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationSigmoidHard.beta)
}

// -------------------------------------------------------------------

// ActivationPReLU

// .CoreML.Specification.WeightParams alpha = 1;
inline bool ActivationPReLU::_internal_has_alpha() const {
  return this != internal_default_instance() && alpha_ != nullptr;
}
inline bool ActivationPReLU::has_alpha() const {
  return _internal_has_alpha();
}
inline void ActivationPReLU::clear_alpha() {
  if (GetArenaForAllocation() == nullptr && alpha_ != nullptr) {
    delete alpha_;
  }
  alpha_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& ActivationPReLU::_internal_alpha() const {
  const ::CoreML::Specification::WeightParams* p = alpha_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& ActivationPReLU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationPReLU.alpha)
  return _internal_alpha();
}
inline void ActivationPReLU::unsafe_arena_set_allocated_alpha(
    ::CoreML::Specification::WeightParams* alpha) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(alpha_);
  }
  alpha_ = alpha;
  if (alpha) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ActivationPReLU.alpha)
}
inline ::CoreML::Specification::WeightParams* ActivationPReLU::release_alpha() {
  
  ::CoreML::Specification::WeightParams* temp = alpha_;
  alpha_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* ActivationPReLU::unsafe_arena_release_alpha() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationPReLU.alpha)
  
  ::CoreML::Specification::WeightParams* temp = alpha_;
  alpha_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* ActivationPReLU::_internal_mutable_alpha() {
  
  if (alpha_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    alpha_ = p;
  }
  return alpha_;
}
inline ::CoreML::Specification::WeightParams* ActivationPReLU::mutable_alpha() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_alpha();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationPReLU.alpha)
  return _msg;
}
inline void ActivationPReLU::set_allocated_alpha(::CoreML::Specification::WeightParams* alpha) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete alpha_;
  }
  if (alpha) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(alpha);
    if (message_arena != submessage_arena) {
      alpha = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, alpha, submessage_arena);
    }
    
  } else {
    
  }
  alpha_ = alpha;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationPReLU.alpha)
}

// -------------------------------------------------------------------

// ActivationELU

// float alpha = 1;
inline void ActivationELU::clear_alpha() {
  alpha_ = 0;
}
inline float ActivationELU::_internal_alpha() const {
  return alpha_;
}
inline float ActivationELU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationELU.alpha)
  return _internal_alpha();
}
inline void ActivationELU::_internal_set_alpha(float value) {
  
  alpha_ = value;
}
inline void ActivationELU::set_alpha(float value) {
  _internal_set_alpha(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationELU.alpha)
}

// -------------------------------------------------------------------

// ActivationThresholdedReLU

// float alpha = 1;
inline void ActivationThresholdedReLU::clear_alpha() {
  alpha_ = 0;
}
inline float ActivationThresholdedReLU::_internal_alpha() const {
  return alpha_;
}
inline float ActivationThresholdedReLU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationThresholdedReLU.alpha)
  return _internal_alpha();
}
inline void ActivationThresholdedReLU::_internal_set_alpha(float value) {
  
  alpha_ = value;
}
inline void ActivationThresholdedReLU::set_alpha(float value) {
  _internal_set_alpha(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationThresholdedReLU.alpha)
}

// -------------------------------------------------------------------

// ActivationSoftsign

// -------------------------------------------------------------------

// ActivationSoftplus

// -------------------------------------------------------------------

// ActivationParametricSoftplus

// .CoreML.Specification.WeightParams alpha = 1;
inline bool ActivationParametricSoftplus::_internal_has_alpha() const {
  return this != internal_default_instance() && alpha_ != nullptr;
}
inline bool ActivationParametricSoftplus::has_alpha() const {
  return _internal_has_alpha();
}
inline void ActivationParametricSoftplus::clear_alpha() {
  if (GetArenaForAllocation() == nullptr && alpha_ != nullptr) {
    delete alpha_;
  }
  alpha_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& ActivationParametricSoftplus::_internal_alpha() const {
  const ::CoreML::Specification::WeightParams* p = alpha_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& ActivationParametricSoftplus::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParametricSoftplus.alpha)
  return _internal_alpha();
}
inline void ActivationParametricSoftplus::unsafe_arena_set_allocated_alpha(
    ::CoreML::Specification::WeightParams* alpha) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(alpha_);
  }
  alpha_ = alpha;
  if (alpha) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ActivationParametricSoftplus.alpha)
}
inline ::CoreML::Specification::WeightParams* ActivationParametricSoftplus::release_alpha() {
  
  ::CoreML::Specification::WeightParams* temp = alpha_;
  alpha_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* ActivationParametricSoftplus::unsafe_arena_release_alpha() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParametricSoftplus.alpha)
  
  ::CoreML::Specification::WeightParams* temp = alpha_;
  alpha_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* ActivationParametricSoftplus::_internal_mutable_alpha() {
  
  if (alpha_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    alpha_ = p;
  }
  return alpha_;
}
inline ::CoreML::Specification::WeightParams* ActivationParametricSoftplus::mutable_alpha() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_alpha();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParametricSoftplus.alpha)
  return _msg;
}
inline void ActivationParametricSoftplus::set_allocated_alpha(::CoreML::Specification::WeightParams* alpha) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete alpha_;
  }
  if (alpha) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(alpha);
    if (message_arena != submessage_arena) {
      alpha = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, alpha, submessage_arena);
    }
    
  } else {
    
  }
  alpha_ = alpha;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParametricSoftplus.alpha)
}

// .CoreML.Specification.WeightParams beta = 2;
inline bool ActivationParametricSoftplus::_internal_has_beta() const {
  return this != internal_default_instance() && beta_ != nullptr;
}
inline bool ActivationParametricSoftplus::has_beta() const {
  return _internal_has_beta();
}
inline void ActivationParametricSoftplus::clear_beta() {
  if (GetArenaForAllocation() == nullptr && beta_ != nullptr) {
    delete beta_;
  }
  beta_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& ActivationParametricSoftplus::_internal_beta() const {
  const ::CoreML::Specification::WeightParams* p = beta_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& ActivationParametricSoftplus::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParametricSoftplus.beta)
  return _internal_beta();
}
inline void ActivationParametricSoftplus::unsafe_arena_set_allocated_beta(
    ::CoreML::Specification::WeightParams* beta) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(beta_);
  }
  beta_ = beta;
  if (beta) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ActivationParametricSoftplus.beta)
}
inline ::CoreML::Specification::WeightParams* ActivationParametricSoftplus::release_beta() {
  
  ::CoreML::Specification::WeightParams* temp = beta_;
  beta_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* ActivationParametricSoftplus::unsafe_arena_release_beta() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParametricSoftplus.beta)
  
  ::CoreML::Specification::WeightParams* temp = beta_;
  beta_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* ActivationParametricSoftplus::_internal_mutable_beta() {
  
  if (beta_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    beta_ = p;
  }
  return beta_;
}
inline ::CoreML::Specification::WeightParams* ActivationParametricSoftplus::mutable_beta() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_beta();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParametricSoftplus.beta)
  return _msg;
}
inline void ActivationParametricSoftplus::set_allocated_beta(::CoreML::Specification::WeightParams* beta) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete beta_;
  }
  if (beta) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(beta);
    if (message_arena != submessage_arena) {
      beta = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, beta, submessage_arena);
    }
    
  } else {
    
  }
  beta_ = beta;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParametricSoftplus.beta)
}

// -------------------------------------------------------------------

// ActivationParams

// .CoreML.Specification.ActivationLinear linear = 5;
inline bool ActivationParams::_internal_has_linear() const {
  return NonlinearityType_case() == kLinear;
}
inline bool ActivationParams::has_linear() const {
  return _internal_has_linear();
}
inline void ActivationParams::set_has_linear() {
  _oneof_case_[0] = kLinear;
}
inline void ActivationParams::clear_linear() {
  if (_internal_has_linear()) {
    if (GetArenaForAllocation() == nullptr) {
      delete NonlinearityType_.linear_;
    }
    clear_has_NonlinearityType();
  }
}
inline ::CoreML::Specification::ActivationLinear* ActivationParams::release_linear() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.linear)
  if (_internal_has_linear()) {
    clear_has_NonlinearityType();
      ::CoreML::Specification::ActivationLinear* temp = NonlinearityType_.linear_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    NonlinearityType_.linear_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ActivationLinear& ActivationParams::_internal_linear() const {
  return _internal_has_linear()
      ? *NonlinearityType_.linear_
      : reinterpret_cast< ::CoreML::Specification::ActivationLinear&>(::CoreML::Specification::_ActivationLinear_default_instance_);
}
inline const ::CoreML::Specification::ActivationLinear& ActivationParams::linear() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.linear)
  return _internal_linear();
}
inline ::CoreML::Specification::ActivationLinear* ActivationParams::unsafe_arena_release_linear() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.ActivationParams.linear)
  if (_internal_has_linear()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationLinear* temp = NonlinearityType_.linear_;
    NonlinearityType_.linear_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ActivationParams::unsafe_arena_set_allocated_linear(::CoreML::Specification::ActivationLinear* linear) {
  clear_NonlinearityType();
  if (linear) {
    set_has_linear();
    NonlinearityType_.linear_ = linear;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ActivationParams.linear)
}
inline ::CoreML::Specification::ActivationLinear* ActivationParams::_internal_mutable_linear() {
  if (!_internal_has_linear()) {
    clear_NonlinearityType();
    set_has_linear();
    NonlinearityType_.linear_ = CreateMaybeMessage< ::CoreML::Specification::ActivationLinear >(GetArenaForAllocation());
  }
  return NonlinearityType_.linear_;
}
inline ::CoreML::Specification::ActivationLinear* ActivationParams::mutable_linear() {
  ::CoreML::Specification::ActivationLinear* _msg = _internal_mutable_linear();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.linear)
  return _msg;
}

// .CoreML.Specification.ActivationReLU ReLU = 10;
inline bool ActivationParams::_internal_has_relu() const {
  return NonlinearityType_case() == kReLU;
}
inline bool ActivationParams::has_relu() const {
  return _internal_has_relu();
}
inline void ActivationParams::set_has_relu() {
  _oneof_case_[0] = kReLU;
}
inline void ActivationParams::clear_relu() {
  if (_internal_has_relu()) {
    if (GetArenaForAllocation() == nullptr) {
      delete NonlinearityType_.relu_;
    }
    clear_has_NonlinearityType();
  }
}
inline ::CoreML::Specification::ActivationReLU* ActivationParams::release_relu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.ReLU)
  if (_internal_has_relu()) {
    clear_has_NonlinearityType();
      ::CoreML::Specification::ActivationReLU* temp = NonlinearityType_.relu_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    NonlinearityType_.relu_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ActivationReLU& ActivationParams::_internal_relu() const {
  return _internal_has_relu()
      ? *NonlinearityType_.relu_
      : reinterpret_cast< ::CoreML::Specification::ActivationReLU&>(::CoreML::Specification::_ActivationReLU_default_instance_);
}
inline const ::CoreML::Specification::ActivationReLU& ActivationParams::relu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.ReLU)
  return _internal_relu();
}
inline ::CoreML::Specification::ActivationReLU* ActivationParams::unsafe_arena_release_relu() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.ActivationParams.ReLU)
  if (_internal_has_relu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationReLU* temp = NonlinearityType_.relu_;
    NonlinearityType_.relu_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ActivationParams::unsafe_arena_set_allocated_relu(::CoreML::Specification::ActivationReLU* relu) {
  clear_NonlinearityType();
  if (relu) {
    set_has_relu();
    NonlinearityType_.relu_ = relu;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ActivationParams.ReLU)
}
inline ::CoreML::Specification::ActivationReLU* ActivationParams::_internal_mutable_relu() {
  if (!_internal_has_relu()) {
    clear_NonlinearityType();
    set_has_relu();
    NonlinearityType_.relu_ = CreateMaybeMessage< ::CoreML::Specification::ActivationReLU >(GetArenaForAllocation());
  }
  return NonlinearityType_.relu_;
}
inline ::CoreML::Specification::ActivationReLU* ActivationParams::mutable_relu() {
  ::CoreML::Specification::ActivationReLU* _msg = _internal_mutable_relu();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.ReLU)
  return _msg;
}

// .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
inline bool ActivationParams::_internal_has_leakyrelu() const {
  return NonlinearityType_case() == kLeakyReLU;
}
inline bool ActivationParams::has_leakyrelu() const {
  return _internal_has_leakyrelu();
}
inline void ActivationParams::set_has_leakyrelu() {
  _oneof_case_[0] = kLeakyReLU;
}
inline void ActivationParams::clear_leakyrelu() {
  if (_internal_has_leakyrelu()) {
    if (GetArenaForAllocation() == nullptr) {
      delete NonlinearityType_.leakyrelu_;
    }
    clear_has_NonlinearityType();
  }
}
inline ::CoreML::Specification::ActivationLeakyReLU* ActivationParams::release_leakyrelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.leakyReLU)
  if (_internal_has_leakyrelu()) {
    clear_has_NonlinearityType();
      ::CoreML::Specification::ActivationLeakyReLU* temp = NonlinearityType_.leakyrelu_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    NonlinearityType_.leakyrelu_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ActivationLeakyReLU& ActivationParams::_internal_leakyrelu() const {
  return _internal_has_leakyrelu()
      ? *NonlinearityType_.leakyrelu_
      : reinterpret_cast< ::CoreML::Specification::ActivationLeakyReLU&>(::CoreML::Specification::_ActivationLeakyReLU_default_instance_);
}
inline const ::CoreML::Specification::ActivationLeakyReLU& ActivationParams::leakyrelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.leakyReLU)
  return _internal_leakyrelu();
}
inline ::CoreML::Specification::ActivationLeakyReLU* ActivationParams::unsafe_arena_release_leakyrelu() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.ActivationParams.leakyReLU)
  if (_internal_has_leakyrelu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationLeakyReLU* temp = NonlinearityType_.leakyrelu_;
    NonlinearityType_.leakyrelu_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ActivationParams::unsafe_arena_set_allocated_leakyrelu(::CoreML::Specification::ActivationLeakyReLU* leakyrelu) {
  clear_NonlinearityType();
  if (leakyrelu) {
    set_has_leakyrelu();
    NonlinearityType_.leakyrelu_ = leakyrelu;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ActivationParams.leakyReLU)
}
inline ::CoreML::Specification::ActivationLeakyReLU* ActivationParams::_internal_mutable_leakyrelu() {
  if (!_internal_has_leakyrelu()) {
    clear_NonlinearityType();
    set_has_leakyrelu();
    NonlinearityType_.leakyrelu_ = CreateMaybeMessage< ::CoreML::Specification::ActivationLeakyReLU >(GetArenaForAllocation());
  }
  return NonlinearityType_.leakyrelu_;
}
inline ::CoreML::Specification::ActivationLeakyReLU* ActivationParams::mutable_leakyrelu() {
  ::CoreML::Specification::ActivationLeakyReLU* _msg = _internal_mutable_leakyrelu();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.leakyReLU)
  return _msg;
}

// .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
inline bool ActivationParams::_internal_has_thresholdedrelu() const {
  return NonlinearityType_case() == kThresholdedReLU;
}
inline bool ActivationParams::has_thresholdedrelu() const {
  return _internal_has_thresholdedrelu();
}
inline void ActivationParams::set_has_thresholdedrelu() {
  _oneof_case_[0] = kThresholdedReLU;
}
inline void ActivationParams::clear_thresholdedrelu() {
  if (_internal_has_thresholdedrelu()) {
    if (GetArenaForAllocation() == nullptr) {
      delete NonlinearityType_.thresholdedrelu_;
    }
    clear_has_NonlinearityType();
  }
}
inline ::CoreML::Specification::ActivationThresholdedReLU* ActivationParams::release_thresholdedrelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.thresholdedReLU)
  if (_internal_has_thresholdedrelu()) {
    clear_has_NonlinearityType();
      ::CoreML::Specification::ActivationThresholdedReLU* temp = NonlinearityType_.thresholdedrelu_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    NonlinearityType_.thresholdedrelu_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ActivationThresholdedReLU& ActivationParams::_internal_thresholdedrelu() const {
  return _internal_has_thresholdedrelu()
      ? *NonlinearityType_.thresholdedrelu_
      : reinterpret_cast< ::CoreML::Specification::ActivationThresholdedReLU&>(::CoreML::Specification::_ActivationThresholdedReLU_default_instance_);
}
inline const ::CoreML::Specification::ActivationThresholdedReLU& ActivationParams::thresholdedrelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.thresholdedReLU)
  return _internal_thresholdedrelu();
}
inline ::CoreML::Specification::ActivationThresholdedReLU* ActivationParams::unsafe_arena_release_thresholdedrelu() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.ActivationParams.thresholdedReLU)
  if (_internal_has_thresholdedrelu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationThresholdedReLU* temp = NonlinearityType_.thresholdedrelu_;
    NonlinearityType_.thresholdedrelu_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ActivationParams::unsafe_arena_set_allocated_thresholdedrelu(::CoreML::Specification::ActivationThresholdedReLU* thresholdedrelu) {
  clear_NonlinearityType();
  if (thresholdedrelu) {
    set_has_thresholdedrelu();
    NonlinearityType_.thresholdedrelu_ = thresholdedrelu;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ActivationParams.thresholdedReLU)
}
inline ::CoreML::Specification::ActivationThresholdedReLU* ActivationParams::_internal_mutable_thresholdedrelu() {
  if (!_internal_has_thresholdedrelu()) {
    clear_NonlinearityType();
    set_has_thresholdedrelu();
    NonlinearityType_.thresholdedrelu_ = CreateMaybeMessage< ::CoreML::Specification::ActivationThresholdedReLU >(GetArenaForAllocation());
  }
  return NonlinearityType_.thresholdedrelu_;
}
inline ::CoreML::Specification::ActivationThresholdedReLU* ActivationParams::mutable_thresholdedrelu() {
  ::CoreML::Specification::ActivationThresholdedReLU* _msg = _internal_mutable_thresholdedrelu();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.thresholdedReLU)
  return _msg;
}

// .CoreML.Specification.ActivationPReLU PReLU = 25;
inline bool ActivationParams::_internal_has_prelu() const {
  return NonlinearityType_case() == kPReLU;
}
inline bool ActivationParams::has_prelu() const {
  return _internal_has_prelu();
}
inline void ActivationParams::set_has_prelu() {
  _oneof_case_[0] = kPReLU;
}
inline void ActivationParams::clear_prelu() {
  if (_internal_has_prelu()) {
    if (GetArenaForAllocation() == nullptr) {
      delete NonlinearityType_.prelu_;
    }
    clear_has_NonlinearityType();
  }
}
inline ::CoreML::Specification::ActivationPReLU* ActivationParams::release_prelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.PReLU)
  if (_internal_has_prelu()) {
    clear_has_NonlinearityType();
      ::CoreML::Specification::ActivationPReLU* temp = NonlinearityType_.prelu_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    NonlinearityType_.prelu_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ActivationPReLU& ActivationParams::_internal_prelu() const {
  return _internal_has_prelu()
      ? *NonlinearityType_.prelu_
      : reinterpret_cast< ::CoreML::Specification::ActivationPReLU&>(::CoreML::Specification::_ActivationPReLU_default_instance_);
}
inline const ::CoreML::Specification::ActivationPReLU& ActivationParams::prelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.PReLU)
  return _internal_prelu();
}
inline ::CoreML::Specification::ActivationPReLU* ActivationParams::unsafe_arena_release_prelu() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.ActivationParams.PReLU)
  if (_internal_has_prelu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationPReLU* temp = NonlinearityType_.prelu_;
    NonlinearityType_.prelu_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ActivationParams::unsafe_arena_set_allocated_prelu(::CoreML::Specification::ActivationPReLU* prelu) {
  clear_NonlinearityType();
  if (prelu) {
    set_has_prelu();
    NonlinearityType_.prelu_ = prelu;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ActivationParams.PReLU)
}
inline ::CoreML::Specification::ActivationPReLU* ActivationParams::_internal_mutable_prelu() {
  if (!_internal_has_prelu()) {
    clear_NonlinearityType();
    set_has_prelu();
    NonlinearityType_.prelu_ = CreateMaybeMessage< ::CoreML::Specification::ActivationPReLU >(GetArenaForAllocation());
  }
  return NonlinearityType_.prelu_;
}
inline ::CoreML::Specification::ActivationPReLU* ActivationParams::mutable_prelu() {
  ::CoreML::Specification::ActivationPReLU* _msg = _internal_mutable_prelu();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.PReLU)
  return _msg;
}

// .CoreML.Specification.ActivationTanh tanh = 30;
inline bool ActivationParams::_internal_has_tanh() const {
  return NonlinearityType_case() == kTanh;
}
inline bool ActivationParams::has_tanh() const {
  return _internal_has_tanh();
}
inline void ActivationParams::set_has_tanh() {
  _oneof_case_[0] = kTanh;
}
inline void ActivationParams::clear_tanh() {
  if (_internal_has_tanh()) {
    if (GetArenaForAllocation() == nullptr) {
      delete NonlinearityType_.tanh_;
    }
    clear_has_NonlinearityType();
  }
}
inline ::CoreML::Specification::ActivationTanh* ActivationParams::release_tanh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.tanh)
  if (_internal_has_tanh()) {
    clear_has_NonlinearityType();
      ::CoreML::Specification::ActivationTanh* temp = NonlinearityType_.tanh_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    NonlinearityType_.tanh_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ActivationTanh& ActivationParams::_internal_tanh() const {
  return _internal_has_tanh()
      ? *NonlinearityType_.tanh_
      : reinterpret_cast< ::CoreML::Specification::ActivationTanh&>(::CoreML::Specification::_ActivationTanh_default_instance_);
}
inline const ::CoreML::Specification::ActivationTanh& ActivationParams::tanh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.tanh)
  return _internal_tanh();
}
inline ::CoreML::Specification::ActivationTanh* ActivationParams::unsafe_arena_release_tanh() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.ActivationParams.tanh)
  if (_internal_has_tanh()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationTanh* temp = NonlinearityType_.tanh_;
    NonlinearityType_.tanh_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ActivationParams::unsafe_arena_set_allocated_tanh(::CoreML::Specification::ActivationTanh* tanh) {
  clear_NonlinearityType();
  if (tanh) {
    set_has_tanh();
    NonlinearityType_.tanh_ = tanh;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ActivationParams.tanh)
}
inline ::CoreML::Specification::ActivationTanh* ActivationParams::_internal_mutable_tanh() {
  if (!_internal_has_tanh()) {
    clear_NonlinearityType();
    set_has_tanh();
    NonlinearityType_.tanh_ = CreateMaybeMessage< ::CoreML::Specification::ActivationTanh >(GetArenaForAllocation());
  }
  return NonlinearityType_.tanh_;
}
inline ::CoreML::Specification::ActivationTanh* ActivationParams::mutable_tanh() {
  ::CoreML::Specification::ActivationTanh* _msg = _internal_mutable_tanh();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.tanh)
  return _msg;
}

// .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
inline bool ActivationParams::_internal_has_scaledtanh() const {
  return NonlinearityType_case() == kScaledTanh;
}
inline bool ActivationParams::has_scaledtanh() const {
  return _internal_has_scaledtanh();
}
inline void ActivationParams::set_has_scaledtanh() {
  _oneof_case_[0] = kScaledTanh;
}
inline void ActivationParams::clear_scaledtanh() {
  if (_internal_has_scaledtanh()) {
    if (GetArenaForAllocation() == nullptr) {
      delete NonlinearityType_.scaledtanh_;
    }
    clear_has_NonlinearityType();
  }
}
inline ::CoreML::Specification::ActivationScaledTanh* ActivationParams::release_scaledtanh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.scaledTanh)
  if (_internal_has_scaledtanh()) {
    clear_has_NonlinearityType();
      ::CoreML::Specification::ActivationScaledTanh* temp = NonlinearityType_.scaledtanh_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    NonlinearityType_.scaledtanh_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ActivationScaledTanh& ActivationParams::_internal_scaledtanh() const {
  return _internal_has_scaledtanh()
      ? *NonlinearityType_.scaledtanh_
      : reinterpret_cast< ::CoreML::Specification::ActivationScaledTanh&>(::CoreML::Specification::_ActivationScaledTanh_default_instance_);
}
inline const ::CoreML::Specification::ActivationScaledTanh& ActivationParams::scaledtanh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.scaledTanh)
  return _internal_scaledtanh();
}
inline ::CoreML::Specification::ActivationScaledTanh* ActivationParams::unsafe_arena_release_scaledtanh() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.ActivationParams.scaledTanh)
  if (_internal_has_scaledtanh()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationScaledTanh* temp = NonlinearityType_.scaledtanh_;
    NonlinearityType_.scaledtanh_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ActivationParams::unsafe_arena_set_allocated_scaledtanh(::CoreML::Specification::ActivationScaledTanh* scaledtanh) {
  clear_NonlinearityType();
  if (scaledtanh) {
    set_has_scaledtanh();
    NonlinearityType_.scaledtanh_ = scaledtanh;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ActivationParams.scaledTanh)
}
inline ::CoreML::Specification::ActivationScaledTanh* ActivationParams::_internal_mutable_scaledtanh() {
  if (!_internal_has_scaledtanh()) {
    clear_NonlinearityType();
    set_has_scaledtanh();
    NonlinearityType_.scaledtanh_ = CreateMaybeMessage< ::CoreML::Specification::ActivationScaledTanh >(GetArenaForAllocation());
  }
  return NonlinearityType_.scaledtanh_;
}
inline ::CoreML::Specification::ActivationScaledTanh* ActivationParams::mutable_scaledtanh() {
  ::CoreML::Specification::ActivationScaledTanh* _msg = _internal_mutable_scaledtanh();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.scaledTanh)
  return _msg;
}

// .CoreML.Specification.ActivationSigmoid sigmoid = 40;
inline bool ActivationParams::_internal_has_sigmoid() const {
  return NonlinearityType_case() == kSigmoid;
}
inline bool ActivationParams::has_sigmoid() const {
  return _internal_has_sigmoid();
}
inline void ActivationParams::set_has_sigmoid() {
  _oneof_case_[0] = kSigmoid;
}
inline void ActivationParams::clear_sigmoid() {
  if (_internal_has_sigmoid()) {
    if (GetArenaForAllocation() == nullptr) {
      delete NonlinearityType_.sigmoid_;
    }
    clear_has_NonlinearityType();
  }
}
inline ::CoreML::Specification::ActivationSigmoid* ActivationParams::release_sigmoid() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.sigmoid)
  if (_internal_has_sigmoid()) {
    clear_has_NonlinearityType();
      ::CoreML::Specification::ActivationSigmoid* temp = NonlinearityType_.sigmoid_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    NonlinearityType_.sigmoid_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ActivationSigmoid& ActivationParams::_internal_sigmoid() const {
  return _internal_has_sigmoid()
      ? *NonlinearityType_.sigmoid_
      : reinterpret_cast< ::CoreML::Specification::ActivationSigmoid&>(::CoreML::Specification::_ActivationSigmoid_default_instance_);
}
inline const ::CoreML::Specification::ActivationSigmoid& ActivationParams::sigmoid() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.sigmoid)
  return _internal_sigmoid();
}
inline ::CoreML::Specification::ActivationSigmoid* ActivationParams::unsafe_arena_release_sigmoid() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.ActivationParams.sigmoid)
  if (_internal_has_sigmoid()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSigmoid* temp = NonlinearityType_.sigmoid_;
    NonlinearityType_.sigmoid_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ActivationParams::unsafe_arena_set_allocated_sigmoid(::CoreML::Specification::ActivationSigmoid* sigmoid) {
  clear_NonlinearityType();
  if (sigmoid) {
    set_has_sigmoid();
    NonlinearityType_.sigmoid_ = sigmoid;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ActivationParams.sigmoid)
}
inline ::CoreML::Specification::ActivationSigmoid* ActivationParams::_internal_mutable_sigmoid() {
  if (!_internal_has_sigmoid()) {
    clear_NonlinearityType();
    set_has_sigmoid();
    NonlinearityType_.sigmoid_ = CreateMaybeMessage< ::CoreML::Specification::ActivationSigmoid >(GetArenaForAllocation());
  }
  return NonlinearityType_.sigmoid_;
}
inline ::CoreML::Specification::ActivationSigmoid* ActivationParams::mutable_sigmoid() {
  ::CoreML::Specification::ActivationSigmoid* _msg = _internal_mutable_sigmoid();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.sigmoid)
  return _msg;
}

// .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
inline bool ActivationParams::_internal_has_sigmoidhard() const {
  return NonlinearityType_case() == kSigmoidHard;
}
inline bool ActivationParams::has_sigmoidhard() const {
  return _internal_has_sigmoidhard();
}
inline void ActivationParams::set_has_sigmoidhard() {
  _oneof_case_[0] = kSigmoidHard;
}
inline void ActivationParams::clear_sigmoidhard() {
  if (_internal_has_sigmoidhard()) {
    if (GetArenaForAllocation() == nullptr) {
      delete NonlinearityType_.sigmoidhard_;
    }
    clear_has_NonlinearityType();
  }
}
inline ::CoreML::Specification::ActivationSigmoidHard* ActivationParams::release_sigmoidhard() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.sigmoidHard)
  if (_internal_has_sigmoidhard()) {
    clear_has_NonlinearityType();
      ::CoreML::Specification::ActivationSigmoidHard* temp = NonlinearityType_.sigmoidhard_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    NonlinearityType_.sigmoidhard_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ActivationSigmoidHard& ActivationParams::_internal_sigmoidhard() const {
  return _internal_has_sigmoidhard()
      ? *NonlinearityType_.sigmoidhard_
      : reinterpret_cast< ::CoreML::Specification::ActivationSigmoidHard&>(::CoreML::Specification::_ActivationSigmoidHard_default_instance_);
}
inline const ::CoreML::Specification::ActivationSigmoidHard& ActivationParams::sigmoidhard() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.sigmoidHard)
  return _internal_sigmoidhard();
}
inline ::CoreML::Specification::ActivationSigmoidHard* ActivationParams::unsafe_arena_release_sigmoidhard() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.ActivationParams.sigmoidHard)
  if (_internal_has_sigmoidhard()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSigmoidHard* temp = NonlinearityType_.sigmoidhard_;
    NonlinearityType_.sigmoidhard_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ActivationParams::unsafe_arena_set_allocated_sigmoidhard(::CoreML::Specification::ActivationSigmoidHard* sigmoidhard) {
  clear_NonlinearityType();
  if (sigmoidhard) {
    set_has_sigmoidhard();
    NonlinearityType_.sigmoidhard_ = sigmoidhard;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ActivationParams.sigmoidHard)
}
inline ::CoreML::Specification::ActivationSigmoidHard* ActivationParams::_internal_mutable_sigmoidhard() {
  if (!_internal_has_sigmoidhard()) {
    clear_NonlinearityType();
    set_has_sigmoidhard();
    NonlinearityType_.sigmoidhard_ = CreateMaybeMessage< ::CoreML::Specification::ActivationSigmoidHard >(GetArenaForAllocation());
  }
  return NonlinearityType_.sigmoidhard_;
}
inline ::CoreML::Specification::ActivationSigmoidHard* ActivationParams::mutable_sigmoidhard() {
  ::CoreML::Specification::ActivationSigmoidHard* _msg = _internal_mutable_sigmoidhard();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.sigmoidHard)
  return _msg;
}

// .CoreML.Specification.ActivationELU ELU = 50;
inline bool ActivationParams::_internal_has_elu() const {
  return NonlinearityType_case() == kELU;
}
inline bool ActivationParams::has_elu() const {
  return _internal_has_elu();
}
inline void ActivationParams::set_has_elu() {
  _oneof_case_[0] = kELU;
}
inline void ActivationParams::clear_elu() {
  if (_internal_has_elu()) {
    if (GetArenaForAllocation() == nullptr) {
      delete NonlinearityType_.elu_;
    }
    clear_has_NonlinearityType();
  }
}
inline ::CoreML::Specification::ActivationELU* ActivationParams::release_elu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.ELU)
  if (_internal_has_elu()) {
    clear_has_NonlinearityType();
      ::CoreML::Specification::ActivationELU* temp = NonlinearityType_.elu_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    NonlinearityType_.elu_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ActivationELU& ActivationParams::_internal_elu() const {
  return _internal_has_elu()
      ? *NonlinearityType_.elu_
      : reinterpret_cast< ::CoreML::Specification::ActivationELU&>(::CoreML::Specification::_ActivationELU_default_instance_);
}
inline const ::CoreML::Specification::ActivationELU& ActivationParams::elu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.ELU)
  return _internal_elu();
}
inline ::CoreML::Specification::ActivationELU* ActivationParams::unsafe_arena_release_elu() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.ActivationParams.ELU)
  if (_internal_has_elu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationELU* temp = NonlinearityType_.elu_;
    NonlinearityType_.elu_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ActivationParams::unsafe_arena_set_allocated_elu(::CoreML::Specification::ActivationELU* elu) {
  clear_NonlinearityType();
  if (elu) {
    set_has_elu();
    NonlinearityType_.elu_ = elu;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ActivationParams.ELU)
}
inline ::CoreML::Specification::ActivationELU* ActivationParams::_internal_mutable_elu() {
  if (!_internal_has_elu()) {
    clear_NonlinearityType();
    set_has_elu();
    NonlinearityType_.elu_ = CreateMaybeMessage< ::CoreML::Specification::ActivationELU >(GetArenaForAllocation());
  }
  return NonlinearityType_.elu_;
}
inline ::CoreML::Specification::ActivationELU* ActivationParams::mutable_elu() {
  ::CoreML::Specification::ActivationELU* _msg = _internal_mutable_elu();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.ELU)
  return _msg;
}

// .CoreML.Specification.ActivationSoftsign softsign = 60;
inline bool ActivationParams::_internal_has_softsign() const {
  return NonlinearityType_case() == kSoftsign;
}
inline bool ActivationParams::has_softsign() const {
  return _internal_has_softsign();
}
inline void ActivationParams::set_has_softsign() {
  _oneof_case_[0] = kSoftsign;
}
inline void ActivationParams::clear_softsign() {
  if (_internal_has_softsign()) {
    if (GetArenaForAllocation() == nullptr) {
      delete NonlinearityType_.softsign_;
    }
    clear_has_NonlinearityType();
  }
}
inline ::CoreML::Specification::ActivationSoftsign* ActivationParams::release_softsign() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.softsign)
  if (_internal_has_softsign()) {
    clear_has_NonlinearityType();
      ::CoreML::Specification::ActivationSoftsign* temp = NonlinearityType_.softsign_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    NonlinearityType_.softsign_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ActivationSoftsign& ActivationParams::_internal_softsign() const {
  return _internal_has_softsign()
      ? *NonlinearityType_.softsign_
      : reinterpret_cast< ::CoreML::Specification::ActivationSoftsign&>(::CoreML::Specification::_ActivationSoftsign_default_instance_);
}
inline const ::CoreML::Specification::ActivationSoftsign& ActivationParams::softsign() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.softsign)
  return _internal_softsign();
}
inline ::CoreML::Specification::ActivationSoftsign* ActivationParams::unsafe_arena_release_softsign() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.ActivationParams.softsign)
  if (_internal_has_softsign()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSoftsign* temp = NonlinearityType_.softsign_;
    NonlinearityType_.softsign_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ActivationParams::unsafe_arena_set_allocated_softsign(::CoreML::Specification::ActivationSoftsign* softsign) {
  clear_NonlinearityType();
  if (softsign) {
    set_has_softsign();
    NonlinearityType_.softsign_ = softsign;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ActivationParams.softsign)
}
inline ::CoreML::Specification::ActivationSoftsign* ActivationParams::_internal_mutable_softsign() {
  if (!_internal_has_softsign()) {
    clear_NonlinearityType();
    set_has_softsign();
    NonlinearityType_.softsign_ = CreateMaybeMessage< ::CoreML::Specification::ActivationSoftsign >(GetArenaForAllocation());
  }
  return NonlinearityType_.softsign_;
}
inline ::CoreML::Specification::ActivationSoftsign* ActivationParams::mutable_softsign() {
  ::CoreML::Specification::ActivationSoftsign* _msg = _internal_mutable_softsign();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.softsign)
  return _msg;
}

// .CoreML.Specification.ActivationSoftplus softplus = 70;
inline bool ActivationParams::_internal_has_softplus() const {
  return NonlinearityType_case() == kSoftplus;
}
inline bool ActivationParams::has_softplus() const {
  return _internal_has_softplus();
}
inline void ActivationParams::set_has_softplus() {
  _oneof_case_[0] = kSoftplus;
}
inline void ActivationParams::clear_softplus() {
  if (_internal_has_softplus()) {
    if (GetArenaForAllocation() == nullptr) {
      delete NonlinearityType_.softplus_;
    }
    clear_has_NonlinearityType();
  }
}
inline ::CoreML::Specification::ActivationSoftplus* ActivationParams::release_softplus() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.softplus)
  if (_internal_has_softplus()) {
    clear_has_NonlinearityType();
      ::CoreML::Specification::ActivationSoftplus* temp = NonlinearityType_.softplus_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    NonlinearityType_.softplus_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ActivationSoftplus& ActivationParams::_internal_softplus() const {
  return _internal_has_softplus()
      ? *NonlinearityType_.softplus_
      : reinterpret_cast< ::CoreML::Specification::ActivationSoftplus&>(::CoreML::Specification::_ActivationSoftplus_default_instance_);
}
inline const ::CoreML::Specification::ActivationSoftplus& ActivationParams::softplus() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.softplus)
  return _internal_softplus();
}
inline ::CoreML::Specification::ActivationSoftplus* ActivationParams::unsafe_arena_release_softplus() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.ActivationParams.softplus)
  if (_internal_has_softplus()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSoftplus* temp = NonlinearityType_.softplus_;
    NonlinearityType_.softplus_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ActivationParams::unsafe_arena_set_allocated_softplus(::CoreML::Specification::ActivationSoftplus* softplus) {
  clear_NonlinearityType();
  if (softplus) {
    set_has_softplus();
    NonlinearityType_.softplus_ = softplus;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ActivationParams.softplus)
}
inline ::CoreML::Specification::ActivationSoftplus* ActivationParams::_internal_mutable_softplus() {
  if (!_internal_has_softplus()) {
    clear_NonlinearityType();
    set_has_softplus();
    NonlinearityType_.softplus_ = CreateMaybeMessage< ::CoreML::Specification::ActivationSoftplus >(GetArenaForAllocation());
  }
  return NonlinearityType_.softplus_;
}
inline ::CoreML::Specification::ActivationSoftplus* ActivationParams::mutable_softplus() {
  ::CoreML::Specification::ActivationSoftplus* _msg = _internal_mutable_softplus();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.softplus)
  return _msg;
}

// .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
inline bool ActivationParams::_internal_has_parametricsoftplus() const {
  return NonlinearityType_case() == kParametricSoftplus;
}
inline bool ActivationParams::has_parametricsoftplus() const {
  return _internal_has_parametricsoftplus();
}
inline void ActivationParams::set_has_parametricsoftplus() {
  _oneof_case_[0] = kParametricSoftplus;
}
inline void ActivationParams::clear_parametricsoftplus() {
  if (_internal_has_parametricsoftplus()) {
    if (GetArenaForAllocation() == nullptr) {
      delete NonlinearityType_.parametricsoftplus_;
    }
    clear_has_NonlinearityType();
  }
}
inline ::CoreML::Specification::ActivationParametricSoftplus* ActivationParams::release_parametricsoftplus() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.parametricSoftplus)
  if (_internal_has_parametricsoftplus()) {
    clear_has_NonlinearityType();
      ::CoreML::Specification::ActivationParametricSoftplus* temp = NonlinearityType_.parametricsoftplus_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    NonlinearityType_.parametricsoftplus_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ActivationParametricSoftplus& ActivationParams::_internal_parametricsoftplus() const {
  return _internal_has_parametricsoftplus()
      ? *NonlinearityType_.parametricsoftplus_
      : reinterpret_cast< ::CoreML::Specification::ActivationParametricSoftplus&>(::CoreML::Specification::_ActivationParametricSoftplus_default_instance_);
}
inline const ::CoreML::Specification::ActivationParametricSoftplus& ActivationParams::parametricsoftplus() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.parametricSoftplus)
  return _internal_parametricsoftplus();
}
inline ::CoreML::Specification::ActivationParametricSoftplus* ActivationParams::unsafe_arena_release_parametricsoftplus() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.ActivationParams.parametricSoftplus)
  if (_internal_has_parametricsoftplus()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationParametricSoftplus* temp = NonlinearityType_.parametricsoftplus_;
    NonlinearityType_.parametricsoftplus_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ActivationParams::unsafe_arena_set_allocated_parametricsoftplus(::CoreML::Specification::ActivationParametricSoftplus* parametricsoftplus) {
  clear_NonlinearityType();
  if (parametricsoftplus) {
    set_has_parametricsoftplus();
    NonlinearityType_.parametricsoftplus_ = parametricsoftplus;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ActivationParams.parametricSoftplus)
}
inline ::CoreML::Specification::ActivationParametricSoftplus* ActivationParams::_internal_mutable_parametricsoftplus() {
  if (!_internal_has_parametricsoftplus()) {
    clear_NonlinearityType();
    set_has_parametricsoftplus();
    NonlinearityType_.parametricsoftplus_ = CreateMaybeMessage< ::CoreML::Specification::ActivationParametricSoftplus >(GetArenaForAllocation());
  }
  return NonlinearityType_.parametricsoftplus_;
}
inline ::CoreML::Specification::ActivationParametricSoftplus* ActivationParams::mutable_parametricsoftplus() {
  ::CoreML::Specification::ActivationParametricSoftplus* _msg = _internal_mutable_parametricsoftplus();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.parametricSoftplus)
  return _msg;
}

inline bool ActivationParams::has_NonlinearityType() const {
  return NonlinearityType_case() != NONLINEARITYTYPE_NOT_SET;
}
inline void ActivationParams::clear_has_NonlinearityType() {
  _oneof_case_[0] = NONLINEARITYTYPE_NOT_SET;
}
inline ActivationParams::NonlinearityTypeCase ActivationParams::NonlinearityType_case() const {
  return ActivationParams::NonlinearityTypeCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// Tensor

// uint32 rank = 1;
inline void Tensor::clear_rank() {
  rank_ = 0u;
}
inline uint32_t Tensor::_internal_rank() const {
  return rank_;
}
inline uint32_t Tensor::rank() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Tensor.rank)
  return _internal_rank();
}
inline void Tensor::_internal_set_rank(uint32_t value) {
  
  rank_ = value;
}
inline void Tensor::set_rank(uint32_t value) {
  _internal_set_rank(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Tensor.rank)
}

// repeated int64 dimValue = 2;
inline int Tensor::_internal_dimvalue_size() const {
  return dimvalue_.size();
}
inline int Tensor::dimvalue_size() const {
  return _internal_dimvalue_size();
}
inline void Tensor::clear_dimvalue() {
  dimvalue_.Clear();
}
inline int64_t Tensor::_internal_dimvalue(int index) const {
  return dimvalue_.Get(index);
}
inline int64_t Tensor::dimvalue(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Tensor.dimValue)
  return _internal_dimvalue(index);
}
inline void Tensor::set_dimvalue(int index, int64_t value) {
  dimvalue_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Tensor.dimValue)
}
inline void Tensor::_internal_add_dimvalue(int64_t value) {
  dimvalue_.Add(value);
}
inline void Tensor::add_dimvalue(int64_t value) {
  _internal_add_dimvalue(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.Tensor.dimValue)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
Tensor::_internal_dimvalue() const {
  return dimvalue_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
Tensor::dimvalue() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.Tensor.dimValue)
  return _internal_dimvalue();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
Tensor::_internal_mutable_dimvalue() {
  return &dimvalue_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
Tensor::mutable_dimvalue() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.Tensor.dimValue)
  return _internal_mutable_dimvalue();
}

// -------------------------------------------------------------------

// NeuralNetworkLayer

// string name = 1;
inline void NeuralNetworkLayer::clear_name() {
  name_.ClearToEmpty();
}
inline const std::string& NeuralNetworkLayer::name() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.name)
  return _internal_name();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void NeuralNetworkLayer::set_name(ArgT0&& arg0, ArgT... args) {
 
 name_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.name)
}
inline std::string* NeuralNetworkLayer::mutable_name() {
  std::string* _s = _internal_mutable_name();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.name)
  return _s;
}
inline const std::string& NeuralNetworkLayer::_internal_name() const {
  return name_.Get();
}
inline void NeuralNetworkLayer::_internal_set_name(const std::string& value) {
  
  name_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* NeuralNetworkLayer::_internal_mutable_name() {
  
  return name_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* NeuralNetworkLayer::release_name() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.name)
  return name_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void NeuralNetworkLayer::set_allocated_name(std::string* name) {
  if (name != nullptr) {
    
  } else {
    
  }
  name_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), name,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (name_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    name_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.name)
}

// repeated string input = 2;
inline int NeuralNetworkLayer::_internal_input_size() const {
  return input_.size();
}
inline int NeuralNetworkLayer::input_size() const {
  return _internal_input_size();
}
inline void NeuralNetworkLayer::clear_input() {
  input_.Clear();
}
inline std::string* NeuralNetworkLayer::add_input() {
  std::string* _s = _internal_add_input();
  // @@protoc_insertion_point(field_add_mutable:CoreML.Specification.NeuralNetworkLayer.input)
  return _s;
}
inline const std::string& NeuralNetworkLayer::_internal_input(int index) const {
  return input_.Get(index);
}
inline const std::string& NeuralNetworkLayer::input(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.input)
  return _internal_input(index);
}
inline std::string* NeuralNetworkLayer::mutable_input(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.input)
  return input_.Mutable(index);
}
inline void NeuralNetworkLayer::set_input(int index, const std::string& value) {
  input_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.input)
}
inline void NeuralNetworkLayer::set_input(int index, std::string&& value) {
  input_.Mutable(index)->assign(std::move(value));
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.input)
}
inline void NeuralNetworkLayer::set_input(int index, const char* value) {
  GOOGLE_DCHECK(value != nullptr);
  input_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkLayer.input)
}
inline void NeuralNetworkLayer::set_input(int index, const char* value, size_t size) {
  input_.Mutable(index)->assign(
    reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkLayer.input)
}
inline std::string* NeuralNetworkLayer::_internal_add_input() {
  return input_.Add();
}
inline void NeuralNetworkLayer::add_input(const std::string& value) {
  input_.Add()->assign(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.input)
}
inline void NeuralNetworkLayer::add_input(std::string&& value) {
  input_.Add(std::move(value));
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.input)
}
inline void NeuralNetworkLayer::add_input(const char* value) {
  GOOGLE_DCHECK(value != nullptr);
  input_.Add()->assign(value);
  // @@protoc_insertion_point(field_add_char:CoreML.Specification.NeuralNetworkLayer.input)
}
inline void NeuralNetworkLayer::add_input(const char* value, size_t size) {
  input_.Add()->assign(reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_add_pointer:CoreML.Specification.NeuralNetworkLayer.input)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField<std::string>&
NeuralNetworkLayer::input() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.input)
  return input_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField<std::string>*
NeuralNetworkLayer::mutable_input() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.input)
  return &input_;
}

// repeated string output = 3;
inline int NeuralNetworkLayer::_internal_output_size() const {
  return output_.size();
}
inline int NeuralNetworkLayer::output_size() const {
  return _internal_output_size();
}
inline void NeuralNetworkLayer::clear_output() {
  output_.Clear();
}
inline std::string* NeuralNetworkLayer::add_output() {
  std::string* _s = _internal_add_output();
  // @@protoc_insertion_point(field_add_mutable:CoreML.Specification.NeuralNetworkLayer.output)
  return _s;
}
inline const std::string& NeuralNetworkLayer::_internal_output(int index) const {
  return output_.Get(index);
}
inline const std::string& NeuralNetworkLayer::output(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.output)
  return _internal_output(index);
}
inline std::string* NeuralNetworkLayer::mutable_output(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.output)
  return output_.Mutable(index);
}
inline void NeuralNetworkLayer::set_output(int index, const std::string& value) {
  output_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.output)
}
inline void NeuralNetworkLayer::set_output(int index, std::string&& value) {
  output_.Mutable(index)->assign(std::move(value));
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.output)
}
inline void NeuralNetworkLayer::set_output(int index, const char* value) {
  GOOGLE_DCHECK(value != nullptr);
  output_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkLayer.output)
}
inline void NeuralNetworkLayer::set_output(int index, const char* value, size_t size) {
  output_.Mutable(index)->assign(
    reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkLayer.output)
}
inline std::string* NeuralNetworkLayer::_internal_add_output() {
  return output_.Add();
}
inline void NeuralNetworkLayer::add_output(const std::string& value) {
  output_.Add()->assign(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.output)
}
inline void NeuralNetworkLayer::add_output(std::string&& value) {
  output_.Add(std::move(value));
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.output)
}
inline void NeuralNetworkLayer::add_output(const char* value) {
  GOOGLE_DCHECK(value != nullptr);
  output_.Add()->assign(value);
  // @@protoc_insertion_point(field_add_char:CoreML.Specification.NeuralNetworkLayer.output)
}
inline void NeuralNetworkLayer::add_output(const char* value, size_t size) {
  output_.Add()->assign(reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_add_pointer:CoreML.Specification.NeuralNetworkLayer.output)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField<std::string>&
NeuralNetworkLayer::output() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.output)
  return output_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField<std::string>*
NeuralNetworkLayer::mutable_output() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.output)
  return &output_;
}

// repeated .CoreML.Specification.Tensor inputTensor = 4;
inline int NeuralNetworkLayer::_internal_inputtensor_size() const {
  return inputtensor_.size();
}
inline int NeuralNetworkLayer::inputtensor_size() const {
  return _internal_inputtensor_size();
}
inline void NeuralNetworkLayer::clear_inputtensor() {
  inputtensor_.Clear();
}
inline ::CoreML::Specification::Tensor* NeuralNetworkLayer::mutable_inputtensor(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.inputTensor)
  return inputtensor_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::Tensor >*
NeuralNetworkLayer::mutable_inputtensor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.inputTensor)
  return &inputtensor_;
}
inline const ::CoreML::Specification::Tensor& NeuralNetworkLayer::_internal_inputtensor(int index) const {
  return inputtensor_.Get(index);
}
inline const ::CoreML::Specification::Tensor& NeuralNetworkLayer::inputtensor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.inputTensor)
  return _internal_inputtensor(index);
}
inline ::CoreML::Specification::Tensor* NeuralNetworkLayer::_internal_add_inputtensor() {
  return inputtensor_.Add();
}
inline ::CoreML::Specification::Tensor* NeuralNetworkLayer::add_inputtensor() {
  ::CoreML::Specification::Tensor* _add = _internal_add_inputtensor();
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.inputTensor)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::Tensor >&
NeuralNetworkLayer::inputtensor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.inputTensor)
  return inputtensor_;
}

// repeated .CoreML.Specification.Tensor outputTensor = 5;
inline int NeuralNetworkLayer::_internal_outputtensor_size() const {
  return outputtensor_.size();
}
inline int NeuralNetworkLayer::outputtensor_size() const {
  return _internal_outputtensor_size();
}
inline void NeuralNetworkLayer::clear_outputtensor() {
  outputtensor_.Clear();
}
inline ::CoreML::Specification::Tensor* NeuralNetworkLayer::mutable_outputtensor(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.outputTensor)
  return outputtensor_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::Tensor >*
NeuralNetworkLayer::mutable_outputtensor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.outputTensor)
  return &outputtensor_;
}
inline const ::CoreML::Specification::Tensor& NeuralNetworkLayer::_internal_outputtensor(int index) const {
  return outputtensor_.Get(index);
}
inline const ::CoreML::Specification::Tensor& NeuralNetworkLayer::outputtensor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.outputTensor)
  return _internal_outputtensor(index);
}
inline ::CoreML::Specification::Tensor* NeuralNetworkLayer::_internal_add_outputtensor() {
  return outputtensor_.Add();
}
inline ::CoreML::Specification::Tensor* NeuralNetworkLayer::add_outputtensor() {
  ::CoreML::Specification::Tensor* _add = _internal_add_outputtensor();
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.outputTensor)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::Tensor >&
NeuralNetworkLayer::outputtensor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.outputTensor)
  return outputtensor_;
}

// bool isUpdatable = 10;
inline void NeuralNetworkLayer::clear_isupdatable() {
  isupdatable_ = false;
}
inline bool NeuralNetworkLayer::_internal_isupdatable() const {
  return isupdatable_;
}
inline bool NeuralNetworkLayer::isupdatable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.isUpdatable)
  return _internal_isupdatable();
}
inline void NeuralNetworkLayer::_internal_set_isupdatable(bool value) {
  
  isupdatable_ = value;
}
inline void NeuralNetworkLayer::set_isupdatable(bool value) {
  _internal_set_isupdatable(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.isUpdatable)
}

// .CoreML.Specification.ConvolutionLayerParams convolution = 100;
inline bool NeuralNetworkLayer::_internal_has_convolution() const {
  return layer_case() == kConvolution;
}
inline bool NeuralNetworkLayer::has_convolution() const {
  return _internal_has_convolution();
}
inline void NeuralNetworkLayer::set_has_convolution() {
  _oneof_case_[0] = kConvolution;
}
inline void NeuralNetworkLayer::clear_convolution() {
  if (_internal_has_convolution()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.convolution_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ConvolutionLayerParams* NeuralNetworkLayer::release_convolution() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.convolution)
  if (_internal_has_convolution()) {
    clear_has_layer();
      ::CoreML::Specification::ConvolutionLayerParams* temp = layer_.convolution_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.convolution_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ConvolutionLayerParams& NeuralNetworkLayer::_internal_convolution() const {
  return _internal_has_convolution()
      ? *layer_.convolution_
      : reinterpret_cast< ::CoreML::Specification::ConvolutionLayerParams&>(::CoreML::Specification::_ConvolutionLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ConvolutionLayerParams& NeuralNetworkLayer::convolution() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.convolution)
  return _internal_convolution();
}
inline ::CoreML::Specification::ConvolutionLayerParams* NeuralNetworkLayer::unsafe_arena_release_convolution() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.convolution)
  if (_internal_has_convolution()) {
    clear_has_layer();
    ::CoreML::Specification::ConvolutionLayerParams* temp = layer_.convolution_;
    layer_.convolution_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_convolution(::CoreML::Specification::ConvolutionLayerParams* convolution) {
  clear_layer();
  if (convolution) {
    set_has_convolution();
    layer_.convolution_ = convolution;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.convolution)
}
inline ::CoreML::Specification::ConvolutionLayerParams* NeuralNetworkLayer::_internal_mutable_convolution() {
  if (!_internal_has_convolution()) {
    clear_layer();
    set_has_convolution();
    layer_.convolution_ = CreateMaybeMessage< ::CoreML::Specification::ConvolutionLayerParams >(GetArenaForAllocation());
  }
  return layer_.convolution_;
}
inline ::CoreML::Specification::ConvolutionLayerParams* NeuralNetworkLayer::mutable_convolution() {
  ::CoreML::Specification::ConvolutionLayerParams* _msg = _internal_mutable_convolution();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.convolution)
  return _msg;
}

// .CoreML.Specification.PoolingLayerParams pooling = 120;
inline bool NeuralNetworkLayer::_internal_has_pooling() const {
  return layer_case() == kPooling;
}
inline bool NeuralNetworkLayer::has_pooling() const {
  return _internal_has_pooling();
}
inline void NeuralNetworkLayer::set_has_pooling() {
  _oneof_case_[0] = kPooling;
}
inline void NeuralNetworkLayer::clear_pooling() {
  if (_internal_has_pooling()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.pooling_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::PoolingLayerParams* NeuralNetworkLayer::release_pooling() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.pooling)
  if (_internal_has_pooling()) {
    clear_has_layer();
      ::CoreML::Specification::PoolingLayerParams* temp = layer_.pooling_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.pooling_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::PoolingLayerParams& NeuralNetworkLayer::_internal_pooling() const {
  return _internal_has_pooling()
      ? *layer_.pooling_
      : reinterpret_cast< ::CoreML::Specification::PoolingLayerParams&>(::CoreML::Specification::_PoolingLayerParams_default_instance_);
}
inline const ::CoreML::Specification::PoolingLayerParams& NeuralNetworkLayer::pooling() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.pooling)
  return _internal_pooling();
}
inline ::CoreML::Specification::PoolingLayerParams* NeuralNetworkLayer::unsafe_arena_release_pooling() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.pooling)
  if (_internal_has_pooling()) {
    clear_has_layer();
    ::CoreML::Specification::PoolingLayerParams* temp = layer_.pooling_;
    layer_.pooling_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_pooling(::CoreML::Specification::PoolingLayerParams* pooling) {
  clear_layer();
  if (pooling) {
    set_has_pooling();
    layer_.pooling_ = pooling;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.pooling)
}
inline ::CoreML::Specification::PoolingLayerParams* NeuralNetworkLayer::_internal_mutable_pooling() {
  if (!_internal_has_pooling()) {
    clear_layer();
    set_has_pooling();
    layer_.pooling_ = CreateMaybeMessage< ::CoreML::Specification::PoolingLayerParams >(GetArenaForAllocation());
  }
  return layer_.pooling_;
}
inline ::CoreML::Specification::PoolingLayerParams* NeuralNetworkLayer::mutable_pooling() {
  ::CoreML::Specification::PoolingLayerParams* _msg = _internal_mutable_pooling();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.pooling)
  return _msg;
}

// .CoreML.Specification.ActivationParams activation = 130;
inline bool NeuralNetworkLayer::_internal_has_activation() const {
  return layer_case() == kActivation;
}
inline bool NeuralNetworkLayer::has_activation() const {
  return _internal_has_activation();
}
inline void NeuralNetworkLayer::set_has_activation() {
  _oneof_case_[0] = kActivation;
}
inline void NeuralNetworkLayer::clear_activation() {
  if (_internal_has_activation()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.activation_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ActivationParams* NeuralNetworkLayer::release_activation() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.activation)
  if (_internal_has_activation()) {
    clear_has_layer();
      ::CoreML::Specification::ActivationParams* temp = layer_.activation_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.activation_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ActivationParams& NeuralNetworkLayer::_internal_activation() const {
  return _internal_has_activation()
      ? *layer_.activation_
      : reinterpret_cast< ::CoreML::Specification::ActivationParams&>(::CoreML::Specification::_ActivationParams_default_instance_);
}
inline const ::CoreML::Specification::ActivationParams& NeuralNetworkLayer::activation() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.activation)
  return _internal_activation();
}
inline ::CoreML::Specification::ActivationParams* NeuralNetworkLayer::unsafe_arena_release_activation() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.activation)
  if (_internal_has_activation()) {
    clear_has_layer();
    ::CoreML::Specification::ActivationParams* temp = layer_.activation_;
    layer_.activation_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_activation(::CoreML::Specification::ActivationParams* activation) {
  clear_layer();
  if (activation) {
    set_has_activation();
    layer_.activation_ = activation;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.activation)
}
inline ::CoreML::Specification::ActivationParams* NeuralNetworkLayer::_internal_mutable_activation() {
  if (!_internal_has_activation()) {
    clear_layer();
    set_has_activation();
    layer_.activation_ = CreateMaybeMessage< ::CoreML::Specification::ActivationParams >(GetArenaForAllocation());
  }
  return layer_.activation_;
}
inline ::CoreML::Specification::ActivationParams* NeuralNetworkLayer::mutable_activation() {
  ::CoreML::Specification::ActivationParams* _msg = _internal_mutable_activation();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.activation)
  return _msg;
}

// .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
inline bool NeuralNetworkLayer::_internal_has_innerproduct() const {
  return layer_case() == kInnerProduct;
}
inline bool NeuralNetworkLayer::has_innerproduct() const {
  return _internal_has_innerproduct();
}
inline void NeuralNetworkLayer::set_has_innerproduct() {
  _oneof_case_[0] = kInnerProduct;
}
inline void NeuralNetworkLayer::clear_innerproduct() {
  if (_internal_has_innerproduct()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.innerproduct_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::InnerProductLayerParams* NeuralNetworkLayer::release_innerproduct() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  if (_internal_has_innerproduct()) {
    clear_has_layer();
      ::CoreML::Specification::InnerProductLayerParams* temp = layer_.innerproduct_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.innerproduct_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::InnerProductLayerParams& NeuralNetworkLayer::_internal_innerproduct() const {
  return _internal_has_innerproduct()
      ? *layer_.innerproduct_
      : reinterpret_cast< ::CoreML::Specification::InnerProductLayerParams&>(::CoreML::Specification::_InnerProductLayerParams_default_instance_);
}
inline const ::CoreML::Specification::InnerProductLayerParams& NeuralNetworkLayer::innerproduct() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  return _internal_innerproduct();
}
inline ::CoreML::Specification::InnerProductLayerParams* NeuralNetworkLayer::unsafe_arena_release_innerproduct() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  if (_internal_has_innerproduct()) {
    clear_has_layer();
    ::CoreML::Specification::InnerProductLayerParams* temp = layer_.innerproduct_;
    layer_.innerproduct_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_innerproduct(::CoreML::Specification::InnerProductLayerParams* innerproduct) {
  clear_layer();
  if (innerproduct) {
    set_has_innerproduct();
    layer_.innerproduct_ = innerproduct;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.innerProduct)
}
inline ::CoreML::Specification::InnerProductLayerParams* NeuralNetworkLayer::_internal_mutable_innerproduct() {
  if (!_internal_has_innerproduct()) {
    clear_layer();
    set_has_innerproduct();
    layer_.innerproduct_ = CreateMaybeMessage< ::CoreML::Specification::InnerProductLayerParams >(GetArenaForAllocation());
  }
  return layer_.innerproduct_;
}
inline ::CoreML::Specification::InnerProductLayerParams* NeuralNetworkLayer::mutable_innerproduct() {
  ::CoreML::Specification::InnerProductLayerParams* _msg = _internal_mutable_innerproduct();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  return _msg;
}

// .CoreML.Specification.EmbeddingLayerParams embedding = 150;
inline bool NeuralNetworkLayer::_internal_has_embedding() const {
  return layer_case() == kEmbedding;
}
inline bool NeuralNetworkLayer::has_embedding() const {
  return _internal_has_embedding();
}
inline void NeuralNetworkLayer::set_has_embedding() {
  _oneof_case_[0] = kEmbedding;
}
inline void NeuralNetworkLayer::clear_embedding() {
  if (_internal_has_embedding()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.embedding_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::EmbeddingLayerParams* NeuralNetworkLayer::release_embedding() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.embedding)
  if (_internal_has_embedding()) {
    clear_has_layer();
      ::CoreML::Specification::EmbeddingLayerParams* temp = layer_.embedding_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.embedding_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::EmbeddingLayerParams& NeuralNetworkLayer::_internal_embedding() const {
  return _internal_has_embedding()
      ? *layer_.embedding_
      : reinterpret_cast< ::CoreML::Specification::EmbeddingLayerParams&>(::CoreML::Specification::_EmbeddingLayerParams_default_instance_);
}
inline const ::CoreML::Specification::EmbeddingLayerParams& NeuralNetworkLayer::embedding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.embedding)
  return _internal_embedding();
}
inline ::CoreML::Specification::EmbeddingLayerParams* NeuralNetworkLayer::unsafe_arena_release_embedding() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.embedding)
  if (_internal_has_embedding()) {
    clear_has_layer();
    ::CoreML::Specification::EmbeddingLayerParams* temp = layer_.embedding_;
    layer_.embedding_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_embedding(::CoreML::Specification::EmbeddingLayerParams* embedding) {
  clear_layer();
  if (embedding) {
    set_has_embedding();
    layer_.embedding_ = embedding;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.embedding)
}
inline ::CoreML::Specification::EmbeddingLayerParams* NeuralNetworkLayer::_internal_mutable_embedding() {
  if (!_internal_has_embedding()) {
    clear_layer();
    set_has_embedding();
    layer_.embedding_ = CreateMaybeMessage< ::CoreML::Specification::EmbeddingLayerParams >(GetArenaForAllocation());
  }
  return layer_.embedding_;
}
inline ::CoreML::Specification::EmbeddingLayerParams* NeuralNetworkLayer::mutable_embedding() {
  ::CoreML::Specification::EmbeddingLayerParams* _msg = _internal_mutable_embedding();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.embedding)
  return _msg;
}

// .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
inline bool NeuralNetworkLayer::_internal_has_batchnorm() const {
  return layer_case() == kBatchnorm;
}
inline bool NeuralNetworkLayer::has_batchnorm() const {
  return _internal_has_batchnorm();
}
inline void NeuralNetworkLayer::set_has_batchnorm() {
  _oneof_case_[0] = kBatchnorm;
}
inline void NeuralNetworkLayer::clear_batchnorm() {
  if (_internal_has_batchnorm()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.batchnorm_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::BatchnormLayerParams* NeuralNetworkLayer::release_batchnorm() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  if (_internal_has_batchnorm()) {
    clear_has_layer();
      ::CoreML::Specification::BatchnormLayerParams* temp = layer_.batchnorm_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.batchnorm_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::BatchnormLayerParams& NeuralNetworkLayer::_internal_batchnorm() const {
  return _internal_has_batchnorm()
      ? *layer_.batchnorm_
      : reinterpret_cast< ::CoreML::Specification::BatchnormLayerParams&>(::CoreML::Specification::_BatchnormLayerParams_default_instance_);
}
inline const ::CoreML::Specification::BatchnormLayerParams& NeuralNetworkLayer::batchnorm() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  return _internal_batchnorm();
}
inline ::CoreML::Specification::BatchnormLayerParams* NeuralNetworkLayer::unsafe_arena_release_batchnorm() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  if (_internal_has_batchnorm()) {
    clear_has_layer();
    ::CoreML::Specification::BatchnormLayerParams* temp = layer_.batchnorm_;
    layer_.batchnorm_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_batchnorm(::CoreML::Specification::BatchnormLayerParams* batchnorm) {
  clear_layer();
  if (batchnorm) {
    set_has_batchnorm();
    layer_.batchnorm_ = batchnorm;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.batchnorm)
}
inline ::CoreML::Specification::BatchnormLayerParams* NeuralNetworkLayer::_internal_mutable_batchnorm() {
  if (!_internal_has_batchnorm()) {
    clear_layer();
    set_has_batchnorm();
    layer_.batchnorm_ = CreateMaybeMessage< ::CoreML::Specification::BatchnormLayerParams >(GetArenaForAllocation());
  }
  return layer_.batchnorm_;
}
inline ::CoreML::Specification::BatchnormLayerParams* NeuralNetworkLayer::mutable_batchnorm() {
  ::CoreML::Specification::BatchnormLayerParams* _msg = _internal_mutable_batchnorm();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  return _msg;
}

// .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
inline bool NeuralNetworkLayer::_internal_has_mvn() const {
  return layer_case() == kMvn;
}
inline bool NeuralNetworkLayer::has_mvn() const {
  return _internal_has_mvn();
}
inline void NeuralNetworkLayer::set_has_mvn() {
  _oneof_case_[0] = kMvn;
}
inline void NeuralNetworkLayer::clear_mvn() {
  if (_internal_has_mvn()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.mvn_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::MeanVarianceNormalizeLayerParams* NeuralNetworkLayer::release_mvn() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.mvn)
  if (_internal_has_mvn()) {
    clear_has_layer();
      ::CoreML::Specification::MeanVarianceNormalizeLayerParams* temp = layer_.mvn_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.mvn_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::MeanVarianceNormalizeLayerParams& NeuralNetworkLayer::_internal_mvn() const {
  return _internal_has_mvn()
      ? *layer_.mvn_
      : reinterpret_cast< ::CoreML::Specification::MeanVarianceNormalizeLayerParams&>(::CoreML::Specification::_MeanVarianceNormalizeLayerParams_default_instance_);
}
inline const ::CoreML::Specification::MeanVarianceNormalizeLayerParams& NeuralNetworkLayer::mvn() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.mvn)
  return _internal_mvn();
}
inline ::CoreML::Specification::MeanVarianceNormalizeLayerParams* NeuralNetworkLayer::unsafe_arena_release_mvn() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.mvn)
  if (_internal_has_mvn()) {
    clear_has_layer();
    ::CoreML::Specification::MeanVarianceNormalizeLayerParams* temp = layer_.mvn_;
    layer_.mvn_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_mvn(::CoreML::Specification::MeanVarianceNormalizeLayerParams* mvn) {
  clear_layer();
  if (mvn) {
    set_has_mvn();
    layer_.mvn_ = mvn;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.mvn)
}
inline ::CoreML::Specification::MeanVarianceNormalizeLayerParams* NeuralNetworkLayer::_internal_mutable_mvn() {
  if (!_internal_has_mvn()) {
    clear_layer();
    set_has_mvn();
    layer_.mvn_ = CreateMaybeMessage< ::CoreML::Specification::MeanVarianceNormalizeLayerParams >(GetArenaForAllocation());
  }
  return layer_.mvn_;
}
inline ::CoreML::Specification::MeanVarianceNormalizeLayerParams* NeuralNetworkLayer::mutable_mvn() {
  ::CoreML::Specification::MeanVarianceNormalizeLayerParams* _msg = _internal_mutable_mvn();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.mvn)
  return _msg;
}

// .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
inline bool NeuralNetworkLayer::_internal_has_l2normalize() const {
  return layer_case() == kL2Normalize;
}
inline bool NeuralNetworkLayer::has_l2normalize() const {
  return _internal_has_l2normalize();
}
inline void NeuralNetworkLayer::set_has_l2normalize() {
  _oneof_case_[0] = kL2Normalize;
}
inline void NeuralNetworkLayer::clear_l2normalize() {
  if (_internal_has_l2normalize()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.l2normalize_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::L2NormalizeLayerParams* NeuralNetworkLayer::release_l2normalize() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  if (_internal_has_l2normalize()) {
    clear_has_layer();
      ::CoreML::Specification::L2NormalizeLayerParams* temp = layer_.l2normalize_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.l2normalize_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::L2NormalizeLayerParams& NeuralNetworkLayer::_internal_l2normalize() const {
  return _internal_has_l2normalize()
      ? *layer_.l2normalize_
      : reinterpret_cast< ::CoreML::Specification::L2NormalizeLayerParams&>(::CoreML::Specification::_L2NormalizeLayerParams_default_instance_);
}
inline const ::CoreML::Specification::L2NormalizeLayerParams& NeuralNetworkLayer::l2normalize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  return _internal_l2normalize();
}
inline ::CoreML::Specification::L2NormalizeLayerParams* NeuralNetworkLayer::unsafe_arena_release_l2normalize() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  if (_internal_has_l2normalize()) {
    clear_has_layer();
    ::CoreML::Specification::L2NormalizeLayerParams* temp = layer_.l2normalize_;
    layer_.l2normalize_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_l2normalize(::CoreML::Specification::L2NormalizeLayerParams* l2normalize) {
  clear_layer();
  if (l2normalize) {
    set_has_l2normalize();
    layer_.l2normalize_ = l2normalize;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.l2normalize)
}
inline ::CoreML::Specification::L2NormalizeLayerParams* NeuralNetworkLayer::_internal_mutable_l2normalize() {
  if (!_internal_has_l2normalize()) {
    clear_layer();
    set_has_l2normalize();
    layer_.l2normalize_ = CreateMaybeMessage< ::CoreML::Specification::L2NormalizeLayerParams >(GetArenaForAllocation());
  }
  return layer_.l2normalize_;
}
inline ::CoreML::Specification::L2NormalizeLayerParams* NeuralNetworkLayer::mutable_l2normalize() {
  ::CoreML::Specification::L2NormalizeLayerParams* _msg = _internal_mutable_l2normalize();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  return _msg;
}

// .CoreML.Specification.SoftmaxLayerParams softmax = 175;
inline bool NeuralNetworkLayer::_internal_has_softmax() const {
  return layer_case() == kSoftmax;
}
inline bool NeuralNetworkLayer::has_softmax() const {
  return _internal_has_softmax();
}
inline void NeuralNetworkLayer::set_has_softmax() {
  _oneof_case_[0] = kSoftmax;
}
inline void NeuralNetworkLayer::clear_softmax() {
  if (_internal_has_softmax()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.softmax_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::SoftmaxLayerParams* NeuralNetworkLayer::release_softmax() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.softmax)
  if (_internal_has_softmax()) {
    clear_has_layer();
      ::CoreML::Specification::SoftmaxLayerParams* temp = layer_.softmax_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.softmax_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::SoftmaxLayerParams& NeuralNetworkLayer::_internal_softmax() const {
  return _internal_has_softmax()
      ? *layer_.softmax_
      : reinterpret_cast< ::CoreML::Specification::SoftmaxLayerParams&>(::CoreML::Specification::_SoftmaxLayerParams_default_instance_);
}
inline const ::CoreML::Specification::SoftmaxLayerParams& NeuralNetworkLayer::softmax() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.softmax)
  return _internal_softmax();
}
inline ::CoreML::Specification::SoftmaxLayerParams* NeuralNetworkLayer::unsafe_arena_release_softmax() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.softmax)
  if (_internal_has_softmax()) {
    clear_has_layer();
    ::CoreML::Specification::SoftmaxLayerParams* temp = layer_.softmax_;
    layer_.softmax_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_softmax(::CoreML::Specification::SoftmaxLayerParams* softmax) {
  clear_layer();
  if (softmax) {
    set_has_softmax();
    layer_.softmax_ = softmax;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.softmax)
}
inline ::CoreML::Specification::SoftmaxLayerParams* NeuralNetworkLayer::_internal_mutable_softmax() {
  if (!_internal_has_softmax()) {
    clear_layer();
    set_has_softmax();
    layer_.softmax_ = CreateMaybeMessage< ::CoreML::Specification::SoftmaxLayerParams >(GetArenaForAllocation());
  }
  return layer_.softmax_;
}
inline ::CoreML::Specification::SoftmaxLayerParams* NeuralNetworkLayer::mutable_softmax() {
  ::CoreML::Specification::SoftmaxLayerParams* _msg = _internal_mutable_softmax();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.softmax)
  return _msg;
}

// .CoreML.Specification.LRNLayerParams lrn = 180;
inline bool NeuralNetworkLayer::_internal_has_lrn() const {
  return layer_case() == kLrn;
}
inline bool NeuralNetworkLayer::has_lrn() const {
  return _internal_has_lrn();
}
inline void NeuralNetworkLayer::set_has_lrn() {
  _oneof_case_[0] = kLrn;
}
inline void NeuralNetworkLayer::clear_lrn() {
  if (_internal_has_lrn()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.lrn_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::LRNLayerParams* NeuralNetworkLayer::release_lrn() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.lrn)
  if (_internal_has_lrn()) {
    clear_has_layer();
      ::CoreML::Specification::LRNLayerParams* temp = layer_.lrn_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.lrn_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::LRNLayerParams& NeuralNetworkLayer::_internal_lrn() const {
  return _internal_has_lrn()
      ? *layer_.lrn_
      : reinterpret_cast< ::CoreML::Specification::LRNLayerParams&>(::CoreML::Specification::_LRNLayerParams_default_instance_);
}
inline const ::CoreML::Specification::LRNLayerParams& NeuralNetworkLayer::lrn() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.lrn)
  return _internal_lrn();
}
inline ::CoreML::Specification::LRNLayerParams* NeuralNetworkLayer::unsafe_arena_release_lrn() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.lrn)
  if (_internal_has_lrn()) {
    clear_has_layer();
    ::CoreML::Specification::LRNLayerParams* temp = layer_.lrn_;
    layer_.lrn_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_lrn(::CoreML::Specification::LRNLayerParams* lrn) {
  clear_layer();
  if (lrn) {
    set_has_lrn();
    layer_.lrn_ = lrn;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.lrn)
}
inline ::CoreML::Specification::LRNLayerParams* NeuralNetworkLayer::_internal_mutable_lrn() {
  if (!_internal_has_lrn()) {
    clear_layer();
    set_has_lrn();
    layer_.lrn_ = CreateMaybeMessage< ::CoreML::Specification::LRNLayerParams >(GetArenaForAllocation());
  }
  return layer_.lrn_;
}
inline ::CoreML::Specification::LRNLayerParams* NeuralNetworkLayer::mutable_lrn() {
  ::CoreML::Specification::LRNLayerParams* _msg = _internal_mutable_lrn();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.lrn)
  return _msg;
}

// .CoreML.Specification.CropLayerParams crop = 190;
inline bool NeuralNetworkLayer::_internal_has_crop() const {
  return layer_case() == kCrop;
}
inline bool NeuralNetworkLayer::has_crop() const {
  return _internal_has_crop();
}
inline void NeuralNetworkLayer::set_has_crop() {
  _oneof_case_[0] = kCrop;
}
inline void NeuralNetworkLayer::clear_crop() {
  if (_internal_has_crop()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.crop_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::CropLayerParams* NeuralNetworkLayer::release_crop() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.crop)
  if (_internal_has_crop()) {
    clear_has_layer();
      ::CoreML::Specification::CropLayerParams* temp = layer_.crop_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.crop_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::CropLayerParams& NeuralNetworkLayer::_internal_crop() const {
  return _internal_has_crop()
      ? *layer_.crop_
      : reinterpret_cast< ::CoreML::Specification::CropLayerParams&>(::CoreML::Specification::_CropLayerParams_default_instance_);
}
inline const ::CoreML::Specification::CropLayerParams& NeuralNetworkLayer::crop() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.crop)
  return _internal_crop();
}
inline ::CoreML::Specification::CropLayerParams* NeuralNetworkLayer::unsafe_arena_release_crop() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.crop)
  if (_internal_has_crop()) {
    clear_has_layer();
    ::CoreML::Specification::CropLayerParams* temp = layer_.crop_;
    layer_.crop_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_crop(::CoreML::Specification::CropLayerParams* crop) {
  clear_layer();
  if (crop) {
    set_has_crop();
    layer_.crop_ = crop;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.crop)
}
inline ::CoreML::Specification::CropLayerParams* NeuralNetworkLayer::_internal_mutable_crop() {
  if (!_internal_has_crop()) {
    clear_layer();
    set_has_crop();
    layer_.crop_ = CreateMaybeMessage< ::CoreML::Specification::CropLayerParams >(GetArenaForAllocation());
  }
  return layer_.crop_;
}
inline ::CoreML::Specification::CropLayerParams* NeuralNetworkLayer::mutable_crop() {
  ::CoreML::Specification::CropLayerParams* _msg = _internal_mutable_crop();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.crop)
  return _msg;
}

// .CoreML.Specification.PaddingLayerParams padding = 200;
inline bool NeuralNetworkLayer::_internal_has_padding() const {
  return layer_case() == kPadding;
}
inline bool NeuralNetworkLayer::has_padding() const {
  return _internal_has_padding();
}
inline void NeuralNetworkLayer::set_has_padding() {
  _oneof_case_[0] = kPadding;
}
inline void NeuralNetworkLayer::clear_padding() {
  if (_internal_has_padding()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.padding_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::PaddingLayerParams* NeuralNetworkLayer::release_padding() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.padding)
  if (_internal_has_padding()) {
    clear_has_layer();
      ::CoreML::Specification::PaddingLayerParams* temp = layer_.padding_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.padding_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::PaddingLayerParams& NeuralNetworkLayer::_internal_padding() const {
  return _internal_has_padding()
      ? *layer_.padding_
      : reinterpret_cast< ::CoreML::Specification::PaddingLayerParams&>(::CoreML::Specification::_PaddingLayerParams_default_instance_);
}
inline const ::CoreML::Specification::PaddingLayerParams& NeuralNetworkLayer::padding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.padding)
  return _internal_padding();
}
inline ::CoreML::Specification::PaddingLayerParams* NeuralNetworkLayer::unsafe_arena_release_padding() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.padding)
  if (_internal_has_padding()) {
    clear_has_layer();
    ::CoreML::Specification::PaddingLayerParams* temp = layer_.padding_;
    layer_.padding_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_padding(::CoreML::Specification::PaddingLayerParams* padding) {
  clear_layer();
  if (padding) {
    set_has_padding();
    layer_.padding_ = padding;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.padding)
}
inline ::CoreML::Specification::PaddingLayerParams* NeuralNetworkLayer::_internal_mutable_padding() {
  if (!_internal_has_padding()) {
    clear_layer();
    set_has_padding();
    layer_.padding_ = CreateMaybeMessage< ::CoreML::Specification::PaddingLayerParams >(GetArenaForAllocation());
  }
  return layer_.padding_;
}
inline ::CoreML::Specification::PaddingLayerParams* NeuralNetworkLayer::mutable_padding() {
  ::CoreML::Specification::PaddingLayerParams* _msg = _internal_mutable_padding();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.padding)
  return _msg;
}

// .CoreML.Specification.UpsampleLayerParams upsample = 210;
inline bool NeuralNetworkLayer::_internal_has_upsample() const {
  return layer_case() == kUpsample;
}
inline bool NeuralNetworkLayer::has_upsample() const {
  return _internal_has_upsample();
}
inline void NeuralNetworkLayer::set_has_upsample() {
  _oneof_case_[0] = kUpsample;
}
inline void NeuralNetworkLayer::clear_upsample() {
  if (_internal_has_upsample()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.upsample_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::UpsampleLayerParams* NeuralNetworkLayer::release_upsample() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.upsample)
  if (_internal_has_upsample()) {
    clear_has_layer();
      ::CoreML::Specification::UpsampleLayerParams* temp = layer_.upsample_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.upsample_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::UpsampleLayerParams& NeuralNetworkLayer::_internal_upsample() const {
  return _internal_has_upsample()
      ? *layer_.upsample_
      : reinterpret_cast< ::CoreML::Specification::UpsampleLayerParams&>(::CoreML::Specification::_UpsampleLayerParams_default_instance_);
}
inline const ::CoreML::Specification::UpsampleLayerParams& NeuralNetworkLayer::upsample() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.upsample)
  return _internal_upsample();
}
inline ::CoreML::Specification::UpsampleLayerParams* NeuralNetworkLayer::unsafe_arena_release_upsample() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.upsample)
  if (_internal_has_upsample()) {
    clear_has_layer();
    ::CoreML::Specification::UpsampleLayerParams* temp = layer_.upsample_;
    layer_.upsample_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_upsample(::CoreML::Specification::UpsampleLayerParams* upsample) {
  clear_layer();
  if (upsample) {
    set_has_upsample();
    layer_.upsample_ = upsample;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.upsample)
}
inline ::CoreML::Specification::UpsampleLayerParams* NeuralNetworkLayer::_internal_mutable_upsample() {
  if (!_internal_has_upsample()) {
    clear_layer();
    set_has_upsample();
    layer_.upsample_ = CreateMaybeMessage< ::CoreML::Specification::UpsampleLayerParams >(GetArenaForAllocation());
  }
  return layer_.upsample_;
}
inline ::CoreML::Specification::UpsampleLayerParams* NeuralNetworkLayer::mutable_upsample() {
  ::CoreML::Specification::UpsampleLayerParams* _msg = _internal_mutable_upsample();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.upsample)
  return _msg;
}

// .CoreML.Specification.ResizeBilinearLayerParams resizeBilinear = 211;
inline bool NeuralNetworkLayer::_internal_has_resizebilinear() const {
  return layer_case() == kResizeBilinear;
}
inline bool NeuralNetworkLayer::has_resizebilinear() const {
  return _internal_has_resizebilinear();
}
inline void NeuralNetworkLayer::set_has_resizebilinear() {
  _oneof_case_[0] = kResizeBilinear;
}
inline void NeuralNetworkLayer::clear_resizebilinear() {
  if (_internal_has_resizebilinear()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.resizebilinear_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ResizeBilinearLayerParams* NeuralNetworkLayer::release_resizebilinear() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.resizeBilinear)
  if (_internal_has_resizebilinear()) {
    clear_has_layer();
      ::CoreML::Specification::ResizeBilinearLayerParams* temp = layer_.resizebilinear_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.resizebilinear_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ResizeBilinearLayerParams& NeuralNetworkLayer::_internal_resizebilinear() const {
  return _internal_has_resizebilinear()
      ? *layer_.resizebilinear_
      : reinterpret_cast< ::CoreML::Specification::ResizeBilinearLayerParams&>(::CoreML::Specification::_ResizeBilinearLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ResizeBilinearLayerParams& NeuralNetworkLayer::resizebilinear() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.resizeBilinear)
  return _internal_resizebilinear();
}
inline ::CoreML::Specification::ResizeBilinearLayerParams* NeuralNetworkLayer::unsafe_arena_release_resizebilinear() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.resizeBilinear)
  if (_internal_has_resizebilinear()) {
    clear_has_layer();
    ::CoreML::Specification::ResizeBilinearLayerParams* temp = layer_.resizebilinear_;
    layer_.resizebilinear_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_resizebilinear(::CoreML::Specification::ResizeBilinearLayerParams* resizebilinear) {
  clear_layer();
  if (resizebilinear) {
    set_has_resizebilinear();
    layer_.resizebilinear_ = resizebilinear;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.resizeBilinear)
}
inline ::CoreML::Specification::ResizeBilinearLayerParams* NeuralNetworkLayer::_internal_mutable_resizebilinear() {
  if (!_internal_has_resizebilinear()) {
    clear_layer();
    set_has_resizebilinear();
    layer_.resizebilinear_ = CreateMaybeMessage< ::CoreML::Specification::ResizeBilinearLayerParams >(GetArenaForAllocation());
  }
  return layer_.resizebilinear_;
}
inline ::CoreML::Specification::ResizeBilinearLayerParams* NeuralNetworkLayer::mutable_resizebilinear() {
  ::CoreML::Specification::ResizeBilinearLayerParams* _msg = _internal_mutable_resizebilinear();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.resizeBilinear)
  return _msg;
}

// .CoreML.Specification.CropResizeLayerParams cropResize = 212;
inline bool NeuralNetworkLayer::_internal_has_cropresize() const {
  return layer_case() == kCropResize;
}
inline bool NeuralNetworkLayer::has_cropresize() const {
  return _internal_has_cropresize();
}
inline void NeuralNetworkLayer::set_has_cropresize() {
  _oneof_case_[0] = kCropResize;
}
inline void NeuralNetworkLayer::clear_cropresize() {
  if (_internal_has_cropresize()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.cropresize_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::CropResizeLayerParams* NeuralNetworkLayer::release_cropresize() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.cropResize)
  if (_internal_has_cropresize()) {
    clear_has_layer();
      ::CoreML::Specification::CropResizeLayerParams* temp = layer_.cropresize_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.cropresize_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::CropResizeLayerParams& NeuralNetworkLayer::_internal_cropresize() const {
  return _internal_has_cropresize()
      ? *layer_.cropresize_
      : reinterpret_cast< ::CoreML::Specification::CropResizeLayerParams&>(::CoreML::Specification::_CropResizeLayerParams_default_instance_);
}
inline const ::CoreML::Specification::CropResizeLayerParams& NeuralNetworkLayer::cropresize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.cropResize)
  return _internal_cropresize();
}
inline ::CoreML::Specification::CropResizeLayerParams* NeuralNetworkLayer::unsafe_arena_release_cropresize() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.cropResize)
  if (_internal_has_cropresize()) {
    clear_has_layer();
    ::CoreML::Specification::CropResizeLayerParams* temp = layer_.cropresize_;
    layer_.cropresize_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_cropresize(::CoreML::Specification::CropResizeLayerParams* cropresize) {
  clear_layer();
  if (cropresize) {
    set_has_cropresize();
    layer_.cropresize_ = cropresize;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.cropResize)
}
inline ::CoreML::Specification::CropResizeLayerParams* NeuralNetworkLayer::_internal_mutable_cropresize() {
  if (!_internal_has_cropresize()) {
    clear_layer();
    set_has_cropresize();
    layer_.cropresize_ = CreateMaybeMessage< ::CoreML::Specification::CropResizeLayerParams >(GetArenaForAllocation());
  }
  return layer_.cropresize_;
}
inline ::CoreML::Specification::CropResizeLayerParams* NeuralNetworkLayer::mutable_cropresize() {
  ::CoreML::Specification::CropResizeLayerParams* _msg = _internal_mutable_cropresize();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.cropResize)
  return _msg;
}

// .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
inline bool NeuralNetworkLayer::_internal_has_unary() const {
  return layer_case() == kUnary;
}
inline bool NeuralNetworkLayer::has_unary() const {
  return _internal_has_unary();
}
inline void NeuralNetworkLayer::set_has_unary() {
  _oneof_case_[0] = kUnary;
}
inline void NeuralNetworkLayer::clear_unary() {
  if (_internal_has_unary()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.unary_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::UnaryFunctionLayerParams* NeuralNetworkLayer::release_unary() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.unary)
  if (_internal_has_unary()) {
    clear_has_layer();
      ::CoreML::Specification::UnaryFunctionLayerParams* temp = layer_.unary_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.unary_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::UnaryFunctionLayerParams& NeuralNetworkLayer::_internal_unary() const {
  return _internal_has_unary()
      ? *layer_.unary_
      : reinterpret_cast< ::CoreML::Specification::UnaryFunctionLayerParams&>(::CoreML::Specification::_UnaryFunctionLayerParams_default_instance_);
}
inline const ::CoreML::Specification::UnaryFunctionLayerParams& NeuralNetworkLayer::unary() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.unary)
  return _internal_unary();
}
inline ::CoreML::Specification::UnaryFunctionLayerParams* NeuralNetworkLayer::unsafe_arena_release_unary() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.unary)
  if (_internal_has_unary()) {
    clear_has_layer();
    ::CoreML::Specification::UnaryFunctionLayerParams* temp = layer_.unary_;
    layer_.unary_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_unary(::CoreML::Specification::UnaryFunctionLayerParams* unary) {
  clear_layer();
  if (unary) {
    set_has_unary();
    layer_.unary_ = unary;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.unary)
}
inline ::CoreML::Specification::UnaryFunctionLayerParams* NeuralNetworkLayer::_internal_mutable_unary() {
  if (!_internal_has_unary()) {
    clear_layer();
    set_has_unary();
    layer_.unary_ = CreateMaybeMessage< ::CoreML::Specification::UnaryFunctionLayerParams >(GetArenaForAllocation());
  }
  return layer_.unary_;
}
inline ::CoreML::Specification::UnaryFunctionLayerParams* NeuralNetworkLayer::mutable_unary() {
  ::CoreML::Specification::UnaryFunctionLayerParams* _msg = _internal_mutable_unary();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.unary)
  return _msg;
}

// .CoreML.Specification.AddLayerParams add = 230;
inline bool NeuralNetworkLayer::_internal_has_add() const {
  return layer_case() == kAdd;
}
inline bool NeuralNetworkLayer::has_add() const {
  return _internal_has_add();
}
inline void NeuralNetworkLayer::set_has_add() {
  _oneof_case_[0] = kAdd;
}
inline void NeuralNetworkLayer::clear_add() {
  if (_internal_has_add()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.add_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::AddLayerParams* NeuralNetworkLayer::release_add() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.add)
  if (_internal_has_add()) {
    clear_has_layer();
      ::CoreML::Specification::AddLayerParams* temp = layer_.add_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.add_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::AddLayerParams& NeuralNetworkLayer::_internal_add() const {
  return _internal_has_add()
      ? *layer_.add_
      : reinterpret_cast< ::CoreML::Specification::AddLayerParams&>(::CoreML::Specification::_AddLayerParams_default_instance_);
}
inline const ::CoreML::Specification::AddLayerParams& NeuralNetworkLayer::add() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.add)
  return _internal_add();
}
inline ::CoreML::Specification::AddLayerParams* NeuralNetworkLayer::unsafe_arena_release_add() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.add)
  if (_internal_has_add()) {
    clear_has_layer();
    ::CoreML::Specification::AddLayerParams* temp = layer_.add_;
    layer_.add_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_add(::CoreML::Specification::AddLayerParams* add) {
  clear_layer();
  if (add) {
    set_has_add();
    layer_.add_ = add;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.add)
}
inline ::CoreML::Specification::AddLayerParams* NeuralNetworkLayer::_internal_mutable_add() {
  if (!_internal_has_add()) {
    clear_layer();
    set_has_add();
    layer_.add_ = CreateMaybeMessage< ::CoreML::Specification::AddLayerParams >(GetArenaForAllocation());
  }
  return layer_.add_;
}
inline ::CoreML::Specification::AddLayerParams* NeuralNetworkLayer::mutable_add() {
  ::CoreML::Specification::AddLayerParams* _msg = _internal_mutable_add();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.add)
  return _msg;
}

// .CoreML.Specification.MultiplyLayerParams multiply = 231;
inline bool NeuralNetworkLayer::_internal_has_multiply() const {
  return layer_case() == kMultiply;
}
inline bool NeuralNetworkLayer::has_multiply() const {
  return _internal_has_multiply();
}
inline void NeuralNetworkLayer::set_has_multiply() {
  _oneof_case_[0] = kMultiply;
}
inline void NeuralNetworkLayer::clear_multiply() {
  if (_internal_has_multiply()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.multiply_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::MultiplyLayerParams* NeuralNetworkLayer::release_multiply() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.multiply)
  if (_internal_has_multiply()) {
    clear_has_layer();
      ::CoreML::Specification::MultiplyLayerParams* temp = layer_.multiply_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.multiply_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::MultiplyLayerParams& NeuralNetworkLayer::_internal_multiply() const {
  return _internal_has_multiply()
      ? *layer_.multiply_
      : reinterpret_cast< ::CoreML::Specification::MultiplyLayerParams&>(::CoreML::Specification::_MultiplyLayerParams_default_instance_);
}
inline const ::CoreML::Specification::MultiplyLayerParams& NeuralNetworkLayer::multiply() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.multiply)
  return _internal_multiply();
}
inline ::CoreML::Specification::MultiplyLayerParams* NeuralNetworkLayer::unsafe_arena_release_multiply() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.multiply)
  if (_internal_has_multiply()) {
    clear_has_layer();
    ::CoreML::Specification::MultiplyLayerParams* temp = layer_.multiply_;
    layer_.multiply_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_multiply(::CoreML::Specification::MultiplyLayerParams* multiply) {
  clear_layer();
  if (multiply) {
    set_has_multiply();
    layer_.multiply_ = multiply;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.multiply)
}
inline ::CoreML::Specification::MultiplyLayerParams* NeuralNetworkLayer::_internal_mutable_multiply() {
  if (!_internal_has_multiply()) {
    clear_layer();
    set_has_multiply();
    layer_.multiply_ = CreateMaybeMessage< ::CoreML::Specification::MultiplyLayerParams >(GetArenaForAllocation());
  }
  return layer_.multiply_;
}
inline ::CoreML::Specification::MultiplyLayerParams* NeuralNetworkLayer::mutable_multiply() {
  ::CoreML::Specification::MultiplyLayerParams* _msg = _internal_mutable_multiply();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.multiply)
  return _msg;
}

// .CoreML.Specification.AverageLayerParams average = 240;
inline bool NeuralNetworkLayer::_internal_has_average() const {
  return layer_case() == kAverage;
}
inline bool NeuralNetworkLayer::has_average() const {
  return _internal_has_average();
}
inline void NeuralNetworkLayer::set_has_average() {
  _oneof_case_[0] = kAverage;
}
inline void NeuralNetworkLayer::clear_average() {
  if (_internal_has_average()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.average_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::AverageLayerParams* NeuralNetworkLayer::release_average() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.average)
  if (_internal_has_average()) {
    clear_has_layer();
      ::CoreML::Specification::AverageLayerParams* temp = layer_.average_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.average_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::AverageLayerParams& NeuralNetworkLayer::_internal_average() const {
  return _internal_has_average()
      ? *layer_.average_
      : reinterpret_cast< ::CoreML::Specification::AverageLayerParams&>(::CoreML::Specification::_AverageLayerParams_default_instance_);
}
inline const ::CoreML::Specification::AverageLayerParams& NeuralNetworkLayer::average() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.average)
  return _internal_average();
}
inline ::CoreML::Specification::AverageLayerParams* NeuralNetworkLayer::unsafe_arena_release_average() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.average)
  if (_internal_has_average()) {
    clear_has_layer();
    ::CoreML::Specification::AverageLayerParams* temp = layer_.average_;
    layer_.average_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_average(::CoreML::Specification::AverageLayerParams* average) {
  clear_layer();
  if (average) {
    set_has_average();
    layer_.average_ = average;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.average)
}
inline ::CoreML::Specification::AverageLayerParams* NeuralNetworkLayer::_internal_mutable_average() {
  if (!_internal_has_average()) {
    clear_layer();
    set_has_average();
    layer_.average_ = CreateMaybeMessage< ::CoreML::Specification::AverageLayerParams >(GetArenaForAllocation());
  }
  return layer_.average_;
}
inline ::CoreML::Specification::AverageLayerParams* NeuralNetworkLayer::mutable_average() {
  ::CoreML::Specification::AverageLayerParams* _msg = _internal_mutable_average();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.average)
  return _msg;
}

// .CoreML.Specification.ScaleLayerParams scale = 245;
inline bool NeuralNetworkLayer::_internal_has_scale() const {
  return layer_case() == kScale;
}
inline bool NeuralNetworkLayer::has_scale() const {
  return _internal_has_scale();
}
inline void NeuralNetworkLayer::set_has_scale() {
  _oneof_case_[0] = kScale;
}
inline void NeuralNetworkLayer::clear_scale() {
  if (_internal_has_scale()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.scale_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ScaleLayerParams* NeuralNetworkLayer::release_scale() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.scale)
  if (_internal_has_scale()) {
    clear_has_layer();
      ::CoreML::Specification::ScaleLayerParams* temp = layer_.scale_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.scale_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ScaleLayerParams& NeuralNetworkLayer::_internal_scale() const {
  return _internal_has_scale()
      ? *layer_.scale_
      : reinterpret_cast< ::CoreML::Specification::ScaleLayerParams&>(::CoreML::Specification::_ScaleLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ScaleLayerParams& NeuralNetworkLayer::scale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.scale)
  return _internal_scale();
}
inline ::CoreML::Specification::ScaleLayerParams* NeuralNetworkLayer::unsafe_arena_release_scale() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.scale)
  if (_internal_has_scale()) {
    clear_has_layer();
    ::CoreML::Specification::ScaleLayerParams* temp = layer_.scale_;
    layer_.scale_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_scale(::CoreML::Specification::ScaleLayerParams* scale) {
  clear_layer();
  if (scale) {
    set_has_scale();
    layer_.scale_ = scale;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.scale)
}
inline ::CoreML::Specification::ScaleLayerParams* NeuralNetworkLayer::_internal_mutable_scale() {
  if (!_internal_has_scale()) {
    clear_layer();
    set_has_scale();
    layer_.scale_ = CreateMaybeMessage< ::CoreML::Specification::ScaleLayerParams >(GetArenaForAllocation());
  }
  return layer_.scale_;
}
inline ::CoreML::Specification::ScaleLayerParams* NeuralNetworkLayer::mutable_scale() {
  ::CoreML::Specification::ScaleLayerParams* _msg = _internal_mutable_scale();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.scale)
  return _msg;
}

// .CoreML.Specification.BiasLayerParams bias = 250;
inline bool NeuralNetworkLayer::_internal_has_bias() const {
  return layer_case() == kBias;
}
inline bool NeuralNetworkLayer::has_bias() const {
  return _internal_has_bias();
}
inline void NeuralNetworkLayer::set_has_bias() {
  _oneof_case_[0] = kBias;
}
inline void NeuralNetworkLayer::clear_bias() {
  if (_internal_has_bias()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.bias_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::BiasLayerParams* NeuralNetworkLayer::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.bias)
  if (_internal_has_bias()) {
    clear_has_layer();
      ::CoreML::Specification::BiasLayerParams* temp = layer_.bias_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.bias_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::BiasLayerParams& NeuralNetworkLayer::_internal_bias() const {
  return _internal_has_bias()
      ? *layer_.bias_
      : reinterpret_cast< ::CoreML::Specification::BiasLayerParams&>(::CoreML::Specification::_BiasLayerParams_default_instance_);
}
inline const ::CoreML::Specification::BiasLayerParams& NeuralNetworkLayer::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.bias)
  return _internal_bias();
}
inline ::CoreML::Specification::BiasLayerParams* NeuralNetworkLayer::unsafe_arena_release_bias() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.bias)
  if (_internal_has_bias()) {
    clear_has_layer();
    ::CoreML::Specification::BiasLayerParams* temp = layer_.bias_;
    layer_.bias_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_bias(::CoreML::Specification::BiasLayerParams* bias) {
  clear_layer();
  if (bias) {
    set_has_bias();
    layer_.bias_ = bias;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.bias)
}
inline ::CoreML::Specification::BiasLayerParams* NeuralNetworkLayer::_internal_mutable_bias() {
  if (!_internal_has_bias()) {
    clear_layer();
    set_has_bias();
    layer_.bias_ = CreateMaybeMessage< ::CoreML::Specification::BiasLayerParams >(GetArenaForAllocation());
  }
  return layer_.bias_;
}
inline ::CoreML::Specification::BiasLayerParams* NeuralNetworkLayer::mutable_bias() {
  ::CoreML::Specification::BiasLayerParams* _msg = _internal_mutable_bias();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.bias)
  return _msg;
}

// .CoreML.Specification.MaxLayerParams max = 260;
inline bool NeuralNetworkLayer::_internal_has_max() const {
  return layer_case() == kMax;
}
inline bool NeuralNetworkLayer::has_max() const {
  return _internal_has_max();
}
inline void NeuralNetworkLayer::set_has_max() {
  _oneof_case_[0] = kMax;
}
inline void NeuralNetworkLayer::clear_max() {
  if (_internal_has_max()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.max_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::MaxLayerParams* NeuralNetworkLayer::release_max() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.max)
  if (_internal_has_max()) {
    clear_has_layer();
      ::CoreML::Specification::MaxLayerParams* temp = layer_.max_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.max_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::MaxLayerParams& NeuralNetworkLayer::_internal_max() const {
  return _internal_has_max()
      ? *layer_.max_
      : reinterpret_cast< ::CoreML::Specification::MaxLayerParams&>(::CoreML::Specification::_MaxLayerParams_default_instance_);
}
inline const ::CoreML::Specification::MaxLayerParams& NeuralNetworkLayer::max() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.max)
  return _internal_max();
}
inline ::CoreML::Specification::MaxLayerParams* NeuralNetworkLayer::unsafe_arena_release_max() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.max)
  if (_internal_has_max()) {
    clear_has_layer();
    ::CoreML::Specification::MaxLayerParams* temp = layer_.max_;
    layer_.max_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_max(::CoreML::Specification::MaxLayerParams* max) {
  clear_layer();
  if (max) {
    set_has_max();
    layer_.max_ = max;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.max)
}
inline ::CoreML::Specification::MaxLayerParams* NeuralNetworkLayer::_internal_mutable_max() {
  if (!_internal_has_max()) {
    clear_layer();
    set_has_max();
    layer_.max_ = CreateMaybeMessage< ::CoreML::Specification::MaxLayerParams >(GetArenaForAllocation());
  }
  return layer_.max_;
}
inline ::CoreML::Specification::MaxLayerParams* NeuralNetworkLayer::mutable_max() {
  ::CoreML::Specification::MaxLayerParams* _msg = _internal_mutable_max();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.max)
  return _msg;
}

// .CoreML.Specification.MinLayerParams min = 261;
inline bool NeuralNetworkLayer::_internal_has_min() const {
  return layer_case() == kMin;
}
inline bool NeuralNetworkLayer::has_min() const {
  return _internal_has_min();
}
inline void NeuralNetworkLayer::set_has_min() {
  _oneof_case_[0] = kMin;
}
inline void NeuralNetworkLayer::clear_min() {
  if (_internal_has_min()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.min_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::MinLayerParams* NeuralNetworkLayer::release_min() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.min)
  if (_internal_has_min()) {
    clear_has_layer();
      ::CoreML::Specification::MinLayerParams* temp = layer_.min_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.min_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::MinLayerParams& NeuralNetworkLayer::_internal_min() const {
  return _internal_has_min()
      ? *layer_.min_
      : reinterpret_cast< ::CoreML::Specification::MinLayerParams&>(::CoreML::Specification::_MinLayerParams_default_instance_);
}
inline const ::CoreML::Specification::MinLayerParams& NeuralNetworkLayer::min() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.min)
  return _internal_min();
}
inline ::CoreML::Specification::MinLayerParams* NeuralNetworkLayer::unsafe_arena_release_min() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.min)
  if (_internal_has_min()) {
    clear_has_layer();
    ::CoreML::Specification::MinLayerParams* temp = layer_.min_;
    layer_.min_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_min(::CoreML::Specification::MinLayerParams* min) {
  clear_layer();
  if (min) {
    set_has_min();
    layer_.min_ = min;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.min)
}
inline ::CoreML::Specification::MinLayerParams* NeuralNetworkLayer::_internal_mutable_min() {
  if (!_internal_has_min()) {
    clear_layer();
    set_has_min();
    layer_.min_ = CreateMaybeMessage< ::CoreML::Specification::MinLayerParams >(GetArenaForAllocation());
  }
  return layer_.min_;
}
inline ::CoreML::Specification::MinLayerParams* NeuralNetworkLayer::mutable_min() {
  ::CoreML::Specification::MinLayerParams* _msg = _internal_mutable_min();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.min)
  return _msg;
}

// .CoreML.Specification.DotProductLayerParams dot = 270;
inline bool NeuralNetworkLayer::_internal_has_dot() const {
  return layer_case() == kDot;
}
inline bool NeuralNetworkLayer::has_dot() const {
  return _internal_has_dot();
}
inline void NeuralNetworkLayer::set_has_dot() {
  _oneof_case_[0] = kDot;
}
inline void NeuralNetworkLayer::clear_dot() {
  if (_internal_has_dot()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.dot_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::DotProductLayerParams* NeuralNetworkLayer::release_dot() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.dot)
  if (_internal_has_dot()) {
    clear_has_layer();
      ::CoreML::Specification::DotProductLayerParams* temp = layer_.dot_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.dot_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::DotProductLayerParams& NeuralNetworkLayer::_internal_dot() const {
  return _internal_has_dot()
      ? *layer_.dot_
      : reinterpret_cast< ::CoreML::Specification::DotProductLayerParams&>(::CoreML::Specification::_DotProductLayerParams_default_instance_);
}
inline const ::CoreML::Specification::DotProductLayerParams& NeuralNetworkLayer::dot() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.dot)
  return _internal_dot();
}
inline ::CoreML::Specification::DotProductLayerParams* NeuralNetworkLayer::unsafe_arena_release_dot() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.dot)
  if (_internal_has_dot()) {
    clear_has_layer();
    ::CoreML::Specification::DotProductLayerParams* temp = layer_.dot_;
    layer_.dot_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_dot(::CoreML::Specification::DotProductLayerParams* dot) {
  clear_layer();
  if (dot) {
    set_has_dot();
    layer_.dot_ = dot;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.dot)
}
inline ::CoreML::Specification::DotProductLayerParams* NeuralNetworkLayer::_internal_mutable_dot() {
  if (!_internal_has_dot()) {
    clear_layer();
    set_has_dot();
    layer_.dot_ = CreateMaybeMessage< ::CoreML::Specification::DotProductLayerParams >(GetArenaForAllocation());
  }
  return layer_.dot_;
}
inline ::CoreML::Specification::DotProductLayerParams* NeuralNetworkLayer::mutable_dot() {
  ::CoreML::Specification::DotProductLayerParams* _msg = _internal_mutable_dot();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.dot)
  return _msg;
}

// .CoreML.Specification.ReduceLayerParams reduce = 280;
inline bool NeuralNetworkLayer::_internal_has_reduce() const {
  return layer_case() == kReduce;
}
inline bool NeuralNetworkLayer::has_reduce() const {
  return _internal_has_reduce();
}
inline void NeuralNetworkLayer::set_has_reduce() {
  _oneof_case_[0] = kReduce;
}
inline void NeuralNetworkLayer::clear_reduce() {
  if (_internal_has_reduce()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.reduce_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ReduceLayerParams* NeuralNetworkLayer::release_reduce() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduce)
  if (_internal_has_reduce()) {
    clear_has_layer();
      ::CoreML::Specification::ReduceLayerParams* temp = layer_.reduce_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.reduce_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ReduceLayerParams& NeuralNetworkLayer::_internal_reduce() const {
  return _internal_has_reduce()
      ? *layer_.reduce_
      : reinterpret_cast< ::CoreML::Specification::ReduceLayerParams&>(::CoreML::Specification::_ReduceLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ReduceLayerParams& NeuralNetworkLayer::reduce() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduce)
  return _internal_reduce();
}
inline ::CoreML::Specification::ReduceLayerParams* NeuralNetworkLayer::unsafe_arena_release_reduce() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.reduce)
  if (_internal_has_reduce()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceLayerParams* temp = layer_.reduce_;
    layer_.reduce_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_reduce(::CoreML::Specification::ReduceLayerParams* reduce) {
  clear_layer();
  if (reduce) {
    set_has_reduce();
    layer_.reduce_ = reduce;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduce)
}
inline ::CoreML::Specification::ReduceLayerParams* NeuralNetworkLayer::_internal_mutable_reduce() {
  if (!_internal_has_reduce()) {
    clear_layer();
    set_has_reduce();
    layer_.reduce_ = CreateMaybeMessage< ::CoreML::Specification::ReduceLayerParams >(GetArenaForAllocation());
  }
  return layer_.reduce_;
}
inline ::CoreML::Specification::ReduceLayerParams* NeuralNetworkLayer::mutable_reduce() {
  ::CoreML::Specification::ReduceLayerParams* _msg = _internal_mutable_reduce();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduce)
  return _msg;
}

// .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
inline bool NeuralNetworkLayer::_internal_has_loadconstant() const {
  return layer_case() == kLoadConstant;
}
inline bool NeuralNetworkLayer::has_loadconstant() const {
  return _internal_has_loadconstant();
}
inline void NeuralNetworkLayer::set_has_loadconstant() {
  _oneof_case_[0] = kLoadConstant;
}
inline void NeuralNetworkLayer::clear_loadconstant() {
  if (_internal_has_loadconstant()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.loadconstant_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::LoadConstantLayerParams* NeuralNetworkLayer::release_loadconstant() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  if (_internal_has_loadconstant()) {
    clear_has_layer();
      ::CoreML::Specification::LoadConstantLayerParams* temp = layer_.loadconstant_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.loadconstant_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::LoadConstantLayerParams& NeuralNetworkLayer::_internal_loadconstant() const {
  return _internal_has_loadconstant()
      ? *layer_.loadconstant_
      : reinterpret_cast< ::CoreML::Specification::LoadConstantLayerParams&>(::CoreML::Specification::_LoadConstantLayerParams_default_instance_);
}
inline const ::CoreML::Specification::LoadConstantLayerParams& NeuralNetworkLayer::loadconstant() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  return _internal_loadconstant();
}
inline ::CoreML::Specification::LoadConstantLayerParams* NeuralNetworkLayer::unsafe_arena_release_loadconstant() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  if (_internal_has_loadconstant()) {
    clear_has_layer();
    ::CoreML::Specification::LoadConstantLayerParams* temp = layer_.loadconstant_;
    layer_.loadconstant_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_loadconstant(::CoreML::Specification::LoadConstantLayerParams* loadconstant) {
  clear_layer();
  if (loadconstant) {
    set_has_loadconstant();
    layer_.loadconstant_ = loadconstant;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.loadConstant)
}
inline ::CoreML::Specification::LoadConstantLayerParams* NeuralNetworkLayer::_internal_mutable_loadconstant() {
  if (!_internal_has_loadconstant()) {
    clear_layer();
    set_has_loadconstant();
    layer_.loadconstant_ = CreateMaybeMessage< ::CoreML::Specification::LoadConstantLayerParams >(GetArenaForAllocation());
  }
  return layer_.loadconstant_;
}
inline ::CoreML::Specification::LoadConstantLayerParams* NeuralNetworkLayer::mutable_loadconstant() {
  ::CoreML::Specification::LoadConstantLayerParams* _msg = _internal_mutable_loadconstant();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  return _msg;
}

// .CoreML.Specification.ReshapeLayerParams reshape = 300;
inline bool NeuralNetworkLayer::_internal_has_reshape() const {
  return layer_case() == kReshape;
}
inline bool NeuralNetworkLayer::has_reshape() const {
  return _internal_has_reshape();
}
inline void NeuralNetworkLayer::set_has_reshape() {
  _oneof_case_[0] = kReshape;
}
inline void NeuralNetworkLayer::clear_reshape() {
  if (_internal_has_reshape()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.reshape_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ReshapeLayerParams* NeuralNetworkLayer::release_reshape() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reshape)
  if (_internal_has_reshape()) {
    clear_has_layer();
      ::CoreML::Specification::ReshapeLayerParams* temp = layer_.reshape_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.reshape_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ReshapeLayerParams& NeuralNetworkLayer::_internal_reshape() const {
  return _internal_has_reshape()
      ? *layer_.reshape_
      : reinterpret_cast< ::CoreML::Specification::ReshapeLayerParams&>(::CoreML::Specification::_ReshapeLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ReshapeLayerParams& NeuralNetworkLayer::reshape() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reshape)
  return _internal_reshape();
}
inline ::CoreML::Specification::ReshapeLayerParams* NeuralNetworkLayer::unsafe_arena_release_reshape() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.reshape)
  if (_internal_has_reshape()) {
    clear_has_layer();
    ::CoreML::Specification::ReshapeLayerParams* temp = layer_.reshape_;
    layer_.reshape_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_reshape(::CoreML::Specification::ReshapeLayerParams* reshape) {
  clear_layer();
  if (reshape) {
    set_has_reshape();
    layer_.reshape_ = reshape;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.reshape)
}
inline ::CoreML::Specification::ReshapeLayerParams* NeuralNetworkLayer::_internal_mutable_reshape() {
  if (!_internal_has_reshape()) {
    clear_layer();
    set_has_reshape();
    layer_.reshape_ = CreateMaybeMessage< ::CoreML::Specification::ReshapeLayerParams >(GetArenaForAllocation());
  }
  return layer_.reshape_;
}
inline ::CoreML::Specification::ReshapeLayerParams* NeuralNetworkLayer::mutable_reshape() {
  ::CoreML::Specification::ReshapeLayerParams* _msg = _internal_mutable_reshape();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reshape)
  return _msg;
}

// .CoreML.Specification.FlattenLayerParams flatten = 301;
inline bool NeuralNetworkLayer::_internal_has_flatten() const {
  return layer_case() == kFlatten;
}
inline bool NeuralNetworkLayer::has_flatten() const {
  return _internal_has_flatten();
}
inline void NeuralNetworkLayer::set_has_flatten() {
  _oneof_case_[0] = kFlatten;
}
inline void NeuralNetworkLayer::clear_flatten() {
  if (_internal_has_flatten()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.flatten_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::FlattenLayerParams* NeuralNetworkLayer::release_flatten() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.flatten)
  if (_internal_has_flatten()) {
    clear_has_layer();
      ::CoreML::Specification::FlattenLayerParams* temp = layer_.flatten_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.flatten_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::FlattenLayerParams& NeuralNetworkLayer::_internal_flatten() const {
  return _internal_has_flatten()
      ? *layer_.flatten_
      : reinterpret_cast< ::CoreML::Specification::FlattenLayerParams&>(::CoreML::Specification::_FlattenLayerParams_default_instance_);
}
inline const ::CoreML::Specification::FlattenLayerParams& NeuralNetworkLayer::flatten() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.flatten)
  return _internal_flatten();
}
inline ::CoreML::Specification::FlattenLayerParams* NeuralNetworkLayer::unsafe_arena_release_flatten() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.flatten)
  if (_internal_has_flatten()) {
    clear_has_layer();
    ::CoreML::Specification::FlattenLayerParams* temp = layer_.flatten_;
    layer_.flatten_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_flatten(::CoreML::Specification::FlattenLayerParams* flatten) {
  clear_layer();
  if (flatten) {
    set_has_flatten();
    layer_.flatten_ = flatten;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.flatten)
}
inline ::CoreML::Specification::FlattenLayerParams* NeuralNetworkLayer::_internal_mutable_flatten() {
  if (!_internal_has_flatten()) {
    clear_layer();
    set_has_flatten();
    layer_.flatten_ = CreateMaybeMessage< ::CoreML::Specification::FlattenLayerParams >(GetArenaForAllocation());
  }
  return layer_.flatten_;
}
inline ::CoreML::Specification::FlattenLayerParams* NeuralNetworkLayer::mutable_flatten() {
  ::CoreML::Specification::FlattenLayerParams* _msg = _internal_mutable_flatten();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.flatten)
  return _msg;
}

// .CoreML.Specification.PermuteLayerParams permute = 310;
inline bool NeuralNetworkLayer::_internal_has_permute() const {
  return layer_case() == kPermute;
}
inline bool NeuralNetworkLayer::has_permute() const {
  return _internal_has_permute();
}
inline void NeuralNetworkLayer::set_has_permute() {
  _oneof_case_[0] = kPermute;
}
inline void NeuralNetworkLayer::clear_permute() {
  if (_internal_has_permute()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.permute_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::PermuteLayerParams* NeuralNetworkLayer::release_permute() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.permute)
  if (_internal_has_permute()) {
    clear_has_layer();
      ::CoreML::Specification::PermuteLayerParams* temp = layer_.permute_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.permute_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::PermuteLayerParams& NeuralNetworkLayer::_internal_permute() const {
  return _internal_has_permute()
      ? *layer_.permute_
      : reinterpret_cast< ::CoreML::Specification::PermuteLayerParams&>(::CoreML::Specification::_PermuteLayerParams_default_instance_);
}
inline const ::CoreML::Specification::PermuteLayerParams& NeuralNetworkLayer::permute() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.permute)
  return _internal_permute();
}
inline ::CoreML::Specification::PermuteLayerParams* NeuralNetworkLayer::unsafe_arena_release_permute() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.permute)
  if (_internal_has_permute()) {
    clear_has_layer();
    ::CoreML::Specification::PermuteLayerParams* temp = layer_.permute_;
    layer_.permute_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_permute(::CoreML::Specification::PermuteLayerParams* permute) {
  clear_layer();
  if (permute) {
    set_has_permute();
    layer_.permute_ = permute;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.permute)
}
inline ::CoreML::Specification::PermuteLayerParams* NeuralNetworkLayer::_internal_mutable_permute() {
  if (!_internal_has_permute()) {
    clear_layer();
    set_has_permute();
    layer_.permute_ = CreateMaybeMessage< ::CoreML::Specification::PermuteLayerParams >(GetArenaForAllocation());
  }
  return layer_.permute_;
}
inline ::CoreML::Specification::PermuteLayerParams* NeuralNetworkLayer::mutable_permute() {
  ::CoreML::Specification::PermuteLayerParams* _msg = _internal_mutable_permute();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.permute)
  return _msg;
}

// .CoreML.Specification.ConcatLayerParams concat = 320;
inline bool NeuralNetworkLayer::_internal_has_concat() const {
  return layer_case() == kConcat;
}
inline bool NeuralNetworkLayer::has_concat() const {
  return _internal_has_concat();
}
inline void NeuralNetworkLayer::set_has_concat() {
  _oneof_case_[0] = kConcat;
}
inline void NeuralNetworkLayer::clear_concat() {
  if (_internal_has_concat()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.concat_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ConcatLayerParams* NeuralNetworkLayer::release_concat() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.concat)
  if (_internal_has_concat()) {
    clear_has_layer();
      ::CoreML::Specification::ConcatLayerParams* temp = layer_.concat_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.concat_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ConcatLayerParams& NeuralNetworkLayer::_internal_concat() const {
  return _internal_has_concat()
      ? *layer_.concat_
      : reinterpret_cast< ::CoreML::Specification::ConcatLayerParams&>(::CoreML::Specification::_ConcatLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ConcatLayerParams& NeuralNetworkLayer::concat() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.concat)
  return _internal_concat();
}
inline ::CoreML::Specification::ConcatLayerParams* NeuralNetworkLayer::unsafe_arena_release_concat() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.concat)
  if (_internal_has_concat()) {
    clear_has_layer();
    ::CoreML::Specification::ConcatLayerParams* temp = layer_.concat_;
    layer_.concat_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_concat(::CoreML::Specification::ConcatLayerParams* concat) {
  clear_layer();
  if (concat) {
    set_has_concat();
    layer_.concat_ = concat;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.concat)
}
inline ::CoreML::Specification::ConcatLayerParams* NeuralNetworkLayer::_internal_mutable_concat() {
  if (!_internal_has_concat()) {
    clear_layer();
    set_has_concat();
    layer_.concat_ = CreateMaybeMessage< ::CoreML::Specification::ConcatLayerParams >(GetArenaForAllocation());
  }
  return layer_.concat_;
}
inline ::CoreML::Specification::ConcatLayerParams* NeuralNetworkLayer::mutable_concat() {
  ::CoreML::Specification::ConcatLayerParams* _msg = _internal_mutable_concat();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.concat)
  return _msg;
}

// .CoreML.Specification.SplitLayerParams split = 330;
inline bool NeuralNetworkLayer::_internal_has_split() const {
  return layer_case() == kSplit;
}
inline bool NeuralNetworkLayer::has_split() const {
  return _internal_has_split();
}
inline void NeuralNetworkLayer::set_has_split() {
  _oneof_case_[0] = kSplit;
}
inline void NeuralNetworkLayer::clear_split() {
  if (_internal_has_split()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.split_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::SplitLayerParams* NeuralNetworkLayer::release_split() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.split)
  if (_internal_has_split()) {
    clear_has_layer();
      ::CoreML::Specification::SplitLayerParams* temp = layer_.split_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.split_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::SplitLayerParams& NeuralNetworkLayer::_internal_split() const {
  return _internal_has_split()
      ? *layer_.split_
      : reinterpret_cast< ::CoreML::Specification::SplitLayerParams&>(::CoreML::Specification::_SplitLayerParams_default_instance_);
}
inline const ::CoreML::Specification::SplitLayerParams& NeuralNetworkLayer::split() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.split)
  return _internal_split();
}
inline ::CoreML::Specification::SplitLayerParams* NeuralNetworkLayer::unsafe_arena_release_split() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.split)
  if (_internal_has_split()) {
    clear_has_layer();
    ::CoreML::Specification::SplitLayerParams* temp = layer_.split_;
    layer_.split_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_split(::CoreML::Specification::SplitLayerParams* split) {
  clear_layer();
  if (split) {
    set_has_split();
    layer_.split_ = split;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.split)
}
inline ::CoreML::Specification::SplitLayerParams* NeuralNetworkLayer::_internal_mutable_split() {
  if (!_internal_has_split()) {
    clear_layer();
    set_has_split();
    layer_.split_ = CreateMaybeMessage< ::CoreML::Specification::SplitLayerParams >(GetArenaForAllocation());
  }
  return layer_.split_;
}
inline ::CoreML::Specification::SplitLayerParams* NeuralNetworkLayer::mutable_split() {
  ::CoreML::Specification::SplitLayerParams* _msg = _internal_mutable_split();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.split)
  return _msg;
}

// .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
inline bool NeuralNetworkLayer::_internal_has_sequencerepeat() const {
  return layer_case() == kSequenceRepeat;
}
inline bool NeuralNetworkLayer::has_sequencerepeat() const {
  return _internal_has_sequencerepeat();
}
inline void NeuralNetworkLayer::set_has_sequencerepeat() {
  _oneof_case_[0] = kSequenceRepeat;
}
inline void NeuralNetworkLayer::clear_sequencerepeat() {
  if (_internal_has_sequencerepeat()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.sequencerepeat_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::SequenceRepeatLayerParams* NeuralNetworkLayer::release_sequencerepeat() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  if (_internal_has_sequencerepeat()) {
    clear_has_layer();
      ::CoreML::Specification::SequenceRepeatLayerParams* temp = layer_.sequencerepeat_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.sequencerepeat_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::SequenceRepeatLayerParams& NeuralNetworkLayer::_internal_sequencerepeat() const {
  return _internal_has_sequencerepeat()
      ? *layer_.sequencerepeat_
      : reinterpret_cast< ::CoreML::Specification::SequenceRepeatLayerParams&>(::CoreML::Specification::_SequenceRepeatLayerParams_default_instance_);
}
inline const ::CoreML::Specification::SequenceRepeatLayerParams& NeuralNetworkLayer::sequencerepeat() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  return _internal_sequencerepeat();
}
inline ::CoreML::Specification::SequenceRepeatLayerParams* NeuralNetworkLayer::unsafe_arena_release_sequencerepeat() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  if (_internal_has_sequencerepeat()) {
    clear_has_layer();
    ::CoreML::Specification::SequenceRepeatLayerParams* temp = layer_.sequencerepeat_;
    layer_.sequencerepeat_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_sequencerepeat(::CoreML::Specification::SequenceRepeatLayerParams* sequencerepeat) {
  clear_layer();
  if (sequencerepeat) {
    set_has_sequencerepeat();
    layer_.sequencerepeat_ = sequencerepeat;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
}
inline ::CoreML::Specification::SequenceRepeatLayerParams* NeuralNetworkLayer::_internal_mutable_sequencerepeat() {
  if (!_internal_has_sequencerepeat()) {
    clear_layer();
    set_has_sequencerepeat();
    layer_.sequencerepeat_ = CreateMaybeMessage< ::CoreML::Specification::SequenceRepeatLayerParams >(GetArenaForAllocation());
  }
  return layer_.sequencerepeat_;
}
inline ::CoreML::Specification::SequenceRepeatLayerParams* NeuralNetworkLayer::mutable_sequencerepeat() {
  ::CoreML::Specification::SequenceRepeatLayerParams* _msg = _internal_mutable_sequencerepeat();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  return _msg;
}

// .CoreML.Specification.ReorganizeDataLayerParams reorganizeData = 345;
inline bool NeuralNetworkLayer::_internal_has_reorganizedata() const {
  return layer_case() == kReorganizeData;
}
inline bool NeuralNetworkLayer::has_reorganizedata() const {
  return _internal_has_reorganizedata();
}
inline void NeuralNetworkLayer::set_has_reorganizedata() {
  _oneof_case_[0] = kReorganizeData;
}
inline void NeuralNetworkLayer::clear_reorganizedata() {
  if (_internal_has_reorganizedata()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.reorganizedata_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ReorganizeDataLayerParams* NeuralNetworkLayer::release_reorganizedata() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reorganizeData)
  if (_internal_has_reorganizedata()) {
    clear_has_layer();
      ::CoreML::Specification::ReorganizeDataLayerParams* temp = layer_.reorganizedata_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.reorganizedata_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ReorganizeDataLayerParams& NeuralNetworkLayer::_internal_reorganizedata() const {
  return _internal_has_reorganizedata()
      ? *layer_.reorganizedata_
      : reinterpret_cast< ::CoreML::Specification::ReorganizeDataLayerParams&>(::CoreML::Specification::_ReorganizeDataLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ReorganizeDataLayerParams& NeuralNetworkLayer::reorganizedata() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reorganizeData)
  return _internal_reorganizedata();
}
inline ::CoreML::Specification::ReorganizeDataLayerParams* NeuralNetworkLayer::unsafe_arena_release_reorganizedata() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.reorganizeData)
  if (_internal_has_reorganizedata()) {
    clear_has_layer();
    ::CoreML::Specification::ReorganizeDataLayerParams* temp = layer_.reorganizedata_;
    layer_.reorganizedata_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_reorganizedata(::CoreML::Specification::ReorganizeDataLayerParams* reorganizedata) {
  clear_layer();
  if (reorganizedata) {
    set_has_reorganizedata();
    layer_.reorganizedata_ = reorganizedata;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.reorganizeData)
}
inline ::CoreML::Specification::ReorganizeDataLayerParams* NeuralNetworkLayer::_internal_mutable_reorganizedata() {
  if (!_internal_has_reorganizedata()) {
    clear_layer();
    set_has_reorganizedata();
    layer_.reorganizedata_ = CreateMaybeMessage< ::CoreML::Specification::ReorganizeDataLayerParams >(GetArenaForAllocation());
  }
  return layer_.reorganizedata_;
}
inline ::CoreML::Specification::ReorganizeDataLayerParams* NeuralNetworkLayer::mutable_reorganizedata() {
  ::CoreML::Specification::ReorganizeDataLayerParams* _msg = _internal_mutable_reorganizedata();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reorganizeData)
  return _msg;
}

// .CoreML.Specification.SliceLayerParams slice = 350;
inline bool NeuralNetworkLayer::_internal_has_slice() const {
  return layer_case() == kSlice;
}
inline bool NeuralNetworkLayer::has_slice() const {
  return _internal_has_slice();
}
inline void NeuralNetworkLayer::set_has_slice() {
  _oneof_case_[0] = kSlice;
}
inline void NeuralNetworkLayer::clear_slice() {
  if (_internal_has_slice()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.slice_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::SliceLayerParams* NeuralNetworkLayer::release_slice() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.slice)
  if (_internal_has_slice()) {
    clear_has_layer();
      ::CoreML::Specification::SliceLayerParams* temp = layer_.slice_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.slice_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::SliceLayerParams& NeuralNetworkLayer::_internal_slice() const {
  return _internal_has_slice()
      ? *layer_.slice_
      : reinterpret_cast< ::CoreML::Specification::SliceLayerParams&>(::CoreML::Specification::_SliceLayerParams_default_instance_);
}
inline const ::CoreML::Specification::SliceLayerParams& NeuralNetworkLayer::slice() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.slice)
  return _internal_slice();
}
inline ::CoreML::Specification::SliceLayerParams* NeuralNetworkLayer::unsafe_arena_release_slice() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.slice)
  if (_internal_has_slice()) {
    clear_has_layer();
    ::CoreML::Specification::SliceLayerParams* temp = layer_.slice_;
    layer_.slice_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_slice(::CoreML::Specification::SliceLayerParams* slice) {
  clear_layer();
  if (slice) {
    set_has_slice();
    layer_.slice_ = slice;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.slice)
}
inline ::CoreML::Specification::SliceLayerParams* NeuralNetworkLayer::_internal_mutable_slice() {
  if (!_internal_has_slice()) {
    clear_layer();
    set_has_slice();
    layer_.slice_ = CreateMaybeMessage< ::CoreML::Specification::SliceLayerParams >(GetArenaForAllocation());
  }
  return layer_.slice_;
}
inline ::CoreML::Specification::SliceLayerParams* NeuralNetworkLayer::mutable_slice() {
  ::CoreML::Specification::SliceLayerParams* _msg = _internal_mutable_slice();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.slice)
  return _msg;
}

// .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
inline bool NeuralNetworkLayer::_internal_has_simplerecurrent() const {
  return layer_case() == kSimpleRecurrent;
}
inline bool NeuralNetworkLayer::has_simplerecurrent() const {
  return _internal_has_simplerecurrent();
}
inline void NeuralNetworkLayer::set_has_simplerecurrent() {
  _oneof_case_[0] = kSimpleRecurrent;
}
inline void NeuralNetworkLayer::clear_simplerecurrent() {
  if (_internal_has_simplerecurrent()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.simplerecurrent_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::SimpleRecurrentLayerParams* NeuralNetworkLayer::release_simplerecurrent() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  if (_internal_has_simplerecurrent()) {
    clear_has_layer();
      ::CoreML::Specification::SimpleRecurrentLayerParams* temp = layer_.simplerecurrent_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.simplerecurrent_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::SimpleRecurrentLayerParams& NeuralNetworkLayer::_internal_simplerecurrent() const {
  return _internal_has_simplerecurrent()
      ? *layer_.simplerecurrent_
      : reinterpret_cast< ::CoreML::Specification::SimpleRecurrentLayerParams&>(::CoreML::Specification::_SimpleRecurrentLayerParams_default_instance_);
}
inline const ::CoreML::Specification::SimpleRecurrentLayerParams& NeuralNetworkLayer::simplerecurrent() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  return _internal_simplerecurrent();
}
inline ::CoreML::Specification::SimpleRecurrentLayerParams* NeuralNetworkLayer::unsafe_arena_release_simplerecurrent() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  if (_internal_has_simplerecurrent()) {
    clear_has_layer();
    ::CoreML::Specification::SimpleRecurrentLayerParams* temp = layer_.simplerecurrent_;
    layer_.simplerecurrent_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_simplerecurrent(::CoreML::Specification::SimpleRecurrentLayerParams* simplerecurrent) {
  clear_layer();
  if (simplerecurrent) {
    set_has_simplerecurrent();
    layer_.simplerecurrent_ = simplerecurrent;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
}
inline ::CoreML::Specification::SimpleRecurrentLayerParams* NeuralNetworkLayer::_internal_mutable_simplerecurrent() {
  if (!_internal_has_simplerecurrent()) {
    clear_layer();
    set_has_simplerecurrent();
    layer_.simplerecurrent_ = CreateMaybeMessage< ::CoreML::Specification::SimpleRecurrentLayerParams >(GetArenaForAllocation());
  }
  return layer_.simplerecurrent_;
}
inline ::CoreML::Specification::SimpleRecurrentLayerParams* NeuralNetworkLayer::mutable_simplerecurrent() {
  ::CoreML::Specification::SimpleRecurrentLayerParams* _msg = _internal_mutable_simplerecurrent();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  return _msg;
}

// .CoreML.Specification.GRULayerParams gru = 410;
inline bool NeuralNetworkLayer::_internal_has_gru() const {
  return layer_case() == kGru;
}
inline bool NeuralNetworkLayer::has_gru() const {
  return _internal_has_gru();
}
inline void NeuralNetworkLayer::set_has_gru() {
  _oneof_case_[0] = kGru;
}
inline void NeuralNetworkLayer::clear_gru() {
  if (_internal_has_gru()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.gru_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::GRULayerParams* NeuralNetworkLayer::release_gru() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.gru)
  if (_internal_has_gru()) {
    clear_has_layer();
      ::CoreML::Specification::GRULayerParams* temp = layer_.gru_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.gru_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::GRULayerParams& NeuralNetworkLayer::_internal_gru() const {
  return _internal_has_gru()
      ? *layer_.gru_
      : reinterpret_cast< ::CoreML::Specification::GRULayerParams&>(::CoreML::Specification::_GRULayerParams_default_instance_);
}
inline const ::CoreML::Specification::GRULayerParams& NeuralNetworkLayer::gru() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.gru)
  return _internal_gru();
}
inline ::CoreML::Specification::GRULayerParams* NeuralNetworkLayer::unsafe_arena_release_gru() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.gru)
  if (_internal_has_gru()) {
    clear_has_layer();
    ::CoreML::Specification::GRULayerParams* temp = layer_.gru_;
    layer_.gru_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_gru(::CoreML::Specification::GRULayerParams* gru) {
  clear_layer();
  if (gru) {
    set_has_gru();
    layer_.gru_ = gru;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.gru)
}
inline ::CoreML::Specification::GRULayerParams* NeuralNetworkLayer::_internal_mutable_gru() {
  if (!_internal_has_gru()) {
    clear_layer();
    set_has_gru();
    layer_.gru_ = CreateMaybeMessage< ::CoreML::Specification::GRULayerParams >(GetArenaForAllocation());
  }
  return layer_.gru_;
}
inline ::CoreML::Specification::GRULayerParams* NeuralNetworkLayer::mutable_gru() {
  ::CoreML::Specification::GRULayerParams* _msg = _internal_mutable_gru();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.gru)
  return _msg;
}

// .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
inline bool NeuralNetworkLayer::_internal_has_unidirectionallstm() const {
  return layer_case() == kUniDirectionalLSTM;
}
inline bool NeuralNetworkLayer::has_unidirectionallstm() const {
  return _internal_has_unidirectionallstm();
}
inline void NeuralNetworkLayer::set_has_unidirectionallstm() {
  _oneof_case_[0] = kUniDirectionalLSTM;
}
inline void NeuralNetworkLayer::clear_unidirectionallstm() {
  if (_internal_has_unidirectionallstm()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.unidirectionallstm_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::UniDirectionalLSTMLayerParams* NeuralNetworkLayer::release_unidirectionallstm() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  if (_internal_has_unidirectionallstm()) {
    clear_has_layer();
      ::CoreML::Specification::UniDirectionalLSTMLayerParams* temp = layer_.unidirectionallstm_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.unidirectionallstm_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::UniDirectionalLSTMLayerParams& NeuralNetworkLayer::_internal_unidirectionallstm() const {
  return _internal_has_unidirectionallstm()
      ? *layer_.unidirectionallstm_
      : reinterpret_cast< ::CoreML::Specification::UniDirectionalLSTMLayerParams&>(::CoreML::Specification::_UniDirectionalLSTMLayerParams_default_instance_);
}
inline const ::CoreML::Specification::UniDirectionalLSTMLayerParams& NeuralNetworkLayer::unidirectionallstm() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  return _internal_unidirectionallstm();
}
inline ::CoreML::Specification::UniDirectionalLSTMLayerParams* NeuralNetworkLayer::unsafe_arena_release_unidirectionallstm() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  if (_internal_has_unidirectionallstm()) {
    clear_has_layer();
    ::CoreML::Specification::UniDirectionalLSTMLayerParams* temp = layer_.unidirectionallstm_;
    layer_.unidirectionallstm_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_unidirectionallstm(::CoreML::Specification::UniDirectionalLSTMLayerParams* unidirectionallstm) {
  clear_layer();
  if (unidirectionallstm) {
    set_has_unidirectionallstm();
    layer_.unidirectionallstm_ = unidirectionallstm;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
}
inline ::CoreML::Specification::UniDirectionalLSTMLayerParams* NeuralNetworkLayer::_internal_mutable_unidirectionallstm() {
  if (!_internal_has_unidirectionallstm()) {
    clear_layer();
    set_has_unidirectionallstm();
    layer_.unidirectionallstm_ = CreateMaybeMessage< ::CoreML::Specification::UniDirectionalLSTMLayerParams >(GetArenaForAllocation());
  }
  return layer_.unidirectionallstm_;
}
inline ::CoreML::Specification::UniDirectionalLSTMLayerParams* NeuralNetworkLayer::mutable_unidirectionallstm() {
  ::CoreML::Specification::UniDirectionalLSTMLayerParams* _msg = _internal_mutable_unidirectionallstm();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  return _msg;
}

// .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
inline bool NeuralNetworkLayer::_internal_has_bidirectionallstm() const {
  return layer_case() == kBiDirectionalLSTM;
}
inline bool NeuralNetworkLayer::has_bidirectionallstm() const {
  return _internal_has_bidirectionallstm();
}
inline void NeuralNetworkLayer::set_has_bidirectionallstm() {
  _oneof_case_[0] = kBiDirectionalLSTM;
}
inline void NeuralNetworkLayer::clear_bidirectionallstm() {
  if (_internal_has_bidirectionallstm()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.bidirectionallstm_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::BiDirectionalLSTMLayerParams* NeuralNetworkLayer::release_bidirectionallstm() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  if (_internal_has_bidirectionallstm()) {
    clear_has_layer();
      ::CoreML::Specification::BiDirectionalLSTMLayerParams* temp = layer_.bidirectionallstm_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.bidirectionallstm_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::BiDirectionalLSTMLayerParams& NeuralNetworkLayer::_internal_bidirectionallstm() const {
  return _internal_has_bidirectionallstm()
      ? *layer_.bidirectionallstm_
      : reinterpret_cast< ::CoreML::Specification::BiDirectionalLSTMLayerParams&>(::CoreML::Specification::_BiDirectionalLSTMLayerParams_default_instance_);
}
inline const ::CoreML::Specification::BiDirectionalLSTMLayerParams& NeuralNetworkLayer::bidirectionallstm() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  return _internal_bidirectionallstm();
}
inline ::CoreML::Specification::BiDirectionalLSTMLayerParams* NeuralNetworkLayer::unsafe_arena_release_bidirectionallstm() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  if (_internal_has_bidirectionallstm()) {
    clear_has_layer();
    ::CoreML::Specification::BiDirectionalLSTMLayerParams* temp = layer_.bidirectionallstm_;
    layer_.bidirectionallstm_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_bidirectionallstm(::CoreML::Specification::BiDirectionalLSTMLayerParams* bidirectionallstm) {
  clear_layer();
  if (bidirectionallstm) {
    set_has_bidirectionallstm();
    layer_.bidirectionallstm_ = bidirectionallstm;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
}
inline ::CoreML::Specification::BiDirectionalLSTMLayerParams* NeuralNetworkLayer::_internal_mutable_bidirectionallstm() {
  if (!_internal_has_bidirectionallstm()) {
    clear_layer();
    set_has_bidirectionallstm();
    layer_.bidirectionallstm_ = CreateMaybeMessage< ::CoreML::Specification::BiDirectionalLSTMLayerParams >(GetArenaForAllocation());
  }
  return layer_.bidirectionallstm_;
}
inline ::CoreML::Specification::BiDirectionalLSTMLayerParams* NeuralNetworkLayer::mutable_bidirectionallstm() {
  ::CoreML::Specification::BiDirectionalLSTMLayerParams* _msg = _internal_mutable_bidirectionallstm();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  return _msg;
}

// .CoreML.Specification.CustomLayerParams custom = 500;
inline bool NeuralNetworkLayer::_internal_has_custom() const {
  return layer_case() == kCustom;
}
inline bool NeuralNetworkLayer::has_custom() const {
  return _internal_has_custom();
}
inline void NeuralNetworkLayer::set_has_custom() {
  _oneof_case_[0] = kCustom;
}
inline void NeuralNetworkLayer::clear_custom() {
  if (_internal_has_custom()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.custom_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::CustomLayerParams* NeuralNetworkLayer::release_custom() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.custom)
  if (_internal_has_custom()) {
    clear_has_layer();
      ::CoreML::Specification::CustomLayerParams* temp = layer_.custom_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.custom_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::CustomLayerParams& NeuralNetworkLayer::_internal_custom() const {
  return _internal_has_custom()
      ? *layer_.custom_
      : reinterpret_cast< ::CoreML::Specification::CustomLayerParams&>(::CoreML::Specification::_CustomLayerParams_default_instance_);
}
inline const ::CoreML::Specification::CustomLayerParams& NeuralNetworkLayer::custom() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.custom)
  return _internal_custom();
}
inline ::CoreML::Specification::CustomLayerParams* NeuralNetworkLayer::unsafe_arena_release_custom() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.custom)
  if (_internal_has_custom()) {
    clear_has_layer();
    ::CoreML::Specification::CustomLayerParams* temp = layer_.custom_;
    layer_.custom_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_custom(::CoreML::Specification::CustomLayerParams* custom) {
  clear_layer();
  if (custom) {
    set_has_custom();
    layer_.custom_ = custom;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.custom)
}
inline ::CoreML::Specification::CustomLayerParams* NeuralNetworkLayer::_internal_mutable_custom() {
  if (!_internal_has_custom()) {
    clear_layer();
    set_has_custom();
    layer_.custom_ = CreateMaybeMessage< ::CoreML::Specification::CustomLayerParams >(GetArenaForAllocation());
  }
  return layer_.custom_;
}
inline ::CoreML::Specification::CustomLayerParams* NeuralNetworkLayer::mutable_custom() {
  ::CoreML::Specification::CustomLayerParams* _msg = _internal_mutable_custom();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.custom)
  return _msg;
}

// .CoreML.Specification.CopyLayerParams copy = 600;
inline bool NeuralNetworkLayer::_internal_has_copy() const {
  return layer_case() == kCopy;
}
inline bool NeuralNetworkLayer::has_copy() const {
  return _internal_has_copy();
}
inline void NeuralNetworkLayer::set_has_copy() {
  _oneof_case_[0] = kCopy;
}
inline void NeuralNetworkLayer::clear_copy() {
  if (_internal_has_copy()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.copy_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::CopyLayerParams* NeuralNetworkLayer::release_copy() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.copy)
  if (_internal_has_copy()) {
    clear_has_layer();
      ::CoreML::Specification::CopyLayerParams* temp = layer_.copy_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.copy_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::CopyLayerParams& NeuralNetworkLayer::_internal_copy() const {
  return _internal_has_copy()
      ? *layer_.copy_
      : reinterpret_cast< ::CoreML::Specification::CopyLayerParams&>(::CoreML::Specification::_CopyLayerParams_default_instance_);
}
inline const ::CoreML::Specification::CopyLayerParams& NeuralNetworkLayer::copy() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.copy)
  return _internal_copy();
}
inline ::CoreML::Specification::CopyLayerParams* NeuralNetworkLayer::unsafe_arena_release_copy() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.copy)
  if (_internal_has_copy()) {
    clear_has_layer();
    ::CoreML::Specification::CopyLayerParams* temp = layer_.copy_;
    layer_.copy_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_copy(::CoreML::Specification::CopyLayerParams* copy) {
  clear_layer();
  if (copy) {
    set_has_copy();
    layer_.copy_ = copy;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.copy)
}
inline ::CoreML::Specification::CopyLayerParams* NeuralNetworkLayer::_internal_mutable_copy() {
  if (!_internal_has_copy()) {
    clear_layer();
    set_has_copy();
    layer_.copy_ = CreateMaybeMessage< ::CoreML::Specification::CopyLayerParams >(GetArenaForAllocation());
  }
  return layer_.copy_;
}
inline ::CoreML::Specification::CopyLayerParams* NeuralNetworkLayer::mutable_copy() {
  ::CoreML::Specification::CopyLayerParams* _msg = _internal_mutable_copy();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.copy)
  return _msg;
}

// .CoreML.Specification.BranchLayerParams branch = 605;
inline bool NeuralNetworkLayer::_internal_has_branch() const {
  return layer_case() == kBranch;
}
inline bool NeuralNetworkLayer::has_branch() const {
  return _internal_has_branch();
}
inline void NeuralNetworkLayer::set_has_branch() {
  _oneof_case_[0] = kBranch;
}
inline void NeuralNetworkLayer::clear_branch() {
  if (_internal_has_branch()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.branch_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::BranchLayerParams* NeuralNetworkLayer::release_branch() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.branch)
  if (_internal_has_branch()) {
    clear_has_layer();
      ::CoreML::Specification::BranchLayerParams* temp = layer_.branch_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.branch_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::BranchLayerParams& NeuralNetworkLayer::_internal_branch() const {
  return _internal_has_branch()
      ? *layer_.branch_
      : reinterpret_cast< ::CoreML::Specification::BranchLayerParams&>(::CoreML::Specification::_BranchLayerParams_default_instance_);
}
inline const ::CoreML::Specification::BranchLayerParams& NeuralNetworkLayer::branch() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.branch)
  return _internal_branch();
}
inline ::CoreML::Specification::BranchLayerParams* NeuralNetworkLayer::unsafe_arena_release_branch() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.branch)
  if (_internal_has_branch()) {
    clear_has_layer();
    ::CoreML::Specification::BranchLayerParams* temp = layer_.branch_;
    layer_.branch_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_branch(::CoreML::Specification::BranchLayerParams* branch) {
  clear_layer();
  if (branch) {
    set_has_branch();
    layer_.branch_ = branch;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.branch)
}
inline ::CoreML::Specification::BranchLayerParams* NeuralNetworkLayer::_internal_mutable_branch() {
  if (!_internal_has_branch()) {
    clear_layer();
    set_has_branch();
    layer_.branch_ = CreateMaybeMessage< ::CoreML::Specification::BranchLayerParams >(GetArenaForAllocation());
  }
  return layer_.branch_;
}
inline ::CoreML::Specification::BranchLayerParams* NeuralNetworkLayer::mutable_branch() {
  ::CoreML::Specification::BranchLayerParams* _msg = _internal_mutable_branch();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.branch)
  return _msg;
}

// .CoreML.Specification.LoopLayerParams loop = 615;
inline bool NeuralNetworkLayer::_internal_has_loop() const {
  return layer_case() == kLoop;
}
inline bool NeuralNetworkLayer::has_loop() const {
  return _internal_has_loop();
}
inline void NeuralNetworkLayer::set_has_loop() {
  _oneof_case_[0] = kLoop;
}
inline void NeuralNetworkLayer::clear_loop() {
  if (_internal_has_loop()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.loop_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::LoopLayerParams* NeuralNetworkLayer::release_loop() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loop)
  if (_internal_has_loop()) {
    clear_has_layer();
      ::CoreML::Specification::LoopLayerParams* temp = layer_.loop_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.loop_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::LoopLayerParams& NeuralNetworkLayer::_internal_loop() const {
  return _internal_has_loop()
      ? *layer_.loop_
      : reinterpret_cast< ::CoreML::Specification::LoopLayerParams&>(::CoreML::Specification::_LoopLayerParams_default_instance_);
}
inline const ::CoreML::Specification::LoopLayerParams& NeuralNetworkLayer::loop() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loop)
  return _internal_loop();
}
inline ::CoreML::Specification::LoopLayerParams* NeuralNetworkLayer::unsafe_arena_release_loop() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.loop)
  if (_internal_has_loop()) {
    clear_has_layer();
    ::CoreML::Specification::LoopLayerParams* temp = layer_.loop_;
    layer_.loop_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_loop(::CoreML::Specification::LoopLayerParams* loop) {
  clear_layer();
  if (loop) {
    set_has_loop();
    layer_.loop_ = loop;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.loop)
}
inline ::CoreML::Specification::LoopLayerParams* NeuralNetworkLayer::_internal_mutable_loop() {
  if (!_internal_has_loop()) {
    clear_layer();
    set_has_loop();
    layer_.loop_ = CreateMaybeMessage< ::CoreML::Specification::LoopLayerParams >(GetArenaForAllocation());
  }
  return layer_.loop_;
}
inline ::CoreML::Specification::LoopLayerParams* NeuralNetworkLayer::mutable_loop() {
  ::CoreML::Specification::LoopLayerParams* _msg = _internal_mutable_loop();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loop)
  return _msg;
}

// .CoreML.Specification.LoopBreakLayerParams loopBreak = 620;
inline bool NeuralNetworkLayer::_internal_has_loopbreak() const {
  return layer_case() == kLoopBreak;
}
inline bool NeuralNetworkLayer::has_loopbreak() const {
  return _internal_has_loopbreak();
}
inline void NeuralNetworkLayer::set_has_loopbreak() {
  _oneof_case_[0] = kLoopBreak;
}
inline void NeuralNetworkLayer::clear_loopbreak() {
  if (_internal_has_loopbreak()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.loopbreak_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::LoopBreakLayerParams* NeuralNetworkLayer::release_loopbreak() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loopBreak)
  if (_internal_has_loopbreak()) {
    clear_has_layer();
      ::CoreML::Specification::LoopBreakLayerParams* temp = layer_.loopbreak_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.loopbreak_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::LoopBreakLayerParams& NeuralNetworkLayer::_internal_loopbreak() const {
  return _internal_has_loopbreak()
      ? *layer_.loopbreak_
      : reinterpret_cast< ::CoreML::Specification::LoopBreakLayerParams&>(::CoreML::Specification::_LoopBreakLayerParams_default_instance_);
}
inline const ::CoreML::Specification::LoopBreakLayerParams& NeuralNetworkLayer::loopbreak() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loopBreak)
  return _internal_loopbreak();
}
inline ::CoreML::Specification::LoopBreakLayerParams* NeuralNetworkLayer::unsafe_arena_release_loopbreak() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.loopBreak)
  if (_internal_has_loopbreak()) {
    clear_has_layer();
    ::CoreML::Specification::LoopBreakLayerParams* temp = layer_.loopbreak_;
    layer_.loopbreak_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_loopbreak(::CoreML::Specification::LoopBreakLayerParams* loopbreak) {
  clear_layer();
  if (loopbreak) {
    set_has_loopbreak();
    layer_.loopbreak_ = loopbreak;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.loopBreak)
}
inline ::CoreML::Specification::LoopBreakLayerParams* NeuralNetworkLayer::_internal_mutable_loopbreak() {
  if (!_internal_has_loopbreak()) {
    clear_layer();
    set_has_loopbreak();
    layer_.loopbreak_ = CreateMaybeMessage< ::CoreML::Specification::LoopBreakLayerParams >(GetArenaForAllocation());
  }
  return layer_.loopbreak_;
}
inline ::CoreML::Specification::LoopBreakLayerParams* NeuralNetworkLayer::mutable_loopbreak() {
  ::CoreML::Specification::LoopBreakLayerParams* _msg = _internal_mutable_loopbreak();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loopBreak)
  return _msg;
}

// .CoreML.Specification.LoopContinueLayerParams loopContinue = 625;
inline bool NeuralNetworkLayer::_internal_has_loopcontinue() const {
  return layer_case() == kLoopContinue;
}
inline bool NeuralNetworkLayer::has_loopcontinue() const {
  return _internal_has_loopcontinue();
}
inline void NeuralNetworkLayer::set_has_loopcontinue() {
  _oneof_case_[0] = kLoopContinue;
}
inline void NeuralNetworkLayer::clear_loopcontinue() {
  if (_internal_has_loopcontinue()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.loopcontinue_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::LoopContinueLayerParams* NeuralNetworkLayer::release_loopcontinue() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loopContinue)
  if (_internal_has_loopcontinue()) {
    clear_has_layer();
      ::CoreML::Specification::LoopContinueLayerParams* temp = layer_.loopcontinue_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.loopcontinue_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::LoopContinueLayerParams& NeuralNetworkLayer::_internal_loopcontinue() const {
  return _internal_has_loopcontinue()
      ? *layer_.loopcontinue_
      : reinterpret_cast< ::CoreML::Specification::LoopContinueLayerParams&>(::CoreML::Specification::_LoopContinueLayerParams_default_instance_);
}
inline const ::CoreML::Specification::LoopContinueLayerParams& NeuralNetworkLayer::loopcontinue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loopContinue)
  return _internal_loopcontinue();
}
inline ::CoreML::Specification::LoopContinueLayerParams* NeuralNetworkLayer::unsafe_arena_release_loopcontinue() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.loopContinue)
  if (_internal_has_loopcontinue()) {
    clear_has_layer();
    ::CoreML::Specification::LoopContinueLayerParams* temp = layer_.loopcontinue_;
    layer_.loopcontinue_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_loopcontinue(::CoreML::Specification::LoopContinueLayerParams* loopcontinue) {
  clear_layer();
  if (loopcontinue) {
    set_has_loopcontinue();
    layer_.loopcontinue_ = loopcontinue;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.loopContinue)
}
inline ::CoreML::Specification::LoopContinueLayerParams* NeuralNetworkLayer::_internal_mutable_loopcontinue() {
  if (!_internal_has_loopcontinue()) {
    clear_layer();
    set_has_loopcontinue();
    layer_.loopcontinue_ = CreateMaybeMessage< ::CoreML::Specification::LoopContinueLayerParams >(GetArenaForAllocation());
  }
  return layer_.loopcontinue_;
}
inline ::CoreML::Specification::LoopContinueLayerParams* NeuralNetworkLayer::mutable_loopcontinue() {
  ::CoreML::Specification::LoopContinueLayerParams* _msg = _internal_mutable_loopcontinue();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loopContinue)
  return _msg;
}

// .CoreML.Specification.RangeStaticLayerParams rangeStatic = 635;
inline bool NeuralNetworkLayer::_internal_has_rangestatic() const {
  return layer_case() == kRangeStatic;
}
inline bool NeuralNetworkLayer::has_rangestatic() const {
  return _internal_has_rangestatic();
}
inline void NeuralNetworkLayer::set_has_rangestatic() {
  _oneof_case_[0] = kRangeStatic;
}
inline void NeuralNetworkLayer::clear_rangestatic() {
  if (_internal_has_rangestatic()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.rangestatic_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::RangeStaticLayerParams* NeuralNetworkLayer::release_rangestatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.rangeStatic)
  if (_internal_has_rangestatic()) {
    clear_has_layer();
      ::CoreML::Specification::RangeStaticLayerParams* temp = layer_.rangestatic_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.rangestatic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::RangeStaticLayerParams& NeuralNetworkLayer::_internal_rangestatic() const {
  return _internal_has_rangestatic()
      ? *layer_.rangestatic_
      : reinterpret_cast< ::CoreML::Specification::RangeStaticLayerParams&>(::CoreML::Specification::_RangeStaticLayerParams_default_instance_);
}
inline const ::CoreML::Specification::RangeStaticLayerParams& NeuralNetworkLayer::rangestatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.rangeStatic)
  return _internal_rangestatic();
}
inline ::CoreML::Specification::RangeStaticLayerParams* NeuralNetworkLayer::unsafe_arena_release_rangestatic() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.rangeStatic)
  if (_internal_has_rangestatic()) {
    clear_has_layer();
    ::CoreML::Specification::RangeStaticLayerParams* temp = layer_.rangestatic_;
    layer_.rangestatic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_rangestatic(::CoreML::Specification::RangeStaticLayerParams* rangestatic) {
  clear_layer();
  if (rangestatic) {
    set_has_rangestatic();
    layer_.rangestatic_ = rangestatic;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.rangeStatic)
}
inline ::CoreML::Specification::RangeStaticLayerParams* NeuralNetworkLayer::_internal_mutable_rangestatic() {
  if (!_internal_has_rangestatic()) {
    clear_layer();
    set_has_rangestatic();
    layer_.rangestatic_ = CreateMaybeMessage< ::CoreML::Specification::RangeStaticLayerParams >(GetArenaForAllocation());
  }
  return layer_.rangestatic_;
}
inline ::CoreML::Specification::RangeStaticLayerParams* NeuralNetworkLayer::mutable_rangestatic() {
  ::CoreML::Specification::RangeStaticLayerParams* _msg = _internal_mutable_rangestatic();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.rangeStatic)
  return _msg;
}

// .CoreML.Specification.RangeDynamicLayerParams rangeDynamic = 640;
inline bool NeuralNetworkLayer::_internal_has_rangedynamic() const {
  return layer_case() == kRangeDynamic;
}
inline bool NeuralNetworkLayer::has_rangedynamic() const {
  return _internal_has_rangedynamic();
}
inline void NeuralNetworkLayer::set_has_rangedynamic() {
  _oneof_case_[0] = kRangeDynamic;
}
inline void NeuralNetworkLayer::clear_rangedynamic() {
  if (_internal_has_rangedynamic()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.rangedynamic_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::RangeDynamicLayerParams* NeuralNetworkLayer::release_rangedynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.rangeDynamic)
  if (_internal_has_rangedynamic()) {
    clear_has_layer();
      ::CoreML::Specification::RangeDynamicLayerParams* temp = layer_.rangedynamic_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.rangedynamic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::RangeDynamicLayerParams& NeuralNetworkLayer::_internal_rangedynamic() const {
  return _internal_has_rangedynamic()
      ? *layer_.rangedynamic_
      : reinterpret_cast< ::CoreML::Specification::RangeDynamicLayerParams&>(::CoreML::Specification::_RangeDynamicLayerParams_default_instance_);
}
inline const ::CoreML::Specification::RangeDynamicLayerParams& NeuralNetworkLayer::rangedynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.rangeDynamic)
  return _internal_rangedynamic();
}
inline ::CoreML::Specification::RangeDynamicLayerParams* NeuralNetworkLayer::unsafe_arena_release_rangedynamic() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.rangeDynamic)
  if (_internal_has_rangedynamic()) {
    clear_has_layer();
    ::CoreML::Specification::RangeDynamicLayerParams* temp = layer_.rangedynamic_;
    layer_.rangedynamic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_rangedynamic(::CoreML::Specification::RangeDynamicLayerParams* rangedynamic) {
  clear_layer();
  if (rangedynamic) {
    set_has_rangedynamic();
    layer_.rangedynamic_ = rangedynamic;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.rangeDynamic)
}
inline ::CoreML::Specification::RangeDynamicLayerParams* NeuralNetworkLayer::_internal_mutable_rangedynamic() {
  if (!_internal_has_rangedynamic()) {
    clear_layer();
    set_has_rangedynamic();
    layer_.rangedynamic_ = CreateMaybeMessage< ::CoreML::Specification::RangeDynamicLayerParams >(GetArenaForAllocation());
  }
  return layer_.rangedynamic_;
}
inline ::CoreML::Specification::RangeDynamicLayerParams* NeuralNetworkLayer::mutable_rangedynamic() {
  ::CoreML::Specification::RangeDynamicLayerParams* _msg = _internal_mutable_rangedynamic();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.rangeDynamic)
  return _msg;
}

// .CoreML.Specification.ClipLayerParams clip = 660;
inline bool NeuralNetworkLayer::_internal_has_clip() const {
  return layer_case() == kClip;
}
inline bool NeuralNetworkLayer::has_clip() const {
  return _internal_has_clip();
}
inline void NeuralNetworkLayer::set_has_clip() {
  _oneof_case_[0] = kClip;
}
inline void NeuralNetworkLayer::clear_clip() {
  if (_internal_has_clip()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.clip_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ClipLayerParams* NeuralNetworkLayer::release_clip() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.clip)
  if (_internal_has_clip()) {
    clear_has_layer();
      ::CoreML::Specification::ClipLayerParams* temp = layer_.clip_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.clip_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ClipLayerParams& NeuralNetworkLayer::_internal_clip() const {
  return _internal_has_clip()
      ? *layer_.clip_
      : reinterpret_cast< ::CoreML::Specification::ClipLayerParams&>(::CoreML::Specification::_ClipLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ClipLayerParams& NeuralNetworkLayer::clip() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.clip)
  return _internal_clip();
}
inline ::CoreML::Specification::ClipLayerParams* NeuralNetworkLayer::unsafe_arena_release_clip() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.clip)
  if (_internal_has_clip()) {
    clear_has_layer();
    ::CoreML::Specification::ClipLayerParams* temp = layer_.clip_;
    layer_.clip_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_clip(::CoreML::Specification::ClipLayerParams* clip) {
  clear_layer();
  if (clip) {
    set_has_clip();
    layer_.clip_ = clip;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.clip)
}
inline ::CoreML::Specification::ClipLayerParams* NeuralNetworkLayer::_internal_mutable_clip() {
  if (!_internal_has_clip()) {
    clear_layer();
    set_has_clip();
    layer_.clip_ = CreateMaybeMessage< ::CoreML::Specification::ClipLayerParams >(GetArenaForAllocation());
  }
  return layer_.clip_;
}
inline ::CoreML::Specification::ClipLayerParams* NeuralNetworkLayer::mutable_clip() {
  ::CoreML::Specification::ClipLayerParams* _msg = _internal_mutable_clip();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.clip)
  return _msg;
}

// .CoreML.Specification.CeilLayerParams ceil = 665;
inline bool NeuralNetworkLayer::_internal_has_ceil() const {
  return layer_case() == kCeil;
}
inline bool NeuralNetworkLayer::has_ceil() const {
  return _internal_has_ceil();
}
inline void NeuralNetworkLayer::set_has_ceil() {
  _oneof_case_[0] = kCeil;
}
inline void NeuralNetworkLayer::clear_ceil() {
  if (_internal_has_ceil()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.ceil_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::CeilLayerParams* NeuralNetworkLayer::release_ceil() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.ceil)
  if (_internal_has_ceil()) {
    clear_has_layer();
      ::CoreML::Specification::CeilLayerParams* temp = layer_.ceil_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.ceil_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::CeilLayerParams& NeuralNetworkLayer::_internal_ceil() const {
  return _internal_has_ceil()
      ? *layer_.ceil_
      : reinterpret_cast< ::CoreML::Specification::CeilLayerParams&>(::CoreML::Specification::_CeilLayerParams_default_instance_);
}
inline const ::CoreML::Specification::CeilLayerParams& NeuralNetworkLayer::ceil() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.ceil)
  return _internal_ceil();
}
inline ::CoreML::Specification::CeilLayerParams* NeuralNetworkLayer::unsafe_arena_release_ceil() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.ceil)
  if (_internal_has_ceil()) {
    clear_has_layer();
    ::CoreML::Specification::CeilLayerParams* temp = layer_.ceil_;
    layer_.ceil_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_ceil(::CoreML::Specification::CeilLayerParams* ceil) {
  clear_layer();
  if (ceil) {
    set_has_ceil();
    layer_.ceil_ = ceil;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.ceil)
}
inline ::CoreML::Specification::CeilLayerParams* NeuralNetworkLayer::_internal_mutable_ceil() {
  if (!_internal_has_ceil()) {
    clear_layer();
    set_has_ceil();
    layer_.ceil_ = CreateMaybeMessage< ::CoreML::Specification::CeilLayerParams >(GetArenaForAllocation());
  }
  return layer_.ceil_;
}
inline ::CoreML::Specification::CeilLayerParams* NeuralNetworkLayer::mutable_ceil() {
  ::CoreML::Specification::CeilLayerParams* _msg = _internal_mutable_ceil();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.ceil)
  return _msg;
}

// .CoreML.Specification.FloorLayerParams floor = 670;
inline bool NeuralNetworkLayer::_internal_has_floor() const {
  return layer_case() == kFloor;
}
inline bool NeuralNetworkLayer::has_floor() const {
  return _internal_has_floor();
}
inline void NeuralNetworkLayer::set_has_floor() {
  _oneof_case_[0] = kFloor;
}
inline void NeuralNetworkLayer::clear_floor() {
  if (_internal_has_floor()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.floor_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::FloorLayerParams* NeuralNetworkLayer::release_floor() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.floor)
  if (_internal_has_floor()) {
    clear_has_layer();
      ::CoreML::Specification::FloorLayerParams* temp = layer_.floor_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.floor_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::FloorLayerParams& NeuralNetworkLayer::_internal_floor() const {
  return _internal_has_floor()
      ? *layer_.floor_
      : reinterpret_cast< ::CoreML::Specification::FloorLayerParams&>(::CoreML::Specification::_FloorLayerParams_default_instance_);
}
inline const ::CoreML::Specification::FloorLayerParams& NeuralNetworkLayer::floor() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.floor)
  return _internal_floor();
}
inline ::CoreML::Specification::FloorLayerParams* NeuralNetworkLayer::unsafe_arena_release_floor() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.floor)
  if (_internal_has_floor()) {
    clear_has_layer();
    ::CoreML::Specification::FloorLayerParams* temp = layer_.floor_;
    layer_.floor_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_floor(::CoreML::Specification::FloorLayerParams* floor) {
  clear_layer();
  if (floor) {
    set_has_floor();
    layer_.floor_ = floor;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.floor)
}
inline ::CoreML::Specification::FloorLayerParams* NeuralNetworkLayer::_internal_mutable_floor() {
  if (!_internal_has_floor()) {
    clear_layer();
    set_has_floor();
    layer_.floor_ = CreateMaybeMessage< ::CoreML::Specification::FloorLayerParams >(GetArenaForAllocation());
  }
  return layer_.floor_;
}
inline ::CoreML::Specification::FloorLayerParams* NeuralNetworkLayer::mutable_floor() {
  ::CoreML::Specification::FloorLayerParams* _msg = _internal_mutable_floor();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.floor)
  return _msg;
}

// .CoreML.Specification.SignLayerParams sign = 680;
inline bool NeuralNetworkLayer::_internal_has_sign() const {
  return layer_case() == kSign;
}
inline bool NeuralNetworkLayer::has_sign() const {
  return _internal_has_sign();
}
inline void NeuralNetworkLayer::set_has_sign() {
  _oneof_case_[0] = kSign;
}
inline void NeuralNetworkLayer::clear_sign() {
  if (_internal_has_sign()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.sign_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::SignLayerParams* NeuralNetworkLayer::release_sign() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sign)
  if (_internal_has_sign()) {
    clear_has_layer();
      ::CoreML::Specification::SignLayerParams* temp = layer_.sign_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.sign_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::SignLayerParams& NeuralNetworkLayer::_internal_sign() const {
  return _internal_has_sign()
      ? *layer_.sign_
      : reinterpret_cast< ::CoreML::Specification::SignLayerParams&>(::CoreML::Specification::_SignLayerParams_default_instance_);
}
inline const ::CoreML::Specification::SignLayerParams& NeuralNetworkLayer::sign() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sign)
  return _internal_sign();
}
inline ::CoreML::Specification::SignLayerParams* NeuralNetworkLayer::unsafe_arena_release_sign() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.sign)
  if (_internal_has_sign()) {
    clear_has_layer();
    ::CoreML::Specification::SignLayerParams* temp = layer_.sign_;
    layer_.sign_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_sign(::CoreML::Specification::SignLayerParams* sign) {
  clear_layer();
  if (sign) {
    set_has_sign();
    layer_.sign_ = sign;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.sign)
}
inline ::CoreML::Specification::SignLayerParams* NeuralNetworkLayer::_internal_mutable_sign() {
  if (!_internal_has_sign()) {
    clear_layer();
    set_has_sign();
    layer_.sign_ = CreateMaybeMessage< ::CoreML::Specification::SignLayerParams >(GetArenaForAllocation());
  }
  return layer_.sign_;
}
inline ::CoreML::Specification::SignLayerParams* NeuralNetworkLayer::mutable_sign() {
  ::CoreML::Specification::SignLayerParams* _msg = _internal_mutable_sign();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sign)
  return _msg;
}

// .CoreML.Specification.RoundLayerParams round = 685;
inline bool NeuralNetworkLayer::_internal_has_round() const {
  return layer_case() == kRound;
}
inline bool NeuralNetworkLayer::has_round() const {
  return _internal_has_round();
}
inline void NeuralNetworkLayer::set_has_round() {
  _oneof_case_[0] = kRound;
}
inline void NeuralNetworkLayer::clear_round() {
  if (_internal_has_round()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.round_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::RoundLayerParams* NeuralNetworkLayer::release_round() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.round)
  if (_internal_has_round()) {
    clear_has_layer();
      ::CoreML::Specification::RoundLayerParams* temp = layer_.round_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.round_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::RoundLayerParams& NeuralNetworkLayer::_internal_round() const {
  return _internal_has_round()
      ? *layer_.round_
      : reinterpret_cast< ::CoreML::Specification::RoundLayerParams&>(::CoreML::Specification::_RoundLayerParams_default_instance_);
}
inline const ::CoreML::Specification::RoundLayerParams& NeuralNetworkLayer::round() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.round)
  return _internal_round();
}
inline ::CoreML::Specification::RoundLayerParams* NeuralNetworkLayer::unsafe_arena_release_round() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.round)
  if (_internal_has_round()) {
    clear_has_layer();
    ::CoreML::Specification::RoundLayerParams* temp = layer_.round_;
    layer_.round_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_round(::CoreML::Specification::RoundLayerParams* round) {
  clear_layer();
  if (round) {
    set_has_round();
    layer_.round_ = round;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.round)
}
inline ::CoreML::Specification::RoundLayerParams* NeuralNetworkLayer::_internal_mutable_round() {
  if (!_internal_has_round()) {
    clear_layer();
    set_has_round();
    layer_.round_ = CreateMaybeMessage< ::CoreML::Specification::RoundLayerParams >(GetArenaForAllocation());
  }
  return layer_.round_;
}
inline ::CoreML::Specification::RoundLayerParams* NeuralNetworkLayer::mutable_round() {
  ::CoreML::Specification::RoundLayerParams* _msg = _internal_mutable_round();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.round)
  return _msg;
}

// .CoreML.Specification.Exp2LayerParams exp2 = 700;
inline bool NeuralNetworkLayer::_internal_has_exp2() const {
  return layer_case() == kExp2;
}
inline bool NeuralNetworkLayer::has_exp2() const {
  return _internal_has_exp2();
}
inline void NeuralNetworkLayer::set_has_exp2() {
  _oneof_case_[0] = kExp2;
}
inline void NeuralNetworkLayer::clear_exp2() {
  if (_internal_has_exp2()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.exp2_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::Exp2LayerParams* NeuralNetworkLayer::release_exp2() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.exp2)
  if (_internal_has_exp2()) {
    clear_has_layer();
      ::CoreML::Specification::Exp2LayerParams* temp = layer_.exp2_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.exp2_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::Exp2LayerParams& NeuralNetworkLayer::_internal_exp2() const {
  return _internal_has_exp2()
      ? *layer_.exp2_
      : reinterpret_cast< ::CoreML::Specification::Exp2LayerParams&>(::CoreML::Specification::_Exp2LayerParams_default_instance_);
}
inline const ::CoreML::Specification::Exp2LayerParams& NeuralNetworkLayer::exp2() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.exp2)
  return _internal_exp2();
}
inline ::CoreML::Specification::Exp2LayerParams* NeuralNetworkLayer::unsafe_arena_release_exp2() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.exp2)
  if (_internal_has_exp2()) {
    clear_has_layer();
    ::CoreML::Specification::Exp2LayerParams* temp = layer_.exp2_;
    layer_.exp2_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_exp2(::CoreML::Specification::Exp2LayerParams* exp2) {
  clear_layer();
  if (exp2) {
    set_has_exp2();
    layer_.exp2_ = exp2;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.exp2)
}
inline ::CoreML::Specification::Exp2LayerParams* NeuralNetworkLayer::_internal_mutable_exp2() {
  if (!_internal_has_exp2()) {
    clear_layer();
    set_has_exp2();
    layer_.exp2_ = CreateMaybeMessage< ::CoreML::Specification::Exp2LayerParams >(GetArenaForAllocation());
  }
  return layer_.exp2_;
}
inline ::CoreML::Specification::Exp2LayerParams* NeuralNetworkLayer::mutable_exp2() {
  ::CoreML::Specification::Exp2LayerParams* _msg = _internal_mutable_exp2();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.exp2)
  return _msg;
}

// .CoreML.Specification.SinLayerParams sin = 710;
inline bool NeuralNetworkLayer::_internal_has_sin() const {
  return layer_case() == kSin;
}
inline bool NeuralNetworkLayer::has_sin() const {
  return _internal_has_sin();
}
inline void NeuralNetworkLayer::set_has_sin() {
  _oneof_case_[0] = kSin;
}
inline void NeuralNetworkLayer::clear_sin() {
  if (_internal_has_sin()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.sin_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::SinLayerParams* NeuralNetworkLayer::release_sin() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sin)
  if (_internal_has_sin()) {
    clear_has_layer();
      ::CoreML::Specification::SinLayerParams* temp = layer_.sin_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.sin_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::SinLayerParams& NeuralNetworkLayer::_internal_sin() const {
  return _internal_has_sin()
      ? *layer_.sin_
      : reinterpret_cast< ::CoreML::Specification::SinLayerParams&>(::CoreML::Specification::_SinLayerParams_default_instance_);
}
inline const ::CoreML::Specification::SinLayerParams& NeuralNetworkLayer::sin() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sin)
  return _internal_sin();
}
inline ::CoreML::Specification::SinLayerParams* NeuralNetworkLayer::unsafe_arena_release_sin() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.sin)
  if (_internal_has_sin()) {
    clear_has_layer();
    ::CoreML::Specification::SinLayerParams* temp = layer_.sin_;
    layer_.sin_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_sin(::CoreML::Specification::SinLayerParams* sin) {
  clear_layer();
  if (sin) {
    set_has_sin();
    layer_.sin_ = sin;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.sin)
}
inline ::CoreML::Specification::SinLayerParams* NeuralNetworkLayer::_internal_mutable_sin() {
  if (!_internal_has_sin()) {
    clear_layer();
    set_has_sin();
    layer_.sin_ = CreateMaybeMessage< ::CoreML::Specification::SinLayerParams >(GetArenaForAllocation());
  }
  return layer_.sin_;
}
inline ::CoreML::Specification::SinLayerParams* NeuralNetworkLayer::mutable_sin() {
  ::CoreML::Specification::SinLayerParams* _msg = _internal_mutable_sin();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sin)
  return _msg;
}

// .CoreML.Specification.CosLayerParams cos = 715;
inline bool NeuralNetworkLayer::_internal_has_cos() const {
  return layer_case() == kCos;
}
inline bool NeuralNetworkLayer::has_cos() const {
  return _internal_has_cos();
}
inline void NeuralNetworkLayer::set_has_cos() {
  _oneof_case_[0] = kCos;
}
inline void NeuralNetworkLayer::clear_cos() {
  if (_internal_has_cos()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.cos_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::CosLayerParams* NeuralNetworkLayer::release_cos() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.cos)
  if (_internal_has_cos()) {
    clear_has_layer();
      ::CoreML::Specification::CosLayerParams* temp = layer_.cos_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.cos_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::CosLayerParams& NeuralNetworkLayer::_internal_cos() const {
  return _internal_has_cos()
      ? *layer_.cos_
      : reinterpret_cast< ::CoreML::Specification::CosLayerParams&>(::CoreML::Specification::_CosLayerParams_default_instance_);
}
inline const ::CoreML::Specification::CosLayerParams& NeuralNetworkLayer::cos() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.cos)
  return _internal_cos();
}
inline ::CoreML::Specification::CosLayerParams* NeuralNetworkLayer::unsafe_arena_release_cos() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.cos)
  if (_internal_has_cos()) {
    clear_has_layer();
    ::CoreML::Specification::CosLayerParams* temp = layer_.cos_;
    layer_.cos_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_cos(::CoreML::Specification::CosLayerParams* cos) {
  clear_layer();
  if (cos) {
    set_has_cos();
    layer_.cos_ = cos;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.cos)
}
inline ::CoreML::Specification::CosLayerParams* NeuralNetworkLayer::_internal_mutable_cos() {
  if (!_internal_has_cos()) {
    clear_layer();
    set_has_cos();
    layer_.cos_ = CreateMaybeMessage< ::CoreML::Specification::CosLayerParams >(GetArenaForAllocation());
  }
  return layer_.cos_;
}
inline ::CoreML::Specification::CosLayerParams* NeuralNetworkLayer::mutable_cos() {
  ::CoreML::Specification::CosLayerParams* _msg = _internal_mutable_cos();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.cos)
  return _msg;
}

// .CoreML.Specification.TanLayerParams tan = 720;
inline bool NeuralNetworkLayer::_internal_has_tan() const {
  return layer_case() == kTan;
}
inline bool NeuralNetworkLayer::has_tan() const {
  return _internal_has_tan();
}
inline void NeuralNetworkLayer::set_has_tan() {
  _oneof_case_[0] = kTan;
}
inline void NeuralNetworkLayer::clear_tan() {
  if (_internal_has_tan()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.tan_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::TanLayerParams* NeuralNetworkLayer::release_tan() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.tan)
  if (_internal_has_tan()) {
    clear_has_layer();
      ::CoreML::Specification::TanLayerParams* temp = layer_.tan_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.tan_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::TanLayerParams& NeuralNetworkLayer::_internal_tan() const {
  return _internal_has_tan()
      ? *layer_.tan_
      : reinterpret_cast< ::CoreML::Specification::TanLayerParams&>(::CoreML::Specification::_TanLayerParams_default_instance_);
}
inline const ::CoreML::Specification::TanLayerParams& NeuralNetworkLayer::tan() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.tan)
  return _internal_tan();
}
inline ::CoreML::Specification::TanLayerParams* NeuralNetworkLayer::unsafe_arena_release_tan() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.tan)
  if (_internal_has_tan()) {
    clear_has_layer();
    ::CoreML::Specification::TanLayerParams* temp = layer_.tan_;
    layer_.tan_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_tan(::CoreML::Specification::TanLayerParams* tan) {
  clear_layer();
  if (tan) {
    set_has_tan();
    layer_.tan_ = tan;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.tan)
}
inline ::CoreML::Specification::TanLayerParams* NeuralNetworkLayer::_internal_mutable_tan() {
  if (!_internal_has_tan()) {
    clear_layer();
    set_has_tan();
    layer_.tan_ = CreateMaybeMessage< ::CoreML::Specification::TanLayerParams >(GetArenaForAllocation());
  }
  return layer_.tan_;
}
inline ::CoreML::Specification::TanLayerParams* NeuralNetworkLayer::mutable_tan() {
  ::CoreML::Specification::TanLayerParams* _msg = _internal_mutable_tan();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.tan)
  return _msg;
}

// .CoreML.Specification.AsinLayerParams asin = 730;
inline bool NeuralNetworkLayer::_internal_has_asin() const {
  return layer_case() == kAsin;
}
inline bool NeuralNetworkLayer::has_asin() const {
  return _internal_has_asin();
}
inline void NeuralNetworkLayer::set_has_asin() {
  _oneof_case_[0] = kAsin;
}
inline void NeuralNetworkLayer::clear_asin() {
  if (_internal_has_asin()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.asin_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::AsinLayerParams* NeuralNetworkLayer::release_asin() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.asin)
  if (_internal_has_asin()) {
    clear_has_layer();
      ::CoreML::Specification::AsinLayerParams* temp = layer_.asin_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.asin_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::AsinLayerParams& NeuralNetworkLayer::_internal_asin() const {
  return _internal_has_asin()
      ? *layer_.asin_
      : reinterpret_cast< ::CoreML::Specification::AsinLayerParams&>(::CoreML::Specification::_AsinLayerParams_default_instance_);
}
inline const ::CoreML::Specification::AsinLayerParams& NeuralNetworkLayer::asin() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.asin)
  return _internal_asin();
}
inline ::CoreML::Specification::AsinLayerParams* NeuralNetworkLayer::unsafe_arena_release_asin() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.asin)
  if (_internal_has_asin()) {
    clear_has_layer();
    ::CoreML::Specification::AsinLayerParams* temp = layer_.asin_;
    layer_.asin_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_asin(::CoreML::Specification::AsinLayerParams* asin) {
  clear_layer();
  if (asin) {
    set_has_asin();
    layer_.asin_ = asin;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.asin)
}
inline ::CoreML::Specification::AsinLayerParams* NeuralNetworkLayer::_internal_mutable_asin() {
  if (!_internal_has_asin()) {
    clear_layer();
    set_has_asin();
    layer_.asin_ = CreateMaybeMessage< ::CoreML::Specification::AsinLayerParams >(GetArenaForAllocation());
  }
  return layer_.asin_;
}
inline ::CoreML::Specification::AsinLayerParams* NeuralNetworkLayer::mutable_asin() {
  ::CoreML::Specification::AsinLayerParams* _msg = _internal_mutable_asin();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.asin)
  return _msg;
}

// .CoreML.Specification.AcosLayerParams acos = 735;
inline bool NeuralNetworkLayer::_internal_has_acos() const {
  return layer_case() == kAcos;
}
inline bool NeuralNetworkLayer::has_acos() const {
  return _internal_has_acos();
}
inline void NeuralNetworkLayer::set_has_acos() {
  _oneof_case_[0] = kAcos;
}
inline void NeuralNetworkLayer::clear_acos() {
  if (_internal_has_acos()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.acos_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::AcosLayerParams* NeuralNetworkLayer::release_acos() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.acos)
  if (_internal_has_acos()) {
    clear_has_layer();
      ::CoreML::Specification::AcosLayerParams* temp = layer_.acos_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.acos_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::AcosLayerParams& NeuralNetworkLayer::_internal_acos() const {
  return _internal_has_acos()
      ? *layer_.acos_
      : reinterpret_cast< ::CoreML::Specification::AcosLayerParams&>(::CoreML::Specification::_AcosLayerParams_default_instance_);
}
inline const ::CoreML::Specification::AcosLayerParams& NeuralNetworkLayer::acos() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.acos)
  return _internal_acos();
}
inline ::CoreML::Specification::AcosLayerParams* NeuralNetworkLayer::unsafe_arena_release_acos() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.acos)
  if (_internal_has_acos()) {
    clear_has_layer();
    ::CoreML::Specification::AcosLayerParams* temp = layer_.acos_;
    layer_.acos_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_acos(::CoreML::Specification::AcosLayerParams* acos) {
  clear_layer();
  if (acos) {
    set_has_acos();
    layer_.acos_ = acos;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.acos)
}
inline ::CoreML::Specification::AcosLayerParams* NeuralNetworkLayer::_internal_mutable_acos() {
  if (!_internal_has_acos()) {
    clear_layer();
    set_has_acos();
    layer_.acos_ = CreateMaybeMessage< ::CoreML::Specification::AcosLayerParams >(GetArenaForAllocation());
  }
  return layer_.acos_;
}
inline ::CoreML::Specification::AcosLayerParams* NeuralNetworkLayer::mutable_acos() {
  ::CoreML::Specification::AcosLayerParams* _msg = _internal_mutable_acos();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.acos)
  return _msg;
}

// .CoreML.Specification.AtanLayerParams atan = 740;
inline bool NeuralNetworkLayer::_internal_has_atan() const {
  return layer_case() == kAtan;
}
inline bool NeuralNetworkLayer::has_atan() const {
  return _internal_has_atan();
}
inline void NeuralNetworkLayer::set_has_atan() {
  _oneof_case_[0] = kAtan;
}
inline void NeuralNetworkLayer::clear_atan() {
  if (_internal_has_atan()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.atan_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::AtanLayerParams* NeuralNetworkLayer::release_atan() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.atan)
  if (_internal_has_atan()) {
    clear_has_layer();
      ::CoreML::Specification::AtanLayerParams* temp = layer_.atan_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.atan_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::AtanLayerParams& NeuralNetworkLayer::_internal_atan() const {
  return _internal_has_atan()
      ? *layer_.atan_
      : reinterpret_cast< ::CoreML::Specification::AtanLayerParams&>(::CoreML::Specification::_AtanLayerParams_default_instance_);
}
inline const ::CoreML::Specification::AtanLayerParams& NeuralNetworkLayer::atan() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.atan)
  return _internal_atan();
}
inline ::CoreML::Specification::AtanLayerParams* NeuralNetworkLayer::unsafe_arena_release_atan() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.atan)
  if (_internal_has_atan()) {
    clear_has_layer();
    ::CoreML::Specification::AtanLayerParams* temp = layer_.atan_;
    layer_.atan_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_atan(::CoreML::Specification::AtanLayerParams* atan) {
  clear_layer();
  if (atan) {
    set_has_atan();
    layer_.atan_ = atan;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.atan)
}
inline ::CoreML::Specification::AtanLayerParams* NeuralNetworkLayer::_internal_mutable_atan() {
  if (!_internal_has_atan()) {
    clear_layer();
    set_has_atan();
    layer_.atan_ = CreateMaybeMessage< ::CoreML::Specification::AtanLayerParams >(GetArenaForAllocation());
  }
  return layer_.atan_;
}
inline ::CoreML::Specification::AtanLayerParams* NeuralNetworkLayer::mutable_atan() {
  ::CoreML::Specification::AtanLayerParams* _msg = _internal_mutable_atan();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.atan)
  return _msg;
}

// .CoreML.Specification.SinhLayerParams sinh = 750;
inline bool NeuralNetworkLayer::_internal_has_sinh() const {
  return layer_case() == kSinh;
}
inline bool NeuralNetworkLayer::has_sinh() const {
  return _internal_has_sinh();
}
inline void NeuralNetworkLayer::set_has_sinh() {
  _oneof_case_[0] = kSinh;
}
inline void NeuralNetworkLayer::clear_sinh() {
  if (_internal_has_sinh()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.sinh_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::SinhLayerParams* NeuralNetworkLayer::release_sinh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sinh)
  if (_internal_has_sinh()) {
    clear_has_layer();
      ::CoreML::Specification::SinhLayerParams* temp = layer_.sinh_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.sinh_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::SinhLayerParams& NeuralNetworkLayer::_internal_sinh() const {
  return _internal_has_sinh()
      ? *layer_.sinh_
      : reinterpret_cast< ::CoreML::Specification::SinhLayerParams&>(::CoreML::Specification::_SinhLayerParams_default_instance_);
}
inline const ::CoreML::Specification::SinhLayerParams& NeuralNetworkLayer::sinh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sinh)
  return _internal_sinh();
}
inline ::CoreML::Specification::SinhLayerParams* NeuralNetworkLayer::unsafe_arena_release_sinh() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.sinh)
  if (_internal_has_sinh()) {
    clear_has_layer();
    ::CoreML::Specification::SinhLayerParams* temp = layer_.sinh_;
    layer_.sinh_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_sinh(::CoreML::Specification::SinhLayerParams* sinh) {
  clear_layer();
  if (sinh) {
    set_has_sinh();
    layer_.sinh_ = sinh;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.sinh)
}
inline ::CoreML::Specification::SinhLayerParams* NeuralNetworkLayer::_internal_mutable_sinh() {
  if (!_internal_has_sinh()) {
    clear_layer();
    set_has_sinh();
    layer_.sinh_ = CreateMaybeMessage< ::CoreML::Specification::SinhLayerParams >(GetArenaForAllocation());
  }
  return layer_.sinh_;
}
inline ::CoreML::Specification::SinhLayerParams* NeuralNetworkLayer::mutable_sinh() {
  ::CoreML::Specification::SinhLayerParams* _msg = _internal_mutable_sinh();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sinh)
  return _msg;
}

// .CoreML.Specification.CoshLayerParams cosh = 755;
inline bool NeuralNetworkLayer::_internal_has_cosh() const {
  return layer_case() == kCosh;
}
inline bool NeuralNetworkLayer::has_cosh() const {
  return _internal_has_cosh();
}
inline void NeuralNetworkLayer::set_has_cosh() {
  _oneof_case_[0] = kCosh;
}
inline void NeuralNetworkLayer::clear_cosh() {
  if (_internal_has_cosh()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.cosh_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::CoshLayerParams* NeuralNetworkLayer::release_cosh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.cosh)
  if (_internal_has_cosh()) {
    clear_has_layer();
      ::CoreML::Specification::CoshLayerParams* temp = layer_.cosh_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.cosh_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::CoshLayerParams& NeuralNetworkLayer::_internal_cosh() const {
  return _internal_has_cosh()
      ? *layer_.cosh_
      : reinterpret_cast< ::CoreML::Specification::CoshLayerParams&>(::CoreML::Specification::_CoshLayerParams_default_instance_);
}
inline const ::CoreML::Specification::CoshLayerParams& NeuralNetworkLayer::cosh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.cosh)
  return _internal_cosh();
}
inline ::CoreML::Specification::CoshLayerParams* NeuralNetworkLayer::unsafe_arena_release_cosh() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.cosh)
  if (_internal_has_cosh()) {
    clear_has_layer();
    ::CoreML::Specification::CoshLayerParams* temp = layer_.cosh_;
    layer_.cosh_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_cosh(::CoreML::Specification::CoshLayerParams* cosh) {
  clear_layer();
  if (cosh) {
    set_has_cosh();
    layer_.cosh_ = cosh;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.cosh)
}
inline ::CoreML::Specification::CoshLayerParams* NeuralNetworkLayer::_internal_mutable_cosh() {
  if (!_internal_has_cosh()) {
    clear_layer();
    set_has_cosh();
    layer_.cosh_ = CreateMaybeMessage< ::CoreML::Specification::CoshLayerParams >(GetArenaForAllocation());
  }
  return layer_.cosh_;
}
inline ::CoreML::Specification::CoshLayerParams* NeuralNetworkLayer::mutable_cosh() {
  ::CoreML::Specification::CoshLayerParams* _msg = _internal_mutable_cosh();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.cosh)
  return _msg;
}

// .CoreML.Specification.TanhLayerParams tanh = 760;
inline bool NeuralNetworkLayer::_internal_has_tanh() const {
  return layer_case() == kTanh;
}
inline bool NeuralNetworkLayer::has_tanh() const {
  return _internal_has_tanh();
}
inline void NeuralNetworkLayer::set_has_tanh() {
  _oneof_case_[0] = kTanh;
}
inline void NeuralNetworkLayer::clear_tanh() {
  if (_internal_has_tanh()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.tanh_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::TanhLayerParams* NeuralNetworkLayer::release_tanh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.tanh)
  if (_internal_has_tanh()) {
    clear_has_layer();
      ::CoreML::Specification::TanhLayerParams* temp = layer_.tanh_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.tanh_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::TanhLayerParams& NeuralNetworkLayer::_internal_tanh() const {
  return _internal_has_tanh()
      ? *layer_.tanh_
      : reinterpret_cast< ::CoreML::Specification::TanhLayerParams&>(::CoreML::Specification::_TanhLayerParams_default_instance_);
}
inline const ::CoreML::Specification::TanhLayerParams& NeuralNetworkLayer::tanh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.tanh)
  return _internal_tanh();
}
inline ::CoreML::Specification::TanhLayerParams* NeuralNetworkLayer::unsafe_arena_release_tanh() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.tanh)
  if (_internal_has_tanh()) {
    clear_has_layer();
    ::CoreML::Specification::TanhLayerParams* temp = layer_.tanh_;
    layer_.tanh_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_tanh(::CoreML::Specification::TanhLayerParams* tanh) {
  clear_layer();
  if (tanh) {
    set_has_tanh();
    layer_.tanh_ = tanh;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.tanh)
}
inline ::CoreML::Specification::TanhLayerParams* NeuralNetworkLayer::_internal_mutable_tanh() {
  if (!_internal_has_tanh()) {
    clear_layer();
    set_has_tanh();
    layer_.tanh_ = CreateMaybeMessage< ::CoreML::Specification::TanhLayerParams >(GetArenaForAllocation());
  }
  return layer_.tanh_;
}
inline ::CoreML::Specification::TanhLayerParams* NeuralNetworkLayer::mutable_tanh() {
  ::CoreML::Specification::TanhLayerParams* _msg = _internal_mutable_tanh();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.tanh)
  return _msg;
}

// .CoreML.Specification.AsinhLayerParams asinh = 770;
inline bool NeuralNetworkLayer::_internal_has_asinh() const {
  return layer_case() == kAsinh;
}
inline bool NeuralNetworkLayer::has_asinh() const {
  return _internal_has_asinh();
}
inline void NeuralNetworkLayer::set_has_asinh() {
  _oneof_case_[0] = kAsinh;
}
inline void NeuralNetworkLayer::clear_asinh() {
  if (_internal_has_asinh()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.asinh_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::AsinhLayerParams* NeuralNetworkLayer::release_asinh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.asinh)
  if (_internal_has_asinh()) {
    clear_has_layer();
      ::CoreML::Specification::AsinhLayerParams* temp = layer_.asinh_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.asinh_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::AsinhLayerParams& NeuralNetworkLayer::_internal_asinh() const {
  return _internal_has_asinh()
      ? *layer_.asinh_
      : reinterpret_cast< ::CoreML::Specification::AsinhLayerParams&>(::CoreML::Specification::_AsinhLayerParams_default_instance_);
}
inline const ::CoreML::Specification::AsinhLayerParams& NeuralNetworkLayer::asinh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.asinh)
  return _internal_asinh();
}
inline ::CoreML::Specification::AsinhLayerParams* NeuralNetworkLayer::unsafe_arena_release_asinh() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.asinh)
  if (_internal_has_asinh()) {
    clear_has_layer();
    ::CoreML::Specification::AsinhLayerParams* temp = layer_.asinh_;
    layer_.asinh_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_asinh(::CoreML::Specification::AsinhLayerParams* asinh) {
  clear_layer();
  if (asinh) {
    set_has_asinh();
    layer_.asinh_ = asinh;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.asinh)
}
inline ::CoreML::Specification::AsinhLayerParams* NeuralNetworkLayer::_internal_mutable_asinh() {
  if (!_internal_has_asinh()) {
    clear_layer();
    set_has_asinh();
    layer_.asinh_ = CreateMaybeMessage< ::CoreML::Specification::AsinhLayerParams >(GetArenaForAllocation());
  }
  return layer_.asinh_;
}
inline ::CoreML::Specification::AsinhLayerParams* NeuralNetworkLayer::mutable_asinh() {
  ::CoreML::Specification::AsinhLayerParams* _msg = _internal_mutable_asinh();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.asinh)
  return _msg;
}

// .CoreML.Specification.AcoshLayerParams acosh = 775;
inline bool NeuralNetworkLayer::_internal_has_acosh() const {
  return layer_case() == kAcosh;
}
inline bool NeuralNetworkLayer::has_acosh() const {
  return _internal_has_acosh();
}
inline void NeuralNetworkLayer::set_has_acosh() {
  _oneof_case_[0] = kAcosh;
}
inline void NeuralNetworkLayer::clear_acosh() {
  if (_internal_has_acosh()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.acosh_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::AcoshLayerParams* NeuralNetworkLayer::release_acosh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.acosh)
  if (_internal_has_acosh()) {
    clear_has_layer();
      ::CoreML::Specification::AcoshLayerParams* temp = layer_.acosh_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.acosh_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::AcoshLayerParams& NeuralNetworkLayer::_internal_acosh() const {
  return _internal_has_acosh()
      ? *layer_.acosh_
      : reinterpret_cast< ::CoreML::Specification::AcoshLayerParams&>(::CoreML::Specification::_AcoshLayerParams_default_instance_);
}
inline const ::CoreML::Specification::AcoshLayerParams& NeuralNetworkLayer::acosh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.acosh)
  return _internal_acosh();
}
inline ::CoreML::Specification::AcoshLayerParams* NeuralNetworkLayer::unsafe_arena_release_acosh() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.acosh)
  if (_internal_has_acosh()) {
    clear_has_layer();
    ::CoreML::Specification::AcoshLayerParams* temp = layer_.acosh_;
    layer_.acosh_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_acosh(::CoreML::Specification::AcoshLayerParams* acosh) {
  clear_layer();
  if (acosh) {
    set_has_acosh();
    layer_.acosh_ = acosh;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.acosh)
}
inline ::CoreML::Specification::AcoshLayerParams* NeuralNetworkLayer::_internal_mutable_acosh() {
  if (!_internal_has_acosh()) {
    clear_layer();
    set_has_acosh();
    layer_.acosh_ = CreateMaybeMessage< ::CoreML::Specification::AcoshLayerParams >(GetArenaForAllocation());
  }
  return layer_.acosh_;
}
inline ::CoreML::Specification::AcoshLayerParams* NeuralNetworkLayer::mutable_acosh() {
  ::CoreML::Specification::AcoshLayerParams* _msg = _internal_mutable_acosh();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.acosh)
  return _msg;
}

// .CoreML.Specification.AtanhLayerParams atanh = 780;
inline bool NeuralNetworkLayer::_internal_has_atanh() const {
  return layer_case() == kAtanh;
}
inline bool NeuralNetworkLayer::has_atanh() const {
  return _internal_has_atanh();
}
inline void NeuralNetworkLayer::set_has_atanh() {
  _oneof_case_[0] = kAtanh;
}
inline void NeuralNetworkLayer::clear_atanh() {
  if (_internal_has_atanh()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.atanh_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::AtanhLayerParams* NeuralNetworkLayer::release_atanh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.atanh)
  if (_internal_has_atanh()) {
    clear_has_layer();
      ::CoreML::Specification::AtanhLayerParams* temp = layer_.atanh_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.atanh_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::AtanhLayerParams& NeuralNetworkLayer::_internal_atanh() const {
  return _internal_has_atanh()
      ? *layer_.atanh_
      : reinterpret_cast< ::CoreML::Specification::AtanhLayerParams&>(::CoreML::Specification::_AtanhLayerParams_default_instance_);
}
inline const ::CoreML::Specification::AtanhLayerParams& NeuralNetworkLayer::atanh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.atanh)
  return _internal_atanh();
}
inline ::CoreML::Specification::AtanhLayerParams* NeuralNetworkLayer::unsafe_arena_release_atanh() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.atanh)
  if (_internal_has_atanh()) {
    clear_has_layer();
    ::CoreML::Specification::AtanhLayerParams* temp = layer_.atanh_;
    layer_.atanh_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_atanh(::CoreML::Specification::AtanhLayerParams* atanh) {
  clear_layer();
  if (atanh) {
    set_has_atanh();
    layer_.atanh_ = atanh;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.atanh)
}
inline ::CoreML::Specification::AtanhLayerParams* NeuralNetworkLayer::_internal_mutable_atanh() {
  if (!_internal_has_atanh()) {
    clear_layer();
    set_has_atanh();
    layer_.atanh_ = CreateMaybeMessage< ::CoreML::Specification::AtanhLayerParams >(GetArenaForAllocation());
  }
  return layer_.atanh_;
}
inline ::CoreML::Specification::AtanhLayerParams* NeuralNetworkLayer::mutable_atanh() {
  ::CoreML::Specification::AtanhLayerParams* _msg = _internal_mutable_atanh();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.atanh)
  return _msg;
}

// .CoreML.Specification.ErfLayerParams erf = 790;
inline bool NeuralNetworkLayer::_internal_has_erf() const {
  return layer_case() == kErf;
}
inline bool NeuralNetworkLayer::has_erf() const {
  return _internal_has_erf();
}
inline void NeuralNetworkLayer::set_has_erf() {
  _oneof_case_[0] = kErf;
}
inline void NeuralNetworkLayer::clear_erf() {
  if (_internal_has_erf()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.erf_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ErfLayerParams* NeuralNetworkLayer::release_erf() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.erf)
  if (_internal_has_erf()) {
    clear_has_layer();
      ::CoreML::Specification::ErfLayerParams* temp = layer_.erf_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.erf_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ErfLayerParams& NeuralNetworkLayer::_internal_erf() const {
  return _internal_has_erf()
      ? *layer_.erf_
      : reinterpret_cast< ::CoreML::Specification::ErfLayerParams&>(::CoreML::Specification::_ErfLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ErfLayerParams& NeuralNetworkLayer::erf() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.erf)
  return _internal_erf();
}
inline ::CoreML::Specification::ErfLayerParams* NeuralNetworkLayer::unsafe_arena_release_erf() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.erf)
  if (_internal_has_erf()) {
    clear_has_layer();
    ::CoreML::Specification::ErfLayerParams* temp = layer_.erf_;
    layer_.erf_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_erf(::CoreML::Specification::ErfLayerParams* erf) {
  clear_layer();
  if (erf) {
    set_has_erf();
    layer_.erf_ = erf;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.erf)
}
inline ::CoreML::Specification::ErfLayerParams* NeuralNetworkLayer::_internal_mutable_erf() {
  if (!_internal_has_erf()) {
    clear_layer();
    set_has_erf();
    layer_.erf_ = CreateMaybeMessage< ::CoreML::Specification::ErfLayerParams >(GetArenaForAllocation());
  }
  return layer_.erf_;
}
inline ::CoreML::Specification::ErfLayerParams* NeuralNetworkLayer::mutable_erf() {
  ::CoreML::Specification::ErfLayerParams* _msg = _internal_mutable_erf();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.erf)
  return _msg;
}

// .CoreML.Specification.GeluLayerParams gelu = 795;
inline bool NeuralNetworkLayer::_internal_has_gelu() const {
  return layer_case() == kGelu;
}
inline bool NeuralNetworkLayer::has_gelu() const {
  return _internal_has_gelu();
}
inline void NeuralNetworkLayer::set_has_gelu() {
  _oneof_case_[0] = kGelu;
}
inline void NeuralNetworkLayer::clear_gelu() {
  if (_internal_has_gelu()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.gelu_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::GeluLayerParams* NeuralNetworkLayer::release_gelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.gelu)
  if (_internal_has_gelu()) {
    clear_has_layer();
      ::CoreML::Specification::GeluLayerParams* temp = layer_.gelu_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.gelu_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::GeluLayerParams& NeuralNetworkLayer::_internal_gelu() const {
  return _internal_has_gelu()
      ? *layer_.gelu_
      : reinterpret_cast< ::CoreML::Specification::GeluLayerParams&>(::CoreML::Specification::_GeluLayerParams_default_instance_);
}
inline const ::CoreML::Specification::GeluLayerParams& NeuralNetworkLayer::gelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.gelu)
  return _internal_gelu();
}
inline ::CoreML::Specification::GeluLayerParams* NeuralNetworkLayer::unsafe_arena_release_gelu() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.gelu)
  if (_internal_has_gelu()) {
    clear_has_layer();
    ::CoreML::Specification::GeluLayerParams* temp = layer_.gelu_;
    layer_.gelu_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_gelu(::CoreML::Specification::GeluLayerParams* gelu) {
  clear_layer();
  if (gelu) {
    set_has_gelu();
    layer_.gelu_ = gelu;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.gelu)
}
inline ::CoreML::Specification::GeluLayerParams* NeuralNetworkLayer::_internal_mutable_gelu() {
  if (!_internal_has_gelu()) {
    clear_layer();
    set_has_gelu();
    layer_.gelu_ = CreateMaybeMessage< ::CoreML::Specification::GeluLayerParams >(GetArenaForAllocation());
  }
  return layer_.gelu_;
}
inline ::CoreML::Specification::GeluLayerParams* NeuralNetworkLayer::mutable_gelu() {
  ::CoreML::Specification::GeluLayerParams* _msg = _internal_mutable_gelu();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.gelu)
  return _msg;
}

// .CoreML.Specification.EqualLayerParams equal = 815;
inline bool NeuralNetworkLayer::_internal_has_equal() const {
  return layer_case() == kEqual;
}
inline bool NeuralNetworkLayer::has_equal() const {
  return _internal_has_equal();
}
inline void NeuralNetworkLayer::set_has_equal() {
  _oneof_case_[0] = kEqual;
}
inline void NeuralNetworkLayer::clear_equal() {
  if (_internal_has_equal()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.equal_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::EqualLayerParams* NeuralNetworkLayer::release_equal() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.equal)
  if (_internal_has_equal()) {
    clear_has_layer();
      ::CoreML::Specification::EqualLayerParams* temp = layer_.equal_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.equal_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::EqualLayerParams& NeuralNetworkLayer::_internal_equal() const {
  return _internal_has_equal()
      ? *layer_.equal_
      : reinterpret_cast< ::CoreML::Specification::EqualLayerParams&>(::CoreML::Specification::_EqualLayerParams_default_instance_);
}
inline const ::CoreML::Specification::EqualLayerParams& NeuralNetworkLayer::equal() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.equal)
  return _internal_equal();
}
inline ::CoreML::Specification::EqualLayerParams* NeuralNetworkLayer::unsafe_arena_release_equal() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.equal)
  if (_internal_has_equal()) {
    clear_has_layer();
    ::CoreML::Specification::EqualLayerParams* temp = layer_.equal_;
    layer_.equal_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_equal(::CoreML::Specification::EqualLayerParams* equal) {
  clear_layer();
  if (equal) {
    set_has_equal();
    layer_.equal_ = equal;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.equal)
}
inline ::CoreML::Specification::EqualLayerParams* NeuralNetworkLayer::_internal_mutable_equal() {
  if (!_internal_has_equal()) {
    clear_layer();
    set_has_equal();
    layer_.equal_ = CreateMaybeMessage< ::CoreML::Specification::EqualLayerParams >(GetArenaForAllocation());
  }
  return layer_.equal_;
}
inline ::CoreML::Specification::EqualLayerParams* NeuralNetworkLayer::mutable_equal() {
  ::CoreML::Specification::EqualLayerParams* _msg = _internal_mutable_equal();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.equal)
  return _msg;
}

// .CoreML.Specification.NotEqualLayerParams notEqual = 820;
inline bool NeuralNetworkLayer::_internal_has_notequal() const {
  return layer_case() == kNotEqual;
}
inline bool NeuralNetworkLayer::has_notequal() const {
  return _internal_has_notequal();
}
inline void NeuralNetworkLayer::set_has_notequal() {
  _oneof_case_[0] = kNotEqual;
}
inline void NeuralNetworkLayer::clear_notequal() {
  if (_internal_has_notequal()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.notequal_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::NotEqualLayerParams* NeuralNetworkLayer::release_notequal() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.notEqual)
  if (_internal_has_notequal()) {
    clear_has_layer();
      ::CoreML::Specification::NotEqualLayerParams* temp = layer_.notequal_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.notequal_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::NotEqualLayerParams& NeuralNetworkLayer::_internal_notequal() const {
  return _internal_has_notequal()
      ? *layer_.notequal_
      : reinterpret_cast< ::CoreML::Specification::NotEqualLayerParams&>(::CoreML::Specification::_NotEqualLayerParams_default_instance_);
}
inline const ::CoreML::Specification::NotEqualLayerParams& NeuralNetworkLayer::notequal() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.notEqual)
  return _internal_notequal();
}
inline ::CoreML::Specification::NotEqualLayerParams* NeuralNetworkLayer::unsafe_arena_release_notequal() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.notEqual)
  if (_internal_has_notequal()) {
    clear_has_layer();
    ::CoreML::Specification::NotEqualLayerParams* temp = layer_.notequal_;
    layer_.notequal_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_notequal(::CoreML::Specification::NotEqualLayerParams* notequal) {
  clear_layer();
  if (notequal) {
    set_has_notequal();
    layer_.notequal_ = notequal;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.notEqual)
}
inline ::CoreML::Specification::NotEqualLayerParams* NeuralNetworkLayer::_internal_mutable_notequal() {
  if (!_internal_has_notequal()) {
    clear_layer();
    set_has_notequal();
    layer_.notequal_ = CreateMaybeMessage< ::CoreML::Specification::NotEqualLayerParams >(GetArenaForAllocation());
  }
  return layer_.notequal_;
}
inline ::CoreML::Specification::NotEqualLayerParams* NeuralNetworkLayer::mutable_notequal() {
  ::CoreML::Specification::NotEqualLayerParams* _msg = _internal_mutable_notequal();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.notEqual)
  return _msg;
}

// .CoreML.Specification.LessThanLayerParams lessThan = 825;
inline bool NeuralNetworkLayer::_internal_has_lessthan() const {
  return layer_case() == kLessThan;
}
inline bool NeuralNetworkLayer::has_lessthan() const {
  return _internal_has_lessthan();
}
inline void NeuralNetworkLayer::set_has_lessthan() {
  _oneof_case_[0] = kLessThan;
}
inline void NeuralNetworkLayer::clear_lessthan() {
  if (_internal_has_lessthan()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.lessthan_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::LessThanLayerParams* NeuralNetworkLayer::release_lessthan() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.lessThan)
  if (_internal_has_lessthan()) {
    clear_has_layer();
      ::CoreML::Specification::LessThanLayerParams* temp = layer_.lessthan_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.lessthan_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::LessThanLayerParams& NeuralNetworkLayer::_internal_lessthan() const {
  return _internal_has_lessthan()
      ? *layer_.lessthan_
      : reinterpret_cast< ::CoreML::Specification::LessThanLayerParams&>(::CoreML::Specification::_LessThanLayerParams_default_instance_);
}
inline const ::CoreML::Specification::LessThanLayerParams& NeuralNetworkLayer::lessthan() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.lessThan)
  return _internal_lessthan();
}
inline ::CoreML::Specification::LessThanLayerParams* NeuralNetworkLayer::unsafe_arena_release_lessthan() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.lessThan)
  if (_internal_has_lessthan()) {
    clear_has_layer();
    ::CoreML::Specification::LessThanLayerParams* temp = layer_.lessthan_;
    layer_.lessthan_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_lessthan(::CoreML::Specification::LessThanLayerParams* lessthan) {
  clear_layer();
  if (lessthan) {
    set_has_lessthan();
    layer_.lessthan_ = lessthan;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.lessThan)
}
inline ::CoreML::Specification::LessThanLayerParams* NeuralNetworkLayer::_internal_mutable_lessthan() {
  if (!_internal_has_lessthan()) {
    clear_layer();
    set_has_lessthan();
    layer_.lessthan_ = CreateMaybeMessage< ::CoreML::Specification::LessThanLayerParams >(GetArenaForAllocation());
  }
  return layer_.lessthan_;
}
inline ::CoreML::Specification::LessThanLayerParams* NeuralNetworkLayer::mutable_lessthan() {
  ::CoreML::Specification::LessThanLayerParams* _msg = _internal_mutable_lessthan();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.lessThan)
  return _msg;
}

// .CoreML.Specification.LessEqualLayerParams lessEqual = 827;
inline bool NeuralNetworkLayer::_internal_has_lessequal() const {
  return layer_case() == kLessEqual;
}
inline bool NeuralNetworkLayer::has_lessequal() const {
  return _internal_has_lessequal();
}
inline void NeuralNetworkLayer::set_has_lessequal() {
  _oneof_case_[0] = kLessEqual;
}
inline void NeuralNetworkLayer::clear_lessequal() {
  if (_internal_has_lessequal()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.lessequal_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::LessEqualLayerParams* NeuralNetworkLayer::release_lessequal() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.lessEqual)
  if (_internal_has_lessequal()) {
    clear_has_layer();
      ::CoreML::Specification::LessEqualLayerParams* temp = layer_.lessequal_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.lessequal_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::LessEqualLayerParams& NeuralNetworkLayer::_internal_lessequal() const {
  return _internal_has_lessequal()
      ? *layer_.lessequal_
      : reinterpret_cast< ::CoreML::Specification::LessEqualLayerParams&>(::CoreML::Specification::_LessEqualLayerParams_default_instance_);
}
inline const ::CoreML::Specification::LessEqualLayerParams& NeuralNetworkLayer::lessequal() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.lessEqual)
  return _internal_lessequal();
}
inline ::CoreML::Specification::LessEqualLayerParams* NeuralNetworkLayer::unsafe_arena_release_lessequal() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.lessEqual)
  if (_internal_has_lessequal()) {
    clear_has_layer();
    ::CoreML::Specification::LessEqualLayerParams* temp = layer_.lessequal_;
    layer_.lessequal_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_lessequal(::CoreML::Specification::LessEqualLayerParams* lessequal) {
  clear_layer();
  if (lessequal) {
    set_has_lessequal();
    layer_.lessequal_ = lessequal;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.lessEqual)
}
inline ::CoreML::Specification::LessEqualLayerParams* NeuralNetworkLayer::_internal_mutable_lessequal() {
  if (!_internal_has_lessequal()) {
    clear_layer();
    set_has_lessequal();
    layer_.lessequal_ = CreateMaybeMessage< ::CoreML::Specification::LessEqualLayerParams >(GetArenaForAllocation());
  }
  return layer_.lessequal_;
}
inline ::CoreML::Specification::LessEqualLayerParams* NeuralNetworkLayer::mutable_lessequal() {
  ::CoreML::Specification::LessEqualLayerParams* _msg = _internal_mutable_lessequal();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.lessEqual)
  return _msg;
}

// .CoreML.Specification.GreaterThanLayerParams greaterThan = 830;
inline bool NeuralNetworkLayer::_internal_has_greaterthan() const {
  return layer_case() == kGreaterThan;
}
inline bool NeuralNetworkLayer::has_greaterthan() const {
  return _internal_has_greaterthan();
}
inline void NeuralNetworkLayer::set_has_greaterthan() {
  _oneof_case_[0] = kGreaterThan;
}
inline void NeuralNetworkLayer::clear_greaterthan() {
  if (_internal_has_greaterthan()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.greaterthan_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::GreaterThanLayerParams* NeuralNetworkLayer::release_greaterthan() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.greaterThan)
  if (_internal_has_greaterthan()) {
    clear_has_layer();
      ::CoreML::Specification::GreaterThanLayerParams* temp = layer_.greaterthan_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.greaterthan_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::GreaterThanLayerParams& NeuralNetworkLayer::_internal_greaterthan() const {
  return _internal_has_greaterthan()
      ? *layer_.greaterthan_
      : reinterpret_cast< ::CoreML::Specification::GreaterThanLayerParams&>(::CoreML::Specification::_GreaterThanLayerParams_default_instance_);
}
inline const ::CoreML::Specification::GreaterThanLayerParams& NeuralNetworkLayer::greaterthan() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.greaterThan)
  return _internal_greaterthan();
}
inline ::CoreML::Specification::GreaterThanLayerParams* NeuralNetworkLayer::unsafe_arena_release_greaterthan() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.greaterThan)
  if (_internal_has_greaterthan()) {
    clear_has_layer();
    ::CoreML::Specification::GreaterThanLayerParams* temp = layer_.greaterthan_;
    layer_.greaterthan_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_greaterthan(::CoreML::Specification::GreaterThanLayerParams* greaterthan) {
  clear_layer();
  if (greaterthan) {
    set_has_greaterthan();
    layer_.greaterthan_ = greaterthan;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.greaterThan)
}
inline ::CoreML::Specification::GreaterThanLayerParams* NeuralNetworkLayer::_internal_mutable_greaterthan() {
  if (!_internal_has_greaterthan()) {
    clear_layer();
    set_has_greaterthan();
    layer_.greaterthan_ = CreateMaybeMessage< ::CoreML::Specification::GreaterThanLayerParams >(GetArenaForAllocation());
  }
  return layer_.greaterthan_;
}
inline ::CoreML::Specification::GreaterThanLayerParams* NeuralNetworkLayer::mutable_greaterthan() {
  ::CoreML::Specification::GreaterThanLayerParams* _msg = _internal_mutable_greaterthan();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.greaterThan)
  return _msg;
}

// .CoreML.Specification.GreaterEqualLayerParams greaterEqual = 832;
inline bool NeuralNetworkLayer::_internal_has_greaterequal() const {
  return layer_case() == kGreaterEqual;
}
inline bool NeuralNetworkLayer::has_greaterequal() const {
  return _internal_has_greaterequal();
}
inline void NeuralNetworkLayer::set_has_greaterequal() {
  _oneof_case_[0] = kGreaterEqual;
}
inline void NeuralNetworkLayer::clear_greaterequal() {
  if (_internal_has_greaterequal()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.greaterequal_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::GreaterEqualLayerParams* NeuralNetworkLayer::release_greaterequal() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.greaterEqual)
  if (_internal_has_greaterequal()) {
    clear_has_layer();
      ::CoreML::Specification::GreaterEqualLayerParams* temp = layer_.greaterequal_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.greaterequal_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::GreaterEqualLayerParams& NeuralNetworkLayer::_internal_greaterequal() const {
  return _internal_has_greaterequal()
      ? *layer_.greaterequal_
      : reinterpret_cast< ::CoreML::Specification::GreaterEqualLayerParams&>(::CoreML::Specification::_GreaterEqualLayerParams_default_instance_);
}
inline const ::CoreML::Specification::GreaterEqualLayerParams& NeuralNetworkLayer::greaterequal() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.greaterEqual)
  return _internal_greaterequal();
}
inline ::CoreML::Specification::GreaterEqualLayerParams* NeuralNetworkLayer::unsafe_arena_release_greaterequal() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.greaterEqual)
  if (_internal_has_greaterequal()) {
    clear_has_layer();
    ::CoreML::Specification::GreaterEqualLayerParams* temp = layer_.greaterequal_;
    layer_.greaterequal_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_greaterequal(::CoreML::Specification::GreaterEqualLayerParams* greaterequal) {
  clear_layer();
  if (greaterequal) {
    set_has_greaterequal();
    layer_.greaterequal_ = greaterequal;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.greaterEqual)
}
inline ::CoreML::Specification::GreaterEqualLayerParams* NeuralNetworkLayer::_internal_mutable_greaterequal() {
  if (!_internal_has_greaterequal()) {
    clear_layer();
    set_has_greaterequal();
    layer_.greaterequal_ = CreateMaybeMessage< ::CoreML::Specification::GreaterEqualLayerParams >(GetArenaForAllocation());
  }
  return layer_.greaterequal_;
}
inline ::CoreML::Specification::GreaterEqualLayerParams* NeuralNetworkLayer::mutable_greaterequal() {
  ::CoreML::Specification::GreaterEqualLayerParams* _msg = _internal_mutable_greaterequal();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.greaterEqual)
  return _msg;
}

// .CoreML.Specification.LogicalOrLayerParams logicalOr = 840;
inline bool NeuralNetworkLayer::_internal_has_logicalor() const {
  return layer_case() == kLogicalOr;
}
inline bool NeuralNetworkLayer::has_logicalor() const {
  return _internal_has_logicalor();
}
inline void NeuralNetworkLayer::set_has_logicalor() {
  _oneof_case_[0] = kLogicalOr;
}
inline void NeuralNetworkLayer::clear_logicalor() {
  if (_internal_has_logicalor()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.logicalor_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::LogicalOrLayerParams* NeuralNetworkLayer::release_logicalor() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.logicalOr)
  if (_internal_has_logicalor()) {
    clear_has_layer();
      ::CoreML::Specification::LogicalOrLayerParams* temp = layer_.logicalor_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.logicalor_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::LogicalOrLayerParams& NeuralNetworkLayer::_internal_logicalor() const {
  return _internal_has_logicalor()
      ? *layer_.logicalor_
      : reinterpret_cast< ::CoreML::Specification::LogicalOrLayerParams&>(::CoreML::Specification::_LogicalOrLayerParams_default_instance_);
}
inline const ::CoreML::Specification::LogicalOrLayerParams& NeuralNetworkLayer::logicalor() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.logicalOr)
  return _internal_logicalor();
}
inline ::CoreML::Specification::LogicalOrLayerParams* NeuralNetworkLayer::unsafe_arena_release_logicalor() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.logicalOr)
  if (_internal_has_logicalor()) {
    clear_has_layer();
    ::CoreML::Specification::LogicalOrLayerParams* temp = layer_.logicalor_;
    layer_.logicalor_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_logicalor(::CoreML::Specification::LogicalOrLayerParams* logicalor) {
  clear_layer();
  if (logicalor) {
    set_has_logicalor();
    layer_.logicalor_ = logicalor;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.logicalOr)
}
inline ::CoreML::Specification::LogicalOrLayerParams* NeuralNetworkLayer::_internal_mutable_logicalor() {
  if (!_internal_has_logicalor()) {
    clear_layer();
    set_has_logicalor();
    layer_.logicalor_ = CreateMaybeMessage< ::CoreML::Specification::LogicalOrLayerParams >(GetArenaForAllocation());
  }
  return layer_.logicalor_;
}
inline ::CoreML::Specification::LogicalOrLayerParams* NeuralNetworkLayer::mutable_logicalor() {
  ::CoreML::Specification::LogicalOrLayerParams* _msg = _internal_mutable_logicalor();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.logicalOr)
  return _msg;
}

// .CoreML.Specification.LogicalXorLayerParams logicalXor = 845;
inline bool NeuralNetworkLayer::_internal_has_logicalxor() const {
  return layer_case() == kLogicalXor;
}
inline bool NeuralNetworkLayer::has_logicalxor() const {
  return _internal_has_logicalxor();
}
inline void NeuralNetworkLayer::set_has_logicalxor() {
  _oneof_case_[0] = kLogicalXor;
}
inline void NeuralNetworkLayer::clear_logicalxor() {
  if (_internal_has_logicalxor()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.logicalxor_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::LogicalXorLayerParams* NeuralNetworkLayer::release_logicalxor() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.logicalXor)
  if (_internal_has_logicalxor()) {
    clear_has_layer();
      ::CoreML::Specification::LogicalXorLayerParams* temp = layer_.logicalxor_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.logicalxor_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::LogicalXorLayerParams& NeuralNetworkLayer::_internal_logicalxor() const {
  return _internal_has_logicalxor()
      ? *layer_.logicalxor_
      : reinterpret_cast< ::CoreML::Specification::LogicalXorLayerParams&>(::CoreML::Specification::_LogicalXorLayerParams_default_instance_);
}
inline const ::CoreML::Specification::LogicalXorLayerParams& NeuralNetworkLayer::logicalxor() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.logicalXor)
  return _internal_logicalxor();
}
inline ::CoreML::Specification::LogicalXorLayerParams* NeuralNetworkLayer::unsafe_arena_release_logicalxor() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.logicalXor)
  if (_internal_has_logicalxor()) {
    clear_has_layer();
    ::CoreML::Specification::LogicalXorLayerParams* temp = layer_.logicalxor_;
    layer_.logicalxor_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_logicalxor(::CoreML::Specification::LogicalXorLayerParams* logicalxor) {
  clear_layer();
  if (logicalxor) {
    set_has_logicalxor();
    layer_.logicalxor_ = logicalxor;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.logicalXor)
}
inline ::CoreML::Specification::LogicalXorLayerParams* NeuralNetworkLayer::_internal_mutable_logicalxor() {
  if (!_internal_has_logicalxor()) {
    clear_layer();
    set_has_logicalxor();
    layer_.logicalxor_ = CreateMaybeMessage< ::CoreML::Specification::LogicalXorLayerParams >(GetArenaForAllocation());
  }
  return layer_.logicalxor_;
}
inline ::CoreML::Specification::LogicalXorLayerParams* NeuralNetworkLayer::mutable_logicalxor() {
  ::CoreML::Specification::LogicalXorLayerParams* _msg = _internal_mutable_logicalxor();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.logicalXor)
  return _msg;
}

// .CoreML.Specification.LogicalNotLayerParams logicalNot = 850;
inline bool NeuralNetworkLayer::_internal_has_logicalnot() const {
  return layer_case() == kLogicalNot;
}
inline bool NeuralNetworkLayer::has_logicalnot() const {
  return _internal_has_logicalnot();
}
inline void NeuralNetworkLayer::set_has_logicalnot() {
  _oneof_case_[0] = kLogicalNot;
}
inline void NeuralNetworkLayer::clear_logicalnot() {
  if (_internal_has_logicalnot()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.logicalnot_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::LogicalNotLayerParams* NeuralNetworkLayer::release_logicalnot() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.logicalNot)
  if (_internal_has_logicalnot()) {
    clear_has_layer();
      ::CoreML::Specification::LogicalNotLayerParams* temp = layer_.logicalnot_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.logicalnot_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::LogicalNotLayerParams& NeuralNetworkLayer::_internal_logicalnot() const {
  return _internal_has_logicalnot()
      ? *layer_.logicalnot_
      : reinterpret_cast< ::CoreML::Specification::LogicalNotLayerParams&>(::CoreML::Specification::_LogicalNotLayerParams_default_instance_);
}
inline const ::CoreML::Specification::LogicalNotLayerParams& NeuralNetworkLayer::logicalnot() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.logicalNot)
  return _internal_logicalnot();
}
inline ::CoreML::Specification::LogicalNotLayerParams* NeuralNetworkLayer::unsafe_arena_release_logicalnot() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.logicalNot)
  if (_internal_has_logicalnot()) {
    clear_has_layer();
    ::CoreML::Specification::LogicalNotLayerParams* temp = layer_.logicalnot_;
    layer_.logicalnot_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_logicalnot(::CoreML::Specification::LogicalNotLayerParams* logicalnot) {
  clear_layer();
  if (logicalnot) {
    set_has_logicalnot();
    layer_.logicalnot_ = logicalnot;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.logicalNot)
}
inline ::CoreML::Specification::LogicalNotLayerParams* NeuralNetworkLayer::_internal_mutable_logicalnot() {
  if (!_internal_has_logicalnot()) {
    clear_layer();
    set_has_logicalnot();
    layer_.logicalnot_ = CreateMaybeMessage< ::CoreML::Specification::LogicalNotLayerParams >(GetArenaForAllocation());
  }
  return layer_.logicalnot_;
}
inline ::CoreML::Specification::LogicalNotLayerParams* NeuralNetworkLayer::mutable_logicalnot() {
  ::CoreML::Specification::LogicalNotLayerParams* _msg = _internal_mutable_logicalnot();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.logicalNot)
  return _msg;
}

// .CoreML.Specification.LogicalAndLayerParams logicalAnd = 855;
inline bool NeuralNetworkLayer::_internal_has_logicaland() const {
  return layer_case() == kLogicalAnd;
}
inline bool NeuralNetworkLayer::has_logicaland() const {
  return _internal_has_logicaland();
}
inline void NeuralNetworkLayer::set_has_logicaland() {
  _oneof_case_[0] = kLogicalAnd;
}
inline void NeuralNetworkLayer::clear_logicaland() {
  if (_internal_has_logicaland()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.logicaland_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::LogicalAndLayerParams* NeuralNetworkLayer::release_logicaland() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.logicalAnd)
  if (_internal_has_logicaland()) {
    clear_has_layer();
      ::CoreML::Specification::LogicalAndLayerParams* temp = layer_.logicaland_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.logicaland_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::LogicalAndLayerParams& NeuralNetworkLayer::_internal_logicaland() const {
  return _internal_has_logicaland()
      ? *layer_.logicaland_
      : reinterpret_cast< ::CoreML::Specification::LogicalAndLayerParams&>(::CoreML::Specification::_LogicalAndLayerParams_default_instance_);
}
inline const ::CoreML::Specification::LogicalAndLayerParams& NeuralNetworkLayer::logicaland() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.logicalAnd)
  return _internal_logicaland();
}
inline ::CoreML::Specification::LogicalAndLayerParams* NeuralNetworkLayer::unsafe_arena_release_logicaland() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.logicalAnd)
  if (_internal_has_logicaland()) {
    clear_has_layer();
    ::CoreML::Specification::LogicalAndLayerParams* temp = layer_.logicaland_;
    layer_.logicaland_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_logicaland(::CoreML::Specification::LogicalAndLayerParams* logicaland) {
  clear_layer();
  if (logicaland) {
    set_has_logicaland();
    layer_.logicaland_ = logicaland;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.logicalAnd)
}
inline ::CoreML::Specification::LogicalAndLayerParams* NeuralNetworkLayer::_internal_mutable_logicaland() {
  if (!_internal_has_logicaland()) {
    clear_layer();
    set_has_logicaland();
    layer_.logicaland_ = CreateMaybeMessage< ::CoreML::Specification::LogicalAndLayerParams >(GetArenaForAllocation());
  }
  return layer_.logicaland_;
}
inline ::CoreML::Specification::LogicalAndLayerParams* NeuralNetworkLayer::mutable_logicaland() {
  ::CoreML::Specification::LogicalAndLayerParams* _msg = _internal_mutable_logicaland();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.logicalAnd)
  return _msg;
}

// .CoreML.Specification.ModBroadcastableLayerParams modBroadcastable = 865;
inline bool NeuralNetworkLayer::_internal_has_modbroadcastable() const {
  return layer_case() == kModBroadcastable;
}
inline bool NeuralNetworkLayer::has_modbroadcastable() const {
  return _internal_has_modbroadcastable();
}
inline void NeuralNetworkLayer::set_has_modbroadcastable() {
  _oneof_case_[0] = kModBroadcastable;
}
inline void NeuralNetworkLayer::clear_modbroadcastable() {
  if (_internal_has_modbroadcastable()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.modbroadcastable_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ModBroadcastableLayerParams* NeuralNetworkLayer::release_modbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.modBroadcastable)
  if (_internal_has_modbroadcastable()) {
    clear_has_layer();
      ::CoreML::Specification::ModBroadcastableLayerParams* temp = layer_.modbroadcastable_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.modbroadcastable_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ModBroadcastableLayerParams& NeuralNetworkLayer::_internal_modbroadcastable() const {
  return _internal_has_modbroadcastable()
      ? *layer_.modbroadcastable_
      : reinterpret_cast< ::CoreML::Specification::ModBroadcastableLayerParams&>(::CoreML::Specification::_ModBroadcastableLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ModBroadcastableLayerParams& NeuralNetworkLayer::modbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.modBroadcastable)
  return _internal_modbroadcastable();
}
inline ::CoreML::Specification::ModBroadcastableLayerParams* NeuralNetworkLayer::unsafe_arena_release_modbroadcastable() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.modBroadcastable)
  if (_internal_has_modbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::ModBroadcastableLayerParams* temp = layer_.modbroadcastable_;
    layer_.modbroadcastable_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_modbroadcastable(::CoreML::Specification::ModBroadcastableLayerParams* modbroadcastable) {
  clear_layer();
  if (modbroadcastable) {
    set_has_modbroadcastable();
    layer_.modbroadcastable_ = modbroadcastable;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.modBroadcastable)
}
inline ::CoreML::Specification::ModBroadcastableLayerParams* NeuralNetworkLayer::_internal_mutable_modbroadcastable() {
  if (!_internal_has_modbroadcastable()) {
    clear_layer();
    set_has_modbroadcastable();
    layer_.modbroadcastable_ = CreateMaybeMessage< ::CoreML::Specification::ModBroadcastableLayerParams >(GetArenaForAllocation());
  }
  return layer_.modbroadcastable_;
}
inline ::CoreML::Specification::ModBroadcastableLayerParams* NeuralNetworkLayer::mutable_modbroadcastable() {
  ::CoreML::Specification::ModBroadcastableLayerParams* _msg = _internal_mutable_modbroadcastable();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.modBroadcastable)
  return _msg;
}

// .CoreML.Specification.MinBroadcastableLayerParams minBroadcastable = 870;
inline bool NeuralNetworkLayer::_internal_has_minbroadcastable() const {
  return layer_case() == kMinBroadcastable;
}
inline bool NeuralNetworkLayer::has_minbroadcastable() const {
  return _internal_has_minbroadcastable();
}
inline void NeuralNetworkLayer::set_has_minbroadcastable() {
  _oneof_case_[0] = kMinBroadcastable;
}
inline void NeuralNetworkLayer::clear_minbroadcastable() {
  if (_internal_has_minbroadcastable()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.minbroadcastable_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::MinBroadcastableLayerParams* NeuralNetworkLayer::release_minbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.minBroadcastable)
  if (_internal_has_minbroadcastable()) {
    clear_has_layer();
      ::CoreML::Specification::MinBroadcastableLayerParams* temp = layer_.minbroadcastable_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.minbroadcastable_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::MinBroadcastableLayerParams& NeuralNetworkLayer::_internal_minbroadcastable() const {
  return _internal_has_minbroadcastable()
      ? *layer_.minbroadcastable_
      : reinterpret_cast< ::CoreML::Specification::MinBroadcastableLayerParams&>(::CoreML::Specification::_MinBroadcastableLayerParams_default_instance_);
}
inline const ::CoreML::Specification::MinBroadcastableLayerParams& NeuralNetworkLayer::minbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.minBroadcastable)
  return _internal_minbroadcastable();
}
inline ::CoreML::Specification::MinBroadcastableLayerParams* NeuralNetworkLayer::unsafe_arena_release_minbroadcastable() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.minBroadcastable)
  if (_internal_has_minbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::MinBroadcastableLayerParams* temp = layer_.minbroadcastable_;
    layer_.minbroadcastable_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_minbroadcastable(::CoreML::Specification::MinBroadcastableLayerParams* minbroadcastable) {
  clear_layer();
  if (minbroadcastable) {
    set_has_minbroadcastable();
    layer_.minbroadcastable_ = minbroadcastable;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.minBroadcastable)
}
inline ::CoreML::Specification::MinBroadcastableLayerParams* NeuralNetworkLayer::_internal_mutable_minbroadcastable() {
  if (!_internal_has_minbroadcastable()) {
    clear_layer();
    set_has_minbroadcastable();
    layer_.minbroadcastable_ = CreateMaybeMessage< ::CoreML::Specification::MinBroadcastableLayerParams >(GetArenaForAllocation());
  }
  return layer_.minbroadcastable_;
}
inline ::CoreML::Specification::MinBroadcastableLayerParams* NeuralNetworkLayer::mutable_minbroadcastable() {
  ::CoreML::Specification::MinBroadcastableLayerParams* _msg = _internal_mutable_minbroadcastable();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.minBroadcastable)
  return _msg;
}

// .CoreML.Specification.MaxBroadcastableLayerParams maxBroadcastable = 875;
inline bool NeuralNetworkLayer::_internal_has_maxbroadcastable() const {
  return layer_case() == kMaxBroadcastable;
}
inline bool NeuralNetworkLayer::has_maxbroadcastable() const {
  return _internal_has_maxbroadcastable();
}
inline void NeuralNetworkLayer::set_has_maxbroadcastable() {
  _oneof_case_[0] = kMaxBroadcastable;
}
inline void NeuralNetworkLayer::clear_maxbroadcastable() {
  if (_internal_has_maxbroadcastable()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.maxbroadcastable_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::MaxBroadcastableLayerParams* NeuralNetworkLayer::release_maxbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.maxBroadcastable)
  if (_internal_has_maxbroadcastable()) {
    clear_has_layer();
      ::CoreML::Specification::MaxBroadcastableLayerParams* temp = layer_.maxbroadcastable_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.maxbroadcastable_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::MaxBroadcastableLayerParams& NeuralNetworkLayer::_internal_maxbroadcastable() const {
  return _internal_has_maxbroadcastable()
      ? *layer_.maxbroadcastable_
      : reinterpret_cast< ::CoreML::Specification::MaxBroadcastableLayerParams&>(::CoreML::Specification::_MaxBroadcastableLayerParams_default_instance_);
}
inline const ::CoreML::Specification::MaxBroadcastableLayerParams& NeuralNetworkLayer::maxbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.maxBroadcastable)
  return _internal_maxbroadcastable();
}
inline ::CoreML::Specification::MaxBroadcastableLayerParams* NeuralNetworkLayer::unsafe_arena_release_maxbroadcastable() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.maxBroadcastable)
  if (_internal_has_maxbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::MaxBroadcastableLayerParams* temp = layer_.maxbroadcastable_;
    layer_.maxbroadcastable_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_maxbroadcastable(::CoreML::Specification::MaxBroadcastableLayerParams* maxbroadcastable) {
  clear_layer();
  if (maxbroadcastable) {
    set_has_maxbroadcastable();
    layer_.maxbroadcastable_ = maxbroadcastable;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.maxBroadcastable)
}
inline ::CoreML::Specification::MaxBroadcastableLayerParams* NeuralNetworkLayer::_internal_mutable_maxbroadcastable() {
  if (!_internal_has_maxbroadcastable()) {
    clear_layer();
    set_has_maxbroadcastable();
    layer_.maxbroadcastable_ = CreateMaybeMessage< ::CoreML::Specification::MaxBroadcastableLayerParams >(GetArenaForAllocation());
  }
  return layer_.maxbroadcastable_;
}
inline ::CoreML::Specification::MaxBroadcastableLayerParams* NeuralNetworkLayer::mutable_maxbroadcastable() {
  ::CoreML::Specification::MaxBroadcastableLayerParams* _msg = _internal_mutable_maxbroadcastable();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.maxBroadcastable)
  return _msg;
}

// .CoreML.Specification.AddBroadcastableLayerParams addBroadcastable = 880;
inline bool NeuralNetworkLayer::_internal_has_addbroadcastable() const {
  return layer_case() == kAddBroadcastable;
}
inline bool NeuralNetworkLayer::has_addbroadcastable() const {
  return _internal_has_addbroadcastable();
}
inline void NeuralNetworkLayer::set_has_addbroadcastable() {
  _oneof_case_[0] = kAddBroadcastable;
}
inline void NeuralNetworkLayer::clear_addbroadcastable() {
  if (_internal_has_addbroadcastable()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.addbroadcastable_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::AddBroadcastableLayerParams* NeuralNetworkLayer::release_addbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.addBroadcastable)
  if (_internal_has_addbroadcastable()) {
    clear_has_layer();
      ::CoreML::Specification::AddBroadcastableLayerParams* temp = layer_.addbroadcastable_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.addbroadcastable_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::AddBroadcastableLayerParams& NeuralNetworkLayer::_internal_addbroadcastable() const {
  return _internal_has_addbroadcastable()
      ? *layer_.addbroadcastable_
      : reinterpret_cast< ::CoreML::Specification::AddBroadcastableLayerParams&>(::CoreML::Specification::_AddBroadcastableLayerParams_default_instance_);
}
inline const ::CoreML::Specification::AddBroadcastableLayerParams& NeuralNetworkLayer::addbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.addBroadcastable)
  return _internal_addbroadcastable();
}
inline ::CoreML::Specification::AddBroadcastableLayerParams* NeuralNetworkLayer::unsafe_arena_release_addbroadcastable() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.addBroadcastable)
  if (_internal_has_addbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::AddBroadcastableLayerParams* temp = layer_.addbroadcastable_;
    layer_.addbroadcastable_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_addbroadcastable(::CoreML::Specification::AddBroadcastableLayerParams* addbroadcastable) {
  clear_layer();
  if (addbroadcastable) {
    set_has_addbroadcastable();
    layer_.addbroadcastable_ = addbroadcastable;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.addBroadcastable)
}
inline ::CoreML::Specification::AddBroadcastableLayerParams* NeuralNetworkLayer::_internal_mutable_addbroadcastable() {
  if (!_internal_has_addbroadcastable()) {
    clear_layer();
    set_has_addbroadcastable();
    layer_.addbroadcastable_ = CreateMaybeMessage< ::CoreML::Specification::AddBroadcastableLayerParams >(GetArenaForAllocation());
  }
  return layer_.addbroadcastable_;
}
inline ::CoreML::Specification::AddBroadcastableLayerParams* NeuralNetworkLayer::mutable_addbroadcastable() {
  ::CoreML::Specification::AddBroadcastableLayerParams* _msg = _internal_mutable_addbroadcastable();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.addBroadcastable)
  return _msg;
}

// .CoreML.Specification.PowBroadcastableLayerParams powBroadcastable = 885;
inline bool NeuralNetworkLayer::_internal_has_powbroadcastable() const {
  return layer_case() == kPowBroadcastable;
}
inline bool NeuralNetworkLayer::has_powbroadcastable() const {
  return _internal_has_powbroadcastable();
}
inline void NeuralNetworkLayer::set_has_powbroadcastable() {
  _oneof_case_[0] = kPowBroadcastable;
}
inline void NeuralNetworkLayer::clear_powbroadcastable() {
  if (_internal_has_powbroadcastable()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.powbroadcastable_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::PowBroadcastableLayerParams* NeuralNetworkLayer::release_powbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.powBroadcastable)
  if (_internal_has_powbroadcastable()) {
    clear_has_layer();
      ::CoreML::Specification::PowBroadcastableLayerParams* temp = layer_.powbroadcastable_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.powbroadcastable_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::PowBroadcastableLayerParams& NeuralNetworkLayer::_internal_powbroadcastable() const {
  return _internal_has_powbroadcastable()
      ? *layer_.powbroadcastable_
      : reinterpret_cast< ::CoreML::Specification::PowBroadcastableLayerParams&>(::CoreML::Specification::_PowBroadcastableLayerParams_default_instance_);
}
inline const ::CoreML::Specification::PowBroadcastableLayerParams& NeuralNetworkLayer::powbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.powBroadcastable)
  return _internal_powbroadcastable();
}
inline ::CoreML::Specification::PowBroadcastableLayerParams* NeuralNetworkLayer::unsafe_arena_release_powbroadcastable() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.powBroadcastable)
  if (_internal_has_powbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::PowBroadcastableLayerParams* temp = layer_.powbroadcastable_;
    layer_.powbroadcastable_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_powbroadcastable(::CoreML::Specification::PowBroadcastableLayerParams* powbroadcastable) {
  clear_layer();
  if (powbroadcastable) {
    set_has_powbroadcastable();
    layer_.powbroadcastable_ = powbroadcastable;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.powBroadcastable)
}
inline ::CoreML::Specification::PowBroadcastableLayerParams* NeuralNetworkLayer::_internal_mutable_powbroadcastable() {
  if (!_internal_has_powbroadcastable()) {
    clear_layer();
    set_has_powbroadcastable();
    layer_.powbroadcastable_ = CreateMaybeMessage< ::CoreML::Specification::PowBroadcastableLayerParams >(GetArenaForAllocation());
  }
  return layer_.powbroadcastable_;
}
inline ::CoreML::Specification::PowBroadcastableLayerParams* NeuralNetworkLayer::mutable_powbroadcastable() {
  ::CoreML::Specification::PowBroadcastableLayerParams* _msg = _internal_mutable_powbroadcastable();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.powBroadcastable)
  return _msg;
}

// .CoreML.Specification.DivideBroadcastableLayerParams divideBroadcastable = 890;
inline bool NeuralNetworkLayer::_internal_has_dividebroadcastable() const {
  return layer_case() == kDivideBroadcastable;
}
inline bool NeuralNetworkLayer::has_dividebroadcastable() const {
  return _internal_has_dividebroadcastable();
}
inline void NeuralNetworkLayer::set_has_dividebroadcastable() {
  _oneof_case_[0] = kDivideBroadcastable;
}
inline void NeuralNetworkLayer::clear_dividebroadcastable() {
  if (_internal_has_dividebroadcastable()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.dividebroadcastable_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::DivideBroadcastableLayerParams* NeuralNetworkLayer::release_dividebroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.divideBroadcastable)
  if (_internal_has_dividebroadcastable()) {
    clear_has_layer();
      ::CoreML::Specification::DivideBroadcastableLayerParams* temp = layer_.dividebroadcastable_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.dividebroadcastable_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::DivideBroadcastableLayerParams& NeuralNetworkLayer::_internal_dividebroadcastable() const {
  return _internal_has_dividebroadcastable()
      ? *layer_.dividebroadcastable_
      : reinterpret_cast< ::CoreML::Specification::DivideBroadcastableLayerParams&>(::CoreML::Specification::_DivideBroadcastableLayerParams_default_instance_);
}
inline const ::CoreML::Specification::DivideBroadcastableLayerParams& NeuralNetworkLayer::dividebroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.divideBroadcastable)
  return _internal_dividebroadcastable();
}
inline ::CoreML::Specification::DivideBroadcastableLayerParams* NeuralNetworkLayer::unsafe_arena_release_dividebroadcastable() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.divideBroadcastable)
  if (_internal_has_dividebroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::DivideBroadcastableLayerParams* temp = layer_.dividebroadcastable_;
    layer_.dividebroadcastable_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_dividebroadcastable(::CoreML::Specification::DivideBroadcastableLayerParams* dividebroadcastable) {
  clear_layer();
  if (dividebroadcastable) {
    set_has_dividebroadcastable();
    layer_.dividebroadcastable_ = dividebroadcastable;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.divideBroadcastable)
}
inline ::CoreML::Specification::DivideBroadcastableLayerParams* NeuralNetworkLayer::_internal_mutable_dividebroadcastable() {
  if (!_internal_has_dividebroadcastable()) {
    clear_layer();
    set_has_dividebroadcastable();
    layer_.dividebroadcastable_ = CreateMaybeMessage< ::CoreML::Specification::DivideBroadcastableLayerParams >(GetArenaForAllocation());
  }
  return layer_.dividebroadcastable_;
}
inline ::CoreML::Specification::DivideBroadcastableLayerParams* NeuralNetworkLayer::mutable_dividebroadcastable() {
  ::CoreML::Specification::DivideBroadcastableLayerParams* _msg = _internal_mutable_dividebroadcastable();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.divideBroadcastable)
  return _msg;
}

// .CoreML.Specification.FloorDivBroadcastableLayerParams floorDivBroadcastable = 895;
inline bool NeuralNetworkLayer::_internal_has_floordivbroadcastable() const {
  return layer_case() == kFloorDivBroadcastable;
}
inline bool NeuralNetworkLayer::has_floordivbroadcastable() const {
  return _internal_has_floordivbroadcastable();
}
inline void NeuralNetworkLayer::set_has_floordivbroadcastable() {
  _oneof_case_[0] = kFloorDivBroadcastable;
}
inline void NeuralNetworkLayer::clear_floordivbroadcastable() {
  if (_internal_has_floordivbroadcastable()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.floordivbroadcastable_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::FloorDivBroadcastableLayerParams* NeuralNetworkLayer::release_floordivbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.floorDivBroadcastable)
  if (_internal_has_floordivbroadcastable()) {
    clear_has_layer();
      ::CoreML::Specification::FloorDivBroadcastableLayerParams* temp = layer_.floordivbroadcastable_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.floordivbroadcastable_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::FloorDivBroadcastableLayerParams& NeuralNetworkLayer::_internal_floordivbroadcastable() const {
  return _internal_has_floordivbroadcastable()
      ? *layer_.floordivbroadcastable_
      : reinterpret_cast< ::CoreML::Specification::FloorDivBroadcastableLayerParams&>(::CoreML::Specification::_FloorDivBroadcastableLayerParams_default_instance_);
}
inline const ::CoreML::Specification::FloorDivBroadcastableLayerParams& NeuralNetworkLayer::floordivbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.floorDivBroadcastable)
  return _internal_floordivbroadcastable();
}
inline ::CoreML::Specification::FloorDivBroadcastableLayerParams* NeuralNetworkLayer::unsafe_arena_release_floordivbroadcastable() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.floorDivBroadcastable)
  if (_internal_has_floordivbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::FloorDivBroadcastableLayerParams* temp = layer_.floordivbroadcastable_;
    layer_.floordivbroadcastable_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_floordivbroadcastable(::CoreML::Specification::FloorDivBroadcastableLayerParams* floordivbroadcastable) {
  clear_layer();
  if (floordivbroadcastable) {
    set_has_floordivbroadcastable();
    layer_.floordivbroadcastable_ = floordivbroadcastable;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.floorDivBroadcastable)
}
inline ::CoreML::Specification::FloorDivBroadcastableLayerParams* NeuralNetworkLayer::_internal_mutable_floordivbroadcastable() {
  if (!_internal_has_floordivbroadcastable()) {
    clear_layer();
    set_has_floordivbroadcastable();
    layer_.floordivbroadcastable_ = CreateMaybeMessage< ::CoreML::Specification::FloorDivBroadcastableLayerParams >(GetArenaForAllocation());
  }
  return layer_.floordivbroadcastable_;
}
inline ::CoreML::Specification::FloorDivBroadcastableLayerParams* NeuralNetworkLayer::mutable_floordivbroadcastable() {
  ::CoreML::Specification::FloorDivBroadcastableLayerParams* _msg = _internal_mutable_floordivbroadcastable();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.floorDivBroadcastable)
  return _msg;
}

// .CoreML.Specification.MultiplyBroadcastableLayerParams multiplyBroadcastable = 900;
inline bool NeuralNetworkLayer::_internal_has_multiplybroadcastable() const {
  return layer_case() == kMultiplyBroadcastable;
}
inline bool NeuralNetworkLayer::has_multiplybroadcastable() const {
  return _internal_has_multiplybroadcastable();
}
inline void NeuralNetworkLayer::set_has_multiplybroadcastable() {
  _oneof_case_[0] = kMultiplyBroadcastable;
}
inline void NeuralNetworkLayer::clear_multiplybroadcastable() {
  if (_internal_has_multiplybroadcastable()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.multiplybroadcastable_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::MultiplyBroadcastableLayerParams* NeuralNetworkLayer::release_multiplybroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.multiplyBroadcastable)
  if (_internal_has_multiplybroadcastable()) {
    clear_has_layer();
      ::CoreML::Specification::MultiplyBroadcastableLayerParams* temp = layer_.multiplybroadcastable_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.multiplybroadcastable_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::MultiplyBroadcastableLayerParams& NeuralNetworkLayer::_internal_multiplybroadcastable() const {
  return _internal_has_multiplybroadcastable()
      ? *layer_.multiplybroadcastable_
      : reinterpret_cast< ::CoreML::Specification::MultiplyBroadcastableLayerParams&>(::CoreML::Specification::_MultiplyBroadcastableLayerParams_default_instance_);
}
inline const ::CoreML::Specification::MultiplyBroadcastableLayerParams& NeuralNetworkLayer::multiplybroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.multiplyBroadcastable)
  return _internal_multiplybroadcastable();
}
inline ::CoreML::Specification::MultiplyBroadcastableLayerParams* NeuralNetworkLayer::unsafe_arena_release_multiplybroadcastable() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.multiplyBroadcastable)
  if (_internal_has_multiplybroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::MultiplyBroadcastableLayerParams* temp = layer_.multiplybroadcastable_;
    layer_.multiplybroadcastable_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_multiplybroadcastable(::CoreML::Specification::MultiplyBroadcastableLayerParams* multiplybroadcastable) {
  clear_layer();
  if (multiplybroadcastable) {
    set_has_multiplybroadcastable();
    layer_.multiplybroadcastable_ = multiplybroadcastable;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.multiplyBroadcastable)
}
inline ::CoreML::Specification::MultiplyBroadcastableLayerParams* NeuralNetworkLayer::_internal_mutable_multiplybroadcastable() {
  if (!_internal_has_multiplybroadcastable()) {
    clear_layer();
    set_has_multiplybroadcastable();
    layer_.multiplybroadcastable_ = CreateMaybeMessage< ::CoreML::Specification::MultiplyBroadcastableLayerParams >(GetArenaForAllocation());
  }
  return layer_.multiplybroadcastable_;
}
inline ::CoreML::Specification::MultiplyBroadcastableLayerParams* NeuralNetworkLayer::mutable_multiplybroadcastable() {
  ::CoreML::Specification::MultiplyBroadcastableLayerParams* _msg = _internal_mutable_multiplybroadcastable();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.multiplyBroadcastable)
  return _msg;
}

// .CoreML.Specification.SubtractBroadcastableLayerParams subtractBroadcastable = 905;
inline bool NeuralNetworkLayer::_internal_has_subtractbroadcastable() const {
  return layer_case() == kSubtractBroadcastable;
}
inline bool NeuralNetworkLayer::has_subtractbroadcastable() const {
  return _internal_has_subtractbroadcastable();
}
inline void NeuralNetworkLayer::set_has_subtractbroadcastable() {
  _oneof_case_[0] = kSubtractBroadcastable;
}
inline void NeuralNetworkLayer::clear_subtractbroadcastable() {
  if (_internal_has_subtractbroadcastable()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.subtractbroadcastable_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::SubtractBroadcastableLayerParams* NeuralNetworkLayer::release_subtractbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.subtractBroadcastable)
  if (_internal_has_subtractbroadcastable()) {
    clear_has_layer();
      ::CoreML::Specification::SubtractBroadcastableLayerParams* temp = layer_.subtractbroadcastable_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.subtractbroadcastable_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::SubtractBroadcastableLayerParams& NeuralNetworkLayer::_internal_subtractbroadcastable() const {
  return _internal_has_subtractbroadcastable()
      ? *layer_.subtractbroadcastable_
      : reinterpret_cast< ::CoreML::Specification::SubtractBroadcastableLayerParams&>(::CoreML::Specification::_SubtractBroadcastableLayerParams_default_instance_);
}
inline const ::CoreML::Specification::SubtractBroadcastableLayerParams& NeuralNetworkLayer::subtractbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.subtractBroadcastable)
  return _internal_subtractbroadcastable();
}
inline ::CoreML::Specification::SubtractBroadcastableLayerParams* NeuralNetworkLayer::unsafe_arena_release_subtractbroadcastable() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.subtractBroadcastable)
  if (_internal_has_subtractbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::SubtractBroadcastableLayerParams* temp = layer_.subtractbroadcastable_;
    layer_.subtractbroadcastable_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_subtractbroadcastable(::CoreML::Specification::SubtractBroadcastableLayerParams* subtractbroadcastable) {
  clear_layer();
  if (subtractbroadcastable) {
    set_has_subtractbroadcastable();
    layer_.subtractbroadcastable_ = subtractbroadcastable;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.subtractBroadcastable)
}
inline ::CoreML::Specification::SubtractBroadcastableLayerParams* NeuralNetworkLayer::_internal_mutable_subtractbroadcastable() {
  if (!_internal_has_subtractbroadcastable()) {
    clear_layer();
    set_has_subtractbroadcastable();
    layer_.subtractbroadcastable_ = CreateMaybeMessage< ::CoreML::Specification::SubtractBroadcastableLayerParams >(GetArenaForAllocation());
  }
  return layer_.subtractbroadcastable_;
}
inline ::CoreML::Specification::SubtractBroadcastableLayerParams* NeuralNetworkLayer::mutable_subtractbroadcastable() {
  ::CoreML::Specification::SubtractBroadcastableLayerParams* _msg = _internal_mutable_subtractbroadcastable();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.subtractBroadcastable)
  return _msg;
}

// .CoreML.Specification.TileLayerParams tile = 920;
inline bool NeuralNetworkLayer::_internal_has_tile() const {
  return layer_case() == kTile;
}
inline bool NeuralNetworkLayer::has_tile() const {
  return _internal_has_tile();
}
inline void NeuralNetworkLayer::set_has_tile() {
  _oneof_case_[0] = kTile;
}
inline void NeuralNetworkLayer::clear_tile() {
  if (_internal_has_tile()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.tile_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::TileLayerParams* NeuralNetworkLayer::release_tile() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.tile)
  if (_internal_has_tile()) {
    clear_has_layer();
      ::CoreML::Specification::TileLayerParams* temp = layer_.tile_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.tile_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::TileLayerParams& NeuralNetworkLayer::_internal_tile() const {
  return _internal_has_tile()
      ? *layer_.tile_
      : reinterpret_cast< ::CoreML::Specification::TileLayerParams&>(::CoreML::Specification::_TileLayerParams_default_instance_);
}
inline const ::CoreML::Specification::TileLayerParams& NeuralNetworkLayer::tile() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.tile)
  return _internal_tile();
}
inline ::CoreML::Specification::TileLayerParams* NeuralNetworkLayer::unsafe_arena_release_tile() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.tile)
  if (_internal_has_tile()) {
    clear_has_layer();
    ::CoreML::Specification::TileLayerParams* temp = layer_.tile_;
    layer_.tile_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_tile(::CoreML::Specification::TileLayerParams* tile) {
  clear_layer();
  if (tile) {
    set_has_tile();
    layer_.tile_ = tile;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.tile)
}
inline ::CoreML::Specification::TileLayerParams* NeuralNetworkLayer::_internal_mutable_tile() {
  if (!_internal_has_tile()) {
    clear_layer();
    set_has_tile();
    layer_.tile_ = CreateMaybeMessage< ::CoreML::Specification::TileLayerParams >(GetArenaForAllocation());
  }
  return layer_.tile_;
}
inline ::CoreML::Specification::TileLayerParams* NeuralNetworkLayer::mutable_tile() {
  ::CoreML::Specification::TileLayerParams* _msg = _internal_mutable_tile();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.tile)
  return _msg;
}

// .CoreML.Specification.StackLayerParams stack = 925;
inline bool NeuralNetworkLayer::_internal_has_stack() const {
  return layer_case() == kStack;
}
inline bool NeuralNetworkLayer::has_stack() const {
  return _internal_has_stack();
}
inline void NeuralNetworkLayer::set_has_stack() {
  _oneof_case_[0] = kStack;
}
inline void NeuralNetworkLayer::clear_stack() {
  if (_internal_has_stack()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.stack_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::StackLayerParams* NeuralNetworkLayer::release_stack() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.stack)
  if (_internal_has_stack()) {
    clear_has_layer();
      ::CoreML::Specification::StackLayerParams* temp = layer_.stack_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.stack_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::StackLayerParams& NeuralNetworkLayer::_internal_stack() const {
  return _internal_has_stack()
      ? *layer_.stack_
      : reinterpret_cast< ::CoreML::Specification::StackLayerParams&>(::CoreML::Specification::_StackLayerParams_default_instance_);
}
inline const ::CoreML::Specification::StackLayerParams& NeuralNetworkLayer::stack() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.stack)
  return _internal_stack();
}
inline ::CoreML::Specification::StackLayerParams* NeuralNetworkLayer::unsafe_arena_release_stack() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.stack)
  if (_internal_has_stack()) {
    clear_has_layer();
    ::CoreML::Specification::StackLayerParams* temp = layer_.stack_;
    layer_.stack_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_stack(::CoreML::Specification::StackLayerParams* stack) {
  clear_layer();
  if (stack) {
    set_has_stack();
    layer_.stack_ = stack;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.stack)
}
inline ::CoreML::Specification::StackLayerParams* NeuralNetworkLayer::_internal_mutable_stack() {
  if (!_internal_has_stack()) {
    clear_layer();
    set_has_stack();
    layer_.stack_ = CreateMaybeMessage< ::CoreML::Specification::StackLayerParams >(GetArenaForAllocation());
  }
  return layer_.stack_;
}
inline ::CoreML::Specification::StackLayerParams* NeuralNetworkLayer::mutable_stack() {
  ::CoreML::Specification::StackLayerParams* _msg = _internal_mutable_stack();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.stack)
  return _msg;
}

// .CoreML.Specification.GatherLayerParams gather = 930;
inline bool NeuralNetworkLayer::_internal_has_gather() const {
  return layer_case() == kGather;
}
inline bool NeuralNetworkLayer::has_gather() const {
  return _internal_has_gather();
}
inline void NeuralNetworkLayer::set_has_gather() {
  _oneof_case_[0] = kGather;
}
inline void NeuralNetworkLayer::clear_gather() {
  if (_internal_has_gather()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.gather_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::GatherLayerParams* NeuralNetworkLayer::release_gather() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.gather)
  if (_internal_has_gather()) {
    clear_has_layer();
      ::CoreML::Specification::GatherLayerParams* temp = layer_.gather_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.gather_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::GatherLayerParams& NeuralNetworkLayer::_internal_gather() const {
  return _internal_has_gather()
      ? *layer_.gather_
      : reinterpret_cast< ::CoreML::Specification::GatherLayerParams&>(::CoreML::Specification::_GatherLayerParams_default_instance_);
}
inline const ::CoreML::Specification::GatherLayerParams& NeuralNetworkLayer::gather() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.gather)
  return _internal_gather();
}
inline ::CoreML::Specification::GatherLayerParams* NeuralNetworkLayer::unsafe_arena_release_gather() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.gather)
  if (_internal_has_gather()) {
    clear_has_layer();
    ::CoreML::Specification::GatherLayerParams* temp = layer_.gather_;
    layer_.gather_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_gather(::CoreML::Specification::GatherLayerParams* gather) {
  clear_layer();
  if (gather) {
    set_has_gather();
    layer_.gather_ = gather;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.gather)
}
inline ::CoreML::Specification::GatherLayerParams* NeuralNetworkLayer::_internal_mutable_gather() {
  if (!_internal_has_gather()) {
    clear_layer();
    set_has_gather();
    layer_.gather_ = CreateMaybeMessage< ::CoreML::Specification::GatherLayerParams >(GetArenaForAllocation());
  }
  return layer_.gather_;
}
inline ::CoreML::Specification::GatherLayerParams* NeuralNetworkLayer::mutable_gather() {
  ::CoreML::Specification::GatherLayerParams* _msg = _internal_mutable_gather();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.gather)
  return _msg;
}

// .CoreML.Specification.ScatterLayerParams scatter = 935;
inline bool NeuralNetworkLayer::_internal_has_scatter() const {
  return layer_case() == kScatter;
}
inline bool NeuralNetworkLayer::has_scatter() const {
  return _internal_has_scatter();
}
inline void NeuralNetworkLayer::set_has_scatter() {
  _oneof_case_[0] = kScatter;
}
inline void NeuralNetworkLayer::clear_scatter() {
  if (_internal_has_scatter()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.scatter_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ScatterLayerParams* NeuralNetworkLayer::release_scatter() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.scatter)
  if (_internal_has_scatter()) {
    clear_has_layer();
      ::CoreML::Specification::ScatterLayerParams* temp = layer_.scatter_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.scatter_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ScatterLayerParams& NeuralNetworkLayer::_internal_scatter() const {
  return _internal_has_scatter()
      ? *layer_.scatter_
      : reinterpret_cast< ::CoreML::Specification::ScatterLayerParams&>(::CoreML::Specification::_ScatterLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ScatterLayerParams& NeuralNetworkLayer::scatter() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.scatter)
  return _internal_scatter();
}
inline ::CoreML::Specification::ScatterLayerParams* NeuralNetworkLayer::unsafe_arena_release_scatter() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.scatter)
  if (_internal_has_scatter()) {
    clear_has_layer();
    ::CoreML::Specification::ScatterLayerParams* temp = layer_.scatter_;
    layer_.scatter_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_scatter(::CoreML::Specification::ScatterLayerParams* scatter) {
  clear_layer();
  if (scatter) {
    set_has_scatter();
    layer_.scatter_ = scatter;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.scatter)
}
inline ::CoreML::Specification::ScatterLayerParams* NeuralNetworkLayer::_internal_mutable_scatter() {
  if (!_internal_has_scatter()) {
    clear_layer();
    set_has_scatter();
    layer_.scatter_ = CreateMaybeMessage< ::CoreML::Specification::ScatterLayerParams >(GetArenaForAllocation());
  }
  return layer_.scatter_;
}
inline ::CoreML::Specification::ScatterLayerParams* NeuralNetworkLayer::mutable_scatter() {
  ::CoreML::Specification::ScatterLayerParams* _msg = _internal_mutable_scatter();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.scatter)
  return _msg;
}

// .CoreML.Specification.GatherNDLayerParams gatherND = 940;
inline bool NeuralNetworkLayer::_internal_has_gathernd() const {
  return layer_case() == kGatherND;
}
inline bool NeuralNetworkLayer::has_gathernd() const {
  return _internal_has_gathernd();
}
inline void NeuralNetworkLayer::set_has_gathernd() {
  _oneof_case_[0] = kGatherND;
}
inline void NeuralNetworkLayer::clear_gathernd() {
  if (_internal_has_gathernd()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.gathernd_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::GatherNDLayerParams* NeuralNetworkLayer::release_gathernd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.gatherND)
  if (_internal_has_gathernd()) {
    clear_has_layer();
      ::CoreML::Specification::GatherNDLayerParams* temp = layer_.gathernd_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.gathernd_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::GatherNDLayerParams& NeuralNetworkLayer::_internal_gathernd() const {
  return _internal_has_gathernd()
      ? *layer_.gathernd_
      : reinterpret_cast< ::CoreML::Specification::GatherNDLayerParams&>(::CoreML::Specification::_GatherNDLayerParams_default_instance_);
}
inline const ::CoreML::Specification::GatherNDLayerParams& NeuralNetworkLayer::gathernd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.gatherND)
  return _internal_gathernd();
}
inline ::CoreML::Specification::GatherNDLayerParams* NeuralNetworkLayer::unsafe_arena_release_gathernd() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.gatherND)
  if (_internal_has_gathernd()) {
    clear_has_layer();
    ::CoreML::Specification::GatherNDLayerParams* temp = layer_.gathernd_;
    layer_.gathernd_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_gathernd(::CoreML::Specification::GatherNDLayerParams* gathernd) {
  clear_layer();
  if (gathernd) {
    set_has_gathernd();
    layer_.gathernd_ = gathernd;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.gatherND)
}
inline ::CoreML::Specification::GatherNDLayerParams* NeuralNetworkLayer::_internal_mutable_gathernd() {
  if (!_internal_has_gathernd()) {
    clear_layer();
    set_has_gathernd();
    layer_.gathernd_ = CreateMaybeMessage< ::CoreML::Specification::GatherNDLayerParams >(GetArenaForAllocation());
  }
  return layer_.gathernd_;
}
inline ::CoreML::Specification::GatherNDLayerParams* NeuralNetworkLayer::mutable_gathernd() {
  ::CoreML::Specification::GatherNDLayerParams* _msg = _internal_mutable_gathernd();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.gatherND)
  return _msg;
}

// .CoreML.Specification.ScatterNDLayerParams scatterND = 945;
inline bool NeuralNetworkLayer::_internal_has_scatternd() const {
  return layer_case() == kScatterND;
}
inline bool NeuralNetworkLayer::has_scatternd() const {
  return _internal_has_scatternd();
}
inline void NeuralNetworkLayer::set_has_scatternd() {
  _oneof_case_[0] = kScatterND;
}
inline void NeuralNetworkLayer::clear_scatternd() {
  if (_internal_has_scatternd()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.scatternd_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ScatterNDLayerParams* NeuralNetworkLayer::release_scatternd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.scatterND)
  if (_internal_has_scatternd()) {
    clear_has_layer();
      ::CoreML::Specification::ScatterNDLayerParams* temp = layer_.scatternd_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.scatternd_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ScatterNDLayerParams& NeuralNetworkLayer::_internal_scatternd() const {
  return _internal_has_scatternd()
      ? *layer_.scatternd_
      : reinterpret_cast< ::CoreML::Specification::ScatterNDLayerParams&>(::CoreML::Specification::_ScatterNDLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ScatterNDLayerParams& NeuralNetworkLayer::scatternd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.scatterND)
  return _internal_scatternd();
}
inline ::CoreML::Specification::ScatterNDLayerParams* NeuralNetworkLayer::unsafe_arena_release_scatternd() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.scatterND)
  if (_internal_has_scatternd()) {
    clear_has_layer();
    ::CoreML::Specification::ScatterNDLayerParams* temp = layer_.scatternd_;
    layer_.scatternd_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_scatternd(::CoreML::Specification::ScatterNDLayerParams* scatternd) {
  clear_layer();
  if (scatternd) {
    set_has_scatternd();
    layer_.scatternd_ = scatternd;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.scatterND)
}
inline ::CoreML::Specification::ScatterNDLayerParams* NeuralNetworkLayer::_internal_mutable_scatternd() {
  if (!_internal_has_scatternd()) {
    clear_layer();
    set_has_scatternd();
    layer_.scatternd_ = CreateMaybeMessage< ::CoreML::Specification::ScatterNDLayerParams >(GetArenaForAllocation());
  }
  return layer_.scatternd_;
}
inline ::CoreML::Specification::ScatterNDLayerParams* NeuralNetworkLayer::mutable_scatternd() {
  ::CoreML::Specification::ScatterNDLayerParams* _msg = _internal_mutable_scatternd();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.scatterND)
  return _msg;
}

// .CoreML.Specification.SoftmaxNDLayerParams softmaxND = 950;
inline bool NeuralNetworkLayer::_internal_has_softmaxnd() const {
  return layer_case() == kSoftmaxND;
}
inline bool NeuralNetworkLayer::has_softmaxnd() const {
  return _internal_has_softmaxnd();
}
inline void NeuralNetworkLayer::set_has_softmaxnd() {
  _oneof_case_[0] = kSoftmaxND;
}
inline void NeuralNetworkLayer::clear_softmaxnd() {
  if (_internal_has_softmaxnd()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.softmaxnd_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::SoftmaxNDLayerParams* NeuralNetworkLayer::release_softmaxnd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.softmaxND)
  if (_internal_has_softmaxnd()) {
    clear_has_layer();
      ::CoreML::Specification::SoftmaxNDLayerParams* temp = layer_.softmaxnd_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.softmaxnd_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::SoftmaxNDLayerParams& NeuralNetworkLayer::_internal_softmaxnd() const {
  return _internal_has_softmaxnd()
      ? *layer_.softmaxnd_
      : reinterpret_cast< ::CoreML::Specification::SoftmaxNDLayerParams&>(::CoreML::Specification::_SoftmaxNDLayerParams_default_instance_);
}
inline const ::CoreML::Specification::SoftmaxNDLayerParams& NeuralNetworkLayer::softmaxnd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.softmaxND)
  return _internal_softmaxnd();
}
inline ::CoreML::Specification::SoftmaxNDLayerParams* NeuralNetworkLayer::unsafe_arena_release_softmaxnd() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.softmaxND)
  if (_internal_has_softmaxnd()) {
    clear_has_layer();
    ::CoreML::Specification::SoftmaxNDLayerParams* temp = layer_.softmaxnd_;
    layer_.softmaxnd_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_softmaxnd(::CoreML::Specification::SoftmaxNDLayerParams* softmaxnd) {
  clear_layer();
  if (softmaxnd) {
    set_has_softmaxnd();
    layer_.softmaxnd_ = softmaxnd;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.softmaxND)
}
inline ::CoreML::Specification::SoftmaxNDLayerParams* NeuralNetworkLayer::_internal_mutable_softmaxnd() {
  if (!_internal_has_softmaxnd()) {
    clear_layer();
    set_has_softmaxnd();
    layer_.softmaxnd_ = CreateMaybeMessage< ::CoreML::Specification::SoftmaxNDLayerParams >(GetArenaForAllocation());
  }
  return layer_.softmaxnd_;
}
inline ::CoreML::Specification::SoftmaxNDLayerParams* NeuralNetworkLayer::mutable_softmaxnd() {
  ::CoreML::Specification::SoftmaxNDLayerParams* _msg = _internal_mutable_softmaxnd();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.softmaxND)
  return _msg;
}

// .CoreML.Specification.GatherAlongAxisLayerParams gatherAlongAxis = 952;
inline bool NeuralNetworkLayer::_internal_has_gatheralongaxis() const {
  return layer_case() == kGatherAlongAxis;
}
inline bool NeuralNetworkLayer::has_gatheralongaxis() const {
  return _internal_has_gatheralongaxis();
}
inline void NeuralNetworkLayer::set_has_gatheralongaxis() {
  _oneof_case_[0] = kGatherAlongAxis;
}
inline void NeuralNetworkLayer::clear_gatheralongaxis() {
  if (_internal_has_gatheralongaxis()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.gatheralongaxis_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::GatherAlongAxisLayerParams* NeuralNetworkLayer::release_gatheralongaxis() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.gatherAlongAxis)
  if (_internal_has_gatheralongaxis()) {
    clear_has_layer();
      ::CoreML::Specification::GatherAlongAxisLayerParams* temp = layer_.gatheralongaxis_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.gatheralongaxis_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::GatherAlongAxisLayerParams& NeuralNetworkLayer::_internal_gatheralongaxis() const {
  return _internal_has_gatheralongaxis()
      ? *layer_.gatheralongaxis_
      : reinterpret_cast< ::CoreML::Specification::GatherAlongAxisLayerParams&>(::CoreML::Specification::_GatherAlongAxisLayerParams_default_instance_);
}
inline const ::CoreML::Specification::GatherAlongAxisLayerParams& NeuralNetworkLayer::gatheralongaxis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.gatherAlongAxis)
  return _internal_gatheralongaxis();
}
inline ::CoreML::Specification::GatherAlongAxisLayerParams* NeuralNetworkLayer::unsafe_arena_release_gatheralongaxis() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.gatherAlongAxis)
  if (_internal_has_gatheralongaxis()) {
    clear_has_layer();
    ::CoreML::Specification::GatherAlongAxisLayerParams* temp = layer_.gatheralongaxis_;
    layer_.gatheralongaxis_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_gatheralongaxis(::CoreML::Specification::GatherAlongAxisLayerParams* gatheralongaxis) {
  clear_layer();
  if (gatheralongaxis) {
    set_has_gatheralongaxis();
    layer_.gatheralongaxis_ = gatheralongaxis;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.gatherAlongAxis)
}
inline ::CoreML::Specification::GatherAlongAxisLayerParams* NeuralNetworkLayer::_internal_mutable_gatheralongaxis() {
  if (!_internal_has_gatheralongaxis()) {
    clear_layer();
    set_has_gatheralongaxis();
    layer_.gatheralongaxis_ = CreateMaybeMessage< ::CoreML::Specification::GatherAlongAxisLayerParams >(GetArenaForAllocation());
  }
  return layer_.gatheralongaxis_;
}
inline ::CoreML::Specification::GatherAlongAxisLayerParams* NeuralNetworkLayer::mutable_gatheralongaxis() {
  ::CoreML::Specification::GatherAlongAxisLayerParams* _msg = _internal_mutable_gatheralongaxis();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.gatherAlongAxis)
  return _msg;
}

// .CoreML.Specification.ScatterAlongAxisLayerParams scatterAlongAxis = 954;
inline bool NeuralNetworkLayer::_internal_has_scatteralongaxis() const {
  return layer_case() == kScatterAlongAxis;
}
inline bool NeuralNetworkLayer::has_scatteralongaxis() const {
  return _internal_has_scatteralongaxis();
}
inline void NeuralNetworkLayer::set_has_scatteralongaxis() {
  _oneof_case_[0] = kScatterAlongAxis;
}
inline void NeuralNetworkLayer::clear_scatteralongaxis() {
  if (_internal_has_scatteralongaxis()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.scatteralongaxis_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ScatterAlongAxisLayerParams* NeuralNetworkLayer::release_scatteralongaxis() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.scatterAlongAxis)
  if (_internal_has_scatteralongaxis()) {
    clear_has_layer();
      ::CoreML::Specification::ScatterAlongAxisLayerParams* temp = layer_.scatteralongaxis_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.scatteralongaxis_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ScatterAlongAxisLayerParams& NeuralNetworkLayer::_internal_scatteralongaxis() const {
  return _internal_has_scatteralongaxis()
      ? *layer_.scatteralongaxis_
      : reinterpret_cast< ::CoreML::Specification::ScatterAlongAxisLayerParams&>(::CoreML::Specification::_ScatterAlongAxisLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ScatterAlongAxisLayerParams& NeuralNetworkLayer::scatteralongaxis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.scatterAlongAxis)
  return _internal_scatteralongaxis();
}
inline ::CoreML::Specification::ScatterAlongAxisLayerParams* NeuralNetworkLayer::unsafe_arena_release_scatteralongaxis() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.scatterAlongAxis)
  if (_internal_has_scatteralongaxis()) {
    clear_has_layer();
    ::CoreML::Specification::ScatterAlongAxisLayerParams* temp = layer_.scatteralongaxis_;
    layer_.scatteralongaxis_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_scatteralongaxis(::CoreML::Specification::ScatterAlongAxisLayerParams* scatteralongaxis) {
  clear_layer();
  if (scatteralongaxis) {
    set_has_scatteralongaxis();
    layer_.scatteralongaxis_ = scatteralongaxis;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.scatterAlongAxis)
}
inline ::CoreML::Specification::ScatterAlongAxisLayerParams* NeuralNetworkLayer::_internal_mutable_scatteralongaxis() {
  if (!_internal_has_scatteralongaxis()) {
    clear_layer();
    set_has_scatteralongaxis();
    layer_.scatteralongaxis_ = CreateMaybeMessage< ::CoreML::Specification::ScatterAlongAxisLayerParams >(GetArenaForAllocation());
  }
  return layer_.scatteralongaxis_;
}
inline ::CoreML::Specification::ScatterAlongAxisLayerParams* NeuralNetworkLayer::mutable_scatteralongaxis() {
  ::CoreML::Specification::ScatterAlongAxisLayerParams* _msg = _internal_mutable_scatteralongaxis();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.scatterAlongAxis)
  return _msg;
}

// .CoreML.Specification.ReverseLayerParams reverse = 960;
inline bool NeuralNetworkLayer::_internal_has_reverse() const {
  return layer_case() == kReverse;
}
inline bool NeuralNetworkLayer::has_reverse() const {
  return _internal_has_reverse();
}
inline void NeuralNetworkLayer::set_has_reverse() {
  _oneof_case_[0] = kReverse;
}
inline void NeuralNetworkLayer::clear_reverse() {
  if (_internal_has_reverse()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.reverse_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ReverseLayerParams* NeuralNetworkLayer::release_reverse() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reverse)
  if (_internal_has_reverse()) {
    clear_has_layer();
      ::CoreML::Specification::ReverseLayerParams* temp = layer_.reverse_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.reverse_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ReverseLayerParams& NeuralNetworkLayer::_internal_reverse() const {
  return _internal_has_reverse()
      ? *layer_.reverse_
      : reinterpret_cast< ::CoreML::Specification::ReverseLayerParams&>(::CoreML::Specification::_ReverseLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ReverseLayerParams& NeuralNetworkLayer::reverse() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reverse)
  return _internal_reverse();
}
inline ::CoreML::Specification::ReverseLayerParams* NeuralNetworkLayer::unsafe_arena_release_reverse() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.reverse)
  if (_internal_has_reverse()) {
    clear_has_layer();
    ::CoreML::Specification::ReverseLayerParams* temp = layer_.reverse_;
    layer_.reverse_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_reverse(::CoreML::Specification::ReverseLayerParams* reverse) {
  clear_layer();
  if (reverse) {
    set_has_reverse();
    layer_.reverse_ = reverse;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.reverse)
}
inline ::CoreML::Specification::ReverseLayerParams* NeuralNetworkLayer::_internal_mutable_reverse() {
  if (!_internal_has_reverse()) {
    clear_layer();
    set_has_reverse();
    layer_.reverse_ = CreateMaybeMessage< ::CoreML::Specification::ReverseLayerParams >(GetArenaForAllocation());
  }
  return layer_.reverse_;
}
inline ::CoreML::Specification::ReverseLayerParams* NeuralNetworkLayer::mutable_reverse() {
  ::CoreML::Specification::ReverseLayerParams* _msg = _internal_mutable_reverse();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reverse)
  return _msg;
}

// .CoreML.Specification.ReverseSeqLayerParams reverseSeq = 965;
inline bool NeuralNetworkLayer::_internal_has_reverseseq() const {
  return layer_case() == kReverseSeq;
}
inline bool NeuralNetworkLayer::has_reverseseq() const {
  return _internal_has_reverseseq();
}
inline void NeuralNetworkLayer::set_has_reverseseq() {
  _oneof_case_[0] = kReverseSeq;
}
inline void NeuralNetworkLayer::clear_reverseseq() {
  if (_internal_has_reverseseq()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.reverseseq_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ReverseSeqLayerParams* NeuralNetworkLayer::release_reverseseq() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reverseSeq)
  if (_internal_has_reverseseq()) {
    clear_has_layer();
      ::CoreML::Specification::ReverseSeqLayerParams* temp = layer_.reverseseq_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.reverseseq_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ReverseSeqLayerParams& NeuralNetworkLayer::_internal_reverseseq() const {
  return _internal_has_reverseseq()
      ? *layer_.reverseseq_
      : reinterpret_cast< ::CoreML::Specification::ReverseSeqLayerParams&>(::CoreML::Specification::_ReverseSeqLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ReverseSeqLayerParams& NeuralNetworkLayer::reverseseq() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reverseSeq)
  return _internal_reverseseq();
}
inline ::CoreML::Specification::ReverseSeqLayerParams* NeuralNetworkLayer::unsafe_arena_release_reverseseq() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.reverseSeq)
  if (_internal_has_reverseseq()) {
    clear_has_layer();
    ::CoreML::Specification::ReverseSeqLayerParams* temp = layer_.reverseseq_;
    layer_.reverseseq_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_reverseseq(::CoreML::Specification::ReverseSeqLayerParams* reverseseq) {
  clear_layer();
  if (reverseseq) {
    set_has_reverseseq();
    layer_.reverseseq_ = reverseseq;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.reverseSeq)
}
inline ::CoreML::Specification::ReverseSeqLayerParams* NeuralNetworkLayer::_internal_mutable_reverseseq() {
  if (!_internal_has_reverseseq()) {
    clear_layer();
    set_has_reverseseq();
    layer_.reverseseq_ = CreateMaybeMessage< ::CoreML::Specification::ReverseSeqLayerParams >(GetArenaForAllocation());
  }
  return layer_.reverseseq_;
}
inline ::CoreML::Specification::ReverseSeqLayerParams* NeuralNetworkLayer::mutable_reverseseq() {
  ::CoreML::Specification::ReverseSeqLayerParams* _msg = _internal_mutable_reverseseq();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reverseSeq)
  return _msg;
}

// .CoreML.Specification.SplitNDLayerParams splitND = 975;
inline bool NeuralNetworkLayer::_internal_has_splitnd() const {
  return layer_case() == kSplitND;
}
inline bool NeuralNetworkLayer::has_splitnd() const {
  return _internal_has_splitnd();
}
inline void NeuralNetworkLayer::set_has_splitnd() {
  _oneof_case_[0] = kSplitND;
}
inline void NeuralNetworkLayer::clear_splitnd() {
  if (_internal_has_splitnd()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.splitnd_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::SplitNDLayerParams* NeuralNetworkLayer::release_splitnd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.splitND)
  if (_internal_has_splitnd()) {
    clear_has_layer();
      ::CoreML::Specification::SplitNDLayerParams* temp = layer_.splitnd_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.splitnd_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::SplitNDLayerParams& NeuralNetworkLayer::_internal_splitnd() const {
  return _internal_has_splitnd()
      ? *layer_.splitnd_
      : reinterpret_cast< ::CoreML::Specification::SplitNDLayerParams&>(::CoreML::Specification::_SplitNDLayerParams_default_instance_);
}
inline const ::CoreML::Specification::SplitNDLayerParams& NeuralNetworkLayer::splitnd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.splitND)
  return _internal_splitnd();
}
inline ::CoreML::Specification::SplitNDLayerParams* NeuralNetworkLayer::unsafe_arena_release_splitnd() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.splitND)
  if (_internal_has_splitnd()) {
    clear_has_layer();
    ::CoreML::Specification::SplitNDLayerParams* temp = layer_.splitnd_;
    layer_.splitnd_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_splitnd(::CoreML::Specification::SplitNDLayerParams* splitnd) {
  clear_layer();
  if (splitnd) {
    set_has_splitnd();
    layer_.splitnd_ = splitnd;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.splitND)
}
inline ::CoreML::Specification::SplitNDLayerParams* NeuralNetworkLayer::_internal_mutable_splitnd() {
  if (!_internal_has_splitnd()) {
    clear_layer();
    set_has_splitnd();
    layer_.splitnd_ = CreateMaybeMessage< ::CoreML::Specification::SplitNDLayerParams >(GetArenaForAllocation());
  }
  return layer_.splitnd_;
}
inline ::CoreML::Specification::SplitNDLayerParams* NeuralNetworkLayer::mutable_splitnd() {
  ::CoreML::Specification::SplitNDLayerParams* _msg = _internal_mutable_splitnd();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.splitND)
  return _msg;
}

// .CoreML.Specification.ConcatNDLayerParams concatND = 980;
inline bool NeuralNetworkLayer::_internal_has_concatnd() const {
  return layer_case() == kConcatND;
}
inline bool NeuralNetworkLayer::has_concatnd() const {
  return _internal_has_concatnd();
}
inline void NeuralNetworkLayer::set_has_concatnd() {
  _oneof_case_[0] = kConcatND;
}
inline void NeuralNetworkLayer::clear_concatnd() {
  if (_internal_has_concatnd()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.concatnd_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ConcatNDLayerParams* NeuralNetworkLayer::release_concatnd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.concatND)
  if (_internal_has_concatnd()) {
    clear_has_layer();
      ::CoreML::Specification::ConcatNDLayerParams* temp = layer_.concatnd_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.concatnd_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ConcatNDLayerParams& NeuralNetworkLayer::_internal_concatnd() const {
  return _internal_has_concatnd()
      ? *layer_.concatnd_
      : reinterpret_cast< ::CoreML::Specification::ConcatNDLayerParams&>(::CoreML::Specification::_ConcatNDLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ConcatNDLayerParams& NeuralNetworkLayer::concatnd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.concatND)
  return _internal_concatnd();
}
inline ::CoreML::Specification::ConcatNDLayerParams* NeuralNetworkLayer::unsafe_arena_release_concatnd() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.concatND)
  if (_internal_has_concatnd()) {
    clear_has_layer();
    ::CoreML::Specification::ConcatNDLayerParams* temp = layer_.concatnd_;
    layer_.concatnd_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_concatnd(::CoreML::Specification::ConcatNDLayerParams* concatnd) {
  clear_layer();
  if (concatnd) {
    set_has_concatnd();
    layer_.concatnd_ = concatnd;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.concatND)
}
inline ::CoreML::Specification::ConcatNDLayerParams* NeuralNetworkLayer::_internal_mutable_concatnd() {
  if (!_internal_has_concatnd()) {
    clear_layer();
    set_has_concatnd();
    layer_.concatnd_ = CreateMaybeMessage< ::CoreML::Specification::ConcatNDLayerParams >(GetArenaForAllocation());
  }
  return layer_.concatnd_;
}
inline ::CoreML::Specification::ConcatNDLayerParams* NeuralNetworkLayer::mutable_concatnd() {
  ::CoreML::Specification::ConcatNDLayerParams* _msg = _internal_mutable_concatnd();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.concatND)
  return _msg;
}

// .CoreML.Specification.TransposeLayerParams transpose = 985;
inline bool NeuralNetworkLayer::_internal_has_transpose() const {
  return layer_case() == kTranspose;
}
inline bool NeuralNetworkLayer::has_transpose() const {
  return _internal_has_transpose();
}
inline void NeuralNetworkLayer::set_has_transpose() {
  _oneof_case_[0] = kTranspose;
}
inline void NeuralNetworkLayer::clear_transpose() {
  if (_internal_has_transpose()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.transpose_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::TransposeLayerParams* NeuralNetworkLayer::release_transpose() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.transpose)
  if (_internal_has_transpose()) {
    clear_has_layer();
      ::CoreML::Specification::TransposeLayerParams* temp = layer_.transpose_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.transpose_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::TransposeLayerParams& NeuralNetworkLayer::_internal_transpose() const {
  return _internal_has_transpose()
      ? *layer_.transpose_
      : reinterpret_cast< ::CoreML::Specification::TransposeLayerParams&>(::CoreML::Specification::_TransposeLayerParams_default_instance_);
}
inline const ::CoreML::Specification::TransposeLayerParams& NeuralNetworkLayer::transpose() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.transpose)
  return _internal_transpose();
}
inline ::CoreML::Specification::TransposeLayerParams* NeuralNetworkLayer::unsafe_arena_release_transpose() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.transpose)
  if (_internal_has_transpose()) {
    clear_has_layer();
    ::CoreML::Specification::TransposeLayerParams* temp = layer_.transpose_;
    layer_.transpose_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_transpose(::CoreML::Specification::TransposeLayerParams* transpose) {
  clear_layer();
  if (transpose) {
    set_has_transpose();
    layer_.transpose_ = transpose;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.transpose)
}
inline ::CoreML::Specification::TransposeLayerParams* NeuralNetworkLayer::_internal_mutable_transpose() {
  if (!_internal_has_transpose()) {
    clear_layer();
    set_has_transpose();
    layer_.transpose_ = CreateMaybeMessage< ::CoreML::Specification::TransposeLayerParams >(GetArenaForAllocation());
  }
  return layer_.transpose_;
}
inline ::CoreML::Specification::TransposeLayerParams* NeuralNetworkLayer::mutable_transpose() {
  ::CoreML::Specification::TransposeLayerParams* _msg = _internal_mutable_transpose();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.transpose)
  return _msg;
}

// .CoreML.Specification.SliceStaticLayerParams sliceStatic = 995;
inline bool NeuralNetworkLayer::_internal_has_slicestatic() const {
  return layer_case() == kSliceStatic;
}
inline bool NeuralNetworkLayer::has_slicestatic() const {
  return _internal_has_slicestatic();
}
inline void NeuralNetworkLayer::set_has_slicestatic() {
  _oneof_case_[0] = kSliceStatic;
}
inline void NeuralNetworkLayer::clear_slicestatic() {
  if (_internal_has_slicestatic()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.slicestatic_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::SliceStaticLayerParams* NeuralNetworkLayer::release_slicestatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sliceStatic)
  if (_internal_has_slicestatic()) {
    clear_has_layer();
      ::CoreML::Specification::SliceStaticLayerParams* temp = layer_.slicestatic_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.slicestatic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::SliceStaticLayerParams& NeuralNetworkLayer::_internal_slicestatic() const {
  return _internal_has_slicestatic()
      ? *layer_.slicestatic_
      : reinterpret_cast< ::CoreML::Specification::SliceStaticLayerParams&>(::CoreML::Specification::_SliceStaticLayerParams_default_instance_);
}
inline const ::CoreML::Specification::SliceStaticLayerParams& NeuralNetworkLayer::slicestatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sliceStatic)
  return _internal_slicestatic();
}
inline ::CoreML::Specification::SliceStaticLayerParams* NeuralNetworkLayer::unsafe_arena_release_slicestatic() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.sliceStatic)
  if (_internal_has_slicestatic()) {
    clear_has_layer();
    ::CoreML::Specification::SliceStaticLayerParams* temp = layer_.slicestatic_;
    layer_.slicestatic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_slicestatic(::CoreML::Specification::SliceStaticLayerParams* slicestatic) {
  clear_layer();
  if (slicestatic) {
    set_has_slicestatic();
    layer_.slicestatic_ = slicestatic;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.sliceStatic)
}
inline ::CoreML::Specification::SliceStaticLayerParams* NeuralNetworkLayer::_internal_mutable_slicestatic() {
  if (!_internal_has_slicestatic()) {
    clear_layer();
    set_has_slicestatic();
    layer_.slicestatic_ = CreateMaybeMessage< ::CoreML::Specification::SliceStaticLayerParams >(GetArenaForAllocation());
  }
  return layer_.slicestatic_;
}
inline ::CoreML::Specification::SliceStaticLayerParams* NeuralNetworkLayer::mutable_slicestatic() {
  ::CoreML::Specification::SliceStaticLayerParams* _msg = _internal_mutable_slicestatic();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sliceStatic)
  return _msg;
}

// .CoreML.Specification.SliceDynamicLayerParams sliceDynamic = 1000;
inline bool NeuralNetworkLayer::_internal_has_slicedynamic() const {
  return layer_case() == kSliceDynamic;
}
inline bool NeuralNetworkLayer::has_slicedynamic() const {
  return _internal_has_slicedynamic();
}
inline void NeuralNetworkLayer::set_has_slicedynamic() {
  _oneof_case_[0] = kSliceDynamic;
}
inline void NeuralNetworkLayer::clear_slicedynamic() {
  if (_internal_has_slicedynamic()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.slicedynamic_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::SliceDynamicLayerParams* NeuralNetworkLayer::release_slicedynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sliceDynamic)
  if (_internal_has_slicedynamic()) {
    clear_has_layer();
      ::CoreML::Specification::SliceDynamicLayerParams* temp = layer_.slicedynamic_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.slicedynamic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::SliceDynamicLayerParams& NeuralNetworkLayer::_internal_slicedynamic() const {
  return _internal_has_slicedynamic()
      ? *layer_.slicedynamic_
      : reinterpret_cast< ::CoreML::Specification::SliceDynamicLayerParams&>(::CoreML::Specification::_SliceDynamicLayerParams_default_instance_);
}
inline const ::CoreML::Specification::SliceDynamicLayerParams& NeuralNetworkLayer::slicedynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sliceDynamic)
  return _internal_slicedynamic();
}
inline ::CoreML::Specification::SliceDynamicLayerParams* NeuralNetworkLayer::unsafe_arena_release_slicedynamic() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.sliceDynamic)
  if (_internal_has_slicedynamic()) {
    clear_has_layer();
    ::CoreML::Specification::SliceDynamicLayerParams* temp = layer_.slicedynamic_;
    layer_.slicedynamic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_slicedynamic(::CoreML::Specification::SliceDynamicLayerParams* slicedynamic) {
  clear_layer();
  if (slicedynamic) {
    set_has_slicedynamic();
    layer_.slicedynamic_ = slicedynamic;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.sliceDynamic)
}
inline ::CoreML::Specification::SliceDynamicLayerParams* NeuralNetworkLayer::_internal_mutable_slicedynamic() {
  if (!_internal_has_slicedynamic()) {
    clear_layer();
    set_has_slicedynamic();
    layer_.slicedynamic_ = CreateMaybeMessage< ::CoreML::Specification::SliceDynamicLayerParams >(GetArenaForAllocation());
  }
  return layer_.slicedynamic_;
}
inline ::CoreML::Specification::SliceDynamicLayerParams* NeuralNetworkLayer::mutable_slicedynamic() {
  ::CoreML::Specification::SliceDynamicLayerParams* _msg = _internal_mutable_slicedynamic();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sliceDynamic)
  return _msg;
}

// .CoreML.Specification.SlidingWindowsLayerParams slidingWindows = 1005;
inline bool NeuralNetworkLayer::_internal_has_slidingwindows() const {
  return layer_case() == kSlidingWindows;
}
inline bool NeuralNetworkLayer::has_slidingwindows() const {
  return _internal_has_slidingwindows();
}
inline void NeuralNetworkLayer::set_has_slidingwindows() {
  _oneof_case_[0] = kSlidingWindows;
}
inline void NeuralNetworkLayer::clear_slidingwindows() {
  if (_internal_has_slidingwindows()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.slidingwindows_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::SlidingWindowsLayerParams* NeuralNetworkLayer::release_slidingwindows() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.slidingWindows)
  if (_internal_has_slidingwindows()) {
    clear_has_layer();
      ::CoreML::Specification::SlidingWindowsLayerParams* temp = layer_.slidingwindows_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.slidingwindows_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::SlidingWindowsLayerParams& NeuralNetworkLayer::_internal_slidingwindows() const {
  return _internal_has_slidingwindows()
      ? *layer_.slidingwindows_
      : reinterpret_cast< ::CoreML::Specification::SlidingWindowsLayerParams&>(::CoreML::Specification::_SlidingWindowsLayerParams_default_instance_);
}
inline const ::CoreML::Specification::SlidingWindowsLayerParams& NeuralNetworkLayer::slidingwindows() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.slidingWindows)
  return _internal_slidingwindows();
}
inline ::CoreML::Specification::SlidingWindowsLayerParams* NeuralNetworkLayer::unsafe_arena_release_slidingwindows() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.slidingWindows)
  if (_internal_has_slidingwindows()) {
    clear_has_layer();
    ::CoreML::Specification::SlidingWindowsLayerParams* temp = layer_.slidingwindows_;
    layer_.slidingwindows_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_slidingwindows(::CoreML::Specification::SlidingWindowsLayerParams* slidingwindows) {
  clear_layer();
  if (slidingwindows) {
    set_has_slidingwindows();
    layer_.slidingwindows_ = slidingwindows;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.slidingWindows)
}
inline ::CoreML::Specification::SlidingWindowsLayerParams* NeuralNetworkLayer::_internal_mutable_slidingwindows() {
  if (!_internal_has_slidingwindows()) {
    clear_layer();
    set_has_slidingwindows();
    layer_.slidingwindows_ = CreateMaybeMessage< ::CoreML::Specification::SlidingWindowsLayerParams >(GetArenaForAllocation());
  }
  return layer_.slidingwindows_;
}
inline ::CoreML::Specification::SlidingWindowsLayerParams* NeuralNetworkLayer::mutable_slidingwindows() {
  ::CoreML::Specification::SlidingWindowsLayerParams* _msg = _internal_mutable_slidingwindows();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.slidingWindows)
  return _msg;
}

// .CoreML.Specification.TopKLayerParams topK = 1015;
inline bool NeuralNetworkLayer::_internal_has_topk() const {
  return layer_case() == kTopK;
}
inline bool NeuralNetworkLayer::has_topk() const {
  return _internal_has_topk();
}
inline void NeuralNetworkLayer::set_has_topk() {
  _oneof_case_[0] = kTopK;
}
inline void NeuralNetworkLayer::clear_topk() {
  if (_internal_has_topk()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.topk_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::TopKLayerParams* NeuralNetworkLayer::release_topk() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.topK)
  if (_internal_has_topk()) {
    clear_has_layer();
      ::CoreML::Specification::TopKLayerParams* temp = layer_.topk_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.topk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::TopKLayerParams& NeuralNetworkLayer::_internal_topk() const {
  return _internal_has_topk()
      ? *layer_.topk_
      : reinterpret_cast< ::CoreML::Specification::TopKLayerParams&>(::CoreML::Specification::_TopKLayerParams_default_instance_);
}
inline const ::CoreML::Specification::TopKLayerParams& NeuralNetworkLayer::topk() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.topK)
  return _internal_topk();
}
inline ::CoreML::Specification::TopKLayerParams* NeuralNetworkLayer::unsafe_arena_release_topk() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.topK)
  if (_internal_has_topk()) {
    clear_has_layer();
    ::CoreML::Specification::TopKLayerParams* temp = layer_.topk_;
    layer_.topk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_topk(::CoreML::Specification::TopKLayerParams* topk) {
  clear_layer();
  if (topk) {
    set_has_topk();
    layer_.topk_ = topk;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.topK)
}
inline ::CoreML::Specification::TopKLayerParams* NeuralNetworkLayer::_internal_mutable_topk() {
  if (!_internal_has_topk()) {
    clear_layer();
    set_has_topk();
    layer_.topk_ = CreateMaybeMessage< ::CoreML::Specification::TopKLayerParams >(GetArenaForAllocation());
  }
  return layer_.topk_;
}
inline ::CoreML::Specification::TopKLayerParams* NeuralNetworkLayer::mutable_topk() {
  ::CoreML::Specification::TopKLayerParams* _msg = _internal_mutable_topk();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.topK)
  return _msg;
}

// .CoreML.Specification.ArgMinLayerParams argMin = 1020;
inline bool NeuralNetworkLayer::_internal_has_argmin() const {
  return layer_case() == kArgMin;
}
inline bool NeuralNetworkLayer::has_argmin() const {
  return _internal_has_argmin();
}
inline void NeuralNetworkLayer::set_has_argmin() {
  _oneof_case_[0] = kArgMin;
}
inline void NeuralNetworkLayer::clear_argmin() {
  if (_internal_has_argmin()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.argmin_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ArgMinLayerParams* NeuralNetworkLayer::release_argmin() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.argMin)
  if (_internal_has_argmin()) {
    clear_has_layer();
      ::CoreML::Specification::ArgMinLayerParams* temp = layer_.argmin_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.argmin_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ArgMinLayerParams& NeuralNetworkLayer::_internal_argmin() const {
  return _internal_has_argmin()
      ? *layer_.argmin_
      : reinterpret_cast< ::CoreML::Specification::ArgMinLayerParams&>(::CoreML::Specification::_ArgMinLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ArgMinLayerParams& NeuralNetworkLayer::argmin() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.argMin)
  return _internal_argmin();
}
inline ::CoreML::Specification::ArgMinLayerParams* NeuralNetworkLayer::unsafe_arena_release_argmin() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.argMin)
  if (_internal_has_argmin()) {
    clear_has_layer();
    ::CoreML::Specification::ArgMinLayerParams* temp = layer_.argmin_;
    layer_.argmin_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_argmin(::CoreML::Specification::ArgMinLayerParams* argmin) {
  clear_layer();
  if (argmin) {
    set_has_argmin();
    layer_.argmin_ = argmin;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.argMin)
}
inline ::CoreML::Specification::ArgMinLayerParams* NeuralNetworkLayer::_internal_mutable_argmin() {
  if (!_internal_has_argmin()) {
    clear_layer();
    set_has_argmin();
    layer_.argmin_ = CreateMaybeMessage< ::CoreML::Specification::ArgMinLayerParams >(GetArenaForAllocation());
  }
  return layer_.argmin_;
}
inline ::CoreML::Specification::ArgMinLayerParams* NeuralNetworkLayer::mutable_argmin() {
  ::CoreML::Specification::ArgMinLayerParams* _msg = _internal_mutable_argmin();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.argMin)
  return _msg;
}

// .CoreML.Specification.ArgMaxLayerParams argMax = 1025;
inline bool NeuralNetworkLayer::_internal_has_argmax() const {
  return layer_case() == kArgMax;
}
inline bool NeuralNetworkLayer::has_argmax() const {
  return _internal_has_argmax();
}
inline void NeuralNetworkLayer::set_has_argmax() {
  _oneof_case_[0] = kArgMax;
}
inline void NeuralNetworkLayer::clear_argmax() {
  if (_internal_has_argmax()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.argmax_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ArgMaxLayerParams* NeuralNetworkLayer::release_argmax() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.argMax)
  if (_internal_has_argmax()) {
    clear_has_layer();
      ::CoreML::Specification::ArgMaxLayerParams* temp = layer_.argmax_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.argmax_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ArgMaxLayerParams& NeuralNetworkLayer::_internal_argmax() const {
  return _internal_has_argmax()
      ? *layer_.argmax_
      : reinterpret_cast< ::CoreML::Specification::ArgMaxLayerParams&>(::CoreML::Specification::_ArgMaxLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ArgMaxLayerParams& NeuralNetworkLayer::argmax() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.argMax)
  return _internal_argmax();
}
inline ::CoreML::Specification::ArgMaxLayerParams* NeuralNetworkLayer::unsafe_arena_release_argmax() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.argMax)
  if (_internal_has_argmax()) {
    clear_has_layer();
    ::CoreML::Specification::ArgMaxLayerParams* temp = layer_.argmax_;
    layer_.argmax_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_argmax(::CoreML::Specification::ArgMaxLayerParams* argmax) {
  clear_layer();
  if (argmax) {
    set_has_argmax();
    layer_.argmax_ = argmax;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.argMax)
}
inline ::CoreML::Specification::ArgMaxLayerParams* NeuralNetworkLayer::_internal_mutable_argmax() {
  if (!_internal_has_argmax()) {
    clear_layer();
    set_has_argmax();
    layer_.argmax_ = CreateMaybeMessage< ::CoreML::Specification::ArgMaxLayerParams >(GetArenaForAllocation());
  }
  return layer_.argmax_;
}
inline ::CoreML::Specification::ArgMaxLayerParams* NeuralNetworkLayer::mutable_argmax() {
  ::CoreML::Specification::ArgMaxLayerParams* _msg = _internal_mutable_argmax();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.argMax)
  return _msg;
}

// .CoreML.Specification.EmbeddingNDLayerParams embeddingND = 1040;
inline bool NeuralNetworkLayer::_internal_has_embeddingnd() const {
  return layer_case() == kEmbeddingND;
}
inline bool NeuralNetworkLayer::has_embeddingnd() const {
  return _internal_has_embeddingnd();
}
inline void NeuralNetworkLayer::set_has_embeddingnd() {
  _oneof_case_[0] = kEmbeddingND;
}
inline void NeuralNetworkLayer::clear_embeddingnd() {
  if (_internal_has_embeddingnd()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.embeddingnd_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::EmbeddingNDLayerParams* NeuralNetworkLayer::release_embeddingnd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.embeddingND)
  if (_internal_has_embeddingnd()) {
    clear_has_layer();
      ::CoreML::Specification::EmbeddingNDLayerParams* temp = layer_.embeddingnd_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.embeddingnd_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::EmbeddingNDLayerParams& NeuralNetworkLayer::_internal_embeddingnd() const {
  return _internal_has_embeddingnd()
      ? *layer_.embeddingnd_
      : reinterpret_cast< ::CoreML::Specification::EmbeddingNDLayerParams&>(::CoreML::Specification::_EmbeddingNDLayerParams_default_instance_);
}
inline const ::CoreML::Specification::EmbeddingNDLayerParams& NeuralNetworkLayer::embeddingnd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.embeddingND)
  return _internal_embeddingnd();
}
inline ::CoreML::Specification::EmbeddingNDLayerParams* NeuralNetworkLayer::unsafe_arena_release_embeddingnd() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.embeddingND)
  if (_internal_has_embeddingnd()) {
    clear_has_layer();
    ::CoreML::Specification::EmbeddingNDLayerParams* temp = layer_.embeddingnd_;
    layer_.embeddingnd_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_embeddingnd(::CoreML::Specification::EmbeddingNDLayerParams* embeddingnd) {
  clear_layer();
  if (embeddingnd) {
    set_has_embeddingnd();
    layer_.embeddingnd_ = embeddingnd;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.embeddingND)
}
inline ::CoreML::Specification::EmbeddingNDLayerParams* NeuralNetworkLayer::_internal_mutable_embeddingnd() {
  if (!_internal_has_embeddingnd()) {
    clear_layer();
    set_has_embeddingnd();
    layer_.embeddingnd_ = CreateMaybeMessage< ::CoreML::Specification::EmbeddingNDLayerParams >(GetArenaForAllocation());
  }
  return layer_.embeddingnd_;
}
inline ::CoreML::Specification::EmbeddingNDLayerParams* NeuralNetworkLayer::mutable_embeddingnd() {
  ::CoreML::Specification::EmbeddingNDLayerParams* _msg = _internal_mutable_embeddingnd();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.embeddingND)
  return _msg;
}

// .CoreML.Specification.BatchedMatMulLayerParams batchedMatmul = 1045;
inline bool NeuralNetworkLayer::_internal_has_batchedmatmul() const {
  return layer_case() == kBatchedMatmul;
}
inline bool NeuralNetworkLayer::has_batchedmatmul() const {
  return _internal_has_batchedmatmul();
}
inline void NeuralNetworkLayer::set_has_batchedmatmul() {
  _oneof_case_[0] = kBatchedMatmul;
}
inline void NeuralNetworkLayer::clear_batchedmatmul() {
  if (_internal_has_batchedmatmul()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.batchedmatmul_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::BatchedMatMulLayerParams* NeuralNetworkLayer::release_batchedmatmul() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.batchedMatmul)
  if (_internal_has_batchedmatmul()) {
    clear_has_layer();
      ::CoreML::Specification::BatchedMatMulLayerParams* temp = layer_.batchedmatmul_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.batchedmatmul_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::BatchedMatMulLayerParams& NeuralNetworkLayer::_internal_batchedmatmul() const {
  return _internal_has_batchedmatmul()
      ? *layer_.batchedmatmul_
      : reinterpret_cast< ::CoreML::Specification::BatchedMatMulLayerParams&>(::CoreML::Specification::_BatchedMatMulLayerParams_default_instance_);
}
inline const ::CoreML::Specification::BatchedMatMulLayerParams& NeuralNetworkLayer::batchedmatmul() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.batchedMatmul)
  return _internal_batchedmatmul();
}
inline ::CoreML::Specification::BatchedMatMulLayerParams* NeuralNetworkLayer::unsafe_arena_release_batchedmatmul() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.batchedMatmul)
  if (_internal_has_batchedmatmul()) {
    clear_has_layer();
    ::CoreML::Specification::BatchedMatMulLayerParams* temp = layer_.batchedmatmul_;
    layer_.batchedmatmul_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_batchedmatmul(::CoreML::Specification::BatchedMatMulLayerParams* batchedmatmul) {
  clear_layer();
  if (batchedmatmul) {
    set_has_batchedmatmul();
    layer_.batchedmatmul_ = batchedmatmul;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.batchedMatmul)
}
inline ::CoreML::Specification::BatchedMatMulLayerParams* NeuralNetworkLayer::_internal_mutable_batchedmatmul() {
  if (!_internal_has_batchedmatmul()) {
    clear_layer();
    set_has_batchedmatmul();
    layer_.batchedmatmul_ = CreateMaybeMessage< ::CoreML::Specification::BatchedMatMulLayerParams >(GetArenaForAllocation());
  }
  return layer_.batchedmatmul_;
}
inline ::CoreML::Specification::BatchedMatMulLayerParams* NeuralNetworkLayer::mutable_batchedmatmul() {
  ::CoreML::Specification::BatchedMatMulLayerParams* _msg = _internal_mutable_batchedmatmul();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.batchedMatmul)
  return _msg;
}

// .CoreML.Specification.GetShapeLayerParams getShape = 1065;
inline bool NeuralNetworkLayer::_internal_has_getshape() const {
  return layer_case() == kGetShape;
}
inline bool NeuralNetworkLayer::has_getshape() const {
  return _internal_has_getshape();
}
inline void NeuralNetworkLayer::set_has_getshape() {
  _oneof_case_[0] = kGetShape;
}
inline void NeuralNetworkLayer::clear_getshape() {
  if (_internal_has_getshape()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.getshape_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::GetShapeLayerParams* NeuralNetworkLayer::release_getshape() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.getShape)
  if (_internal_has_getshape()) {
    clear_has_layer();
      ::CoreML::Specification::GetShapeLayerParams* temp = layer_.getshape_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.getshape_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::GetShapeLayerParams& NeuralNetworkLayer::_internal_getshape() const {
  return _internal_has_getshape()
      ? *layer_.getshape_
      : reinterpret_cast< ::CoreML::Specification::GetShapeLayerParams&>(::CoreML::Specification::_GetShapeLayerParams_default_instance_);
}
inline const ::CoreML::Specification::GetShapeLayerParams& NeuralNetworkLayer::getshape() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.getShape)
  return _internal_getshape();
}
inline ::CoreML::Specification::GetShapeLayerParams* NeuralNetworkLayer::unsafe_arena_release_getshape() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.getShape)
  if (_internal_has_getshape()) {
    clear_has_layer();
    ::CoreML::Specification::GetShapeLayerParams* temp = layer_.getshape_;
    layer_.getshape_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_getshape(::CoreML::Specification::GetShapeLayerParams* getshape) {
  clear_layer();
  if (getshape) {
    set_has_getshape();
    layer_.getshape_ = getshape;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.getShape)
}
inline ::CoreML::Specification::GetShapeLayerParams* NeuralNetworkLayer::_internal_mutable_getshape() {
  if (!_internal_has_getshape()) {
    clear_layer();
    set_has_getshape();
    layer_.getshape_ = CreateMaybeMessage< ::CoreML::Specification::GetShapeLayerParams >(GetArenaForAllocation());
  }
  return layer_.getshape_;
}
inline ::CoreML::Specification::GetShapeLayerParams* NeuralNetworkLayer::mutable_getshape() {
  ::CoreML::Specification::GetShapeLayerParams* _msg = _internal_mutable_getshape();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.getShape)
  return _msg;
}

// .CoreML.Specification.LoadConstantNDLayerParams loadConstantND = 1070;
inline bool NeuralNetworkLayer::_internal_has_loadconstantnd() const {
  return layer_case() == kLoadConstantND;
}
inline bool NeuralNetworkLayer::has_loadconstantnd() const {
  return _internal_has_loadconstantnd();
}
inline void NeuralNetworkLayer::set_has_loadconstantnd() {
  _oneof_case_[0] = kLoadConstantND;
}
inline void NeuralNetworkLayer::clear_loadconstantnd() {
  if (_internal_has_loadconstantnd()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.loadconstantnd_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::LoadConstantNDLayerParams* NeuralNetworkLayer::release_loadconstantnd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loadConstantND)
  if (_internal_has_loadconstantnd()) {
    clear_has_layer();
      ::CoreML::Specification::LoadConstantNDLayerParams* temp = layer_.loadconstantnd_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.loadconstantnd_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::LoadConstantNDLayerParams& NeuralNetworkLayer::_internal_loadconstantnd() const {
  return _internal_has_loadconstantnd()
      ? *layer_.loadconstantnd_
      : reinterpret_cast< ::CoreML::Specification::LoadConstantNDLayerParams&>(::CoreML::Specification::_LoadConstantNDLayerParams_default_instance_);
}
inline const ::CoreML::Specification::LoadConstantNDLayerParams& NeuralNetworkLayer::loadconstantnd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loadConstantND)
  return _internal_loadconstantnd();
}
inline ::CoreML::Specification::LoadConstantNDLayerParams* NeuralNetworkLayer::unsafe_arena_release_loadconstantnd() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.loadConstantND)
  if (_internal_has_loadconstantnd()) {
    clear_has_layer();
    ::CoreML::Specification::LoadConstantNDLayerParams* temp = layer_.loadconstantnd_;
    layer_.loadconstantnd_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_loadconstantnd(::CoreML::Specification::LoadConstantNDLayerParams* loadconstantnd) {
  clear_layer();
  if (loadconstantnd) {
    set_has_loadconstantnd();
    layer_.loadconstantnd_ = loadconstantnd;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.loadConstantND)
}
inline ::CoreML::Specification::LoadConstantNDLayerParams* NeuralNetworkLayer::_internal_mutable_loadconstantnd() {
  if (!_internal_has_loadconstantnd()) {
    clear_layer();
    set_has_loadconstantnd();
    layer_.loadconstantnd_ = CreateMaybeMessage< ::CoreML::Specification::LoadConstantNDLayerParams >(GetArenaForAllocation());
  }
  return layer_.loadconstantnd_;
}
inline ::CoreML::Specification::LoadConstantNDLayerParams* NeuralNetworkLayer::mutable_loadconstantnd() {
  ::CoreML::Specification::LoadConstantNDLayerParams* _msg = _internal_mutable_loadconstantnd();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loadConstantND)
  return _msg;
}

// .CoreML.Specification.FillLikeLayerParams fillLike = 1080;
inline bool NeuralNetworkLayer::_internal_has_filllike() const {
  return layer_case() == kFillLike;
}
inline bool NeuralNetworkLayer::has_filllike() const {
  return _internal_has_filllike();
}
inline void NeuralNetworkLayer::set_has_filllike() {
  _oneof_case_[0] = kFillLike;
}
inline void NeuralNetworkLayer::clear_filllike() {
  if (_internal_has_filllike()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.filllike_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::FillLikeLayerParams* NeuralNetworkLayer::release_filllike() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.fillLike)
  if (_internal_has_filllike()) {
    clear_has_layer();
      ::CoreML::Specification::FillLikeLayerParams* temp = layer_.filllike_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.filllike_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::FillLikeLayerParams& NeuralNetworkLayer::_internal_filllike() const {
  return _internal_has_filllike()
      ? *layer_.filllike_
      : reinterpret_cast< ::CoreML::Specification::FillLikeLayerParams&>(::CoreML::Specification::_FillLikeLayerParams_default_instance_);
}
inline const ::CoreML::Specification::FillLikeLayerParams& NeuralNetworkLayer::filllike() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.fillLike)
  return _internal_filllike();
}
inline ::CoreML::Specification::FillLikeLayerParams* NeuralNetworkLayer::unsafe_arena_release_filllike() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.fillLike)
  if (_internal_has_filllike()) {
    clear_has_layer();
    ::CoreML::Specification::FillLikeLayerParams* temp = layer_.filllike_;
    layer_.filllike_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_filllike(::CoreML::Specification::FillLikeLayerParams* filllike) {
  clear_layer();
  if (filllike) {
    set_has_filllike();
    layer_.filllike_ = filllike;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.fillLike)
}
inline ::CoreML::Specification::FillLikeLayerParams* NeuralNetworkLayer::_internal_mutable_filllike() {
  if (!_internal_has_filllike()) {
    clear_layer();
    set_has_filllike();
    layer_.filllike_ = CreateMaybeMessage< ::CoreML::Specification::FillLikeLayerParams >(GetArenaForAllocation());
  }
  return layer_.filllike_;
}
inline ::CoreML::Specification::FillLikeLayerParams* NeuralNetworkLayer::mutable_filllike() {
  ::CoreML::Specification::FillLikeLayerParams* _msg = _internal_mutable_filllike();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.fillLike)
  return _msg;
}

// .CoreML.Specification.FillStaticLayerParams fillStatic = 1085;
inline bool NeuralNetworkLayer::_internal_has_fillstatic() const {
  return layer_case() == kFillStatic;
}
inline bool NeuralNetworkLayer::has_fillstatic() const {
  return _internal_has_fillstatic();
}
inline void NeuralNetworkLayer::set_has_fillstatic() {
  _oneof_case_[0] = kFillStatic;
}
inline void NeuralNetworkLayer::clear_fillstatic() {
  if (_internal_has_fillstatic()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.fillstatic_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::FillStaticLayerParams* NeuralNetworkLayer::release_fillstatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.fillStatic)
  if (_internal_has_fillstatic()) {
    clear_has_layer();
      ::CoreML::Specification::FillStaticLayerParams* temp = layer_.fillstatic_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.fillstatic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::FillStaticLayerParams& NeuralNetworkLayer::_internal_fillstatic() const {
  return _internal_has_fillstatic()
      ? *layer_.fillstatic_
      : reinterpret_cast< ::CoreML::Specification::FillStaticLayerParams&>(::CoreML::Specification::_FillStaticLayerParams_default_instance_);
}
inline const ::CoreML::Specification::FillStaticLayerParams& NeuralNetworkLayer::fillstatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.fillStatic)
  return _internal_fillstatic();
}
inline ::CoreML::Specification::FillStaticLayerParams* NeuralNetworkLayer::unsafe_arena_release_fillstatic() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.fillStatic)
  if (_internal_has_fillstatic()) {
    clear_has_layer();
    ::CoreML::Specification::FillStaticLayerParams* temp = layer_.fillstatic_;
    layer_.fillstatic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_fillstatic(::CoreML::Specification::FillStaticLayerParams* fillstatic) {
  clear_layer();
  if (fillstatic) {
    set_has_fillstatic();
    layer_.fillstatic_ = fillstatic;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.fillStatic)
}
inline ::CoreML::Specification::FillStaticLayerParams* NeuralNetworkLayer::_internal_mutable_fillstatic() {
  if (!_internal_has_fillstatic()) {
    clear_layer();
    set_has_fillstatic();
    layer_.fillstatic_ = CreateMaybeMessage< ::CoreML::Specification::FillStaticLayerParams >(GetArenaForAllocation());
  }
  return layer_.fillstatic_;
}
inline ::CoreML::Specification::FillStaticLayerParams* NeuralNetworkLayer::mutable_fillstatic() {
  ::CoreML::Specification::FillStaticLayerParams* _msg = _internal_mutable_fillstatic();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.fillStatic)
  return _msg;
}

// .CoreML.Specification.FillDynamicLayerParams fillDynamic = 1090;
inline bool NeuralNetworkLayer::_internal_has_filldynamic() const {
  return layer_case() == kFillDynamic;
}
inline bool NeuralNetworkLayer::has_filldynamic() const {
  return _internal_has_filldynamic();
}
inline void NeuralNetworkLayer::set_has_filldynamic() {
  _oneof_case_[0] = kFillDynamic;
}
inline void NeuralNetworkLayer::clear_filldynamic() {
  if (_internal_has_filldynamic()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.filldynamic_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::FillDynamicLayerParams* NeuralNetworkLayer::release_filldynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.fillDynamic)
  if (_internal_has_filldynamic()) {
    clear_has_layer();
      ::CoreML::Specification::FillDynamicLayerParams* temp = layer_.filldynamic_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.filldynamic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::FillDynamicLayerParams& NeuralNetworkLayer::_internal_filldynamic() const {
  return _internal_has_filldynamic()
      ? *layer_.filldynamic_
      : reinterpret_cast< ::CoreML::Specification::FillDynamicLayerParams&>(::CoreML::Specification::_FillDynamicLayerParams_default_instance_);
}
inline const ::CoreML::Specification::FillDynamicLayerParams& NeuralNetworkLayer::filldynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.fillDynamic)
  return _internal_filldynamic();
}
inline ::CoreML::Specification::FillDynamicLayerParams* NeuralNetworkLayer::unsafe_arena_release_filldynamic() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.fillDynamic)
  if (_internal_has_filldynamic()) {
    clear_has_layer();
    ::CoreML::Specification::FillDynamicLayerParams* temp = layer_.filldynamic_;
    layer_.filldynamic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_filldynamic(::CoreML::Specification::FillDynamicLayerParams* filldynamic) {
  clear_layer();
  if (filldynamic) {
    set_has_filldynamic();
    layer_.filldynamic_ = filldynamic;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.fillDynamic)
}
inline ::CoreML::Specification::FillDynamicLayerParams* NeuralNetworkLayer::_internal_mutable_filldynamic() {
  if (!_internal_has_filldynamic()) {
    clear_layer();
    set_has_filldynamic();
    layer_.filldynamic_ = CreateMaybeMessage< ::CoreML::Specification::FillDynamicLayerParams >(GetArenaForAllocation());
  }
  return layer_.filldynamic_;
}
inline ::CoreML::Specification::FillDynamicLayerParams* NeuralNetworkLayer::mutable_filldynamic() {
  ::CoreML::Specification::FillDynamicLayerParams* _msg = _internal_mutable_filldynamic();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.fillDynamic)
  return _msg;
}

// .CoreML.Specification.BroadcastToLikeLayerParams broadcastToLike = 1100;
inline bool NeuralNetworkLayer::_internal_has_broadcasttolike() const {
  return layer_case() == kBroadcastToLike;
}
inline bool NeuralNetworkLayer::has_broadcasttolike() const {
  return _internal_has_broadcasttolike();
}
inline void NeuralNetworkLayer::set_has_broadcasttolike() {
  _oneof_case_[0] = kBroadcastToLike;
}
inline void NeuralNetworkLayer::clear_broadcasttolike() {
  if (_internal_has_broadcasttolike()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.broadcasttolike_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::BroadcastToLikeLayerParams* NeuralNetworkLayer::release_broadcasttolike() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.broadcastToLike)
  if (_internal_has_broadcasttolike()) {
    clear_has_layer();
      ::CoreML::Specification::BroadcastToLikeLayerParams* temp = layer_.broadcasttolike_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.broadcasttolike_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::BroadcastToLikeLayerParams& NeuralNetworkLayer::_internal_broadcasttolike() const {
  return _internal_has_broadcasttolike()
      ? *layer_.broadcasttolike_
      : reinterpret_cast< ::CoreML::Specification::BroadcastToLikeLayerParams&>(::CoreML::Specification::_BroadcastToLikeLayerParams_default_instance_);
}
inline const ::CoreML::Specification::BroadcastToLikeLayerParams& NeuralNetworkLayer::broadcasttolike() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.broadcastToLike)
  return _internal_broadcasttolike();
}
inline ::CoreML::Specification::BroadcastToLikeLayerParams* NeuralNetworkLayer::unsafe_arena_release_broadcasttolike() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.broadcastToLike)
  if (_internal_has_broadcasttolike()) {
    clear_has_layer();
    ::CoreML::Specification::BroadcastToLikeLayerParams* temp = layer_.broadcasttolike_;
    layer_.broadcasttolike_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_broadcasttolike(::CoreML::Specification::BroadcastToLikeLayerParams* broadcasttolike) {
  clear_layer();
  if (broadcasttolike) {
    set_has_broadcasttolike();
    layer_.broadcasttolike_ = broadcasttolike;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.broadcastToLike)
}
inline ::CoreML::Specification::BroadcastToLikeLayerParams* NeuralNetworkLayer::_internal_mutable_broadcasttolike() {
  if (!_internal_has_broadcasttolike()) {
    clear_layer();
    set_has_broadcasttolike();
    layer_.broadcasttolike_ = CreateMaybeMessage< ::CoreML::Specification::BroadcastToLikeLayerParams >(GetArenaForAllocation());
  }
  return layer_.broadcasttolike_;
}
inline ::CoreML::Specification::BroadcastToLikeLayerParams* NeuralNetworkLayer::mutable_broadcasttolike() {
  ::CoreML::Specification::BroadcastToLikeLayerParams* _msg = _internal_mutable_broadcasttolike();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.broadcastToLike)
  return _msg;
}

// .CoreML.Specification.BroadcastToStaticLayerParams broadcastToStatic = 1105;
inline bool NeuralNetworkLayer::_internal_has_broadcasttostatic() const {
  return layer_case() == kBroadcastToStatic;
}
inline bool NeuralNetworkLayer::has_broadcasttostatic() const {
  return _internal_has_broadcasttostatic();
}
inline void NeuralNetworkLayer::set_has_broadcasttostatic() {
  _oneof_case_[0] = kBroadcastToStatic;
}
inline void NeuralNetworkLayer::clear_broadcasttostatic() {
  if (_internal_has_broadcasttostatic()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.broadcasttostatic_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::BroadcastToStaticLayerParams* NeuralNetworkLayer::release_broadcasttostatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.broadcastToStatic)
  if (_internal_has_broadcasttostatic()) {
    clear_has_layer();
      ::CoreML::Specification::BroadcastToStaticLayerParams* temp = layer_.broadcasttostatic_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.broadcasttostatic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::BroadcastToStaticLayerParams& NeuralNetworkLayer::_internal_broadcasttostatic() const {
  return _internal_has_broadcasttostatic()
      ? *layer_.broadcasttostatic_
      : reinterpret_cast< ::CoreML::Specification::BroadcastToStaticLayerParams&>(::CoreML::Specification::_BroadcastToStaticLayerParams_default_instance_);
}
inline const ::CoreML::Specification::BroadcastToStaticLayerParams& NeuralNetworkLayer::broadcasttostatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.broadcastToStatic)
  return _internal_broadcasttostatic();
}
inline ::CoreML::Specification::BroadcastToStaticLayerParams* NeuralNetworkLayer::unsafe_arena_release_broadcasttostatic() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.broadcastToStatic)
  if (_internal_has_broadcasttostatic()) {
    clear_has_layer();
    ::CoreML::Specification::BroadcastToStaticLayerParams* temp = layer_.broadcasttostatic_;
    layer_.broadcasttostatic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_broadcasttostatic(::CoreML::Specification::BroadcastToStaticLayerParams* broadcasttostatic) {
  clear_layer();
  if (broadcasttostatic) {
    set_has_broadcasttostatic();
    layer_.broadcasttostatic_ = broadcasttostatic;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.broadcastToStatic)
}
inline ::CoreML::Specification::BroadcastToStaticLayerParams* NeuralNetworkLayer::_internal_mutable_broadcasttostatic() {
  if (!_internal_has_broadcasttostatic()) {
    clear_layer();
    set_has_broadcasttostatic();
    layer_.broadcasttostatic_ = CreateMaybeMessage< ::CoreML::Specification::BroadcastToStaticLayerParams >(GetArenaForAllocation());
  }
  return layer_.broadcasttostatic_;
}
inline ::CoreML::Specification::BroadcastToStaticLayerParams* NeuralNetworkLayer::mutable_broadcasttostatic() {
  ::CoreML::Specification::BroadcastToStaticLayerParams* _msg = _internal_mutable_broadcasttostatic();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.broadcastToStatic)
  return _msg;
}

// .CoreML.Specification.BroadcastToDynamicLayerParams broadcastToDynamic = 1110;
inline bool NeuralNetworkLayer::_internal_has_broadcasttodynamic() const {
  return layer_case() == kBroadcastToDynamic;
}
inline bool NeuralNetworkLayer::has_broadcasttodynamic() const {
  return _internal_has_broadcasttodynamic();
}
inline void NeuralNetworkLayer::set_has_broadcasttodynamic() {
  _oneof_case_[0] = kBroadcastToDynamic;
}
inline void NeuralNetworkLayer::clear_broadcasttodynamic() {
  if (_internal_has_broadcasttodynamic()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.broadcasttodynamic_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::BroadcastToDynamicLayerParams* NeuralNetworkLayer::release_broadcasttodynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.broadcastToDynamic)
  if (_internal_has_broadcasttodynamic()) {
    clear_has_layer();
      ::CoreML::Specification::BroadcastToDynamicLayerParams* temp = layer_.broadcasttodynamic_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.broadcasttodynamic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::BroadcastToDynamicLayerParams& NeuralNetworkLayer::_internal_broadcasttodynamic() const {
  return _internal_has_broadcasttodynamic()
      ? *layer_.broadcasttodynamic_
      : reinterpret_cast< ::CoreML::Specification::BroadcastToDynamicLayerParams&>(::CoreML::Specification::_BroadcastToDynamicLayerParams_default_instance_);
}
inline const ::CoreML::Specification::BroadcastToDynamicLayerParams& NeuralNetworkLayer::broadcasttodynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.broadcastToDynamic)
  return _internal_broadcasttodynamic();
}
inline ::CoreML::Specification::BroadcastToDynamicLayerParams* NeuralNetworkLayer::unsafe_arena_release_broadcasttodynamic() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.broadcastToDynamic)
  if (_internal_has_broadcasttodynamic()) {
    clear_has_layer();
    ::CoreML::Specification::BroadcastToDynamicLayerParams* temp = layer_.broadcasttodynamic_;
    layer_.broadcasttodynamic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_broadcasttodynamic(::CoreML::Specification::BroadcastToDynamicLayerParams* broadcasttodynamic) {
  clear_layer();
  if (broadcasttodynamic) {
    set_has_broadcasttodynamic();
    layer_.broadcasttodynamic_ = broadcasttodynamic;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.broadcastToDynamic)
}
inline ::CoreML::Specification::BroadcastToDynamicLayerParams* NeuralNetworkLayer::_internal_mutable_broadcasttodynamic() {
  if (!_internal_has_broadcasttodynamic()) {
    clear_layer();
    set_has_broadcasttodynamic();
    layer_.broadcasttodynamic_ = CreateMaybeMessage< ::CoreML::Specification::BroadcastToDynamicLayerParams >(GetArenaForAllocation());
  }
  return layer_.broadcasttodynamic_;
}
inline ::CoreML::Specification::BroadcastToDynamicLayerParams* NeuralNetworkLayer::mutable_broadcasttodynamic() {
  ::CoreML::Specification::BroadcastToDynamicLayerParams* _msg = _internal_mutable_broadcasttodynamic();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.broadcastToDynamic)
  return _msg;
}

// .CoreML.Specification.SqueezeLayerParams squeeze = 1120;
inline bool NeuralNetworkLayer::_internal_has_squeeze() const {
  return layer_case() == kSqueeze;
}
inline bool NeuralNetworkLayer::has_squeeze() const {
  return _internal_has_squeeze();
}
inline void NeuralNetworkLayer::set_has_squeeze() {
  _oneof_case_[0] = kSqueeze;
}
inline void NeuralNetworkLayer::clear_squeeze() {
  if (_internal_has_squeeze()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.squeeze_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::SqueezeLayerParams* NeuralNetworkLayer::release_squeeze() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.squeeze)
  if (_internal_has_squeeze()) {
    clear_has_layer();
      ::CoreML::Specification::SqueezeLayerParams* temp = layer_.squeeze_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.squeeze_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::SqueezeLayerParams& NeuralNetworkLayer::_internal_squeeze() const {
  return _internal_has_squeeze()
      ? *layer_.squeeze_
      : reinterpret_cast< ::CoreML::Specification::SqueezeLayerParams&>(::CoreML::Specification::_SqueezeLayerParams_default_instance_);
}
inline const ::CoreML::Specification::SqueezeLayerParams& NeuralNetworkLayer::squeeze() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.squeeze)
  return _internal_squeeze();
}
inline ::CoreML::Specification::SqueezeLayerParams* NeuralNetworkLayer::unsafe_arena_release_squeeze() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.squeeze)
  if (_internal_has_squeeze()) {
    clear_has_layer();
    ::CoreML::Specification::SqueezeLayerParams* temp = layer_.squeeze_;
    layer_.squeeze_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_squeeze(::CoreML::Specification::SqueezeLayerParams* squeeze) {
  clear_layer();
  if (squeeze) {
    set_has_squeeze();
    layer_.squeeze_ = squeeze;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.squeeze)
}
inline ::CoreML::Specification::SqueezeLayerParams* NeuralNetworkLayer::_internal_mutable_squeeze() {
  if (!_internal_has_squeeze()) {
    clear_layer();
    set_has_squeeze();
    layer_.squeeze_ = CreateMaybeMessage< ::CoreML::Specification::SqueezeLayerParams >(GetArenaForAllocation());
  }
  return layer_.squeeze_;
}
inline ::CoreML::Specification::SqueezeLayerParams* NeuralNetworkLayer::mutable_squeeze() {
  ::CoreML::Specification::SqueezeLayerParams* _msg = _internal_mutable_squeeze();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.squeeze)
  return _msg;
}

// .CoreML.Specification.ExpandDimsLayerParams expandDims = 1125;
inline bool NeuralNetworkLayer::_internal_has_expanddims() const {
  return layer_case() == kExpandDims;
}
inline bool NeuralNetworkLayer::has_expanddims() const {
  return _internal_has_expanddims();
}
inline void NeuralNetworkLayer::set_has_expanddims() {
  _oneof_case_[0] = kExpandDims;
}
inline void NeuralNetworkLayer::clear_expanddims() {
  if (_internal_has_expanddims()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.expanddims_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ExpandDimsLayerParams* NeuralNetworkLayer::release_expanddims() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.expandDims)
  if (_internal_has_expanddims()) {
    clear_has_layer();
      ::CoreML::Specification::ExpandDimsLayerParams* temp = layer_.expanddims_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.expanddims_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ExpandDimsLayerParams& NeuralNetworkLayer::_internal_expanddims() const {
  return _internal_has_expanddims()
      ? *layer_.expanddims_
      : reinterpret_cast< ::CoreML::Specification::ExpandDimsLayerParams&>(::CoreML::Specification::_ExpandDimsLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ExpandDimsLayerParams& NeuralNetworkLayer::expanddims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.expandDims)
  return _internal_expanddims();
}
inline ::CoreML::Specification::ExpandDimsLayerParams* NeuralNetworkLayer::unsafe_arena_release_expanddims() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.expandDims)
  if (_internal_has_expanddims()) {
    clear_has_layer();
    ::CoreML::Specification::ExpandDimsLayerParams* temp = layer_.expanddims_;
    layer_.expanddims_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_expanddims(::CoreML::Specification::ExpandDimsLayerParams* expanddims) {
  clear_layer();
  if (expanddims) {
    set_has_expanddims();
    layer_.expanddims_ = expanddims;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.expandDims)
}
inline ::CoreML::Specification::ExpandDimsLayerParams* NeuralNetworkLayer::_internal_mutable_expanddims() {
  if (!_internal_has_expanddims()) {
    clear_layer();
    set_has_expanddims();
    layer_.expanddims_ = CreateMaybeMessage< ::CoreML::Specification::ExpandDimsLayerParams >(GetArenaForAllocation());
  }
  return layer_.expanddims_;
}
inline ::CoreML::Specification::ExpandDimsLayerParams* NeuralNetworkLayer::mutable_expanddims() {
  ::CoreML::Specification::ExpandDimsLayerParams* _msg = _internal_mutable_expanddims();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.expandDims)
  return _msg;
}

// .CoreML.Specification.FlattenTo2DLayerParams flattenTo2D = 1130;
inline bool NeuralNetworkLayer::_internal_has_flattento2d() const {
  return layer_case() == kFlattenTo2D;
}
inline bool NeuralNetworkLayer::has_flattento2d() const {
  return _internal_has_flattento2d();
}
inline void NeuralNetworkLayer::set_has_flattento2d() {
  _oneof_case_[0] = kFlattenTo2D;
}
inline void NeuralNetworkLayer::clear_flattento2d() {
  if (_internal_has_flattento2d()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.flattento2d_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::FlattenTo2DLayerParams* NeuralNetworkLayer::release_flattento2d() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.flattenTo2D)
  if (_internal_has_flattento2d()) {
    clear_has_layer();
      ::CoreML::Specification::FlattenTo2DLayerParams* temp = layer_.flattento2d_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.flattento2d_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::FlattenTo2DLayerParams& NeuralNetworkLayer::_internal_flattento2d() const {
  return _internal_has_flattento2d()
      ? *layer_.flattento2d_
      : reinterpret_cast< ::CoreML::Specification::FlattenTo2DLayerParams&>(::CoreML::Specification::_FlattenTo2DLayerParams_default_instance_);
}
inline const ::CoreML::Specification::FlattenTo2DLayerParams& NeuralNetworkLayer::flattento2d() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.flattenTo2D)
  return _internal_flattento2d();
}
inline ::CoreML::Specification::FlattenTo2DLayerParams* NeuralNetworkLayer::unsafe_arena_release_flattento2d() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.flattenTo2D)
  if (_internal_has_flattento2d()) {
    clear_has_layer();
    ::CoreML::Specification::FlattenTo2DLayerParams* temp = layer_.flattento2d_;
    layer_.flattento2d_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_flattento2d(::CoreML::Specification::FlattenTo2DLayerParams* flattento2d) {
  clear_layer();
  if (flattento2d) {
    set_has_flattento2d();
    layer_.flattento2d_ = flattento2d;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.flattenTo2D)
}
inline ::CoreML::Specification::FlattenTo2DLayerParams* NeuralNetworkLayer::_internal_mutable_flattento2d() {
  if (!_internal_has_flattento2d()) {
    clear_layer();
    set_has_flattento2d();
    layer_.flattento2d_ = CreateMaybeMessage< ::CoreML::Specification::FlattenTo2DLayerParams >(GetArenaForAllocation());
  }
  return layer_.flattento2d_;
}
inline ::CoreML::Specification::FlattenTo2DLayerParams* NeuralNetworkLayer::mutable_flattento2d() {
  ::CoreML::Specification::FlattenTo2DLayerParams* _msg = _internal_mutable_flattento2d();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.flattenTo2D)
  return _msg;
}

// .CoreML.Specification.ReshapeLikeLayerParams reshapeLike = 1135;
inline bool NeuralNetworkLayer::_internal_has_reshapelike() const {
  return layer_case() == kReshapeLike;
}
inline bool NeuralNetworkLayer::has_reshapelike() const {
  return _internal_has_reshapelike();
}
inline void NeuralNetworkLayer::set_has_reshapelike() {
  _oneof_case_[0] = kReshapeLike;
}
inline void NeuralNetworkLayer::clear_reshapelike() {
  if (_internal_has_reshapelike()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.reshapelike_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ReshapeLikeLayerParams* NeuralNetworkLayer::release_reshapelike() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reshapeLike)
  if (_internal_has_reshapelike()) {
    clear_has_layer();
      ::CoreML::Specification::ReshapeLikeLayerParams* temp = layer_.reshapelike_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.reshapelike_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ReshapeLikeLayerParams& NeuralNetworkLayer::_internal_reshapelike() const {
  return _internal_has_reshapelike()
      ? *layer_.reshapelike_
      : reinterpret_cast< ::CoreML::Specification::ReshapeLikeLayerParams&>(::CoreML::Specification::_ReshapeLikeLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ReshapeLikeLayerParams& NeuralNetworkLayer::reshapelike() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reshapeLike)
  return _internal_reshapelike();
}
inline ::CoreML::Specification::ReshapeLikeLayerParams* NeuralNetworkLayer::unsafe_arena_release_reshapelike() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.reshapeLike)
  if (_internal_has_reshapelike()) {
    clear_has_layer();
    ::CoreML::Specification::ReshapeLikeLayerParams* temp = layer_.reshapelike_;
    layer_.reshapelike_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_reshapelike(::CoreML::Specification::ReshapeLikeLayerParams* reshapelike) {
  clear_layer();
  if (reshapelike) {
    set_has_reshapelike();
    layer_.reshapelike_ = reshapelike;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.reshapeLike)
}
inline ::CoreML::Specification::ReshapeLikeLayerParams* NeuralNetworkLayer::_internal_mutable_reshapelike() {
  if (!_internal_has_reshapelike()) {
    clear_layer();
    set_has_reshapelike();
    layer_.reshapelike_ = CreateMaybeMessage< ::CoreML::Specification::ReshapeLikeLayerParams >(GetArenaForAllocation());
  }
  return layer_.reshapelike_;
}
inline ::CoreML::Specification::ReshapeLikeLayerParams* NeuralNetworkLayer::mutable_reshapelike() {
  ::CoreML::Specification::ReshapeLikeLayerParams* _msg = _internal_mutable_reshapelike();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reshapeLike)
  return _msg;
}

// .CoreML.Specification.ReshapeStaticLayerParams reshapeStatic = 1140;
inline bool NeuralNetworkLayer::_internal_has_reshapestatic() const {
  return layer_case() == kReshapeStatic;
}
inline bool NeuralNetworkLayer::has_reshapestatic() const {
  return _internal_has_reshapestatic();
}
inline void NeuralNetworkLayer::set_has_reshapestatic() {
  _oneof_case_[0] = kReshapeStatic;
}
inline void NeuralNetworkLayer::clear_reshapestatic() {
  if (_internal_has_reshapestatic()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.reshapestatic_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ReshapeStaticLayerParams* NeuralNetworkLayer::release_reshapestatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reshapeStatic)
  if (_internal_has_reshapestatic()) {
    clear_has_layer();
      ::CoreML::Specification::ReshapeStaticLayerParams* temp = layer_.reshapestatic_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.reshapestatic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ReshapeStaticLayerParams& NeuralNetworkLayer::_internal_reshapestatic() const {
  return _internal_has_reshapestatic()
      ? *layer_.reshapestatic_
      : reinterpret_cast< ::CoreML::Specification::ReshapeStaticLayerParams&>(::CoreML::Specification::_ReshapeStaticLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ReshapeStaticLayerParams& NeuralNetworkLayer::reshapestatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reshapeStatic)
  return _internal_reshapestatic();
}
inline ::CoreML::Specification::ReshapeStaticLayerParams* NeuralNetworkLayer::unsafe_arena_release_reshapestatic() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.reshapeStatic)
  if (_internal_has_reshapestatic()) {
    clear_has_layer();
    ::CoreML::Specification::ReshapeStaticLayerParams* temp = layer_.reshapestatic_;
    layer_.reshapestatic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_reshapestatic(::CoreML::Specification::ReshapeStaticLayerParams* reshapestatic) {
  clear_layer();
  if (reshapestatic) {
    set_has_reshapestatic();
    layer_.reshapestatic_ = reshapestatic;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.reshapeStatic)
}
inline ::CoreML::Specification::ReshapeStaticLayerParams* NeuralNetworkLayer::_internal_mutable_reshapestatic() {
  if (!_internal_has_reshapestatic()) {
    clear_layer();
    set_has_reshapestatic();
    layer_.reshapestatic_ = CreateMaybeMessage< ::CoreML::Specification::ReshapeStaticLayerParams >(GetArenaForAllocation());
  }
  return layer_.reshapestatic_;
}
inline ::CoreML::Specification::ReshapeStaticLayerParams* NeuralNetworkLayer::mutable_reshapestatic() {
  ::CoreML::Specification::ReshapeStaticLayerParams* _msg = _internal_mutable_reshapestatic();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reshapeStatic)
  return _msg;
}

// .CoreML.Specification.ReshapeDynamicLayerParams reshapeDynamic = 1145;
inline bool NeuralNetworkLayer::_internal_has_reshapedynamic() const {
  return layer_case() == kReshapeDynamic;
}
inline bool NeuralNetworkLayer::has_reshapedynamic() const {
  return _internal_has_reshapedynamic();
}
inline void NeuralNetworkLayer::set_has_reshapedynamic() {
  _oneof_case_[0] = kReshapeDynamic;
}
inline void NeuralNetworkLayer::clear_reshapedynamic() {
  if (_internal_has_reshapedynamic()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.reshapedynamic_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ReshapeDynamicLayerParams* NeuralNetworkLayer::release_reshapedynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reshapeDynamic)
  if (_internal_has_reshapedynamic()) {
    clear_has_layer();
      ::CoreML::Specification::ReshapeDynamicLayerParams* temp = layer_.reshapedynamic_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.reshapedynamic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ReshapeDynamicLayerParams& NeuralNetworkLayer::_internal_reshapedynamic() const {
  return _internal_has_reshapedynamic()
      ? *layer_.reshapedynamic_
      : reinterpret_cast< ::CoreML::Specification::ReshapeDynamicLayerParams&>(::CoreML::Specification::_ReshapeDynamicLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ReshapeDynamicLayerParams& NeuralNetworkLayer::reshapedynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reshapeDynamic)
  return _internal_reshapedynamic();
}
inline ::CoreML::Specification::ReshapeDynamicLayerParams* NeuralNetworkLayer::unsafe_arena_release_reshapedynamic() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.reshapeDynamic)
  if (_internal_has_reshapedynamic()) {
    clear_has_layer();
    ::CoreML::Specification::ReshapeDynamicLayerParams* temp = layer_.reshapedynamic_;
    layer_.reshapedynamic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_reshapedynamic(::CoreML::Specification::ReshapeDynamicLayerParams* reshapedynamic) {
  clear_layer();
  if (reshapedynamic) {
    set_has_reshapedynamic();
    layer_.reshapedynamic_ = reshapedynamic;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.reshapeDynamic)
}
inline ::CoreML::Specification::ReshapeDynamicLayerParams* NeuralNetworkLayer::_internal_mutable_reshapedynamic() {
  if (!_internal_has_reshapedynamic()) {
    clear_layer();
    set_has_reshapedynamic();
    layer_.reshapedynamic_ = CreateMaybeMessage< ::CoreML::Specification::ReshapeDynamicLayerParams >(GetArenaForAllocation());
  }
  return layer_.reshapedynamic_;
}
inline ::CoreML::Specification::ReshapeDynamicLayerParams* NeuralNetworkLayer::mutable_reshapedynamic() {
  ::CoreML::Specification::ReshapeDynamicLayerParams* _msg = _internal_mutable_reshapedynamic();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reshapeDynamic)
  return _msg;
}

// .CoreML.Specification.RankPreservingReshapeLayerParams rankPreservingReshape = 1150;
inline bool NeuralNetworkLayer::_internal_has_rankpreservingreshape() const {
  return layer_case() == kRankPreservingReshape;
}
inline bool NeuralNetworkLayer::has_rankpreservingreshape() const {
  return _internal_has_rankpreservingreshape();
}
inline void NeuralNetworkLayer::set_has_rankpreservingreshape() {
  _oneof_case_[0] = kRankPreservingReshape;
}
inline void NeuralNetworkLayer::clear_rankpreservingreshape() {
  if (_internal_has_rankpreservingreshape()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.rankpreservingreshape_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::RankPreservingReshapeLayerParams* NeuralNetworkLayer::release_rankpreservingreshape() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.rankPreservingReshape)
  if (_internal_has_rankpreservingreshape()) {
    clear_has_layer();
      ::CoreML::Specification::RankPreservingReshapeLayerParams* temp = layer_.rankpreservingreshape_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.rankpreservingreshape_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::RankPreservingReshapeLayerParams& NeuralNetworkLayer::_internal_rankpreservingreshape() const {
  return _internal_has_rankpreservingreshape()
      ? *layer_.rankpreservingreshape_
      : reinterpret_cast< ::CoreML::Specification::RankPreservingReshapeLayerParams&>(::CoreML::Specification::_RankPreservingReshapeLayerParams_default_instance_);
}
inline const ::CoreML::Specification::RankPreservingReshapeLayerParams& NeuralNetworkLayer::rankpreservingreshape() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.rankPreservingReshape)
  return _internal_rankpreservingreshape();
}
inline ::CoreML::Specification::RankPreservingReshapeLayerParams* NeuralNetworkLayer::unsafe_arena_release_rankpreservingreshape() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.rankPreservingReshape)
  if (_internal_has_rankpreservingreshape()) {
    clear_has_layer();
    ::CoreML::Specification::RankPreservingReshapeLayerParams* temp = layer_.rankpreservingreshape_;
    layer_.rankpreservingreshape_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_rankpreservingreshape(::CoreML::Specification::RankPreservingReshapeLayerParams* rankpreservingreshape) {
  clear_layer();
  if (rankpreservingreshape) {
    set_has_rankpreservingreshape();
    layer_.rankpreservingreshape_ = rankpreservingreshape;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.rankPreservingReshape)
}
inline ::CoreML::Specification::RankPreservingReshapeLayerParams* NeuralNetworkLayer::_internal_mutable_rankpreservingreshape() {
  if (!_internal_has_rankpreservingreshape()) {
    clear_layer();
    set_has_rankpreservingreshape();
    layer_.rankpreservingreshape_ = CreateMaybeMessage< ::CoreML::Specification::RankPreservingReshapeLayerParams >(GetArenaForAllocation());
  }
  return layer_.rankpreservingreshape_;
}
inline ::CoreML::Specification::RankPreservingReshapeLayerParams* NeuralNetworkLayer::mutable_rankpreservingreshape() {
  ::CoreML::Specification::RankPreservingReshapeLayerParams* _msg = _internal_mutable_rankpreservingreshape();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.rankPreservingReshape)
  return _msg;
}

// .CoreML.Specification.ConstantPaddingLayerParams constantPad = 1155;
inline bool NeuralNetworkLayer::_internal_has_constantpad() const {
  return layer_case() == kConstantPad;
}
inline bool NeuralNetworkLayer::has_constantpad() const {
  return _internal_has_constantpad();
}
inline void NeuralNetworkLayer::set_has_constantpad() {
  _oneof_case_[0] = kConstantPad;
}
inline void NeuralNetworkLayer::clear_constantpad() {
  if (_internal_has_constantpad()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.constantpad_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ConstantPaddingLayerParams* NeuralNetworkLayer::release_constantpad() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.constantPad)
  if (_internal_has_constantpad()) {
    clear_has_layer();
      ::CoreML::Specification::ConstantPaddingLayerParams* temp = layer_.constantpad_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.constantpad_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ConstantPaddingLayerParams& NeuralNetworkLayer::_internal_constantpad() const {
  return _internal_has_constantpad()
      ? *layer_.constantpad_
      : reinterpret_cast< ::CoreML::Specification::ConstantPaddingLayerParams&>(::CoreML::Specification::_ConstantPaddingLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ConstantPaddingLayerParams& NeuralNetworkLayer::constantpad() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.constantPad)
  return _internal_constantpad();
}
inline ::CoreML::Specification::ConstantPaddingLayerParams* NeuralNetworkLayer::unsafe_arena_release_constantpad() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.constantPad)
  if (_internal_has_constantpad()) {
    clear_has_layer();
    ::CoreML::Specification::ConstantPaddingLayerParams* temp = layer_.constantpad_;
    layer_.constantpad_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_constantpad(::CoreML::Specification::ConstantPaddingLayerParams* constantpad) {
  clear_layer();
  if (constantpad) {
    set_has_constantpad();
    layer_.constantpad_ = constantpad;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.constantPad)
}
inline ::CoreML::Specification::ConstantPaddingLayerParams* NeuralNetworkLayer::_internal_mutable_constantpad() {
  if (!_internal_has_constantpad()) {
    clear_layer();
    set_has_constantpad();
    layer_.constantpad_ = CreateMaybeMessage< ::CoreML::Specification::ConstantPaddingLayerParams >(GetArenaForAllocation());
  }
  return layer_.constantpad_;
}
inline ::CoreML::Specification::ConstantPaddingLayerParams* NeuralNetworkLayer::mutable_constantpad() {
  ::CoreML::Specification::ConstantPaddingLayerParams* _msg = _internal_mutable_constantpad();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.constantPad)
  return _msg;
}

// .CoreML.Specification.RandomNormalLikeLayerParams randomNormalLike = 1170;
inline bool NeuralNetworkLayer::_internal_has_randomnormallike() const {
  return layer_case() == kRandomNormalLike;
}
inline bool NeuralNetworkLayer::has_randomnormallike() const {
  return _internal_has_randomnormallike();
}
inline void NeuralNetworkLayer::set_has_randomnormallike() {
  _oneof_case_[0] = kRandomNormalLike;
}
inline void NeuralNetworkLayer::clear_randomnormallike() {
  if (_internal_has_randomnormallike()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.randomnormallike_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::RandomNormalLikeLayerParams* NeuralNetworkLayer::release_randomnormallike() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomNormalLike)
  if (_internal_has_randomnormallike()) {
    clear_has_layer();
      ::CoreML::Specification::RandomNormalLikeLayerParams* temp = layer_.randomnormallike_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.randomnormallike_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::RandomNormalLikeLayerParams& NeuralNetworkLayer::_internal_randomnormallike() const {
  return _internal_has_randomnormallike()
      ? *layer_.randomnormallike_
      : reinterpret_cast< ::CoreML::Specification::RandomNormalLikeLayerParams&>(::CoreML::Specification::_RandomNormalLikeLayerParams_default_instance_);
}
inline const ::CoreML::Specification::RandomNormalLikeLayerParams& NeuralNetworkLayer::randomnormallike() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomNormalLike)
  return _internal_randomnormallike();
}
inline ::CoreML::Specification::RandomNormalLikeLayerParams* NeuralNetworkLayer::unsafe_arena_release_randomnormallike() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.randomNormalLike)
  if (_internal_has_randomnormallike()) {
    clear_has_layer();
    ::CoreML::Specification::RandomNormalLikeLayerParams* temp = layer_.randomnormallike_;
    layer_.randomnormallike_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_randomnormallike(::CoreML::Specification::RandomNormalLikeLayerParams* randomnormallike) {
  clear_layer();
  if (randomnormallike) {
    set_has_randomnormallike();
    layer_.randomnormallike_ = randomnormallike;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomNormalLike)
}
inline ::CoreML::Specification::RandomNormalLikeLayerParams* NeuralNetworkLayer::_internal_mutable_randomnormallike() {
  if (!_internal_has_randomnormallike()) {
    clear_layer();
    set_has_randomnormallike();
    layer_.randomnormallike_ = CreateMaybeMessage< ::CoreML::Specification::RandomNormalLikeLayerParams >(GetArenaForAllocation());
  }
  return layer_.randomnormallike_;
}
inline ::CoreML::Specification::RandomNormalLikeLayerParams* NeuralNetworkLayer::mutable_randomnormallike() {
  ::CoreML::Specification::RandomNormalLikeLayerParams* _msg = _internal_mutable_randomnormallike();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomNormalLike)
  return _msg;
}

// .CoreML.Specification.RandomNormalStaticLayerParams randomNormalStatic = 1175;
inline bool NeuralNetworkLayer::_internal_has_randomnormalstatic() const {
  return layer_case() == kRandomNormalStatic;
}
inline bool NeuralNetworkLayer::has_randomnormalstatic() const {
  return _internal_has_randomnormalstatic();
}
inline void NeuralNetworkLayer::set_has_randomnormalstatic() {
  _oneof_case_[0] = kRandomNormalStatic;
}
inline void NeuralNetworkLayer::clear_randomnormalstatic() {
  if (_internal_has_randomnormalstatic()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.randomnormalstatic_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::RandomNormalStaticLayerParams* NeuralNetworkLayer::release_randomnormalstatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomNormalStatic)
  if (_internal_has_randomnormalstatic()) {
    clear_has_layer();
      ::CoreML::Specification::RandomNormalStaticLayerParams* temp = layer_.randomnormalstatic_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.randomnormalstatic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::RandomNormalStaticLayerParams& NeuralNetworkLayer::_internal_randomnormalstatic() const {
  return _internal_has_randomnormalstatic()
      ? *layer_.randomnormalstatic_
      : reinterpret_cast< ::CoreML::Specification::RandomNormalStaticLayerParams&>(::CoreML::Specification::_RandomNormalStaticLayerParams_default_instance_);
}
inline const ::CoreML::Specification::RandomNormalStaticLayerParams& NeuralNetworkLayer::randomnormalstatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomNormalStatic)
  return _internal_randomnormalstatic();
}
inline ::CoreML::Specification::RandomNormalStaticLayerParams* NeuralNetworkLayer::unsafe_arena_release_randomnormalstatic() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.randomNormalStatic)
  if (_internal_has_randomnormalstatic()) {
    clear_has_layer();
    ::CoreML::Specification::RandomNormalStaticLayerParams* temp = layer_.randomnormalstatic_;
    layer_.randomnormalstatic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_randomnormalstatic(::CoreML::Specification::RandomNormalStaticLayerParams* randomnormalstatic) {
  clear_layer();
  if (randomnormalstatic) {
    set_has_randomnormalstatic();
    layer_.randomnormalstatic_ = randomnormalstatic;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomNormalStatic)
}
inline ::CoreML::Specification::RandomNormalStaticLayerParams* NeuralNetworkLayer::_internal_mutable_randomnormalstatic() {
  if (!_internal_has_randomnormalstatic()) {
    clear_layer();
    set_has_randomnormalstatic();
    layer_.randomnormalstatic_ = CreateMaybeMessage< ::CoreML::Specification::RandomNormalStaticLayerParams >(GetArenaForAllocation());
  }
  return layer_.randomnormalstatic_;
}
inline ::CoreML::Specification::RandomNormalStaticLayerParams* NeuralNetworkLayer::mutable_randomnormalstatic() {
  ::CoreML::Specification::RandomNormalStaticLayerParams* _msg = _internal_mutable_randomnormalstatic();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomNormalStatic)
  return _msg;
}

// .CoreML.Specification.RandomNormalDynamicLayerParams randomNormalDynamic = 1180;
inline bool NeuralNetworkLayer::_internal_has_randomnormaldynamic() const {
  return layer_case() == kRandomNormalDynamic;
}
inline bool NeuralNetworkLayer::has_randomnormaldynamic() const {
  return _internal_has_randomnormaldynamic();
}
inline void NeuralNetworkLayer::set_has_randomnormaldynamic() {
  _oneof_case_[0] = kRandomNormalDynamic;
}
inline void NeuralNetworkLayer::clear_randomnormaldynamic() {
  if (_internal_has_randomnormaldynamic()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.randomnormaldynamic_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::RandomNormalDynamicLayerParams* NeuralNetworkLayer::release_randomnormaldynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomNormalDynamic)
  if (_internal_has_randomnormaldynamic()) {
    clear_has_layer();
      ::CoreML::Specification::RandomNormalDynamicLayerParams* temp = layer_.randomnormaldynamic_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.randomnormaldynamic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::RandomNormalDynamicLayerParams& NeuralNetworkLayer::_internal_randomnormaldynamic() const {
  return _internal_has_randomnormaldynamic()
      ? *layer_.randomnormaldynamic_
      : reinterpret_cast< ::CoreML::Specification::RandomNormalDynamicLayerParams&>(::CoreML::Specification::_RandomNormalDynamicLayerParams_default_instance_);
}
inline const ::CoreML::Specification::RandomNormalDynamicLayerParams& NeuralNetworkLayer::randomnormaldynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomNormalDynamic)
  return _internal_randomnormaldynamic();
}
inline ::CoreML::Specification::RandomNormalDynamicLayerParams* NeuralNetworkLayer::unsafe_arena_release_randomnormaldynamic() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.randomNormalDynamic)
  if (_internal_has_randomnormaldynamic()) {
    clear_has_layer();
    ::CoreML::Specification::RandomNormalDynamicLayerParams* temp = layer_.randomnormaldynamic_;
    layer_.randomnormaldynamic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_randomnormaldynamic(::CoreML::Specification::RandomNormalDynamicLayerParams* randomnormaldynamic) {
  clear_layer();
  if (randomnormaldynamic) {
    set_has_randomnormaldynamic();
    layer_.randomnormaldynamic_ = randomnormaldynamic;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomNormalDynamic)
}
inline ::CoreML::Specification::RandomNormalDynamicLayerParams* NeuralNetworkLayer::_internal_mutable_randomnormaldynamic() {
  if (!_internal_has_randomnormaldynamic()) {
    clear_layer();
    set_has_randomnormaldynamic();
    layer_.randomnormaldynamic_ = CreateMaybeMessage< ::CoreML::Specification::RandomNormalDynamicLayerParams >(GetArenaForAllocation());
  }
  return layer_.randomnormaldynamic_;
}
inline ::CoreML::Specification::RandomNormalDynamicLayerParams* NeuralNetworkLayer::mutable_randomnormaldynamic() {
  ::CoreML::Specification::RandomNormalDynamicLayerParams* _msg = _internal_mutable_randomnormaldynamic();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomNormalDynamic)
  return _msg;
}

// .CoreML.Specification.RandomUniformLikeLayerParams randomUniformLike = 1190;
inline bool NeuralNetworkLayer::_internal_has_randomuniformlike() const {
  return layer_case() == kRandomUniformLike;
}
inline bool NeuralNetworkLayer::has_randomuniformlike() const {
  return _internal_has_randomuniformlike();
}
inline void NeuralNetworkLayer::set_has_randomuniformlike() {
  _oneof_case_[0] = kRandomUniformLike;
}
inline void NeuralNetworkLayer::clear_randomuniformlike() {
  if (_internal_has_randomuniformlike()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.randomuniformlike_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::RandomUniformLikeLayerParams* NeuralNetworkLayer::release_randomuniformlike() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomUniformLike)
  if (_internal_has_randomuniformlike()) {
    clear_has_layer();
      ::CoreML::Specification::RandomUniformLikeLayerParams* temp = layer_.randomuniformlike_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.randomuniformlike_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::RandomUniformLikeLayerParams& NeuralNetworkLayer::_internal_randomuniformlike() const {
  return _internal_has_randomuniformlike()
      ? *layer_.randomuniformlike_
      : reinterpret_cast< ::CoreML::Specification::RandomUniformLikeLayerParams&>(::CoreML::Specification::_RandomUniformLikeLayerParams_default_instance_);
}
inline const ::CoreML::Specification::RandomUniformLikeLayerParams& NeuralNetworkLayer::randomuniformlike() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomUniformLike)
  return _internal_randomuniformlike();
}
inline ::CoreML::Specification::RandomUniformLikeLayerParams* NeuralNetworkLayer::unsafe_arena_release_randomuniformlike() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.randomUniformLike)
  if (_internal_has_randomuniformlike()) {
    clear_has_layer();
    ::CoreML::Specification::RandomUniformLikeLayerParams* temp = layer_.randomuniformlike_;
    layer_.randomuniformlike_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_randomuniformlike(::CoreML::Specification::RandomUniformLikeLayerParams* randomuniformlike) {
  clear_layer();
  if (randomuniformlike) {
    set_has_randomuniformlike();
    layer_.randomuniformlike_ = randomuniformlike;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomUniformLike)
}
inline ::CoreML::Specification::RandomUniformLikeLayerParams* NeuralNetworkLayer::_internal_mutable_randomuniformlike() {
  if (!_internal_has_randomuniformlike()) {
    clear_layer();
    set_has_randomuniformlike();
    layer_.randomuniformlike_ = CreateMaybeMessage< ::CoreML::Specification::RandomUniformLikeLayerParams >(GetArenaForAllocation());
  }
  return layer_.randomuniformlike_;
}
inline ::CoreML::Specification::RandomUniformLikeLayerParams* NeuralNetworkLayer::mutable_randomuniformlike() {
  ::CoreML::Specification::RandomUniformLikeLayerParams* _msg = _internal_mutable_randomuniformlike();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomUniformLike)
  return _msg;
}

// .CoreML.Specification.RandomUniformStaticLayerParams randomUniformStatic = 1195;
inline bool NeuralNetworkLayer::_internal_has_randomuniformstatic() const {
  return layer_case() == kRandomUniformStatic;
}
inline bool NeuralNetworkLayer::has_randomuniformstatic() const {
  return _internal_has_randomuniformstatic();
}
inline void NeuralNetworkLayer::set_has_randomuniformstatic() {
  _oneof_case_[0] = kRandomUniformStatic;
}
inline void NeuralNetworkLayer::clear_randomuniformstatic() {
  if (_internal_has_randomuniformstatic()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.randomuniformstatic_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::RandomUniformStaticLayerParams* NeuralNetworkLayer::release_randomuniformstatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomUniformStatic)
  if (_internal_has_randomuniformstatic()) {
    clear_has_layer();
      ::CoreML::Specification::RandomUniformStaticLayerParams* temp = layer_.randomuniformstatic_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.randomuniformstatic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::RandomUniformStaticLayerParams& NeuralNetworkLayer::_internal_randomuniformstatic() const {
  return _internal_has_randomuniformstatic()
      ? *layer_.randomuniformstatic_
      : reinterpret_cast< ::CoreML::Specification::RandomUniformStaticLayerParams&>(::CoreML::Specification::_RandomUniformStaticLayerParams_default_instance_);
}
inline const ::CoreML::Specification::RandomUniformStaticLayerParams& NeuralNetworkLayer::randomuniformstatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomUniformStatic)
  return _internal_randomuniformstatic();
}
inline ::CoreML::Specification::RandomUniformStaticLayerParams* NeuralNetworkLayer::unsafe_arena_release_randomuniformstatic() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.randomUniformStatic)
  if (_internal_has_randomuniformstatic()) {
    clear_has_layer();
    ::CoreML::Specification::RandomUniformStaticLayerParams* temp = layer_.randomuniformstatic_;
    layer_.randomuniformstatic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_randomuniformstatic(::CoreML::Specification::RandomUniformStaticLayerParams* randomuniformstatic) {
  clear_layer();
  if (randomuniformstatic) {
    set_has_randomuniformstatic();
    layer_.randomuniformstatic_ = randomuniformstatic;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomUniformStatic)
}
inline ::CoreML::Specification::RandomUniformStaticLayerParams* NeuralNetworkLayer::_internal_mutable_randomuniformstatic() {
  if (!_internal_has_randomuniformstatic()) {
    clear_layer();
    set_has_randomuniformstatic();
    layer_.randomuniformstatic_ = CreateMaybeMessage< ::CoreML::Specification::RandomUniformStaticLayerParams >(GetArenaForAllocation());
  }
  return layer_.randomuniformstatic_;
}
inline ::CoreML::Specification::RandomUniformStaticLayerParams* NeuralNetworkLayer::mutable_randomuniformstatic() {
  ::CoreML::Specification::RandomUniformStaticLayerParams* _msg = _internal_mutable_randomuniformstatic();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomUniformStatic)
  return _msg;
}

// .CoreML.Specification.RandomUniformDynamicLayerParams randomUniformDynamic = 1200;
inline bool NeuralNetworkLayer::_internal_has_randomuniformdynamic() const {
  return layer_case() == kRandomUniformDynamic;
}
inline bool NeuralNetworkLayer::has_randomuniformdynamic() const {
  return _internal_has_randomuniformdynamic();
}
inline void NeuralNetworkLayer::set_has_randomuniformdynamic() {
  _oneof_case_[0] = kRandomUniformDynamic;
}
inline void NeuralNetworkLayer::clear_randomuniformdynamic() {
  if (_internal_has_randomuniformdynamic()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.randomuniformdynamic_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::RandomUniformDynamicLayerParams* NeuralNetworkLayer::release_randomuniformdynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomUniformDynamic)
  if (_internal_has_randomuniformdynamic()) {
    clear_has_layer();
      ::CoreML::Specification::RandomUniformDynamicLayerParams* temp = layer_.randomuniformdynamic_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.randomuniformdynamic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::RandomUniformDynamicLayerParams& NeuralNetworkLayer::_internal_randomuniformdynamic() const {
  return _internal_has_randomuniformdynamic()
      ? *layer_.randomuniformdynamic_
      : reinterpret_cast< ::CoreML::Specification::RandomUniformDynamicLayerParams&>(::CoreML::Specification::_RandomUniformDynamicLayerParams_default_instance_);
}
inline const ::CoreML::Specification::RandomUniformDynamicLayerParams& NeuralNetworkLayer::randomuniformdynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomUniformDynamic)
  return _internal_randomuniformdynamic();
}
inline ::CoreML::Specification::RandomUniformDynamicLayerParams* NeuralNetworkLayer::unsafe_arena_release_randomuniformdynamic() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.randomUniformDynamic)
  if (_internal_has_randomuniformdynamic()) {
    clear_has_layer();
    ::CoreML::Specification::RandomUniformDynamicLayerParams* temp = layer_.randomuniformdynamic_;
    layer_.randomuniformdynamic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_randomuniformdynamic(::CoreML::Specification::RandomUniformDynamicLayerParams* randomuniformdynamic) {
  clear_layer();
  if (randomuniformdynamic) {
    set_has_randomuniformdynamic();
    layer_.randomuniformdynamic_ = randomuniformdynamic;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomUniformDynamic)
}
inline ::CoreML::Specification::RandomUniformDynamicLayerParams* NeuralNetworkLayer::_internal_mutable_randomuniformdynamic() {
  if (!_internal_has_randomuniformdynamic()) {
    clear_layer();
    set_has_randomuniformdynamic();
    layer_.randomuniformdynamic_ = CreateMaybeMessage< ::CoreML::Specification::RandomUniformDynamicLayerParams >(GetArenaForAllocation());
  }
  return layer_.randomuniformdynamic_;
}
inline ::CoreML::Specification::RandomUniformDynamicLayerParams* NeuralNetworkLayer::mutable_randomuniformdynamic() {
  ::CoreML::Specification::RandomUniformDynamicLayerParams* _msg = _internal_mutable_randomuniformdynamic();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomUniformDynamic)
  return _msg;
}

// .CoreML.Specification.RandomBernoulliLikeLayerParams randomBernoulliLike = 1210;
inline bool NeuralNetworkLayer::_internal_has_randombernoullilike() const {
  return layer_case() == kRandomBernoulliLike;
}
inline bool NeuralNetworkLayer::has_randombernoullilike() const {
  return _internal_has_randombernoullilike();
}
inline void NeuralNetworkLayer::set_has_randombernoullilike() {
  _oneof_case_[0] = kRandomBernoulliLike;
}
inline void NeuralNetworkLayer::clear_randombernoullilike() {
  if (_internal_has_randombernoullilike()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.randombernoullilike_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::RandomBernoulliLikeLayerParams* NeuralNetworkLayer::release_randombernoullilike() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomBernoulliLike)
  if (_internal_has_randombernoullilike()) {
    clear_has_layer();
      ::CoreML::Specification::RandomBernoulliLikeLayerParams* temp = layer_.randombernoullilike_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.randombernoullilike_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::RandomBernoulliLikeLayerParams& NeuralNetworkLayer::_internal_randombernoullilike() const {
  return _internal_has_randombernoullilike()
      ? *layer_.randombernoullilike_
      : reinterpret_cast< ::CoreML::Specification::RandomBernoulliLikeLayerParams&>(::CoreML::Specification::_RandomBernoulliLikeLayerParams_default_instance_);
}
inline const ::CoreML::Specification::RandomBernoulliLikeLayerParams& NeuralNetworkLayer::randombernoullilike() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomBernoulliLike)
  return _internal_randombernoullilike();
}
inline ::CoreML::Specification::RandomBernoulliLikeLayerParams* NeuralNetworkLayer::unsafe_arena_release_randombernoullilike() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.randomBernoulliLike)
  if (_internal_has_randombernoullilike()) {
    clear_has_layer();
    ::CoreML::Specification::RandomBernoulliLikeLayerParams* temp = layer_.randombernoullilike_;
    layer_.randombernoullilike_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_randombernoullilike(::CoreML::Specification::RandomBernoulliLikeLayerParams* randombernoullilike) {
  clear_layer();
  if (randombernoullilike) {
    set_has_randombernoullilike();
    layer_.randombernoullilike_ = randombernoullilike;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomBernoulliLike)
}
inline ::CoreML::Specification::RandomBernoulliLikeLayerParams* NeuralNetworkLayer::_internal_mutable_randombernoullilike() {
  if (!_internal_has_randombernoullilike()) {
    clear_layer();
    set_has_randombernoullilike();
    layer_.randombernoullilike_ = CreateMaybeMessage< ::CoreML::Specification::RandomBernoulliLikeLayerParams >(GetArenaForAllocation());
  }
  return layer_.randombernoullilike_;
}
inline ::CoreML::Specification::RandomBernoulliLikeLayerParams* NeuralNetworkLayer::mutable_randombernoullilike() {
  ::CoreML::Specification::RandomBernoulliLikeLayerParams* _msg = _internal_mutable_randombernoullilike();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomBernoulliLike)
  return _msg;
}

// .CoreML.Specification.RandomBernoulliStaticLayerParams randomBernoulliStatic = 1215;
inline bool NeuralNetworkLayer::_internal_has_randombernoullistatic() const {
  return layer_case() == kRandomBernoulliStatic;
}
inline bool NeuralNetworkLayer::has_randombernoullistatic() const {
  return _internal_has_randombernoullistatic();
}
inline void NeuralNetworkLayer::set_has_randombernoullistatic() {
  _oneof_case_[0] = kRandomBernoulliStatic;
}
inline void NeuralNetworkLayer::clear_randombernoullistatic() {
  if (_internal_has_randombernoullistatic()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.randombernoullistatic_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::RandomBernoulliStaticLayerParams* NeuralNetworkLayer::release_randombernoullistatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomBernoulliStatic)
  if (_internal_has_randombernoullistatic()) {
    clear_has_layer();
      ::CoreML::Specification::RandomBernoulliStaticLayerParams* temp = layer_.randombernoullistatic_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.randombernoullistatic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::RandomBernoulliStaticLayerParams& NeuralNetworkLayer::_internal_randombernoullistatic() const {
  return _internal_has_randombernoullistatic()
      ? *layer_.randombernoullistatic_
      : reinterpret_cast< ::CoreML::Specification::RandomBernoulliStaticLayerParams&>(::CoreML::Specification::_RandomBernoulliStaticLayerParams_default_instance_);
}
inline const ::CoreML::Specification::RandomBernoulliStaticLayerParams& NeuralNetworkLayer::randombernoullistatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomBernoulliStatic)
  return _internal_randombernoullistatic();
}
inline ::CoreML::Specification::RandomBernoulliStaticLayerParams* NeuralNetworkLayer::unsafe_arena_release_randombernoullistatic() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.randomBernoulliStatic)
  if (_internal_has_randombernoullistatic()) {
    clear_has_layer();
    ::CoreML::Specification::RandomBernoulliStaticLayerParams* temp = layer_.randombernoullistatic_;
    layer_.randombernoullistatic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_randombernoullistatic(::CoreML::Specification::RandomBernoulliStaticLayerParams* randombernoullistatic) {
  clear_layer();
  if (randombernoullistatic) {
    set_has_randombernoullistatic();
    layer_.randombernoullistatic_ = randombernoullistatic;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomBernoulliStatic)
}
inline ::CoreML::Specification::RandomBernoulliStaticLayerParams* NeuralNetworkLayer::_internal_mutable_randombernoullistatic() {
  if (!_internal_has_randombernoullistatic()) {
    clear_layer();
    set_has_randombernoullistatic();
    layer_.randombernoullistatic_ = CreateMaybeMessage< ::CoreML::Specification::RandomBernoulliStaticLayerParams >(GetArenaForAllocation());
  }
  return layer_.randombernoullistatic_;
}
inline ::CoreML::Specification::RandomBernoulliStaticLayerParams* NeuralNetworkLayer::mutable_randombernoullistatic() {
  ::CoreML::Specification::RandomBernoulliStaticLayerParams* _msg = _internal_mutable_randombernoullistatic();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomBernoulliStatic)
  return _msg;
}

// .CoreML.Specification.RandomBernoulliDynamicLayerParams randomBernoulliDynamic = 1220;
inline bool NeuralNetworkLayer::_internal_has_randombernoullidynamic() const {
  return layer_case() == kRandomBernoulliDynamic;
}
inline bool NeuralNetworkLayer::has_randombernoullidynamic() const {
  return _internal_has_randombernoullidynamic();
}
inline void NeuralNetworkLayer::set_has_randombernoullidynamic() {
  _oneof_case_[0] = kRandomBernoulliDynamic;
}
inline void NeuralNetworkLayer::clear_randombernoullidynamic() {
  if (_internal_has_randombernoullidynamic()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.randombernoullidynamic_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::RandomBernoulliDynamicLayerParams* NeuralNetworkLayer::release_randombernoullidynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomBernoulliDynamic)
  if (_internal_has_randombernoullidynamic()) {
    clear_has_layer();
      ::CoreML::Specification::RandomBernoulliDynamicLayerParams* temp = layer_.randombernoullidynamic_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.randombernoullidynamic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::RandomBernoulliDynamicLayerParams& NeuralNetworkLayer::_internal_randombernoullidynamic() const {
  return _internal_has_randombernoullidynamic()
      ? *layer_.randombernoullidynamic_
      : reinterpret_cast< ::CoreML::Specification::RandomBernoulliDynamicLayerParams&>(::CoreML::Specification::_RandomBernoulliDynamicLayerParams_default_instance_);
}
inline const ::CoreML::Specification::RandomBernoulliDynamicLayerParams& NeuralNetworkLayer::randombernoullidynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomBernoulliDynamic)
  return _internal_randombernoullidynamic();
}
inline ::CoreML::Specification::RandomBernoulliDynamicLayerParams* NeuralNetworkLayer::unsafe_arena_release_randombernoullidynamic() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.randomBernoulliDynamic)
  if (_internal_has_randombernoullidynamic()) {
    clear_has_layer();
    ::CoreML::Specification::RandomBernoulliDynamicLayerParams* temp = layer_.randombernoullidynamic_;
    layer_.randombernoullidynamic_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_randombernoullidynamic(::CoreML::Specification::RandomBernoulliDynamicLayerParams* randombernoullidynamic) {
  clear_layer();
  if (randombernoullidynamic) {
    set_has_randombernoullidynamic();
    layer_.randombernoullidynamic_ = randombernoullidynamic;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomBernoulliDynamic)
}
inline ::CoreML::Specification::RandomBernoulliDynamicLayerParams* NeuralNetworkLayer::_internal_mutable_randombernoullidynamic() {
  if (!_internal_has_randombernoullidynamic()) {
    clear_layer();
    set_has_randombernoullidynamic();
    layer_.randombernoullidynamic_ = CreateMaybeMessage< ::CoreML::Specification::RandomBernoulliDynamicLayerParams >(GetArenaForAllocation());
  }
  return layer_.randombernoullidynamic_;
}
inline ::CoreML::Specification::RandomBernoulliDynamicLayerParams* NeuralNetworkLayer::mutable_randombernoullidynamic() {
  ::CoreML::Specification::RandomBernoulliDynamicLayerParams* _msg = _internal_mutable_randombernoullidynamic();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomBernoulliDynamic)
  return _msg;
}

// .CoreML.Specification.CategoricalDistributionLayerParams categoricalDistribution = 1230;
inline bool NeuralNetworkLayer::_internal_has_categoricaldistribution() const {
  return layer_case() == kCategoricalDistribution;
}
inline bool NeuralNetworkLayer::has_categoricaldistribution() const {
  return _internal_has_categoricaldistribution();
}
inline void NeuralNetworkLayer::set_has_categoricaldistribution() {
  _oneof_case_[0] = kCategoricalDistribution;
}
inline void NeuralNetworkLayer::clear_categoricaldistribution() {
  if (_internal_has_categoricaldistribution()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.categoricaldistribution_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::CategoricalDistributionLayerParams* NeuralNetworkLayer::release_categoricaldistribution() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.categoricalDistribution)
  if (_internal_has_categoricaldistribution()) {
    clear_has_layer();
      ::CoreML::Specification::CategoricalDistributionLayerParams* temp = layer_.categoricaldistribution_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.categoricaldistribution_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::CategoricalDistributionLayerParams& NeuralNetworkLayer::_internal_categoricaldistribution() const {
  return _internal_has_categoricaldistribution()
      ? *layer_.categoricaldistribution_
      : reinterpret_cast< ::CoreML::Specification::CategoricalDistributionLayerParams&>(::CoreML::Specification::_CategoricalDistributionLayerParams_default_instance_);
}
inline const ::CoreML::Specification::CategoricalDistributionLayerParams& NeuralNetworkLayer::categoricaldistribution() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.categoricalDistribution)
  return _internal_categoricaldistribution();
}
inline ::CoreML::Specification::CategoricalDistributionLayerParams* NeuralNetworkLayer::unsafe_arena_release_categoricaldistribution() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.categoricalDistribution)
  if (_internal_has_categoricaldistribution()) {
    clear_has_layer();
    ::CoreML::Specification::CategoricalDistributionLayerParams* temp = layer_.categoricaldistribution_;
    layer_.categoricaldistribution_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_categoricaldistribution(::CoreML::Specification::CategoricalDistributionLayerParams* categoricaldistribution) {
  clear_layer();
  if (categoricaldistribution) {
    set_has_categoricaldistribution();
    layer_.categoricaldistribution_ = categoricaldistribution;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.categoricalDistribution)
}
inline ::CoreML::Specification::CategoricalDistributionLayerParams* NeuralNetworkLayer::_internal_mutable_categoricaldistribution() {
  if (!_internal_has_categoricaldistribution()) {
    clear_layer();
    set_has_categoricaldistribution();
    layer_.categoricaldistribution_ = CreateMaybeMessage< ::CoreML::Specification::CategoricalDistributionLayerParams >(GetArenaForAllocation());
  }
  return layer_.categoricaldistribution_;
}
inline ::CoreML::Specification::CategoricalDistributionLayerParams* NeuralNetworkLayer::mutable_categoricaldistribution() {
  ::CoreML::Specification::CategoricalDistributionLayerParams* _msg = _internal_mutable_categoricaldistribution();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.categoricalDistribution)
  return _msg;
}

// .CoreML.Specification.ReduceL1LayerParams reduceL1 = 1250;
inline bool NeuralNetworkLayer::_internal_has_reducel1() const {
  return layer_case() == kReduceL1;
}
inline bool NeuralNetworkLayer::has_reducel1() const {
  return _internal_has_reducel1();
}
inline void NeuralNetworkLayer::set_has_reducel1() {
  _oneof_case_[0] = kReduceL1;
}
inline void NeuralNetworkLayer::clear_reducel1() {
  if (_internal_has_reducel1()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.reducel1_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ReduceL1LayerParams* NeuralNetworkLayer::release_reducel1() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceL1)
  if (_internal_has_reducel1()) {
    clear_has_layer();
      ::CoreML::Specification::ReduceL1LayerParams* temp = layer_.reducel1_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.reducel1_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ReduceL1LayerParams& NeuralNetworkLayer::_internal_reducel1() const {
  return _internal_has_reducel1()
      ? *layer_.reducel1_
      : reinterpret_cast< ::CoreML::Specification::ReduceL1LayerParams&>(::CoreML::Specification::_ReduceL1LayerParams_default_instance_);
}
inline const ::CoreML::Specification::ReduceL1LayerParams& NeuralNetworkLayer::reducel1() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceL1)
  return _internal_reducel1();
}
inline ::CoreML::Specification::ReduceL1LayerParams* NeuralNetworkLayer::unsafe_arena_release_reducel1() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.reduceL1)
  if (_internal_has_reducel1()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceL1LayerParams* temp = layer_.reducel1_;
    layer_.reducel1_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_reducel1(::CoreML::Specification::ReduceL1LayerParams* reducel1) {
  clear_layer();
  if (reducel1) {
    set_has_reducel1();
    layer_.reducel1_ = reducel1;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceL1)
}
inline ::CoreML::Specification::ReduceL1LayerParams* NeuralNetworkLayer::_internal_mutable_reducel1() {
  if (!_internal_has_reducel1()) {
    clear_layer();
    set_has_reducel1();
    layer_.reducel1_ = CreateMaybeMessage< ::CoreML::Specification::ReduceL1LayerParams >(GetArenaForAllocation());
  }
  return layer_.reducel1_;
}
inline ::CoreML::Specification::ReduceL1LayerParams* NeuralNetworkLayer::mutable_reducel1() {
  ::CoreML::Specification::ReduceL1LayerParams* _msg = _internal_mutable_reducel1();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceL1)
  return _msg;
}

// .CoreML.Specification.ReduceL2LayerParams reduceL2 = 1255;
inline bool NeuralNetworkLayer::_internal_has_reducel2() const {
  return layer_case() == kReduceL2;
}
inline bool NeuralNetworkLayer::has_reducel2() const {
  return _internal_has_reducel2();
}
inline void NeuralNetworkLayer::set_has_reducel2() {
  _oneof_case_[0] = kReduceL2;
}
inline void NeuralNetworkLayer::clear_reducel2() {
  if (_internal_has_reducel2()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.reducel2_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ReduceL2LayerParams* NeuralNetworkLayer::release_reducel2() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceL2)
  if (_internal_has_reducel2()) {
    clear_has_layer();
      ::CoreML::Specification::ReduceL2LayerParams* temp = layer_.reducel2_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.reducel2_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ReduceL2LayerParams& NeuralNetworkLayer::_internal_reducel2() const {
  return _internal_has_reducel2()
      ? *layer_.reducel2_
      : reinterpret_cast< ::CoreML::Specification::ReduceL2LayerParams&>(::CoreML::Specification::_ReduceL2LayerParams_default_instance_);
}
inline const ::CoreML::Specification::ReduceL2LayerParams& NeuralNetworkLayer::reducel2() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceL2)
  return _internal_reducel2();
}
inline ::CoreML::Specification::ReduceL2LayerParams* NeuralNetworkLayer::unsafe_arena_release_reducel2() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.reduceL2)
  if (_internal_has_reducel2()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceL2LayerParams* temp = layer_.reducel2_;
    layer_.reducel2_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_reducel2(::CoreML::Specification::ReduceL2LayerParams* reducel2) {
  clear_layer();
  if (reducel2) {
    set_has_reducel2();
    layer_.reducel2_ = reducel2;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceL2)
}
inline ::CoreML::Specification::ReduceL2LayerParams* NeuralNetworkLayer::_internal_mutable_reducel2() {
  if (!_internal_has_reducel2()) {
    clear_layer();
    set_has_reducel2();
    layer_.reducel2_ = CreateMaybeMessage< ::CoreML::Specification::ReduceL2LayerParams >(GetArenaForAllocation());
  }
  return layer_.reducel2_;
}
inline ::CoreML::Specification::ReduceL2LayerParams* NeuralNetworkLayer::mutable_reducel2() {
  ::CoreML::Specification::ReduceL2LayerParams* _msg = _internal_mutable_reducel2();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceL2)
  return _msg;
}

// .CoreML.Specification.ReduceMaxLayerParams reduceMax = 1260;
inline bool NeuralNetworkLayer::_internal_has_reducemax() const {
  return layer_case() == kReduceMax;
}
inline bool NeuralNetworkLayer::has_reducemax() const {
  return _internal_has_reducemax();
}
inline void NeuralNetworkLayer::set_has_reducemax() {
  _oneof_case_[0] = kReduceMax;
}
inline void NeuralNetworkLayer::clear_reducemax() {
  if (_internal_has_reducemax()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.reducemax_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ReduceMaxLayerParams* NeuralNetworkLayer::release_reducemax() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceMax)
  if (_internal_has_reducemax()) {
    clear_has_layer();
      ::CoreML::Specification::ReduceMaxLayerParams* temp = layer_.reducemax_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.reducemax_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ReduceMaxLayerParams& NeuralNetworkLayer::_internal_reducemax() const {
  return _internal_has_reducemax()
      ? *layer_.reducemax_
      : reinterpret_cast< ::CoreML::Specification::ReduceMaxLayerParams&>(::CoreML::Specification::_ReduceMaxLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ReduceMaxLayerParams& NeuralNetworkLayer::reducemax() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceMax)
  return _internal_reducemax();
}
inline ::CoreML::Specification::ReduceMaxLayerParams* NeuralNetworkLayer::unsafe_arena_release_reducemax() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.reduceMax)
  if (_internal_has_reducemax()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceMaxLayerParams* temp = layer_.reducemax_;
    layer_.reducemax_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_reducemax(::CoreML::Specification::ReduceMaxLayerParams* reducemax) {
  clear_layer();
  if (reducemax) {
    set_has_reducemax();
    layer_.reducemax_ = reducemax;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceMax)
}
inline ::CoreML::Specification::ReduceMaxLayerParams* NeuralNetworkLayer::_internal_mutable_reducemax() {
  if (!_internal_has_reducemax()) {
    clear_layer();
    set_has_reducemax();
    layer_.reducemax_ = CreateMaybeMessage< ::CoreML::Specification::ReduceMaxLayerParams >(GetArenaForAllocation());
  }
  return layer_.reducemax_;
}
inline ::CoreML::Specification::ReduceMaxLayerParams* NeuralNetworkLayer::mutable_reducemax() {
  ::CoreML::Specification::ReduceMaxLayerParams* _msg = _internal_mutable_reducemax();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceMax)
  return _msg;
}

// .CoreML.Specification.ReduceMinLayerParams reduceMin = 1265;
inline bool NeuralNetworkLayer::_internal_has_reducemin() const {
  return layer_case() == kReduceMin;
}
inline bool NeuralNetworkLayer::has_reducemin() const {
  return _internal_has_reducemin();
}
inline void NeuralNetworkLayer::set_has_reducemin() {
  _oneof_case_[0] = kReduceMin;
}
inline void NeuralNetworkLayer::clear_reducemin() {
  if (_internal_has_reducemin()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.reducemin_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ReduceMinLayerParams* NeuralNetworkLayer::release_reducemin() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceMin)
  if (_internal_has_reducemin()) {
    clear_has_layer();
      ::CoreML::Specification::ReduceMinLayerParams* temp = layer_.reducemin_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.reducemin_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ReduceMinLayerParams& NeuralNetworkLayer::_internal_reducemin() const {
  return _internal_has_reducemin()
      ? *layer_.reducemin_
      : reinterpret_cast< ::CoreML::Specification::ReduceMinLayerParams&>(::CoreML::Specification::_ReduceMinLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ReduceMinLayerParams& NeuralNetworkLayer::reducemin() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceMin)
  return _internal_reducemin();
}
inline ::CoreML::Specification::ReduceMinLayerParams* NeuralNetworkLayer::unsafe_arena_release_reducemin() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.reduceMin)
  if (_internal_has_reducemin()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceMinLayerParams* temp = layer_.reducemin_;
    layer_.reducemin_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_reducemin(::CoreML::Specification::ReduceMinLayerParams* reducemin) {
  clear_layer();
  if (reducemin) {
    set_has_reducemin();
    layer_.reducemin_ = reducemin;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceMin)
}
inline ::CoreML::Specification::ReduceMinLayerParams* NeuralNetworkLayer::_internal_mutable_reducemin() {
  if (!_internal_has_reducemin()) {
    clear_layer();
    set_has_reducemin();
    layer_.reducemin_ = CreateMaybeMessage< ::CoreML::Specification::ReduceMinLayerParams >(GetArenaForAllocation());
  }
  return layer_.reducemin_;
}
inline ::CoreML::Specification::ReduceMinLayerParams* NeuralNetworkLayer::mutable_reducemin() {
  ::CoreML::Specification::ReduceMinLayerParams* _msg = _internal_mutable_reducemin();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceMin)
  return _msg;
}

// .CoreML.Specification.ReduceSumLayerParams reduceSum = 1270;
inline bool NeuralNetworkLayer::_internal_has_reducesum() const {
  return layer_case() == kReduceSum;
}
inline bool NeuralNetworkLayer::has_reducesum() const {
  return _internal_has_reducesum();
}
inline void NeuralNetworkLayer::set_has_reducesum() {
  _oneof_case_[0] = kReduceSum;
}
inline void NeuralNetworkLayer::clear_reducesum() {
  if (_internal_has_reducesum()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.reducesum_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ReduceSumLayerParams* NeuralNetworkLayer::release_reducesum() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceSum)
  if (_internal_has_reducesum()) {
    clear_has_layer();
      ::CoreML::Specification::ReduceSumLayerParams* temp = layer_.reducesum_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.reducesum_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ReduceSumLayerParams& NeuralNetworkLayer::_internal_reducesum() const {
  return _internal_has_reducesum()
      ? *layer_.reducesum_
      : reinterpret_cast< ::CoreML::Specification::ReduceSumLayerParams&>(::CoreML::Specification::_ReduceSumLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ReduceSumLayerParams& NeuralNetworkLayer::reducesum() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceSum)
  return _internal_reducesum();
}
inline ::CoreML::Specification::ReduceSumLayerParams* NeuralNetworkLayer::unsafe_arena_release_reducesum() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.reduceSum)
  if (_internal_has_reducesum()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceSumLayerParams* temp = layer_.reducesum_;
    layer_.reducesum_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_reducesum(::CoreML::Specification::ReduceSumLayerParams* reducesum) {
  clear_layer();
  if (reducesum) {
    set_has_reducesum();
    layer_.reducesum_ = reducesum;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceSum)
}
inline ::CoreML::Specification::ReduceSumLayerParams* NeuralNetworkLayer::_internal_mutable_reducesum() {
  if (!_internal_has_reducesum()) {
    clear_layer();
    set_has_reducesum();
    layer_.reducesum_ = CreateMaybeMessage< ::CoreML::Specification::ReduceSumLayerParams >(GetArenaForAllocation());
  }
  return layer_.reducesum_;
}
inline ::CoreML::Specification::ReduceSumLayerParams* NeuralNetworkLayer::mutable_reducesum() {
  ::CoreML::Specification::ReduceSumLayerParams* _msg = _internal_mutable_reducesum();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceSum)
  return _msg;
}

// .CoreML.Specification.ReduceProdLayerParams reduceProd = 1275;
inline bool NeuralNetworkLayer::_internal_has_reduceprod() const {
  return layer_case() == kReduceProd;
}
inline bool NeuralNetworkLayer::has_reduceprod() const {
  return _internal_has_reduceprod();
}
inline void NeuralNetworkLayer::set_has_reduceprod() {
  _oneof_case_[0] = kReduceProd;
}
inline void NeuralNetworkLayer::clear_reduceprod() {
  if (_internal_has_reduceprod()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.reduceprod_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ReduceProdLayerParams* NeuralNetworkLayer::release_reduceprod() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceProd)
  if (_internal_has_reduceprod()) {
    clear_has_layer();
      ::CoreML::Specification::ReduceProdLayerParams* temp = layer_.reduceprod_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.reduceprod_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ReduceProdLayerParams& NeuralNetworkLayer::_internal_reduceprod() const {
  return _internal_has_reduceprod()
      ? *layer_.reduceprod_
      : reinterpret_cast< ::CoreML::Specification::ReduceProdLayerParams&>(::CoreML::Specification::_ReduceProdLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ReduceProdLayerParams& NeuralNetworkLayer::reduceprod() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceProd)
  return _internal_reduceprod();
}
inline ::CoreML::Specification::ReduceProdLayerParams* NeuralNetworkLayer::unsafe_arena_release_reduceprod() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.reduceProd)
  if (_internal_has_reduceprod()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceProdLayerParams* temp = layer_.reduceprod_;
    layer_.reduceprod_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_reduceprod(::CoreML::Specification::ReduceProdLayerParams* reduceprod) {
  clear_layer();
  if (reduceprod) {
    set_has_reduceprod();
    layer_.reduceprod_ = reduceprod;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceProd)
}
inline ::CoreML::Specification::ReduceProdLayerParams* NeuralNetworkLayer::_internal_mutable_reduceprod() {
  if (!_internal_has_reduceprod()) {
    clear_layer();
    set_has_reduceprod();
    layer_.reduceprod_ = CreateMaybeMessage< ::CoreML::Specification::ReduceProdLayerParams >(GetArenaForAllocation());
  }
  return layer_.reduceprod_;
}
inline ::CoreML::Specification::ReduceProdLayerParams* NeuralNetworkLayer::mutable_reduceprod() {
  ::CoreML::Specification::ReduceProdLayerParams* _msg = _internal_mutable_reduceprod();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceProd)
  return _msg;
}

// .CoreML.Specification.ReduceMeanLayerParams reduceMean = 1280;
inline bool NeuralNetworkLayer::_internal_has_reducemean() const {
  return layer_case() == kReduceMean;
}
inline bool NeuralNetworkLayer::has_reducemean() const {
  return _internal_has_reducemean();
}
inline void NeuralNetworkLayer::set_has_reducemean() {
  _oneof_case_[0] = kReduceMean;
}
inline void NeuralNetworkLayer::clear_reducemean() {
  if (_internal_has_reducemean()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.reducemean_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ReduceMeanLayerParams* NeuralNetworkLayer::release_reducemean() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceMean)
  if (_internal_has_reducemean()) {
    clear_has_layer();
      ::CoreML::Specification::ReduceMeanLayerParams* temp = layer_.reducemean_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.reducemean_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ReduceMeanLayerParams& NeuralNetworkLayer::_internal_reducemean() const {
  return _internal_has_reducemean()
      ? *layer_.reducemean_
      : reinterpret_cast< ::CoreML::Specification::ReduceMeanLayerParams&>(::CoreML::Specification::_ReduceMeanLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ReduceMeanLayerParams& NeuralNetworkLayer::reducemean() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceMean)
  return _internal_reducemean();
}
inline ::CoreML::Specification::ReduceMeanLayerParams* NeuralNetworkLayer::unsafe_arena_release_reducemean() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.reduceMean)
  if (_internal_has_reducemean()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceMeanLayerParams* temp = layer_.reducemean_;
    layer_.reducemean_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_reducemean(::CoreML::Specification::ReduceMeanLayerParams* reducemean) {
  clear_layer();
  if (reducemean) {
    set_has_reducemean();
    layer_.reducemean_ = reducemean;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceMean)
}
inline ::CoreML::Specification::ReduceMeanLayerParams* NeuralNetworkLayer::_internal_mutable_reducemean() {
  if (!_internal_has_reducemean()) {
    clear_layer();
    set_has_reducemean();
    layer_.reducemean_ = CreateMaybeMessage< ::CoreML::Specification::ReduceMeanLayerParams >(GetArenaForAllocation());
  }
  return layer_.reducemean_;
}
inline ::CoreML::Specification::ReduceMeanLayerParams* NeuralNetworkLayer::mutable_reducemean() {
  ::CoreML::Specification::ReduceMeanLayerParams* _msg = _internal_mutable_reducemean();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceMean)
  return _msg;
}

// .CoreML.Specification.ReduceLogSumLayerParams reduceLogSum = 1285;
inline bool NeuralNetworkLayer::_internal_has_reducelogsum() const {
  return layer_case() == kReduceLogSum;
}
inline bool NeuralNetworkLayer::has_reducelogsum() const {
  return _internal_has_reducelogsum();
}
inline void NeuralNetworkLayer::set_has_reducelogsum() {
  _oneof_case_[0] = kReduceLogSum;
}
inline void NeuralNetworkLayer::clear_reducelogsum() {
  if (_internal_has_reducelogsum()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.reducelogsum_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ReduceLogSumLayerParams* NeuralNetworkLayer::release_reducelogsum() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceLogSum)
  if (_internal_has_reducelogsum()) {
    clear_has_layer();
      ::CoreML::Specification::ReduceLogSumLayerParams* temp = layer_.reducelogsum_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.reducelogsum_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ReduceLogSumLayerParams& NeuralNetworkLayer::_internal_reducelogsum() const {
  return _internal_has_reducelogsum()
      ? *layer_.reducelogsum_
      : reinterpret_cast< ::CoreML::Specification::ReduceLogSumLayerParams&>(::CoreML::Specification::_ReduceLogSumLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ReduceLogSumLayerParams& NeuralNetworkLayer::reducelogsum() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceLogSum)
  return _internal_reducelogsum();
}
inline ::CoreML::Specification::ReduceLogSumLayerParams* NeuralNetworkLayer::unsafe_arena_release_reducelogsum() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.reduceLogSum)
  if (_internal_has_reducelogsum()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceLogSumLayerParams* temp = layer_.reducelogsum_;
    layer_.reducelogsum_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_reducelogsum(::CoreML::Specification::ReduceLogSumLayerParams* reducelogsum) {
  clear_layer();
  if (reducelogsum) {
    set_has_reducelogsum();
    layer_.reducelogsum_ = reducelogsum;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceLogSum)
}
inline ::CoreML::Specification::ReduceLogSumLayerParams* NeuralNetworkLayer::_internal_mutable_reducelogsum() {
  if (!_internal_has_reducelogsum()) {
    clear_layer();
    set_has_reducelogsum();
    layer_.reducelogsum_ = CreateMaybeMessage< ::CoreML::Specification::ReduceLogSumLayerParams >(GetArenaForAllocation());
  }
  return layer_.reducelogsum_;
}
inline ::CoreML::Specification::ReduceLogSumLayerParams* NeuralNetworkLayer::mutable_reducelogsum() {
  ::CoreML::Specification::ReduceLogSumLayerParams* _msg = _internal_mutable_reducelogsum();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceLogSum)
  return _msg;
}

// .CoreML.Specification.ReduceSumSquareLayerParams reduceSumSquare = 1290;
inline bool NeuralNetworkLayer::_internal_has_reducesumsquare() const {
  return layer_case() == kReduceSumSquare;
}
inline bool NeuralNetworkLayer::has_reducesumsquare() const {
  return _internal_has_reducesumsquare();
}
inline void NeuralNetworkLayer::set_has_reducesumsquare() {
  _oneof_case_[0] = kReduceSumSquare;
}
inline void NeuralNetworkLayer::clear_reducesumsquare() {
  if (_internal_has_reducesumsquare()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.reducesumsquare_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ReduceSumSquareLayerParams* NeuralNetworkLayer::release_reducesumsquare() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceSumSquare)
  if (_internal_has_reducesumsquare()) {
    clear_has_layer();
      ::CoreML::Specification::ReduceSumSquareLayerParams* temp = layer_.reducesumsquare_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.reducesumsquare_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ReduceSumSquareLayerParams& NeuralNetworkLayer::_internal_reducesumsquare() const {
  return _internal_has_reducesumsquare()
      ? *layer_.reducesumsquare_
      : reinterpret_cast< ::CoreML::Specification::ReduceSumSquareLayerParams&>(::CoreML::Specification::_ReduceSumSquareLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ReduceSumSquareLayerParams& NeuralNetworkLayer::reducesumsquare() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceSumSquare)
  return _internal_reducesumsquare();
}
inline ::CoreML::Specification::ReduceSumSquareLayerParams* NeuralNetworkLayer::unsafe_arena_release_reducesumsquare() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.reduceSumSquare)
  if (_internal_has_reducesumsquare()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceSumSquareLayerParams* temp = layer_.reducesumsquare_;
    layer_.reducesumsquare_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_reducesumsquare(::CoreML::Specification::ReduceSumSquareLayerParams* reducesumsquare) {
  clear_layer();
  if (reducesumsquare) {
    set_has_reducesumsquare();
    layer_.reducesumsquare_ = reducesumsquare;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceSumSquare)
}
inline ::CoreML::Specification::ReduceSumSquareLayerParams* NeuralNetworkLayer::_internal_mutable_reducesumsquare() {
  if (!_internal_has_reducesumsquare()) {
    clear_layer();
    set_has_reducesumsquare();
    layer_.reducesumsquare_ = CreateMaybeMessage< ::CoreML::Specification::ReduceSumSquareLayerParams >(GetArenaForAllocation());
  }
  return layer_.reducesumsquare_;
}
inline ::CoreML::Specification::ReduceSumSquareLayerParams* NeuralNetworkLayer::mutable_reducesumsquare() {
  ::CoreML::Specification::ReduceSumSquareLayerParams* _msg = _internal_mutable_reducesumsquare();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceSumSquare)
  return _msg;
}

// .CoreML.Specification.ReduceLogSumExpLayerParams reduceLogSumExp = 1295;
inline bool NeuralNetworkLayer::_internal_has_reducelogsumexp() const {
  return layer_case() == kReduceLogSumExp;
}
inline bool NeuralNetworkLayer::has_reducelogsumexp() const {
  return _internal_has_reducelogsumexp();
}
inline void NeuralNetworkLayer::set_has_reducelogsumexp() {
  _oneof_case_[0] = kReduceLogSumExp;
}
inline void NeuralNetworkLayer::clear_reducelogsumexp() {
  if (_internal_has_reducelogsumexp()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.reducelogsumexp_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ReduceLogSumExpLayerParams* NeuralNetworkLayer::release_reducelogsumexp() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceLogSumExp)
  if (_internal_has_reducelogsumexp()) {
    clear_has_layer();
      ::CoreML::Specification::ReduceLogSumExpLayerParams* temp = layer_.reducelogsumexp_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.reducelogsumexp_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ReduceLogSumExpLayerParams& NeuralNetworkLayer::_internal_reducelogsumexp() const {
  return _internal_has_reducelogsumexp()
      ? *layer_.reducelogsumexp_
      : reinterpret_cast< ::CoreML::Specification::ReduceLogSumExpLayerParams&>(::CoreML::Specification::_ReduceLogSumExpLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ReduceLogSumExpLayerParams& NeuralNetworkLayer::reducelogsumexp() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceLogSumExp)
  return _internal_reducelogsumexp();
}
inline ::CoreML::Specification::ReduceLogSumExpLayerParams* NeuralNetworkLayer::unsafe_arena_release_reducelogsumexp() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.reduceLogSumExp)
  if (_internal_has_reducelogsumexp()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceLogSumExpLayerParams* temp = layer_.reducelogsumexp_;
    layer_.reducelogsumexp_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_reducelogsumexp(::CoreML::Specification::ReduceLogSumExpLayerParams* reducelogsumexp) {
  clear_layer();
  if (reducelogsumexp) {
    set_has_reducelogsumexp();
    layer_.reducelogsumexp_ = reducelogsumexp;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceLogSumExp)
}
inline ::CoreML::Specification::ReduceLogSumExpLayerParams* NeuralNetworkLayer::_internal_mutable_reducelogsumexp() {
  if (!_internal_has_reducelogsumexp()) {
    clear_layer();
    set_has_reducelogsumexp();
    layer_.reducelogsumexp_ = CreateMaybeMessage< ::CoreML::Specification::ReduceLogSumExpLayerParams >(GetArenaForAllocation());
  }
  return layer_.reducelogsumexp_;
}
inline ::CoreML::Specification::ReduceLogSumExpLayerParams* NeuralNetworkLayer::mutable_reducelogsumexp() {
  ::CoreML::Specification::ReduceLogSumExpLayerParams* _msg = _internal_mutable_reducelogsumexp();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceLogSumExp)
  return _msg;
}

// .CoreML.Specification.WhereNonZeroLayerParams whereNonZero = 1313;
inline bool NeuralNetworkLayer::_internal_has_wherenonzero() const {
  return layer_case() == kWhereNonZero;
}
inline bool NeuralNetworkLayer::has_wherenonzero() const {
  return _internal_has_wherenonzero();
}
inline void NeuralNetworkLayer::set_has_wherenonzero() {
  _oneof_case_[0] = kWhereNonZero;
}
inline void NeuralNetworkLayer::clear_wherenonzero() {
  if (_internal_has_wherenonzero()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.wherenonzero_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::WhereNonZeroLayerParams* NeuralNetworkLayer::release_wherenonzero() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.whereNonZero)
  if (_internal_has_wherenonzero()) {
    clear_has_layer();
      ::CoreML::Specification::WhereNonZeroLayerParams* temp = layer_.wherenonzero_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.wherenonzero_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::WhereNonZeroLayerParams& NeuralNetworkLayer::_internal_wherenonzero() const {
  return _internal_has_wherenonzero()
      ? *layer_.wherenonzero_
      : reinterpret_cast< ::CoreML::Specification::WhereNonZeroLayerParams&>(::CoreML::Specification::_WhereNonZeroLayerParams_default_instance_);
}
inline const ::CoreML::Specification::WhereNonZeroLayerParams& NeuralNetworkLayer::wherenonzero() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.whereNonZero)
  return _internal_wherenonzero();
}
inline ::CoreML::Specification::WhereNonZeroLayerParams* NeuralNetworkLayer::unsafe_arena_release_wherenonzero() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.whereNonZero)
  if (_internal_has_wherenonzero()) {
    clear_has_layer();
    ::CoreML::Specification::WhereNonZeroLayerParams* temp = layer_.wherenonzero_;
    layer_.wherenonzero_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_wherenonzero(::CoreML::Specification::WhereNonZeroLayerParams* wherenonzero) {
  clear_layer();
  if (wherenonzero) {
    set_has_wherenonzero();
    layer_.wherenonzero_ = wherenonzero;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.whereNonZero)
}
inline ::CoreML::Specification::WhereNonZeroLayerParams* NeuralNetworkLayer::_internal_mutable_wherenonzero() {
  if (!_internal_has_wherenonzero()) {
    clear_layer();
    set_has_wherenonzero();
    layer_.wherenonzero_ = CreateMaybeMessage< ::CoreML::Specification::WhereNonZeroLayerParams >(GetArenaForAllocation());
  }
  return layer_.wherenonzero_;
}
inline ::CoreML::Specification::WhereNonZeroLayerParams* NeuralNetworkLayer::mutable_wherenonzero() {
  ::CoreML::Specification::WhereNonZeroLayerParams* _msg = _internal_mutable_wherenonzero();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.whereNonZero)
  return _msg;
}

// .CoreML.Specification.MatrixBandPartLayerParams matrixBandPart = 1315;
inline bool NeuralNetworkLayer::_internal_has_matrixbandpart() const {
  return layer_case() == kMatrixBandPart;
}
inline bool NeuralNetworkLayer::has_matrixbandpart() const {
  return _internal_has_matrixbandpart();
}
inline void NeuralNetworkLayer::set_has_matrixbandpart() {
  _oneof_case_[0] = kMatrixBandPart;
}
inline void NeuralNetworkLayer::clear_matrixbandpart() {
  if (_internal_has_matrixbandpart()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.matrixbandpart_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::MatrixBandPartLayerParams* NeuralNetworkLayer::release_matrixbandpart() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.matrixBandPart)
  if (_internal_has_matrixbandpart()) {
    clear_has_layer();
      ::CoreML::Specification::MatrixBandPartLayerParams* temp = layer_.matrixbandpart_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.matrixbandpart_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::MatrixBandPartLayerParams& NeuralNetworkLayer::_internal_matrixbandpart() const {
  return _internal_has_matrixbandpart()
      ? *layer_.matrixbandpart_
      : reinterpret_cast< ::CoreML::Specification::MatrixBandPartLayerParams&>(::CoreML::Specification::_MatrixBandPartLayerParams_default_instance_);
}
inline const ::CoreML::Specification::MatrixBandPartLayerParams& NeuralNetworkLayer::matrixbandpart() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.matrixBandPart)
  return _internal_matrixbandpart();
}
inline ::CoreML::Specification::MatrixBandPartLayerParams* NeuralNetworkLayer::unsafe_arena_release_matrixbandpart() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.matrixBandPart)
  if (_internal_has_matrixbandpart()) {
    clear_has_layer();
    ::CoreML::Specification::MatrixBandPartLayerParams* temp = layer_.matrixbandpart_;
    layer_.matrixbandpart_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_matrixbandpart(::CoreML::Specification::MatrixBandPartLayerParams* matrixbandpart) {
  clear_layer();
  if (matrixbandpart) {
    set_has_matrixbandpart();
    layer_.matrixbandpart_ = matrixbandpart;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.matrixBandPart)
}
inline ::CoreML::Specification::MatrixBandPartLayerParams* NeuralNetworkLayer::_internal_mutable_matrixbandpart() {
  if (!_internal_has_matrixbandpart()) {
    clear_layer();
    set_has_matrixbandpart();
    layer_.matrixbandpart_ = CreateMaybeMessage< ::CoreML::Specification::MatrixBandPartLayerParams >(GetArenaForAllocation());
  }
  return layer_.matrixbandpart_;
}
inline ::CoreML::Specification::MatrixBandPartLayerParams* NeuralNetworkLayer::mutable_matrixbandpart() {
  ::CoreML::Specification::MatrixBandPartLayerParams* _msg = _internal_mutable_matrixbandpart();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.matrixBandPart)
  return _msg;
}

// .CoreML.Specification.LowerTriangularLayerParams lowerTriangular = 1320;
inline bool NeuralNetworkLayer::_internal_has_lowertriangular() const {
  return layer_case() == kLowerTriangular;
}
inline bool NeuralNetworkLayer::has_lowertriangular() const {
  return _internal_has_lowertriangular();
}
inline void NeuralNetworkLayer::set_has_lowertriangular() {
  _oneof_case_[0] = kLowerTriangular;
}
inline void NeuralNetworkLayer::clear_lowertriangular() {
  if (_internal_has_lowertriangular()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.lowertriangular_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::LowerTriangularLayerParams* NeuralNetworkLayer::release_lowertriangular() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.lowerTriangular)
  if (_internal_has_lowertriangular()) {
    clear_has_layer();
      ::CoreML::Specification::LowerTriangularLayerParams* temp = layer_.lowertriangular_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.lowertriangular_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::LowerTriangularLayerParams& NeuralNetworkLayer::_internal_lowertriangular() const {
  return _internal_has_lowertriangular()
      ? *layer_.lowertriangular_
      : reinterpret_cast< ::CoreML::Specification::LowerTriangularLayerParams&>(::CoreML::Specification::_LowerTriangularLayerParams_default_instance_);
}
inline const ::CoreML::Specification::LowerTriangularLayerParams& NeuralNetworkLayer::lowertriangular() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.lowerTriangular)
  return _internal_lowertriangular();
}
inline ::CoreML::Specification::LowerTriangularLayerParams* NeuralNetworkLayer::unsafe_arena_release_lowertriangular() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.lowerTriangular)
  if (_internal_has_lowertriangular()) {
    clear_has_layer();
    ::CoreML::Specification::LowerTriangularLayerParams* temp = layer_.lowertriangular_;
    layer_.lowertriangular_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_lowertriangular(::CoreML::Specification::LowerTriangularLayerParams* lowertriangular) {
  clear_layer();
  if (lowertriangular) {
    set_has_lowertriangular();
    layer_.lowertriangular_ = lowertriangular;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.lowerTriangular)
}
inline ::CoreML::Specification::LowerTriangularLayerParams* NeuralNetworkLayer::_internal_mutable_lowertriangular() {
  if (!_internal_has_lowertriangular()) {
    clear_layer();
    set_has_lowertriangular();
    layer_.lowertriangular_ = CreateMaybeMessage< ::CoreML::Specification::LowerTriangularLayerParams >(GetArenaForAllocation());
  }
  return layer_.lowertriangular_;
}
inline ::CoreML::Specification::LowerTriangularLayerParams* NeuralNetworkLayer::mutable_lowertriangular() {
  ::CoreML::Specification::LowerTriangularLayerParams* _msg = _internal_mutable_lowertriangular();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.lowerTriangular)
  return _msg;
}

// .CoreML.Specification.UpperTriangularLayerParams upperTriangular = 1325;
inline bool NeuralNetworkLayer::_internal_has_uppertriangular() const {
  return layer_case() == kUpperTriangular;
}
inline bool NeuralNetworkLayer::has_uppertriangular() const {
  return _internal_has_uppertriangular();
}
inline void NeuralNetworkLayer::set_has_uppertriangular() {
  _oneof_case_[0] = kUpperTriangular;
}
inline void NeuralNetworkLayer::clear_uppertriangular() {
  if (_internal_has_uppertriangular()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.uppertriangular_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::UpperTriangularLayerParams* NeuralNetworkLayer::release_uppertriangular() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.upperTriangular)
  if (_internal_has_uppertriangular()) {
    clear_has_layer();
      ::CoreML::Specification::UpperTriangularLayerParams* temp = layer_.uppertriangular_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.uppertriangular_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::UpperTriangularLayerParams& NeuralNetworkLayer::_internal_uppertriangular() const {
  return _internal_has_uppertriangular()
      ? *layer_.uppertriangular_
      : reinterpret_cast< ::CoreML::Specification::UpperTriangularLayerParams&>(::CoreML::Specification::_UpperTriangularLayerParams_default_instance_);
}
inline const ::CoreML::Specification::UpperTriangularLayerParams& NeuralNetworkLayer::uppertriangular() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.upperTriangular)
  return _internal_uppertriangular();
}
inline ::CoreML::Specification::UpperTriangularLayerParams* NeuralNetworkLayer::unsafe_arena_release_uppertriangular() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.upperTriangular)
  if (_internal_has_uppertriangular()) {
    clear_has_layer();
    ::CoreML::Specification::UpperTriangularLayerParams* temp = layer_.uppertriangular_;
    layer_.uppertriangular_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_uppertriangular(::CoreML::Specification::UpperTriangularLayerParams* uppertriangular) {
  clear_layer();
  if (uppertriangular) {
    set_has_uppertriangular();
    layer_.uppertriangular_ = uppertriangular;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.upperTriangular)
}
inline ::CoreML::Specification::UpperTriangularLayerParams* NeuralNetworkLayer::_internal_mutable_uppertriangular() {
  if (!_internal_has_uppertriangular()) {
    clear_layer();
    set_has_uppertriangular();
    layer_.uppertriangular_ = CreateMaybeMessage< ::CoreML::Specification::UpperTriangularLayerParams >(GetArenaForAllocation());
  }
  return layer_.uppertriangular_;
}
inline ::CoreML::Specification::UpperTriangularLayerParams* NeuralNetworkLayer::mutable_uppertriangular() {
  ::CoreML::Specification::UpperTriangularLayerParams* _msg = _internal_mutable_uppertriangular();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.upperTriangular)
  return _msg;
}

// .CoreML.Specification.WhereBroadcastableLayerParams whereBroadcastable = 1330;
inline bool NeuralNetworkLayer::_internal_has_wherebroadcastable() const {
  return layer_case() == kWhereBroadcastable;
}
inline bool NeuralNetworkLayer::has_wherebroadcastable() const {
  return _internal_has_wherebroadcastable();
}
inline void NeuralNetworkLayer::set_has_wherebroadcastable() {
  _oneof_case_[0] = kWhereBroadcastable;
}
inline void NeuralNetworkLayer::clear_wherebroadcastable() {
  if (_internal_has_wherebroadcastable()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.wherebroadcastable_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::WhereBroadcastableLayerParams* NeuralNetworkLayer::release_wherebroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.whereBroadcastable)
  if (_internal_has_wherebroadcastable()) {
    clear_has_layer();
      ::CoreML::Specification::WhereBroadcastableLayerParams* temp = layer_.wherebroadcastable_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.wherebroadcastable_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::WhereBroadcastableLayerParams& NeuralNetworkLayer::_internal_wherebroadcastable() const {
  return _internal_has_wherebroadcastable()
      ? *layer_.wherebroadcastable_
      : reinterpret_cast< ::CoreML::Specification::WhereBroadcastableLayerParams&>(::CoreML::Specification::_WhereBroadcastableLayerParams_default_instance_);
}
inline const ::CoreML::Specification::WhereBroadcastableLayerParams& NeuralNetworkLayer::wherebroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.whereBroadcastable)
  return _internal_wherebroadcastable();
}
inline ::CoreML::Specification::WhereBroadcastableLayerParams* NeuralNetworkLayer::unsafe_arena_release_wherebroadcastable() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.whereBroadcastable)
  if (_internal_has_wherebroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::WhereBroadcastableLayerParams* temp = layer_.wherebroadcastable_;
    layer_.wherebroadcastable_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_wherebroadcastable(::CoreML::Specification::WhereBroadcastableLayerParams* wherebroadcastable) {
  clear_layer();
  if (wherebroadcastable) {
    set_has_wherebroadcastable();
    layer_.wherebroadcastable_ = wherebroadcastable;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.whereBroadcastable)
}
inline ::CoreML::Specification::WhereBroadcastableLayerParams* NeuralNetworkLayer::_internal_mutable_wherebroadcastable() {
  if (!_internal_has_wherebroadcastable()) {
    clear_layer();
    set_has_wherebroadcastable();
    layer_.wherebroadcastable_ = CreateMaybeMessage< ::CoreML::Specification::WhereBroadcastableLayerParams >(GetArenaForAllocation());
  }
  return layer_.wherebroadcastable_;
}
inline ::CoreML::Specification::WhereBroadcastableLayerParams* NeuralNetworkLayer::mutable_wherebroadcastable() {
  ::CoreML::Specification::WhereBroadcastableLayerParams* _msg = _internal_mutable_wherebroadcastable();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.whereBroadcastable)
  return _msg;
}

// .CoreML.Specification.LayerNormalizationLayerParams layerNormalization = 1350;
inline bool NeuralNetworkLayer::_internal_has_layernormalization() const {
  return layer_case() == kLayerNormalization;
}
inline bool NeuralNetworkLayer::has_layernormalization() const {
  return _internal_has_layernormalization();
}
inline void NeuralNetworkLayer::set_has_layernormalization() {
  _oneof_case_[0] = kLayerNormalization;
}
inline void NeuralNetworkLayer::clear_layernormalization() {
  if (_internal_has_layernormalization()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.layernormalization_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::LayerNormalizationLayerParams* NeuralNetworkLayer::release_layernormalization() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.layerNormalization)
  if (_internal_has_layernormalization()) {
    clear_has_layer();
      ::CoreML::Specification::LayerNormalizationLayerParams* temp = layer_.layernormalization_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.layernormalization_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::LayerNormalizationLayerParams& NeuralNetworkLayer::_internal_layernormalization() const {
  return _internal_has_layernormalization()
      ? *layer_.layernormalization_
      : reinterpret_cast< ::CoreML::Specification::LayerNormalizationLayerParams&>(::CoreML::Specification::_LayerNormalizationLayerParams_default_instance_);
}
inline const ::CoreML::Specification::LayerNormalizationLayerParams& NeuralNetworkLayer::layernormalization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.layerNormalization)
  return _internal_layernormalization();
}
inline ::CoreML::Specification::LayerNormalizationLayerParams* NeuralNetworkLayer::unsafe_arena_release_layernormalization() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.layerNormalization)
  if (_internal_has_layernormalization()) {
    clear_has_layer();
    ::CoreML::Specification::LayerNormalizationLayerParams* temp = layer_.layernormalization_;
    layer_.layernormalization_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_layernormalization(::CoreML::Specification::LayerNormalizationLayerParams* layernormalization) {
  clear_layer();
  if (layernormalization) {
    set_has_layernormalization();
    layer_.layernormalization_ = layernormalization;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.layerNormalization)
}
inline ::CoreML::Specification::LayerNormalizationLayerParams* NeuralNetworkLayer::_internal_mutable_layernormalization() {
  if (!_internal_has_layernormalization()) {
    clear_layer();
    set_has_layernormalization();
    layer_.layernormalization_ = CreateMaybeMessage< ::CoreML::Specification::LayerNormalizationLayerParams >(GetArenaForAllocation());
  }
  return layer_.layernormalization_;
}
inline ::CoreML::Specification::LayerNormalizationLayerParams* NeuralNetworkLayer::mutable_layernormalization() {
  ::CoreML::Specification::LayerNormalizationLayerParams* _msg = _internal_mutable_layernormalization();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.layerNormalization)
  return _msg;
}

// .CoreML.Specification.NonMaximumSuppressionLayerParams NonMaximumSuppression = 1400;
inline bool NeuralNetworkLayer::_internal_has_nonmaximumsuppression() const {
  return layer_case() == kNonMaximumSuppression;
}
inline bool NeuralNetworkLayer::has_nonmaximumsuppression() const {
  return _internal_has_nonmaximumsuppression();
}
inline void NeuralNetworkLayer::set_has_nonmaximumsuppression() {
  _oneof_case_[0] = kNonMaximumSuppression;
}
inline void NeuralNetworkLayer::clear_nonmaximumsuppression() {
  if (_internal_has_nonmaximumsuppression()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.nonmaximumsuppression_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::NonMaximumSuppressionLayerParams* NeuralNetworkLayer::release_nonmaximumsuppression() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.NonMaximumSuppression)
  if (_internal_has_nonmaximumsuppression()) {
    clear_has_layer();
      ::CoreML::Specification::NonMaximumSuppressionLayerParams* temp = layer_.nonmaximumsuppression_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.nonmaximumsuppression_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::NonMaximumSuppressionLayerParams& NeuralNetworkLayer::_internal_nonmaximumsuppression() const {
  return _internal_has_nonmaximumsuppression()
      ? *layer_.nonmaximumsuppression_
      : reinterpret_cast< ::CoreML::Specification::NonMaximumSuppressionLayerParams&>(::CoreML::Specification::_NonMaximumSuppressionLayerParams_default_instance_);
}
inline const ::CoreML::Specification::NonMaximumSuppressionLayerParams& NeuralNetworkLayer::nonmaximumsuppression() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.NonMaximumSuppression)
  return _internal_nonmaximumsuppression();
}
inline ::CoreML::Specification::NonMaximumSuppressionLayerParams* NeuralNetworkLayer::unsafe_arena_release_nonmaximumsuppression() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.NonMaximumSuppression)
  if (_internal_has_nonmaximumsuppression()) {
    clear_has_layer();
    ::CoreML::Specification::NonMaximumSuppressionLayerParams* temp = layer_.nonmaximumsuppression_;
    layer_.nonmaximumsuppression_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_nonmaximumsuppression(::CoreML::Specification::NonMaximumSuppressionLayerParams* nonmaximumsuppression) {
  clear_layer();
  if (nonmaximumsuppression) {
    set_has_nonmaximumsuppression();
    layer_.nonmaximumsuppression_ = nonmaximumsuppression;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.NonMaximumSuppression)
}
inline ::CoreML::Specification::NonMaximumSuppressionLayerParams* NeuralNetworkLayer::_internal_mutable_nonmaximumsuppression() {
  if (!_internal_has_nonmaximumsuppression()) {
    clear_layer();
    set_has_nonmaximumsuppression();
    layer_.nonmaximumsuppression_ = CreateMaybeMessage< ::CoreML::Specification::NonMaximumSuppressionLayerParams >(GetArenaForAllocation());
  }
  return layer_.nonmaximumsuppression_;
}
inline ::CoreML::Specification::NonMaximumSuppressionLayerParams* NeuralNetworkLayer::mutable_nonmaximumsuppression() {
  ::CoreML::Specification::NonMaximumSuppressionLayerParams* _msg = _internal_mutable_nonmaximumsuppression();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.NonMaximumSuppression)
  return _msg;
}

// .CoreML.Specification.OneHotLayerParams oneHot = 1450;
inline bool NeuralNetworkLayer::_internal_has_onehot() const {
  return layer_case() == kOneHot;
}
inline bool NeuralNetworkLayer::has_onehot() const {
  return _internal_has_onehot();
}
inline void NeuralNetworkLayer::set_has_onehot() {
  _oneof_case_[0] = kOneHot;
}
inline void NeuralNetworkLayer::clear_onehot() {
  if (_internal_has_onehot()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.onehot_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::OneHotLayerParams* NeuralNetworkLayer::release_onehot() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.oneHot)
  if (_internal_has_onehot()) {
    clear_has_layer();
      ::CoreML::Specification::OneHotLayerParams* temp = layer_.onehot_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.onehot_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::OneHotLayerParams& NeuralNetworkLayer::_internal_onehot() const {
  return _internal_has_onehot()
      ? *layer_.onehot_
      : reinterpret_cast< ::CoreML::Specification::OneHotLayerParams&>(::CoreML::Specification::_OneHotLayerParams_default_instance_);
}
inline const ::CoreML::Specification::OneHotLayerParams& NeuralNetworkLayer::onehot() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.oneHot)
  return _internal_onehot();
}
inline ::CoreML::Specification::OneHotLayerParams* NeuralNetworkLayer::unsafe_arena_release_onehot() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.oneHot)
  if (_internal_has_onehot()) {
    clear_has_layer();
    ::CoreML::Specification::OneHotLayerParams* temp = layer_.onehot_;
    layer_.onehot_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_onehot(::CoreML::Specification::OneHotLayerParams* onehot) {
  clear_layer();
  if (onehot) {
    set_has_onehot();
    layer_.onehot_ = onehot;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.oneHot)
}
inline ::CoreML::Specification::OneHotLayerParams* NeuralNetworkLayer::_internal_mutable_onehot() {
  if (!_internal_has_onehot()) {
    clear_layer();
    set_has_onehot();
    layer_.onehot_ = CreateMaybeMessage< ::CoreML::Specification::OneHotLayerParams >(GetArenaForAllocation());
  }
  return layer_.onehot_;
}
inline ::CoreML::Specification::OneHotLayerParams* NeuralNetworkLayer::mutable_onehot() {
  ::CoreML::Specification::OneHotLayerParams* _msg = _internal_mutable_onehot();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.oneHot)
  return _msg;
}

// .CoreML.Specification.CumSumLayerParams cumSum = 1455;
inline bool NeuralNetworkLayer::_internal_has_cumsum() const {
  return layer_case() == kCumSum;
}
inline bool NeuralNetworkLayer::has_cumsum() const {
  return _internal_has_cumsum();
}
inline void NeuralNetworkLayer::set_has_cumsum() {
  _oneof_case_[0] = kCumSum;
}
inline void NeuralNetworkLayer::clear_cumsum() {
  if (_internal_has_cumsum()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.cumsum_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::CumSumLayerParams* NeuralNetworkLayer::release_cumsum() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.cumSum)
  if (_internal_has_cumsum()) {
    clear_has_layer();
      ::CoreML::Specification::CumSumLayerParams* temp = layer_.cumsum_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.cumsum_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::CumSumLayerParams& NeuralNetworkLayer::_internal_cumsum() const {
  return _internal_has_cumsum()
      ? *layer_.cumsum_
      : reinterpret_cast< ::CoreML::Specification::CumSumLayerParams&>(::CoreML::Specification::_CumSumLayerParams_default_instance_);
}
inline const ::CoreML::Specification::CumSumLayerParams& NeuralNetworkLayer::cumsum() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.cumSum)
  return _internal_cumsum();
}
inline ::CoreML::Specification::CumSumLayerParams* NeuralNetworkLayer::unsafe_arena_release_cumsum() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.cumSum)
  if (_internal_has_cumsum()) {
    clear_has_layer();
    ::CoreML::Specification::CumSumLayerParams* temp = layer_.cumsum_;
    layer_.cumsum_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_cumsum(::CoreML::Specification::CumSumLayerParams* cumsum) {
  clear_layer();
  if (cumsum) {
    set_has_cumsum();
    layer_.cumsum_ = cumsum;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.cumSum)
}
inline ::CoreML::Specification::CumSumLayerParams* NeuralNetworkLayer::_internal_mutable_cumsum() {
  if (!_internal_has_cumsum()) {
    clear_layer();
    set_has_cumsum();
    layer_.cumsum_ = CreateMaybeMessage< ::CoreML::Specification::CumSumLayerParams >(GetArenaForAllocation());
  }
  return layer_.cumsum_;
}
inline ::CoreML::Specification::CumSumLayerParams* NeuralNetworkLayer::mutable_cumsum() {
  ::CoreML::Specification::CumSumLayerParams* _msg = _internal_mutable_cumsum();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.cumSum)
  return _msg;
}

// .CoreML.Specification.ClampedReLULayerParams clampedReLU = 1460;
inline bool NeuralNetworkLayer::_internal_has_clampedrelu() const {
  return layer_case() == kClampedReLU;
}
inline bool NeuralNetworkLayer::has_clampedrelu() const {
  return _internal_has_clampedrelu();
}
inline void NeuralNetworkLayer::set_has_clampedrelu() {
  _oneof_case_[0] = kClampedReLU;
}
inline void NeuralNetworkLayer::clear_clampedrelu() {
  if (_internal_has_clampedrelu()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.clampedrelu_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ClampedReLULayerParams* NeuralNetworkLayer::release_clampedrelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.clampedReLU)
  if (_internal_has_clampedrelu()) {
    clear_has_layer();
      ::CoreML::Specification::ClampedReLULayerParams* temp = layer_.clampedrelu_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.clampedrelu_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ClampedReLULayerParams& NeuralNetworkLayer::_internal_clampedrelu() const {
  return _internal_has_clampedrelu()
      ? *layer_.clampedrelu_
      : reinterpret_cast< ::CoreML::Specification::ClampedReLULayerParams&>(::CoreML::Specification::_ClampedReLULayerParams_default_instance_);
}
inline const ::CoreML::Specification::ClampedReLULayerParams& NeuralNetworkLayer::clampedrelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.clampedReLU)
  return _internal_clampedrelu();
}
inline ::CoreML::Specification::ClampedReLULayerParams* NeuralNetworkLayer::unsafe_arena_release_clampedrelu() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.clampedReLU)
  if (_internal_has_clampedrelu()) {
    clear_has_layer();
    ::CoreML::Specification::ClampedReLULayerParams* temp = layer_.clampedrelu_;
    layer_.clampedrelu_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_clampedrelu(::CoreML::Specification::ClampedReLULayerParams* clampedrelu) {
  clear_layer();
  if (clampedrelu) {
    set_has_clampedrelu();
    layer_.clampedrelu_ = clampedrelu;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.clampedReLU)
}
inline ::CoreML::Specification::ClampedReLULayerParams* NeuralNetworkLayer::_internal_mutable_clampedrelu() {
  if (!_internal_has_clampedrelu()) {
    clear_layer();
    set_has_clampedrelu();
    layer_.clampedrelu_ = CreateMaybeMessage< ::CoreML::Specification::ClampedReLULayerParams >(GetArenaForAllocation());
  }
  return layer_.clampedrelu_;
}
inline ::CoreML::Specification::ClampedReLULayerParams* NeuralNetworkLayer::mutable_clampedrelu() {
  ::CoreML::Specification::ClampedReLULayerParams* _msg = _internal_mutable_clampedrelu();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.clampedReLU)
  return _msg;
}

// .CoreML.Specification.ArgSortLayerParams argSort = 1461;
inline bool NeuralNetworkLayer::_internal_has_argsort() const {
  return layer_case() == kArgSort;
}
inline bool NeuralNetworkLayer::has_argsort() const {
  return _internal_has_argsort();
}
inline void NeuralNetworkLayer::set_has_argsort() {
  _oneof_case_[0] = kArgSort;
}
inline void NeuralNetworkLayer::clear_argsort() {
  if (_internal_has_argsort()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.argsort_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::ArgSortLayerParams* NeuralNetworkLayer::release_argsort() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.argSort)
  if (_internal_has_argsort()) {
    clear_has_layer();
      ::CoreML::Specification::ArgSortLayerParams* temp = layer_.argsort_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.argsort_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ArgSortLayerParams& NeuralNetworkLayer::_internal_argsort() const {
  return _internal_has_argsort()
      ? *layer_.argsort_
      : reinterpret_cast< ::CoreML::Specification::ArgSortLayerParams&>(::CoreML::Specification::_ArgSortLayerParams_default_instance_);
}
inline const ::CoreML::Specification::ArgSortLayerParams& NeuralNetworkLayer::argsort() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.argSort)
  return _internal_argsort();
}
inline ::CoreML::Specification::ArgSortLayerParams* NeuralNetworkLayer::unsafe_arena_release_argsort() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.argSort)
  if (_internal_has_argsort()) {
    clear_has_layer();
    ::CoreML::Specification::ArgSortLayerParams* temp = layer_.argsort_;
    layer_.argsort_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_argsort(::CoreML::Specification::ArgSortLayerParams* argsort) {
  clear_layer();
  if (argsort) {
    set_has_argsort();
    layer_.argsort_ = argsort;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.argSort)
}
inline ::CoreML::Specification::ArgSortLayerParams* NeuralNetworkLayer::_internal_mutable_argsort() {
  if (!_internal_has_argsort()) {
    clear_layer();
    set_has_argsort();
    layer_.argsort_ = CreateMaybeMessage< ::CoreML::Specification::ArgSortLayerParams >(GetArenaForAllocation());
  }
  return layer_.argsort_;
}
inline ::CoreML::Specification::ArgSortLayerParams* NeuralNetworkLayer::mutable_argsort() {
  ::CoreML::Specification::ArgSortLayerParams* _msg = _internal_mutable_argsort();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.argSort)
  return _msg;
}

// .CoreML.Specification.Pooling3DLayerParams pooling3d = 1465;
inline bool NeuralNetworkLayer::_internal_has_pooling3d() const {
  return layer_case() == kPooling3D;
}
inline bool NeuralNetworkLayer::has_pooling3d() const {
  return _internal_has_pooling3d();
}
inline void NeuralNetworkLayer::set_has_pooling3d() {
  _oneof_case_[0] = kPooling3D;
}
inline void NeuralNetworkLayer::clear_pooling3d() {
  if (_internal_has_pooling3d()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.pooling3d_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::Pooling3DLayerParams* NeuralNetworkLayer::release_pooling3d() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.pooling3d)
  if (_internal_has_pooling3d()) {
    clear_has_layer();
      ::CoreML::Specification::Pooling3DLayerParams* temp = layer_.pooling3d_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.pooling3d_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::Pooling3DLayerParams& NeuralNetworkLayer::_internal_pooling3d() const {
  return _internal_has_pooling3d()
      ? *layer_.pooling3d_
      : reinterpret_cast< ::CoreML::Specification::Pooling3DLayerParams&>(::CoreML::Specification::_Pooling3DLayerParams_default_instance_);
}
inline const ::CoreML::Specification::Pooling3DLayerParams& NeuralNetworkLayer::pooling3d() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.pooling3d)
  return _internal_pooling3d();
}
inline ::CoreML::Specification::Pooling3DLayerParams* NeuralNetworkLayer::unsafe_arena_release_pooling3d() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.pooling3d)
  if (_internal_has_pooling3d()) {
    clear_has_layer();
    ::CoreML::Specification::Pooling3DLayerParams* temp = layer_.pooling3d_;
    layer_.pooling3d_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_pooling3d(::CoreML::Specification::Pooling3DLayerParams* pooling3d) {
  clear_layer();
  if (pooling3d) {
    set_has_pooling3d();
    layer_.pooling3d_ = pooling3d;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.pooling3d)
}
inline ::CoreML::Specification::Pooling3DLayerParams* NeuralNetworkLayer::_internal_mutable_pooling3d() {
  if (!_internal_has_pooling3d()) {
    clear_layer();
    set_has_pooling3d();
    layer_.pooling3d_ = CreateMaybeMessage< ::CoreML::Specification::Pooling3DLayerParams >(GetArenaForAllocation());
  }
  return layer_.pooling3d_;
}
inline ::CoreML::Specification::Pooling3DLayerParams* NeuralNetworkLayer::mutable_pooling3d() {
  ::CoreML::Specification::Pooling3DLayerParams* _msg = _internal_mutable_pooling3d();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.pooling3d)
  return _msg;
}

// .CoreML.Specification.GlobalPooling3DLayerParams globalPooling3d = 1466;
inline bool NeuralNetworkLayer::_internal_has_globalpooling3d() const {
  return layer_case() == kGlobalPooling3D;
}
inline bool NeuralNetworkLayer::has_globalpooling3d() const {
  return _internal_has_globalpooling3d();
}
inline void NeuralNetworkLayer::set_has_globalpooling3d() {
  _oneof_case_[0] = kGlobalPooling3D;
}
inline void NeuralNetworkLayer::clear_globalpooling3d() {
  if (_internal_has_globalpooling3d()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.globalpooling3d_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::GlobalPooling3DLayerParams* NeuralNetworkLayer::release_globalpooling3d() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.globalPooling3d)
  if (_internal_has_globalpooling3d()) {
    clear_has_layer();
      ::CoreML::Specification::GlobalPooling3DLayerParams* temp = layer_.globalpooling3d_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.globalpooling3d_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::GlobalPooling3DLayerParams& NeuralNetworkLayer::_internal_globalpooling3d() const {
  return _internal_has_globalpooling3d()
      ? *layer_.globalpooling3d_
      : reinterpret_cast< ::CoreML::Specification::GlobalPooling3DLayerParams&>(::CoreML::Specification::_GlobalPooling3DLayerParams_default_instance_);
}
inline const ::CoreML::Specification::GlobalPooling3DLayerParams& NeuralNetworkLayer::globalpooling3d() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.globalPooling3d)
  return _internal_globalpooling3d();
}
inline ::CoreML::Specification::GlobalPooling3DLayerParams* NeuralNetworkLayer::unsafe_arena_release_globalpooling3d() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.globalPooling3d)
  if (_internal_has_globalpooling3d()) {
    clear_has_layer();
    ::CoreML::Specification::GlobalPooling3DLayerParams* temp = layer_.globalpooling3d_;
    layer_.globalpooling3d_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_globalpooling3d(::CoreML::Specification::GlobalPooling3DLayerParams* globalpooling3d) {
  clear_layer();
  if (globalpooling3d) {
    set_has_globalpooling3d();
    layer_.globalpooling3d_ = globalpooling3d;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.globalPooling3d)
}
inline ::CoreML::Specification::GlobalPooling3DLayerParams* NeuralNetworkLayer::_internal_mutable_globalpooling3d() {
  if (!_internal_has_globalpooling3d()) {
    clear_layer();
    set_has_globalpooling3d();
    layer_.globalpooling3d_ = CreateMaybeMessage< ::CoreML::Specification::GlobalPooling3DLayerParams >(GetArenaForAllocation());
  }
  return layer_.globalpooling3d_;
}
inline ::CoreML::Specification::GlobalPooling3DLayerParams* NeuralNetworkLayer::mutable_globalpooling3d() {
  ::CoreML::Specification::GlobalPooling3DLayerParams* _msg = _internal_mutable_globalpooling3d();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.globalPooling3d)
  return _msg;
}

// .CoreML.Specification.SliceBySizeLayerParams sliceBySize = 1470;
inline bool NeuralNetworkLayer::_internal_has_slicebysize() const {
  return layer_case() == kSliceBySize;
}
inline bool NeuralNetworkLayer::has_slicebysize() const {
  return _internal_has_slicebysize();
}
inline void NeuralNetworkLayer::set_has_slicebysize() {
  _oneof_case_[0] = kSliceBySize;
}
inline void NeuralNetworkLayer::clear_slicebysize() {
  if (_internal_has_slicebysize()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.slicebysize_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::SliceBySizeLayerParams* NeuralNetworkLayer::release_slicebysize() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sliceBySize)
  if (_internal_has_slicebysize()) {
    clear_has_layer();
      ::CoreML::Specification::SliceBySizeLayerParams* temp = layer_.slicebysize_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.slicebysize_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::SliceBySizeLayerParams& NeuralNetworkLayer::_internal_slicebysize() const {
  return _internal_has_slicebysize()
      ? *layer_.slicebysize_
      : reinterpret_cast< ::CoreML::Specification::SliceBySizeLayerParams&>(::CoreML::Specification::_SliceBySizeLayerParams_default_instance_);
}
inline const ::CoreML::Specification::SliceBySizeLayerParams& NeuralNetworkLayer::slicebysize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sliceBySize)
  return _internal_slicebysize();
}
inline ::CoreML::Specification::SliceBySizeLayerParams* NeuralNetworkLayer::unsafe_arena_release_slicebysize() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.sliceBySize)
  if (_internal_has_slicebysize()) {
    clear_has_layer();
    ::CoreML::Specification::SliceBySizeLayerParams* temp = layer_.slicebysize_;
    layer_.slicebysize_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_slicebysize(::CoreML::Specification::SliceBySizeLayerParams* slicebysize) {
  clear_layer();
  if (slicebysize) {
    set_has_slicebysize();
    layer_.slicebysize_ = slicebysize;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.sliceBySize)
}
inline ::CoreML::Specification::SliceBySizeLayerParams* NeuralNetworkLayer::_internal_mutable_slicebysize() {
  if (!_internal_has_slicebysize()) {
    clear_layer();
    set_has_slicebysize();
    layer_.slicebysize_ = CreateMaybeMessage< ::CoreML::Specification::SliceBySizeLayerParams >(GetArenaForAllocation());
  }
  return layer_.slicebysize_;
}
inline ::CoreML::Specification::SliceBySizeLayerParams* NeuralNetworkLayer::mutable_slicebysize() {
  ::CoreML::Specification::SliceBySizeLayerParams* _msg = _internal_mutable_slicebysize();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sliceBySize)
  return _msg;
}

// .CoreML.Specification.Convolution3DLayerParams convolution3d = 1471;
inline bool NeuralNetworkLayer::_internal_has_convolution3d() const {
  return layer_case() == kConvolution3D;
}
inline bool NeuralNetworkLayer::has_convolution3d() const {
  return _internal_has_convolution3d();
}
inline void NeuralNetworkLayer::set_has_convolution3d() {
  _oneof_case_[0] = kConvolution3D;
}
inline void NeuralNetworkLayer::clear_convolution3d() {
  if (_internal_has_convolution3d()) {
    if (GetArenaForAllocation() == nullptr) {
      delete layer_.convolution3d_;
    }
    clear_has_layer();
  }
}
inline ::CoreML::Specification::Convolution3DLayerParams* NeuralNetworkLayer::release_convolution3d() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.convolution3d)
  if (_internal_has_convolution3d()) {
    clear_has_layer();
      ::CoreML::Specification::Convolution3DLayerParams* temp = layer_.convolution3d_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    layer_.convolution3d_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::Convolution3DLayerParams& NeuralNetworkLayer::_internal_convolution3d() const {
  return _internal_has_convolution3d()
      ? *layer_.convolution3d_
      : reinterpret_cast< ::CoreML::Specification::Convolution3DLayerParams&>(::CoreML::Specification::_Convolution3DLayerParams_default_instance_);
}
inline const ::CoreML::Specification::Convolution3DLayerParams& NeuralNetworkLayer::convolution3d() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.convolution3d)
  return _internal_convolution3d();
}
inline ::CoreML::Specification::Convolution3DLayerParams* NeuralNetworkLayer::unsafe_arena_release_convolution3d() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkLayer.convolution3d)
  if (_internal_has_convolution3d()) {
    clear_has_layer();
    ::CoreML::Specification::Convolution3DLayerParams* temp = layer_.convolution3d_;
    layer_.convolution3d_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkLayer::unsafe_arena_set_allocated_convolution3d(::CoreML::Specification::Convolution3DLayerParams* convolution3d) {
  clear_layer();
  if (convolution3d) {
    set_has_convolution3d();
    layer_.convolution3d_ = convolution3d;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkLayer.convolution3d)
}
inline ::CoreML::Specification::Convolution3DLayerParams* NeuralNetworkLayer::_internal_mutable_convolution3d() {
  if (!_internal_has_convolution3d()) {
    clear_layer();
    set_has_convolution3d();
    layer_.convolution3d_ = CreateMaybeMessage< ::CoreML::Specification::Convolution3DLayerParams >(GetArenaForAllocation());
  }
  return layer_.convolution3d_;
}
inline ::CoreML::Specification::Convolution3DLayerParams* NeuralNetworkLayer::mutable_convolution3d() {
  ::CoreML::Specification::Convolution3DLayerParams* _msg = _internal_mutable_convolution3d();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.convolution3d)
  return _msg;
}

inline bool NeuralNetworkLayer::has_layer() const {
  return layer_case() != LAYER_NOT_SET;
}
inline void NeuralNetworkLayer::clear_has_layer() {
  _oneof_case_[0] = LAYER_NOT_SET;
}
inline NeuralNetworkLayer::LayerCase NeuralNetworkLayer::layer_case() const {
  return NeuralNetworkLayer::LayerCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// BranchLayerParams

// .CoreML.Specification.NeuralNetwork ifBranch = 1;
inline bool BranchLayerParams::_internal_has_ifbranch() const {
  return this != internal_default_instance() && ifbranch_ != nullptr;
}
inline bool BranchLayerParams::has_ifbranch() const {
  return _internal_has_ifbranch();
}
inline void BranchLayerParams::clear_ifbranch() {
  if (GetArenaForAllocation() == nullptr && ifbranch_ != nullptr) {
    delete ifbranch_;
  }
  ifbranch_ = nullptr;
}
inline const ::CoreML::Specification::NeuralNetwork& BranchLayerParams::_internal_ifbranch() const {
  const ::CoreML::Specification::NeuralNetwork* p = ifbranch_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::NeuralNetwork&>(
      ::CoreML::Specification::_NeuralNetwork_default_instance_);
}
inline const ::CoreML::Specification::NeuralNetwork& BranchLayerParams::ifbranch() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BranchLayerParams.ifBranch)
  return _internal_ifbranch();
}
inline void BranchLayerParams::unsafe_arena_set_allocated_ifbranch(
    ::CoreML::Specification::NeuralNetwork* ifbranch) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(ifbranch_);
  }
  ifbranch_ = ifbranch;
  if (ifbranch) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.BranchLayerParams.ifBranch)
}
inline ::CoreML::Specification::NeuralNetwork* BranchLayerParams::release_ifbranch() {
  
  ::CoreML::Specification::NeuralNetwork* temp = ifbranch_;
  ifbranch_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::NeuralNetwork* BranchLayerParams::unsafe_arena_release_ifbranch() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BranchLayerParams.ifBranch)
  
  ::CoreML::Specification::NeuralNetwork* temp = ifbranch_;
  ifbranch_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::NeuralNetwork* BranchLayerParams::_internal_mutable_ifbranch() {
  
  if (ifbranch_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::NeuralNetwork>(GetArenaForAllocation());
    ifbranch_ = p;
  }
  return ifbranch_;
}
inline ::CoreML::Specification::NeuralNetwork* BranchLayerParams::mutable_ifbranch() {
  ::CoreML::Specification::NeuralNetwork* _msg = _internal_mutable_ifbranch();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BranchLayerParams.ifBranch)
  return _msg;
}
inline void BranchLayerParams::set_allocated_ifbranch(::CoreML::Specification::NeuralNetwork* ifbranch) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete ifbranch_;
  }
  if (ifbranch) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::NeuralNetwork>::GetOwningArena(ifbranch);
    if (message_arena != submessage_arena) {
      ifbranch = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, ifbranch, submessage_arena);
    }
    
  } else {
    
  }
  ifbranch_ = ifbranch;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BranchLayerParams.ifBranch)
}

// .CoreML.Specification.NeuralNetwork elseBranch = 2;
inline bool BranchLayerParams::_internal_has_elsebranch() const {
  return this != internal_default_instance() && elsebranch_ != nullptr;
}
inline bool BranchLayerParams::has_elsebranch() const {
  return _internal_has_elsebranch();
}
inline void BranchLayerParams::clear_elsebranch() {
  if (GetArenaForAllocation() == nullptr && elsebranch_ != nullptr) {
    delete elsebranch_;
  }
  elsebranch_ = nullptr;
}
inline const ::CoreML::Specification::NeuralNetwork& BranchLayerParams::_internal_elsebranch() const {
  const ::CoreML::Specification::NeuralNetwork* p = elsebranch_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::NeuralNetwork&>(
      ::CoreML::Specification::_NeuralNetwork_default_instance_);
}
inline const ::CoreML::Specification::NeuralNetwork& BranchLayerParams::elsebranch() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BranchLayerParams.elseBranch)
  return _internal_elsebranch();
}
inline void BranchLayerParams::unsafe_arena_set_allocated_elsebranch(
    ::CoreML::Specification::NeuralNetwork* elsebranch) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(elsebranch_);
  }
  elsebranch_ = elsebranch;
  if (elsebranch) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.BranchLayerParams.elseBranch)
}
inline ::CoreML::Specification::NeuralNetwork* BranchLayerParams::release_elsebranch() {
  
  ::CoreML::Specification::NeuralNetwork* temp = elsebranch_;
  elsebranch_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::NeuralNetwork* BranchLayerParams::unsafe_arena_release_elsebranch() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BranchLayerParams.elseBranch)
  
  ::CoreML::Specification::NeuralNetwork* temp = elsebranch_;
  elsebranch_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::NeuralNetwork* BranchLayerParams::_internal_mutable_elsebranch() {
  
  if (elsebranch_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::NeuralNetwork>(GetArenaForAllocation());
    elsebranch_ = p;
  }
  return elsebranch_;
}
inline ::CoreML::Specification::NeuralNetwork* BranchLayerParams::mutable_elsebranch() {
  ::CoreML::Specification::NeuralNetwork* _msg = _internal_mutable_elsebranch();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BranchLayerParams.elseBranch)
  return _msg;
}
inline void BranchLayerParams::set_allocated_elsebranch(::CoreML::Specification::NeuralNetwork* elsebranch) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete elsebranch_;
  }
  if (elsebranch) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::NeuralNetwork>::GetOwningArena(elsebranch);
    if (message_arena != submessage_arena) {
      elsebranch = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, elsebranch, submessage_arena);
    }
    
  } else {
    
  }
  elsebranch_ = elsebranch;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BranchLayerParams.elseBranch)
}

// -------------------------------------------------------------------

// LoopLayerParams

// uint64 maxLoopIterations = 1;
inline void LoopLayerParams::clear_maxloopiterations() {
  maxloopiterations_ = uint64_t{0u};
}
inline uint64_t LoopLayerParams::_internal_maxloopiterations() const {
  return maxloopiterations_;
}
inline uint64_t LoopLayerParams::maxloopiterations() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoopLayerParams.maxLoopIterations)
  return _internal_maxloopiterations();
}
inline void LoopLayerParams::_internal_set_maxloopiterations(uint64_t value) {
  
  maxloopiterations_ = value;
}
inline void LoopLayerParams::set_maxloopiterations(uint64_t value) {
  _internal_set_maxloopiterations(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LoopLayerParams.maxLoopIterations)
}

// string conditionVar = 2;
inline void LoopLayerParams::clear_conditionvar() {
  conditionvar_.ClearToEmpty();
}
inline const std::string& LoopLayerParams::conditionvar() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoopLayerParams.conditionVar)
  return _internal_conditionvar();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void LoopLayerParams::set_conditionvar(ArgT0&& arg0, ArgT... args) {
 
 conditionvar_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:CoreML.Specification.LoopLayerParams.conditionVar)
}
inline std::string* LoopLayerParams::mutable_conditionvar() {
  std::string* _s = _internal_mutable_conditionvar();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoopLayerParams.conditionVar)
  return _s;
}
inline const std::string& LoopLayerParams::_internal_conditionvar() const {
  return conditionvar_.Get();
}
inline void LoopLayerParams::_internal_set_conditionvar(const std::string& value) {
  
  conditionvar_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* LoopLayerParams::_internal_mutable_conditionvar() {
  
  return conditionvar_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* LoopLayerParams::release_conditionvar() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoopLayerParams.conditionVar)
  return conditionvar_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void LoopLayerParams::set_allocated_conditionvar(std::string* conditionvar) {
  if (conditionvar != nullptr) {
    
  } else {
    
  }
  conditionvar_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), conditionvar,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (conditionvar_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    conditionvar_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoopLayerParams.conditionVar)
}

// .CoreML.Specification.NeuralNetwork conditionNetwork = 3;
inline bool LoopLayerParams::_internal_has_conditionnetwork() const {
  return this != internal_default_instance() && conditionnetwork_ != nullptr;
}
inline bool LoopLayerParams::has_conditionnetwork() const {
  return _internal_has_conditionnetwork();
}
inline void LoopLayerParams::clear_conditionnetwork() {
  if (GetArenaForAllocation() == nullptr && conditionnetwork_ != nullptr) {
    delete conditionnetwork_;
  }
  conditionnetwork_ = nullptr;
}
inline const ::CoreML::Specification::NeuralNetwork& LoopLayerParams::_internal_conditionnetwork() const {
  const ::CoreML::Specification::NeuralNetwork* p = conditionnetwork_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::NeuralNetwork&>(
      ::CoreML::Specification::_NeuralNetwork_default_instance_);
}
inline const ::CoreML::Specification::NeuralNetwork& LoopLayerParams::conditionnetwork() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoopLayerParams.conditionNetwork)
  return _internal_conditionnetwork();
}
inline void LoopLayerParams::unsafe_arena_set_allocated_conditionnetwork(
    ::CoreML::Specification::NeuralNetwork* conditionnetwork) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(conditionnetwork_);
  }
  conditionnetwork_ = conditionnetwork;
  if (conditionnetwork) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LoopLayerParams.conditionNetwork)
}
inline ::CoreML::Specification::NeuralNetwork* LoopLayerParams::release_conditionnetwork() {
  
  ::CoreML::Specification::NeuralNetwork* temp = conditionnetwork_;
  conditionnetwork_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::NeuralNetwork* LoopLayerParams::unsafe_arena_release_conditionnetwork() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoopLayerParams.conditionNetwork)
  
  ::CoreML::Specification::NeuralNetwork* temp = conditionnetwork_;
  conditionnetwork_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::NeuralNetwork* LoopLayerParams::_internal_mutable_conditionnetwork() {
  
  if (conditionnetwork_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::NeuralNetwork>(GetArenaForAllocation());
    conditionnetwork_ = p;
  }
  return conditionnetwork_;
}
inline ::CoreML::Specification::NeuralNetwork* LoopLayerParams::mutable_conditionnetwork() {
  ::CoreML::Specification::NeuralNetwork* _msg = _internal_mutable_conditionnetwork();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoopLayerParams.conditionNetwork)
  return _msg;
}
inline void LoopLayerParams::set_allocated_conditionnetwork(::CoreML::Specification::NeuralNetwork* conditionnetwork) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete conditionnetwork_;
  }
  if (conditionnetwork) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::NeuralNetwork>::GetOwningArena(conditionnetwork);
    if (message_arena != submessage_arena) {
      conditionnetwork = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, conditionnetwork, submessage_arena);
    }
    
  } else {
    
  }
  conditionnetwork_ = conditionnetwork;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoopLayerParams.conditionNetwork)
}

// .CoreML.Specification.NeuralNetwork bodyNetwork = 4;
inline bool LoopLayerParams::_internal_has_bodynetwork() const {
  return this != internal_default_instance() && bodynetwork_ != nullptr;
}
inline bool LoopLayerParams::has_bodynetwork() const {
  return _internal_has_bodynetwork();
}
inline void LoopLayerParams::clear_bodynetwork() {
  if (GetArenaForAllocation() == nullptr && bodynetwork_ != nullptr) {
    delete bodynetwork_;
  }
  bodynetwork_ = nullptr;
}
inline const ::CoreML::Specification::NeuralNetwork& LoopLayerParams::_internal_bodynetwork() const {
  const ::CoreML::Specification::NeuralNetwork* p = bodynetwork_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::NeuralNetwork&>(
      ::CoreML::Specification::_NeuralNetwork_default_instance_);
}
inline const ::CoreML::Specification::NeuralNetwork& LoopLayerParams::bodynetwork() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoopLayerParams.bodyNetwork)
  return _internal_bodynetwork();
}
inline void LoopLayerParams::unsafe_arena_set_allocated_bodynetwork(
    ::CoreML::Specification::NeuralNetwork* bodynetwork) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(bodynetwork_);
  }
  bodynetwork_ = bodynetwork;
  if (bodynetwork) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LoopLayerParams.bodyNetwork)
}
inline ::CoreML::Specification::NeuralNetwork* LoopLayerParams::release_bodynetwork() {
  
  ::CoreML::Specification::NeuralNetwork* temp = bodynetwork_;
  bodynetwork_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::NeuralNetwork* LoopLayerParams::unsafe_arena_release_bodynetwork() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoopLayerParams.bodyNetwork)
  
  ::CoreML::Specification::NeuralNetwork* temp = bodynetwork_;
  bodynetwork_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::NeuralNetwork* LoopLayerParams::_internal_mutable_bodynetwork() {
  
  if (bodynetwork_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::NeuralNetwork>(GetArenaForAllocation());
    bodynetwork_ = p;
  }
  return bodynetwork_;
}
inline ::CoreML::Specification::NeuralNetwork* LoopLayerParams::mutable_bodynetwork() {
  ::CoreML::Specification::NeuralNetwork* _msg = _internal_mutable_bodynetwork();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoopLayerParams.bodyNetwork)
  return _msg;
}
inline void LoopLayerParams::set_allocated_bodynetwork(::CoreML::Specification::NeuralNetwork* bodynetwork) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete bodynetwork_;
  }
  if (bodynetwork) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::NeuralNetwork>::GetOwningArena(bodynetwork);
    if (message_arena != submessage_arena) {
      bodynetwork = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, bodynetwork, submessage_arena);
    }
    
  } else {
    
  }
  bodynetwork_ = bodynetwork;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoopLayerParams.bodyNetwork)
}

// -------------------------------------------------------------------

// LoopBreakLayerParams

// -------------------------------------------------------------------

// LoopContinueLayerParams

// -------------------------------------------------------------------

// CopyLayerParams

// -------------------------------------------------------------------

// GreaterThanLayerParams

// float alpha = 2;
inline void GreaterThanLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float GreaterThanLayerParams::_internal_alpha() const {
  return alpha_;
}
inline float GreaterThanLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GreaterThanLayerParams.alpha)
  return _internal_alpha();
}
inline void GreaterThanLayerParams::_internal_set_alpha(float value) {
  
  alpha_ = value;
}
inline void GreaterThanLayerParams::set_alpha(float value) {
  _internal_set_alpha(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.GreaterThanLayerParams.alpha)
}

// -------------------------------------------------------------------

// GreaterEqualLayerParams

// float alpha = 2;
inline void GreaterEqualLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float GreaterEqualLayerParams::_internal_alpha() const {
  return alpha_;
}
inline float GreaterEqualLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GreaterEqualLayerParams.alpha)
  return _internal_alpha();
}
inline void GreaterEqualLayerParams::_internal_set_alpha(float value) {
  
  alpha_ = value;
}
inline void GreaterEqualLayerParams::set_alpha(float value) {
  _internal_set_alpha(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.GreaterEqualLayerParams.alpha)
}

// -------------------------------------------------------------------

// LessThanLayerParams

// float alpha = 2;
inline void LessThanLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float LessThanLayerParams::_internal_alpha() const {
  return alpha_;
}
inline float LessThanLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LessThanLayerParams.alpha)
  return _internal_alpha();
}
inline void LessThanLayerParams::_internal_set_alpha(float value) {
  
  alpha_ = value;
}
inline void LessThanLayerParams::set_alpha(float value) {
  _internal_set_alpha(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LessThanLayerParams.alpha)
}

// -------------------------------------------------------------------

// LessEqualLayerParams

// float alpha = 2;
inline void LessEqualLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float LessEqualLayerParams::_internal_alpha() const {
  return alpha_;
}
inline float LessEqualLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LessEqualLayerParams.alpha)
  return _internal_alpha();
}
inline void LessEqualLayerParams::_internal_set_alpha(float value) {
  
  alpha_ = value;
}
inline void LessEqualLayerParams::set_alpha(float value) {
  _internal_set_alpha(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LessEqualLayerParams.alpha)
}

// -------------------------------------------------------------------

// EqualLayerParams

// float alpha = 1;
inline void EqualLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float EqualLayerParams::_internal_alpha() const {
  return alpha_;
}
inline float EqualLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EqualLayerParams.alpha)
  return _internal_alpha();
}
inline void EqualLayerParams::_internal_set_alpha(float value) {
  
  alpha_ = value;
}
inline void EqualLayerParams::set_alpha(float value) {
  _internal_set_alpha(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.EqualLayerParams.alpha)
}

// -------------------------------------------------------------------

// NotEqualLayerParams

// float alpha = 1;
inline void NotEqualLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float NotEqualLayerParams::_internal_alpha() const {
  return alpha_;
}
inline float NotEqualLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NotEqualLayerParams.alpha)
  return _internal_alpha();
}
inline void NotEqualLayerParams::_internal_set_alpha(float value) {
  
  alpha_ = value;
}
inline void NotEqualLayerParams::set_alpha(float value) {
  _internal_set_alpha(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NotEqualLayerParams.alpha)
}

// -------------------------------------------------------------------

// LogicalAndLayerParams

// -------------------------------------------------------------------

// LogicalOrLayerParams

// -------------------------------------------------------------------

// LogicalXorLayerParams

// -------------------------------------------------------------------

// LogicalNotLayerParams

// -------------------------------------------------------------------

// BorderAmounts_EdgeSizes

// uint64 startEdgeSize = 1;
inline void BorderAmounts_EdgeSizes::clear_startedgesize() {
  startedgesize_ = uint64_t{0u};
}
inline uint64_t BorderAmounts_EdgeSizes::_internal_startedgesize() const {
  return startedgesize_;
}
inline uint64_t BorderAmounts_EdgeSizes::startedgesize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BorderAmounts.EdgeSizes.startEdgeSize)
  return _internal_startedgesize();
}
inline void BorderAmounts_EdgeSizes::_internal_set_startedgesize(uint64_t value) {
  
  startedgesize_ = value;
}
inline void BorderAmounts_EdgeSizes::set_startedgesize(uint64_t value) {
  _internal_set_startedgesize(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BorderAmounts.EdgeSizes.startEdgeSize)
}

// uint64 endEdgeSize = 2;
inline void BorderAmounts_EdgeSizes::clear_endedgesize() {
  endedgesize_ = uint64_t{0u};
}
inline uint64_t BorderAmounts_EdgeSizes::_internal_endedgesize() const {
  return endedgesize_;
}
inline uint64_t BorderAmounts_EdgeSizes::endedgesize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BorderAmounts.EdgeSizes.endEdgeSize)
  return _internal_endedgesize();
}
inline void BorderAmounts_EdgeSizes::_internal_set_endedgesize(uint64_t value) {
  
  endedgesize_ = value;
}
inline void BorderAmounts_EdgeSizes::set_endedgesize(uint64_t value) {
  _internal_set_endedgesize(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BorderAmounts.EdgeSizes.endEdgeSize)
}

// -------------------------------------------------------------------

// BorderAmounts

// repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
inline int BorderAmounts::_internal_borderamounts_size() const {
  return borderamounts_.size();
}
inline int BorderAmounts::borderamounts_size() const {
  return _internal_borderamounts_size();
}
inline void BorderAmounts::clear_borderamounts() {
  borderamounts_.Clear();
}
inline ::CoreML::Specification::BorderAmounts_EdgeSizes* BorderAmounts::mutable_borderamounts(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes >*
BorderAmounts::mutable_borderamounts() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BorderAmounts.borderAmounts)
  return &borderamounts_;
}
inline const ::CoreML::Specification::BorderAmounts_EdgeSizes& BorderAmounts::_internal_borderamounts(int index) const {
  return borderamounts_.Get(index);
}
inline const ::CoreML::Specification::BorderAmounts_EdgeSizes& BorderAmounts::borderamounts(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BorderAmounts.borderAmounts)
  return _internal_borderamounts(index);
}
inline ::CoreML::Specification::BorderAmounts_EdgeSizes* BorderAmounts::_internal_add_borderamounts() {
  return borderamounts_.Add();
}
inline ::CoreML::Specification::BorderAmounts_EdgeSizes* BorderAmounts::add_borderamounts() {
  ::CoreML::Specification::BorderAmounts_EdgeSizes* _add = _internal_add_borderamounts();
  // @@protoc_insertion_point(field_add:CoreML.Specification.BorderAmounts.borderAmounts)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes >&
BorderAmounts::borderamounts() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_;
}

// -------------------------------------------------------------------

// ValidPadding

// .CoreML.Specification.BorderAmounts paddingAmounts = 1;
inline bool ValidPadding::_internal_has_paddingamounts() const {
  return this != internal_default_instance() && paddingamounts_ != nullptr;
}
inline bool ValidPadding::has_paddingamounts() const {
  return _internal_has_paddingamounts();
}
inline void ValidPadding::clear_paddingamounts() {
  if (GetArenaForAllocation() == nullptr && paddingamounts_ != nullptr) {
    delete paddingamounts_;
  }
  paddingamounts_ = nullptr;
}
inline const ::CoreML::Specification::BorderAmounts& ValidPadding::_internal_paddingamounts() const {
  const ::CoreML::Specification::BorderAmounts* p = paddingamounts_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::BorderAmounts&>(
      ::CoreML::Specification::_BorderAmounts_default_instance_);
}
inline const ::CoreML::Specification::BorderAmounts& ValidPadding::paddingamounts() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ValidPadding.paddingAmounts)
  return _internal_paddingamounts();
}
inline void ValidPadding::unsafe_arena_set_allocated_paddingamounts(
    ::CoreML::Specification::BorderAmounts* paddingamounts) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(paddingamounts_);
  }
  paddingamounts_ = paddingamounts;
  if (paddingamounts) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ValidPadding.paddingAmounts)
}
inline ::CoreML::Specification::BorderAmounts* ValidPadding::release_paddingamounts() {
  
  ::CoreML::Specification::BorderAmounts* temp = paddingamounts_;
  paddingamounts_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::BorderAmounts* ValidPadding::unsafe_arena_release_paddingamounts() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ValidPadding.paddingAmounts)
  
  ::CoreML::Specification::BorderAmounts* temp = paddingamounts_;
  paddingamounts_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::BorderAmounts* ValidPadding::_internal_mutable_paddingamounts() {
  
  if (paddingamounts_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::BorderAmounts>(GetArenaForAllocation());
    paddingamounts_ = p;
  }
  return paddingamounts_;
}
inline ::CoreML::Specification::BorderAmounts* ValidPadding::mutable_paddingamounts() {
  ::CoreML::Specification::BorderAmounts* _msg = _internal_mutable_paddingamounts();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ValidPadding.paddingAmounts)
  return _msg;
}
inline void ValidPadding::set_allocated_paddingamounts(::CoreML::Specification::BorderAmounts* paddingamounts) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete paddingamounts_;
  }
  if (paddingamounts) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::BorderAmounts>::GetOwningArena(paddingamounts);
    if (message_arena != submessage_arena) {
      paddingamounts = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, paddingamounts, submessage_arena);
    }
    
  } else {
    
  }
  paddingamounts_ = paddingamounts;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ValidPadding.paddingAmounts)
}

// -------------------------------------------------------------------

// SamePadding

// .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
inline void SamePadding::clear_asymmetrymode() {
  asymmetrymode_ = 0;
}
inline ::CoreML::Specification::SamePadding_SamePaddingMode SamePadding::_internal_asymmetrymode() const {
  return static_cast< ::CoreML::Specification::SamePadding_SamePaddingMode >(asymmetrymode_);
}
inline ::CoreML::Specification::SamePadding_SamePaddingMode SamePadding::asymmetrymode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SamePadding.asymmetryMode)
  return _internal_asymmetrymode();
}
inline void SamePadding::_internal_set_asymmetrymode(::CoreML::Specification::SamePadding_SamePaddingMode value) {
  
  asymmetrymode_ = value;
}
inline void SamePadding::set_asymmetrymode(::CoreML::Specification::SamePadding_SamePaddingMode value) {
  _internal_set_asymmetrymode(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SamePadding.asymmetryMode)
}

// -------------------------------------------------------------------

// SamplingMode

// .CoreML.Specification.SamplingMode.Method samplingMethod = 1;
inline void SamplingMode::clear_samplingmethod() {
  samplingmethod_ = 0;
}
inline ::CoreML::Specification::SamplingMode_Method SamplingMode::_internal_samplingmethod() const {
  return static_cast< ::CoreML::Specification::SamplingMode_Method >(samplingmethod_);
}
inline ::CoreML::Specification::SamplingMode_Method SamplingMode::samplingmethod() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SamplingMode.samplingMethod)
  return _internal_samplingmethod();
}
inline void SamplingMode::_internal_set_samplingmethod(::CoreML::Specification::SamplingMode_Method value) {
  
  samplingmethod_ = value;
}
inline void SamplingMode::set_samplingmethod(::CoreML::Specification::SamplingMode_Method value) {
  _internal_set_samplingmethod(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SamplingMode.samplingMethod)
}

// -------------------------------------------------------------------

// BoxCoordinatesMode

// .CoreML.Specification.BoxCoordinatesMode.Coordinates boxMode = 1;
inline void BoxCoordinatesMode::clear_boxmode() {
  boxmode_ = 0;
}
inline ::CoreML::Specification::BoxCoordinatesMode_Coordinates BoxCoordinatesMode::_internal_boxmode() const {
  return static_cast< ::CoreML::Specification::BoxCoordinatesMode_Coordinates >(boxmode_);
}
inline ::CoreML::Specification::BoxCoordinatesMode_Coordinates BoxCoordinatesMode::boxmode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BoxCoordinatesMode.boxMode)
  return _internal_boxmode();
}
inline void BoxCoordinatesMode::_internal_set_boxmode(::CoreML::Specification::BoxCoordinatesMode_Coordinates value) {
  
  boxmode_ = value;
}
inline void BoxCoordinatesMode::set_boxmode(::CoreML::Specification::BoxCoordinatesMode_Coordinates value) {
  _internal_set_boxmode(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BoxCoordinatesMode.boxMode)
}

// -------------------------------------------------------------------

// WeightParams

// repeated float floatValue = 1;
inline int WeightParams::_internal_floatvalue_size() const {
  return floatvalue_.size();
}
inline int WeightParams::floatvalue_size() const {
  return _internal_floatvalue_size();
}
inline void WeightParams::clear_floatvalue() {
  floatvalue_.Clear();
}
inline float WeightParams::_internal_floatvalue(int index) const {
  return floatvalue_.Get(index);
}
inline float WeightParams::floatvalue(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.floatValue)
  return _internal_floatvalue(index);
}
inline void WeightParams::set_floatvalue(int index, float value) {
  floatvalue_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.floatValue)
}
inline void WeightParams::_internal_add_floatvalue(float value) {
  floatvalue_.Add(value);
}
inline void WeightParams::add_floatvalue(float value) {
  _internal_add_floatvalue(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.WeightParams.floatValue)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
WeightParams::_internal_floatvalue() const {
  return floatvalue_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
WeightParams::floatvalue() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.WeightParams.floatValue)
  return _internal_floatvalue();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
WeightParams::_internal_mutable_floatvalue() {
  return &floatvalue_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
WeightParams::mutable_floatvalue() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.WeightParams.floatValue)
  return _internal_mutable_floatvalue();
}

// bytes float16Value = 2;
inline void WeightParams::clear_float16value() {
  float16value_.ClearToEmpty();
}
inline const std::string& WeightParams::float16value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.float16Value)
  return _internal_float16value();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void WeightParams::set_float16value(ArgT0&& arg0, ArgT... args) {
 
 float16value_.SetBytes(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.float16Value)
}
inline std::string* WeightParams::mutable_float16value() {
  std::string* _s = _internal_mutable_float16value();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.WeightParams.float16Value)
  return _s;
}
inline const std::string& WeightParams::_internal_float16value() const {
  return float16value_.Get();
}
inline void WeightParams::_internal_set_float16value(const std::string& value) {
  
  float16value_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* WeightParams::_internal_mutable_float16value() {
  
  return float16value_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* WeightParams::release_float16value() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.WeightParams.float16Value)
  return float16value_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void WeightParams::set_allocated_float16value(std::string* float16value) {
  if (float16value != nullptr) {
    
  } else {
    
  }
  float16value_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), float16value,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (float16value_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    float16value_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.WeightParams.float16Value)
}

// bytes rawValue = 30;
inline void WeightParams::clear_rawvalue() {
  rawvalue_.ClearToEmpty();
}
inline const std::string& WeightParams::rawvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.rawValue)
  return _internal_rawvalue();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void WeightParams::set_rawvalue(ArgT0&& arg0, ArgT... args) {
 
 rawvalue_.SetBytes(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.rawValue)
}
inline std::string* WeightParams::mutable_rawvalue() {
  std::string* _s = _internal_mutable_rawvalue();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.WeightParams.rawValue)
  return _s;
}
inline const std::string& WeightParams::_internal_rawvalue() const {
  return rawvalue_.Get();
}
inline void WeightParams::_internal_set_rawvalue(const std::string& value) {
  
  rawvalue_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* WeightParams::_internal_mutable_rawvalue() {
  
  return rawvalue_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* WeightParams::release_rawvalue() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.WeightParams.rawValue)
  return rawvalue_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void WeightParams::set_allocated_rawvalue(std::string* rawvalue) {
  if (rawvalue != nullptr) {
    
  } else {
    
  }
  rawvalue_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), rawvalue,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (rawvalue_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    rawvalue_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.WeightParams.rawValue)
}

// bytes int8RawValue = 31;
inline void WeightParams::clear_int8rawvalue() {
  int8rawvalue_.ClearToEmpty();
}
inline const std::string& WeightParams::int8rawvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.int8RawValue)
  return _internal_int8rawvalue();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void WeightParams::set_int8rawvalue(ArgT0&& arg0, ArgT... args) {
 
 int8rawvalue_.SetBytes(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.int8RawValue)
}
inline std::string* WeightParams::mutable_int8rawvalue() {
  std::string* _s = _internal_mutable_int8rawvalue();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.WeightParams.int8RawValue)
  return _s;
}
inline const std::string& WeightParams::_internal_int8rawvalue() const {
  return int8rawvalue_.Get();
}
inline void WeightParams::_internal_set_int8rawvalue(const std::string& value) {
  
  int8rawvalue_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* WeightParams::_internal_mutable_int8rawvalue() {
  
  return int8rawvalue_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* WeightParams::release_int8rawvalue() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.WeightParams.int8RawValue)
  return int8rawvalue_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void WeightParams::set_allocated_int8rawvalue(std::string* int8rawvalue) {
  if (int8rawvalue != nullptr) {
    
  } else {
    
  }
  int8rawvalue_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), int8rawvalue,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (int8rawvalue_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    int8rawvalue_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.WeightParams.int8RawValue)
}

// .CoreML.Specification.QuantizationParams quantization = 40;
inline bool WeightParams::_internal_has_quantization() const {
  return this != internal_default_instance() && quantization_ != nullptr;
}
inline bool WeightParams::has_quantization() const {
  return _internal_has_quantization();
}
inline void WeightParams::clear_quantization() {
  if (GetArenaForAllocation() == nullptr && quantization_ != nullptr) {
    delete quantization_;
  }
  quantization_ = nullptr;
}
inline const ::CoreML::Specification::QuantizationParams& WeightParams::_internal_quantization() const {
  const ::CoreML::Specification::QuantizationParams* p = quantization_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::QuantizationParams&>(
      ::CoreML::Specification::_QuantizationParams_default_instance_);
}
inline const ::CoreML::Specification::QuantizationParams& WeightParams::quantization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.quantization)
  return _internal_quantization();
}
inline void WeightParams::unsafe_arena_set_allocated_quantization(
    ::CoreML::Specification::QuantizationParams* quantization) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(quantization_);
  }
  quantization_ = quantization;
  if (quantization) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.WeightParams.quantization)
}
inline ::CoreML::Specification::QuantizationParams* WeightParams::release_quantization() {
  
  ::CoreML::Specification::QuantizationParams* temp = quantization_;
  quantization_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::QuantizationParams* WeightParams::unsafe_arena_release_quantization() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.WeightParams.quantization)
  
  ::CoreML::Specification::QuantizationParams* temp = quantization_;
  quantization_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::QuantizationParams* WeightParams::_internal_mutable_quantization() {
  
  if (quantization_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::QuantizationParams>(GetArenaForAllocation());
    quantization_ = p;
  }
  return quantization_;
}
inline ::CoreML::Specification::QuantizationParams* WeightParams::mutable_quantization() {
  ::CoreML::Specification::QuantizationParams* _msg = _internal_mutable_quantization();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.WeightParams.quantization)
  return _msg;
}
inline void WeightParams::set_allocated_quantization(::CoreML::Specification::QuantizationParams* quantization) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete quantization_;
  }
  if (quantization) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::QuantizationParams>::GetOwningArena(quantization);
    if (message_arena != submessage_arena) {
      quantization = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, quantization, submessage_arena);
    }
    
  } else {
    
  }
  quantization_ = quantization;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.WeightParams.quantization)
}

// bool isUpdatable = 50;
inline void WeightParams::clear_isupdatable() {
  isupdatable_ = false;
}
inline bool WeightParams::_internal_isupdatable() const {
  return isupdatable_;
}
inline bool WeightParams::isupdatable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.isUpdatable)
  return _internal_isupdatable();
}
inline void WeightParams::_internal_set_isupdatable(bool value) {
  
  isupdatable_ = value;
}
inline void WeightParams::set_isupdatable(bool value) {
  _internal_set_isupdatable(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.isUpdatable)
}

// -------------------------------------------------------------------

// QuantizationParams

// uint64 numberOfBits = 1;
inline void QuantizationParams::clear_numberofbits() {
  numberofbits_ = uint64_t{0u};
}
inline uint64_t QuantizationParams::_internal_numberofbits() const {
  return numberofbits_;
}
inline uint64_t QuantizationParams::numberofbits() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.QuantizationParams.numberOfBits)
  return _internal_numberofbits();
}
inline void QuantizationParams::_internal_set_numberofbits(uint64_t value) {
  
  numberofbits_ = value;
}
inline void QuantizationParams::set_numberofbits(uint64_t value) {
  _internal_set_numberofbits(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.QuantizationParams.numberOfBits)
}

// .CoreML.Specification.LinearQuantizationParams linearQuantization = 101;
inline bool QuantizationParams::_internal_has_linearquantization() const {
  return QuantizationType_case() == kLinearQuantization;
}
inline bool QuantizationParams::has_linearquantization() const {
  return _internal_has_linearquantization();
}
inline void QuantizationParams::set_has_linearquantization() {
  _oneof_case_[0] = kLinearQuantization;
}
inline void QuantizationParams::clear_linearquantization() {
  if (_internal_has_linearquantization()) {
    if (GetArenaForAllocation() == nullptr) {
      delete QuantizationType_.linearquantization_;
    }
    clear_has_QuantizationType();
  }
}
inline ::CoreML::Specification::LinearQuantizationParams* QuantizationParams::release_linearquantization() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.QuantizationParams.linearQuantization)
  if (_internal_has_linearquantization()) {
    clear_has_QuantizationType();
      ::CoreML::Specification::LinearQuantizationParams* temp = QuantizationType_.linearquantization_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    QuantizationType_.linearquantization_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::LinearQuantizationParams& QuantizationParams::_internal_linearquantization() const {
  return _internal_has_linearquantization()
      ? *QuantizationType_.linearquantization_
      : reinterpret_cast< ::CoreML::Specification::LinearQuantizationParams&>(::CoreML::Specification::_LinearQuantizationParams_default_instance_);
}
inline const ::CoreML::Specification::LinearQuantizationParams& QuantizationParams::linearquantization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.QuantizationParams.linearQuantization)
  return _internal_linearquantization();
}
inline ::CoreML::Specification::LinearQuantizationParams* QuantizationParams::unsafe_arena_release_linearquantization() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.QuantizationParams.linearQuantization)
  if (_internal_has_linearquantization()) {
    clear_has_QuantizationType();
    ::CoreML::Specification::LinearQuantizationParams* temp = QuantizationType_.linearquantization_;
    QuantizationType_.linearquantization_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void QuantizationParams::unsafe_arena_set_allocated_linearquantization(::CoreML::Specification::LinearQuantizationParams* linearquantization) {
  clear_QuantizationType();
  if (linearquantization) {
    set_has_linearquantization();
    QuantizationType_.linearquantization_ = linearquantization;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.QuantizationParams.linearQuantization)
}
inline ::CoreML::Specification::LinearQuantizationParams* QuantizationParams::_internal_mutable_linearquantization() {
  if (!_internal_has_linearquantization()) {
    clear_QuantizationType();
    set_has_linearquantization();
    QuantizationType_.linearquantization_ = CreateMaybeMessage< ::CoreML::Specification::LinearQuantizationParams >(GetArenaForAllocation());
  }
  return QuantizationType_.linearquantization_;
}
inline ::CoreML::Specification::LinearQuantizationParams* QuantizationParams::mutable_linearquantization() {
  ::CoreML::Specification::LinearQuantizationParams* _msg = _internal_mutable_linearquantization();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.QuantizationParams.linearQuantization)
  return _msg;
}

// .CoreML.Specification.LookUpTableQuantizationParams lookupTableQuantization = 102;
inline bool QuantizationParams::_internal_has_lookuptablequantization() const {
  return QuantizationType_case() == kLookupTableQuantization;
}
inline bool QuantizationParams::has_lookuptablequantization() const {
  return _internal_has_lookuptablequantization();
}
inline void QuantizationParams::set_has_lookuptablequantization() {
  _oneof_case_[0] = kLookupTableQuantization;
}
inline void QuantizationParams::clear_lookuptablequantization() {
  if (_internal_has_lookuptablequantization()) {
    if (GetArenaForAllocation() == nullptr) {
      delete QuantizationType_.lookuptablequantization_;
    }
    clear_has_QuantizationType();
  }
}
inline ::CoreML::Specification::LookUpTableQuantizationParams* QuantizationParams::release_lookuptablequantization() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.QuantizationParams.lookupTableQuantization)
  if (_internal_has_lookuptablequantization()) {
    clear_has_QuantizationType();
      ::CoreML::Specification::LookUpTableQuantizationParams* temp = QuantizationType_.lookuptablequantization_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    QuantizationType_.lookuptablequantization_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::LookUpTableQuantizationParams& QuantizationParams::_internal_lookuptablequantization() const {
  return _internal_has_lookuptablequantization()
      ? *QuantizationType_.lookuptablequantization_
      : reinterpret_cast< ::CoreML::Specification::LookUpTableQuantizationParams&>(::CoreML::Specification::_LookUpTableQuantizationParams_default_instance_);
}
inline const ::CoreML::Specification::LookUpTableQuantizationParams& QuantizationParams::lookuptablequantization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.QuantizationParams.lookupTableQuantization)
  return _internal_lookuptablequantization();
}
inline ::CoreML::Specification::LookUpTableQuantizationParams* QuantizationParams::unsafe_arena_release_lookuptablequantization() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.QuantizationParams.lookupTableQuantization)
  if (_internal_has_lookuptablequantization()) {
    clear_has_QuantizationType();
    ::CoreML::Specification::LookUpTableQuantizationParams* temp = QuantizationType_.lookuptablequantization_;
    QuantizationType_.lookuptablequantization_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void QuantizationParams::unsafe_arena_set_allocated_lookuptablequantization(::CoreML::Specification::LookUpTableQuantizationParams* lookuptablequantization) {
  clear_QuantizationType();
  if (lookuptablequantization) {
    set_has_lookuptablequantization();
    QuantizationType_.lookuptablequantization_ = lookuptablequantization;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.QuantizationParams.lookupTableQuantization)
}
inline ::CoreML::Specification::LookUpTableQuantizationParams* QuantizationParams::_internal_mutable_lookuptablequantization() {
  if (!_internal_has_lookuptablequantization()) {
    clear_QuantizationType();
    set_has_lookuptablequantization();
    QuantizationType_.lookuptablequantization_ = CreateMaybeMessage< ::CoreML::Specification::LookUpTableQuantizationParams >(GetArenaForAllocation());
  }
  return QuantizationType_.lookuptablequantization_;
}
inline ::CoreML::Specification::LookUpTableQuantizationParams* QuantizationParams::mutable_lookuptablequantization() {
  ::CoreML::Specification::LookUpTableQuantizationParams* _msg = _internal_mutable_lookuptablequantization();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.QuantizationParams.lookupTableQuantization)
  return _msg;
}

inline bool QuantizationParams::has_QuantizationType() const {
  return QuantizationType_case() != QUANTIZATIONTYPE_NOT_SET;
}
inline void QuantizationParams::clear_has_QuantizationType() {
  _oneof_case_[0] = QUANTIZATIONTYPE_NOT_SET;
}
inline QuantizationParams::QuantizationTypeCase QuantizationParams::QuantizationType_case() const {
  return QuantizationParams::QuantizationTypeCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// LinearQuantizationParams

// repeated float scale = 1;
inline int LinearQuantizationParams::_internal_scale_size() const {
  return scale_.size();
}
inline int LinearQuantizationParams::scale_size() const {
  return _internal_scale_size();
}
inline void LinearQuantizationParams::clear_scale() {
  scale_.Clear();
}
inline float LinearQuantizationParams::_internal_scale(int index) const {
  return scale_.Get(index);
}
inline float LinearQuantizationParams::scale(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LinearQuantizationParams.scale)
  return _internal_scale(index);
}
inline void LinearQuantizationParams::set_scale(int index, float value) {
  scale_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LinearQuantizationParams.scale)
}
inline void LinearQuantizationParams::_internal_add_scale(float value) {
  scale_.Add(value);
}
inline void LinearQuantizationParams::add_scale(float value) {
  _internal_add_scale(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LinearQuantizationParams.scale)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
LinearQuantizationParams::_internal_scale() const {
  return scale_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
LinearQuantizationParams::scale() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LinearQuantizationParams.scale)
  return _internal_scale();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
LinearQuantizationParams::_internal_mutable_scale() {
  return &scale_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
LinearQuantizationParams::mutable_scale() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LinearQuantizationParams.scale)
  return _internal_mutable_scale();
}

// repeated float bias = 2;
inline int LinearQuantizationParams::_internal_bias_size() const {
  return bias_.size();
}
inline int LinearQuantizationParams::bias_size() const {
  return _internal_bias_size();
}
inline void LinearQuantizationParams::clear_bias() {
  bias_.Clear();
}
inline float LinearQuantizationParams::_internal_bias(int index) const {
  return bias_.Get(index);
}
inline float LinearQuantizationParams::bias(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LinearQuantizationParams.bias)
  return _internal_bias(index);
}
inline void LinearQuantizationParams::set_bias(int index, float value) {
  bias_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LinearQuantizationParams.bias)
}
inline void LinearQuantizationParams::_internal_add_bias(float value) {
  bias_.Add(value);
}
inline void LinearQuantizationParams::add_bias(float value) {
  _internal_add_bias(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LinearQuantizationParams.bias)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
LinearQuantizationParams::_internal_bias() const {
  return bias_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
LinearQuantizationParams::bias() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LinearQuantizationParams.bias)
  return _internal_bias();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
LinearQuantizationParams::_internal_mutable_bias() {
  return &bias_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
LinearQuantizationParams::mutable_bias() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LinearQuantizationParams.bias)
  return _internal_mutable_bias();
}

// -------------------------------------------------------------------

// LookUpTableQuantizationParams

// repeated float floatValue = 1;
inline int LookUpTableQuantizationParams::_internal_floatvalue_size() const {
  return floatvalue_.size();
}
inline int LookUpTableQuantizationParams::floatvalue_size() const {
  return _internal_floatvalue_size();
}
inline void LookUpTableQuantizationParams::clear_floatvalue() {
  floatvalue_.Clear();
}
inline float LookUpTableQuantizationParams::_internal_floatvalue(int index) const {
  return floatvalue_.Get(index);
}
inline float LookUpTableQuantizationParams::floatvalue(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
  return _internal_floatvalue(index);
}
inline void LookUpTableQuantizationParams::set_floatvalue(int index, float value) {
  floatvalue_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
}
inline void LookUpTableQuantizationParams::_internal_add_floatvalue(float value) {
  floatvalue_.Add(value);
}
inline void LookUpTableQuantizationParams::add_floatvalue(float value) {
  _internal_add_floatvalue(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
LookUpTableQuantizationParams::_internal_floatvalue() const {
  return floatvalue_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
LookUpTableQuantizationParams::floatvalue() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
  return _internal_floatvalue();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
LookUpTableQuantizationParams::_internal_mutable_floatvalue() {
  return &floatvalue_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
LookUpTableQuantizationParams::mutable_floatvalue() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
  return _internal_mutable_floatvalue();
}

// -------------------------------------------------------------------

// ConvolutionLayerParams

// uint64 outputChannels = 1;
inline void ConvolutionLayerParams::clear_outputchannels() {
  outputchannels_ = uint64_t{0u};
}
inline uint64_t ConvolutionLayerParams::_internal_outputchannels() const {
  return outputchannels_;
}
inline uint64_t ConvolutionLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.outputChannels)
  return _internal_outputchannels();
}
inline void ConvolutionLayerParams::_internal_set_outputchannels(uint64_t value) {
  
  outputchannels_ = value;
}
inline void ConvolutionLayerParams::set_outputchannels(uint64_t value) {
  _internal_set_outputchannels(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.outputChannels)
}

// uint64 kernelChannels = 2;
inline void ConvolutionLayerParams::clear_kernelchannels() {
  kernelchannels_ = uint64_t{0u};
}
inline uint64_t ConvolutionLayerParams::_internal_kernelchannels() const {
  return kernelchannels_;
}
inline uint64_t ConvolutionLayerParams::kernelchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.kernelChannels)
  return _internal_kernelchannels();
}
inline void ConvolutionLayerParams::_internal_set_kernelchannels(uint64_t value) {
  
  kernelchannels_ = value;
}
inline void ConvolutionLayerParams::set_kernelchannels(uint64_t value) {
  _internal_set_kernelchannels(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.kernelChannels)
}

// uint64 nGroups = 10;
inline void ConvolutionLayerParams::clear_ngroups() {
  ngroups_ = uint64_t{0u};
}
inline uint64_t ConvolutionLayerParams::_internal_ngroups() const {
  return ngroups_;
}
inline uint64_t ConvolutionLayerParams::ngroups() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.nGroups)
  return _internal_ngroups();
}
inline void ConvolutionLayerParams::_internal_set_ngroups(uint64_t value) {
  
  ngroups_ = value;
}
inline void ConvolutionLayerParams::set_ngroups(uint64_t value) {
  _internal_set_ngroups(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.nGroups)
}

// repeated uint64 kernelSize = 20;
inline int ConvolutionLayerParams::_internal_kernelsize_size() const {
  return kernelsize_.size();
}
inline int ConvolutionLayerParams::kernelsize_size() const {
  return _internal_kernelsize_size();
}
inline void ConvolutionLayerParams::clear_kernelsize() {
  kernelsize_.Clear();
}
inline uint64_t ConvolutionLayerParams::_internal_kernelsize(int index) const {
  return kernelsize_.Get(index);
}
inline uint64_t ConvolutionLayerParams::kernelsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.kernelSize)
  return _internal_kernelsize(index);
}
inline void ConvolutionLayerParams::set_kernelsize(int index, uint64_t value) {
  kernelsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.kernelSize)
}
inline void ConvolutionLayerParams::_internal_add_kernelsize(uint64_t value) {
  kernelsize_.Add(value);
}
inline void ConvolutionLayerParams::add_kernelsize(uint64_t value) {
  _internal_add_kernelsize(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.kernelSize)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
ConvolutionLayerParams::_internal_kernelsize() const {
  return kernelsize_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
ConvolutionLayerParams::kernelsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.kernelSize)
  return _internal_kernelsize();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
ConvolutionLayerParams::_internal_mutable_kernelsize() {
  return &kernelsize_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
ConvolutionLayerParams::mutable_kernelsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.kernelSize)
  return _internal_mutable_kernelsize();
}

// repeated uint64 stride = 30;
inline int ConvolutionLayerParams::_internal_stride_size() const {
  return stride_.size();
}
inline int ConvolutionLayerParams::stride_size() const {
  return _internal_stride_size();
}
inline void ConvolutionLayerParams::clear_stride() {
  stride_.Clear();
}
inline uint64_t ConvolutionLayerParams::_internal_stride(int index) const {
  return stride_.Get(index);
}
inline uint64_t ConvolutionLayerParams::stride(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.stride)
  return _internal_stride(index);
}
inline void ConvolutionLayerParams::set_stride(int index, uint64_t value) {
  stride_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.stride)
}
inline void ConvolutionLayerParams::_internal_add_stride(uint64_t value) {
  stride_.Add(value);
}
inline void ConvolutionLayerParams::add_stride(uint64_t value) {
  _internal_add_stride(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.stride)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
ConvolutionLayerParams::_internal_stride() const {
  return stride_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
ConvolutionLayerParams::stride() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.stride)
  return _internal_stride();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
ConvolutionLayerParams::_internal_mutable_stride() {
  return &stride_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
ConvolutionLayerParams::mutable_stride() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.stride)
  return _internal_mutable_stride();
}

// repeated uint64 dilationFactor = 40;
inline int ConvolutionLayerParams::_internal_dilationfactor_size() const {
  return dilationfactor_.size();
}
inline int ConvolutionLayerParams::dilationfactor_size() const {
  return _internal_dilationfactor_size();
}
inline void ConvolutionLayerParams::clear_dilationfactor() {
  dilationfactor_.Clear();
}
inline uint64_t ConvolutionLayerParams::_internal_dilationfactor(int index) const {
  return dilationfactor_.Get(index);
}
inline uint64_t ConvolutionLayerParams::dilationfactor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
  return _internal_dilationfactor(index);
}
inline void ConvolutionLayerParams::set_dilationfactor(int index, uint64_t value) {
  dilationfactor_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
}
inline void ConvolutionLayerParams::_internal_add_dilationfactor(uint64_t value) {
  dilationfactor_.Add(value);
}
inline void ConvolutionLayerParams::add_dilationfactor(uint64_t value) {
  _internal_add_dilationfactor(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
ConvolutionLayerParams::_internal_dilationfactor() const {
  return dilationfactor_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
ConvolutionLayerParams::dilationfactor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
  return _internal_dilationfactor();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
ConvolutionLayerParams::_internal_mutable_dilationfactor() {
  return &dilationfactor_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
ConvolutionLayerParams::mutable_dilationfactor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
  return _internal_mutable_dilationfactor();
}

// .CoreML.Specification.ValidPadding valid = 50;
inline bool ConvolutionLayerParams::_internal_has_valid() const {
  return ConvolutionPaddingType_case() == kValid;
}
inline bool ConvolutionLayerParams::has_valid() const {
  return _internal_has_valid();
}
inline void ConvolutionLayerParams::set_has_valid() {
  _oneof_case_[0] = kValid;
}
inline void ConvolutionLayerParams::clear_valid() {
  if (_internal_has_valid()) {
    if (GetArenaForAllocation() == nullptr) {
      delete ConvolutionPaddingType_.valid_;
    }
    clear_has_ConvolutionPaddingType();
  }
}
inline ::CoreML::Specification::ValidPadding* ConvolutionLayerParams::release_valid() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.valid)
  if (_internal_has_valid()) {
    clear_has_ConvolutionPaddingType();
      ::CoreML::Specification::ValidPadding* temp = ConvolutionPaddingType_.valid_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    ConvolutionPaddingType_.valid_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ValidPadding& ConvolutionLayerParams::_internal_valid() const {
  return _internal_has_valid()
      ? *ConvolutionPaddingType_.valid_
      : reinterpret_cast< ::CoreML::Specification::ValidPadding&>(::CoreML::Specification::_ValidPadding_default_instance_);
}
inline const ::CoreML::Specification::ValidPadding& ConvolutionLayerParams::valid() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.valid)
  return _internal_valid();
}
inline ::CoreML::Specification::ValidPadding* ConvolutionLayerParams::unsafe_arena_release_valid() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.ConvolutionLayerParams.valid)
  if (_internal_has_valid()) {
    clear_has_ConvolutionPaddingType();
    ::CoreML::Specification::ValidPadding* temp = ConvolutionPaddingType_.valid_;
    ConvolutionPaddingType_.valid_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ConvolutionLayerParams::unsafe_arena_set_allocated_valid(::CoreML::Specification::ValidPadding* valid) {
  clear_ConvolutionPaddingType();
  if (valid) {
    set_has_valid();
    ConvolutionPaddingType_.valid_ = valid;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ConvolutionLayerParams.valid)
}
inline ::CoreML::Specification::ValidPadding* ConvolutionLayerParams::_internal_mutable_valid() {
  if (!_internal_has_valid()) {
    clear_ConvolutionPaddingType();
    set_has_valid();
    ConvolutionPaddingType_.valid_ = CreateMaybeMessage< ::CoreML::Specification::ValidPadding >(GetArenaForAllocation());
  }
  return ConvolutionPaddingType_.valid_;
}
inline ::CoreML::Specification::ValidPadding* ConvolutionLayerParams::mutable_valid() {
  ::CoreML::Specification::ValidPadding* _msg = _internal_mutable_valid();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.valid)
  return _msg;
}

// .CoreML.Specification.SamePadding same = 51;
inline bool ConvolutionLayerParams::_internal_has_same() const {
  return ConvolutionPaddingType_case() == kSame;
}
inline bool ConvolutionLayerParams::has_same() const {
  return _internal_has_same();
}
inline void ConvolutionLayerParams::set_has_same() {
  _oneof_case_[0] = kSame;
}
inline void ConvolutionLayerParams::clear_same() {
  if (_internal_has_same()) {
    if (GetArenaForAllocation() == nullptr) {
      delete ConvolutionPaddingType_.same_;
    }
    clear_has_ConvolutionPaddingType();
  }
}
inline ::CoreML::Specification::SamePadding* ConvolutionLayerParams::release_same() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.same)
  if (_internal_has_same()) {
    clear_has_ConvolutionPaddingType();
      ::CoreML::Specification::SamePadding* temp = ConvolutionPaddingType_.same_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    ConvolutionPaddingType_.same_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::SamePadding& ConvolutionLayerParams::_internal_same() const {
  return _internal_has_same()
      ? *ConvolutionPaddingType_.same_
      : reinterpret_cast< ::CoreML::Specification::SamePadding&>(::CoreML::Specification::_SamePadding_default_instance_);
}
inline const ::CoreML::Specification::SamePadding& ConvolutionLayerParams::same() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.same)
  return _internal_same();
}
inline ::CoreML::Specification::SamePadding* ConvolutionLayerParams::unsafe_arena_release_same() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.ConvolutionLayerParams.same)
  if (_internal_has_same()) {
    clear_has_ConvolutionPaddingType();
    ::CoreML::Specification::SamePadding* temp = ConvolutionPaddingType_.same_;
    ConvolutionPaddingType_.same_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ConvolutionLayerParams::unsafe_arena_set_allocated_same(::CoreML::Specification::SamePadding* same) {
  clear_ConvolutionPaddingType();
  if (same) {
    set_has_same();
    ConvolutionPaddingType_.same_ = same;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ConvolutionLayerParams.same)
}
inline ::CoreML::Specification::SamePadding* ConvolutionLayerParams::_internal_mutable_same() {
  if (!_internal_has_same()) {
    clear_ConvolutionPaddingType();
    set_has_same();
    ConvolutionPaddingType_.same_ = CreateMaybeMessage< ::CoreML::Specification::SamePadding >(GetArenaForAllocation());
  }
  return ConvolutionPaddingType_.same_;
}
inline ::CoreML::Specification::SamePadding* ConvolutionLayerParams::mutable_same() {
  ::CoreML::Specification::SamePadding* _msg = _internal_mutable_same();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.same)
  return _msg;
}

// bool isDeconvolution = 60;
inline void ConvolutionLayerParams::clear_isdeconvolution() {
  isdeconvolution_ = false;
}
inline bool ConvolutionLayerParams::_internal_isdeconvolution() const {
  return isdeconvolution_;
}
inline bool ConvolutionLayerParams::isdeconvolution() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.isDeconvolution)
  return _internal_isdeconvolution();
}
inline void ConvolutionLayerParams::_internal_set_isdeconvolution(bool value) {
  
  isdeconvolution_ = value;
}
inline void ConvolutionLayerParams::set_isdeconvolution(bool value) {
  _internal_set_isdeconvolution(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.isDeconvolution)
}

// bool hasBias = 70;
inline void ConvolutionLayerParams::clear_hasbias() {
  hasbias_ = false;
}
inline bool ConvolutionLayerParams::_internal_hasbias() const {
  return hasbias_;
}
inline bool ConvolutionLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.hasBias)
  return _internal_hasbias();
}
inline void ConvolutionLayerParams::_internal_set_hasbias(bool value) {
  
  hasbias_ = value;
}
inline void ConvolutionLayerParams::set_hasbias(bool value) {
  _internal_set_hasbias(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 90;
inline bool ConvolutionLayerParams::_internal_has_weights() const {
  return this != internal_default_instance() && weights_ != nullptr;
}
inline bool ConvolutionLayerParams::has_weights() const {
  return _internal_has_weights();
}
inline void ConvolutionLayerParams::clear_weights() {
  if (GetArenaForAllocation() == nullptr && weights_ != nullptr) {
    delete weights_;
  }
  weights_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& ConvolutionLayerParams::_internal_weights() const {
  const ::CoreML::Specification::WeightParams* p = weights_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& ConvolutionLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.weights)
  return _internal_weights();
}
inline void ConvolutionLayerParams::unsafe_arena_set_allocated_weights(
    ::CoreML::Specification::WeightParams* weights) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(weights_);
  }
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ConvolutionLayerParams.weights)
}
inline ::CoreML::Specification::WeightParams* ConvolutionLayerParams::release_weights() {
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* ConvolutionLayerParams::unsafe_arena_release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* ConvolutionLayerParams::_internal_mutable_weights() {
  
  if (weights_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    weights_ = p;
  }
  return weights_;
}
inline ::CoreML::Specification::WeightParams* ConvolutionLayerParams::mutable_weights() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_weights();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.weights)
  return _msg;
}
inline void ConvolutionLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete weights_;
  }
  if (weights) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(weights);
    if (message_arena != submessage_arena) {
      weights = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, weights, submessage_arena);
    }
    
  } else {
    
  }
  weights_ = weights;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 91;
inline bool ConvolutionLayerParams::_internal_has_bias() const {
  return this != internal_default_instance() && bias_ != nullptr;
}
inline bool ConvolutionLayerParams::has_bias() const {
  return _internal_has_bias();
}
inline void ConvolutionLayerParams::clear_bias() {
  if (GetArenaForAllocation() == nullptr && bias_ != nullptr) {
    delete bias_;
  }
  bias_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& ConvolutionLayerParams::_internal_bias() const {
  const ::CoreML::Specification::WeightParams* p = bias_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& ConvolutionLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.bias)
  return _internal_bias();
}
inline void ConvolutionLayerParams::unsafe_arena_set_allocated_bias(
    ::CoreML::Specification::WeightParams* bias) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(bias_);
  }
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ConvolutionLayerParams.bias)
}
inline ::CoreML::Specification::WeightParams* ConvolutionLayerParams::release_bias() {
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* ConvolutionLayerParams::unsafe_arena_release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* ConvolutionLayerParams::_internal_mutable_bias() {
  
  if (bias_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    bias_ = p;
  }
  return bias_;
}
inline ::CoreML::Specification::WeightParams* ConvolutionLayerParams::mutable_bias() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_bias();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.bias)
  return _msg;
}
inline void ConvolutionLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete bias_;
  }
  if (bias) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(bias);
    if (message_arena != submessage_arena) {
      bias = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, bias, submessage_arena);
    }
    
  } else {
    
  }
  bias_ = bias;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.bias)
}

// repeated uint64 outputShape = 100;
inline int ConvolutionLayerParams::_internal_outputshape_size() const {
  return outputshape_.size();
}
inline int ConvolutionLayerParams::outputshape_size() const {
  return _internal_outputshape_size();
}
inline void ConvolutionLayerParams::clear_outputshape() {
  outputshape_.Clear();
}
inline uint64_t ConvolutionLayerParams::_internal_outputshape(int index) const {
  return outputshape_.Get(index);
}
inline uint64_t ConvolutionLayerParams::outputshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.outputShape)
  return _internal_outputshape(index);
}
inline void ConvolutionLayerParams::set_outputshape(int index, uint64_t value) {
  outputshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.outputShape)
}
inline void ConvolutionLayerParams::_internal_add_outputshape(uint64_t value) {
  outputshape_.Add(value);
}
inline void ConvolutionLayerParams::add_outputshape(uint64_t value) {
  _internal_add_outputshape(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.outputShape)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
ConvolutionLayerParams::_internal_outputshape() const {
  return outputshape_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
ConvolutionLayerParams::outputshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.outputShape)
  return _internal_outputshape();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
ConvolutionLayerParams::_internal_mutable_outputshape() {
  return &outputshape_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
ConvolutionLayerParams::mutable_outputshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.outputShape)
  return _internal_mutable_outputshape();
}

inline bool ConvolutionLayerParams::has_ConvolutionPaddingType() const {
  return ConvolutionPaddingType_case() != CONVOLUTIONPADDINGTYPE_NOT_SET;
}
inline void ConvolutionLayerParams::clear_has_ConvolutionPaddingType() {
  _oneof_case_[0] = CONVOLUTIONPADDINGTYPE_NOT_SET;
}
inline ConvolutionLayerParams::ConvolutionPaddingTypeCase ConvolutionLayerParams::ConvolutionPaddingType_case() const {
  return ConvolutionLayerParams::ConvolutionPaddingTypeCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// Convolution3DLayerParams

// int32 outputChannels = 1;
inline void Convolution3DLayerParams::clear_outputchannels() {
  outputchannels_ = 0;
}
inline int32_t Convolution3DLayerParams::_internal_outputchannels() const {
  return outputchannels_;
}
inline int32_t Convolution3DLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.outputChannels)
  return _internal_outputchannels();
}
inline void Convolution3DLayerParams::_internal_set_outputchannels(int32_t value) {
  
  outputchannels_ = value;
}
inline void Convolution3DLayerParams::set_outputchannels(int32_t value) {
  _internal_set_outputchannels(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.outputChannels)
}

// int32 inputChannels = 2;
inline void Convolution3DLayerParams::clear_inputchannels() {
  inputchannels_ = 0;
}
inline int32_t Convolution3DLayerParams::_internal_inputchannels() const {
  return inputchannels_;
}
inline int32_t Convolution3DLayerParams::inputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.inputChannels)
  return _internal_inputchannels();
}
inline void Convolution3DLayerParams::_internal_set_inputchannels(int32_t value) {
  
  inputchannels_ = value;
}
inline void Convolution3DLayerParams::set_inputchannels(int32_t value) {
  _internal_set_inputchannels(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.inputChannels)
}

// int32 nGroups = 10;
inline void Convolution3DLayerParams::clear_ngroups() {
  ngroups_ = 0;
}
inline int32_t Convolution3DLayerParams::_internal_ngroups() const {
  return ngroups_;
}
inline int32_t Convolution3DLayerParams::ngroups() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.nGroups)
  return _internal_ngroups();
}
inline void Convolution3DLayerParams::_internal_set_ngroups(int32_t value) {
  
  ngroups_ = value;
}
inline void Convolution3DLayerParams::set_ngroups(int32_t value) {
  _internal_set_ngroups(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.nGroups)
}

// int32 kernelDepth = 20;
inline void Convolution3DLayerParams::clear_kerneldepth() {
  kerneldepth_ = 0;
}
inline int32_t Convolution3DLayerParams::_internal_kerneldepth() const {
  return kerneldepth_;
}
inline int32_t Convolution3DLayerParams::kerneldepth() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.kernelDepth)
  return _internal_kerneldepth();
}
inline void Convolution3DLayerParams::_internal_set_kerneldepth(int32_t value) {
  
  kerneldepth_ = value;
}
inline void Convolution3DLayerParams::set_kerneldepth(int32_t value) {
  _internal_set_kerneldepth(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.kernelDepth)
}

// int32 kernelHeight = 21;
inline void Convolution3DLayerParams::clear_kernelheight() {
  kernelheight_ = 0;
}
inline int32_t Convolution3DLayerParams::_internal_kernelheight() const {
  return kernelheight_;
}
inline int32_t Convolution3DLayerParams::kernelheight() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.kernelHeight)
  return _internal_kernelheight();
}
inline void Convolution3DLayerParams::_internal_set_kernelheight(int32_t value) {
  
  kernelheight_ = value;
}
inline void Convolution3DLayerParams::set_kernelheight(int32_t value) {
  _internal_set_kernelheight(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.kernelHeight)
}

// int32 kernelWidth = 22;
inline void Convolution3DLayerParams::clear_kernelwidth() {
  kernelwidth_ = 0;
}
inline int32_t Convolution3DLayerParams::_internal_kernelwidth() const {
  return kernelwidth_;
}
inline int32_t Convolution3DLayerParams::kernelwidth() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.kernelWidth)
  return _internal_kernelwidth();
}
inline void Convolution3DLayerParams::_internal_set_kernelwidth(int32_t value) {
  
  kernelwidth_ = value;
}
inline void Convolution3DLayerParams::set_kernelwidth(int32_t value) {
  _internal_set_kernelwidth(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.kernelWidth)
}

// int32 strideDepth = 31;
inline void Convolution3DLayerParams::clear_stridedepth() {
  stridedepth_ = 0;
}
inline int32_t Convolution3DLayerParams::_internal_stridedepth() const {
  return stridedepth_;
}
inline int32_t Convolution3DLayerParams::stridedepth() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.strideDepth)
  return _internal_stridedepth();
}
inline void Convolution3DLayerParams::_internal_set_stridedepth(int32_t value) {
  
  stridedepth_ = value;
}
inline void Convolution3DLayerParams::set_stridedepth(int32_t value) {
  _internal_set_stridedepth(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.strideDepth)
}

// int32 strideHeight = 32;
inline void Convolution3DLayerParams::clear_strideheight() {
  strideheight_ = 0;
}
inline int32_t Convolution3DLayerParams::_internal_strideheight() const {
  return strideheight_;
}
inline int32_t Convolution3DLayerParams::strideheight() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.strideHeight)
  return _internal_strideheight();
}
inline void Convolution3DLayerParams::_internal_set_strideheight(int32_t value) {
  
  strideheight_ = value;
}
inline void Convolution3DLayerParams::set_strideheight(int32_t value) {
  _internal_set_strideheight(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.strideHeight)
}

// int32 strideWidth = 33;
inline void Convolution3DLayerParams::clear_stridewidth() {
  stridewidth_ = 0;
}
inline int32_t Convolution3DLayerParams::_internal_stridewidth() const {
  return stridewidth_;
}
inline int32_t Convolution3DLayerParams::stridewidth() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.strideWidth)
  return _internal_stridewidth();
}
inline void Convolution3DLayerParams::_internal_set_stridewidth(int32_t value) {
  
  stridewidth_ = value;
}
inline void Convolution3DLayerParams::set_stridewidth(int32_t value) {
  _internal_set_stridewidth(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.strideWidth)
}

// int32 dilationDepth = 40;
inline void Convolution3DLayerParams::clear_dilationdepth() {
  dilationdepth_ = 0;
}
inline int32_t Convolution3DLayerParams::_internal_dilationdepth() const {
  return dilationdepth_;
}
inline int32_t Convolution3DLayerParams::dilationdepth() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.dilationDepth)
  return _internal_dilationdepth();
}
inline void Convolution3DLayerParams::_internal_set_dilationdepth(int32_t value) {
  
  dilationdepth_ = value;
}
inline void Convolution3DLayerParams::set_dilationdepth(int32_t value) {
  _internal_set_dilationdepth(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.dilationDepth)
}

// int32 dilationHeight = 41;
inline void Convolution3DLayerParams::clear_dilationheight() {
  dilationheight_ = 0;
}
inline int32_t Convolution3DLayerParams::_internal_dilationheight() const {
  return dilationheight_;
}
inline int32_t Convolution3DLayerParams::dilationheight() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.dilationHeight)
  return _internal_dilationheight();
}
inline void Convolution3DLayerParams::_internal_set_dilationheight(int32_t value) {
  
  dilationheight_ = value;
}
inline void Convolution3DLayerParams::set_dilationheight(int32_t value) {
  _internal_set_dilationheight(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.dilationHeight)
}

// int32 dilationWidth = 42;
inline void Convolution3DLayerParams::clear_dilationwidth() {
  dilationwidth_ = 0;
}
inline int32_t Convolution3DLayerParams::_internal_dilationwidth() const {
  return dilationwidth_;
}
inline int32_t Convolution3DLayerParams::dilationwidth() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.dilationWidth)
  return _internal_dilationwidth();
}
inline void Convolution3DLayerParams::_internal_set_dilationwidth(int32_t value) {
  
  dilationwidth_ = value;
}
inline void Convolution3DLayerParams::set_dilationwidth(int32_t value) {
  _internal_set_dilationwidth(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.dilationWidth)
}

// bool hasBias = 50;
inline void Convolution3DLayerParams::clear_hasbias() {
  hasbias_ = false;
}
inline bool Convolution3DLayerParams::_internal_hasbias() const {
  return hasbias_;
}
inline bool Convolution3DLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.hasBias)
  return _internal_hasbias();
}
inline void Convolution3DLayerParams::_internal_set_hasbias(bool value) {
  
  hasbias_ = value;
}
inline void Convolution3DLayerParams::set_hasbias(bool value) {
  _internal_set_hasbias(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 60;
inline bool Convolution3DLayerParams::_internal_has_weights() const {
  return this != internal_default_instance() && weights_ != nullptr;
}
inline bool Convolution3DLayerParams::has_weights() const {
  return _internal_has_weights();
}
inline void Convolution3DLayerParams::clear_weights() {
  if (GetArenaForAllocation() == nullptr && weights_ != nullptr) {
    delete weights_;
  }
  weights_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& Convolution3DLayerParams::_internal_weights() const {
  const ::CoreML::Specification::WeightParams* p = weights_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& Convolution3DLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.weights)
  return _internal_weights();
}
inline void Convolution3DLayerParams::unsafe_arena_set_allocated_weights(
    ::CoreML::Specification::WeightParams* weights) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(weights_);
  }
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.Convolution3DLayerParams.weights)
}
inline ::CoreML::Specification::WeightParams* Convolution3DLayerParams::release_weights() {
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* Convolution3DLayerParams::unsafe_arena_release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.Convolution3DLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* Convolution3DLayerParams::_internal_mutable_weights() {
  
  if (weights_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    weights_ = p;
  }
  return weights_;
}
inline ::CoreML::Specification::WeightParams* Convolution3DLayerParams::mutable_weights() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_weights();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.Convolution3DLayerParams.weights)
  return _msg;
}
inline void Convolution3DLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete weights_;
  }
  if (weights) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(weights);
    if (message_arena != submessage_arena) {
      weights = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, weights, submessage_arena);
    }
    
  } else {
    
  }
  weights_ = weights;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.Convolution3DLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 61;
inline bool Convolution3DLayerParams::_internal_has_bias() const {
  return this != internal_default_instance() && bias_ != nullptr;
}
inline bool Convolution3DLayerParams::has_bias() const {
  return _internal_has_bias();
}
inline void Convolution3DLayerParams::clear_bias() {
  if (GetArenaForAllocation() == nullptr && bias_ != nullptr) {
    delete bias_;
  }
  bias_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& Convolution3DLayerParams::_internal_bias() const {
  const ::CoreML::Specification::WeightParams* p = bias_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& Convolution3DLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.bias)
  return _internal_bias();
}
inline void Convolution3DLayerParams::unsafe_arena_set_allocated_bias(
    ::CoreML::Specification::WeightParams* bias) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(bias_);
  }
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.Convolution3DLayerParams.bias)
}
inline ::CoreML::Specification::WeightParams* Convolution3DLayerParams::release_bias() {
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* Convolution3DLayerParams::unsafe_arena_release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.Convolution3DLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* Convolution3DLayerParams::_internal_mutable_bias() {
  
  if (bias_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    bias_ = p;
  }
  return bias_;
}
inline ::CoreML::Specification::WeightParams* Convolution3DLayerParams::mutable_bias() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_bias();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.Convolution3DLayerParams.bias)
  return _msg;
}
inline void Convolution3DLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete bias_;
  }
  if (bias) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(bias);
    if (message_arena != submessage_arena) {
      bias = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, bias, submessage_arena);
    }
    
  } else {
    
  }
  bias_ = bias;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.Convolution3DLayerParams.bias)
}

// .CoreML.Specification.Convolution3DLayerParams.PaddingType paddingType = 70;
inline void Convolution3DLayerParams::clear_paddingtype() {
  paddingtype_ = 0;
}
inline ::CoreML::Specification::Convolution3DLayerParams_PaddingType Convolution3DLayerParams::_internal_paddingtype() const {
  return static_cast< ::CoreML::Specification::Convolution3DLayerParams_PaddingType >(paddingtype_);
}
inline ::CoreML::Specification::Convolution3DLayerParams_PaddingType Convolution3DLayerParams::paddingtype() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.paddingType)
  return _internal_paddingtype();
}
inline void Convolution3DLayerParams::_internal_set_paddingtype(::CoreML::Specification::Convolution3DLayerParams_PaddingType value) {
  
  paddingtype_ = value;
}
inline void Convolution3DLayerParams::set_paddingtype(::CoreML::Specification::Convolution3DLayerParams_PaddingType value) {
  _internal_set_paddingtype(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.paddingType)
}

// int32 customPaddingFront = 80;
inline void Convolution3DLayerParams::clear_custompaddingfront() {
  custompaddingfront_ = 0;
}
inline int32_t Convolution3DLayerParams::_internal_custompaddingfront() const {
  return custompaddingfront_;
}
inline int32_t Convolution3DLayerParams::custompaddingfront() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.customPaddingFront)
  return _internal_custompaddingfront();
}
inline void Convolution3DLayerParams::_internal_set_custompaddingfront(int32_t value) {
  
  custompaddingfront_ = value;
}
inline void Convolution3DLayerParams::set_custompaddingfront(int32_t value) {
  _internal_set_custompaddingfront(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.customPaddingFront)
}

// int32 customPaddingBack = 81;
inline void Convolution3DLayerParams::clear_custompaddingback() {
  custompaddingback_ = 0;
}
inline int32_t Convolution3DLayerParams::_internal_custompaddingback() const {
  return custompaddingback_;
}
inline int32_t Convolution3DLayerParams::custompaddingback() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.customPaddingBack)
  return _internal_custompaddingback();
}
inline void Convolution3DLayerParams::_internal_set_custompaddingback(int32_t value) {
  
  custompaddingback_ = value;
}
inline void Convolution3DLayerParams::set_custompaddingback(int32_t value) {
  _internal_set_custompaddingback(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.customPaddingBack)
}

// int32 customPaddingTop = 82;
inline void Convolution3DLayerParams::clear_custompaddingtop() {
  custompaddingtop_ = 0;
}
inline int32_t Convolution3DLayerParams::_internal_custompaddingtop() const {
  return custompaddingtop_;
}
inline int32_t Convolution3DLayerParams::custompaddingtop() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.customPaddingTop)
  return _internal_custompaddingtop();
}
inline void Convolution3DLayerParams::_internal_set_custompaddingtop(int32_t value) {
  
  custompaddingtop_ = value;
}
inline void Convolution3DLayerParams::set_custompaddingtop(int32_t value) {
  _internal_set_custompaddingtop(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.customPaddingTop)
}

// int32 customPaddingBottom = 83;
inline void Convolution3DLayerParams::clear_custompaddingbottom() {
  custompaddingbottom_ = 0;
}
inline int32_t Convolution3DLayerParams::_internal_custompaddingbottom() const {
  return custompaddingbottom_;
}
inline int32_t Convolution3DLayerParams::custompaddingbottom() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.customPaddingBottom)
  return _internal_custompaddingbottom();
}
inline void Convolution3DLayerParams::_internal_set_custompaddingbottom(int32_t value) {
  
  custompaddingbottom_ = value;
}
inline void Convolution3DLayerParams::set_custompaddingbottom(int32_t value) {
  _internal_set_custompaddingbottom(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.customPaddingBottom)
}

// int32 customPaddingLeft = 84;
inline void Convolution3DLayerParams::clear_custompaddingleft() {
  custompaddingleft_ = 0;
}
inline int32_t Convolution3DLayerParams::_internal_custompaddingleft() const {
  return custompaddingleft_;
}
inline int32_t Convolution3DLayerParams::custompaddingleft() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.customPaddingLeft)
  return _internal_custompaddingleft();
}
inline void Convolution3DLayerParams::_internal_set_custompaddingleft(int32_t value) {
  
  custompaddingleft_ = value;
}
inline void Convolution3DLayerParams::set_custompaddingleft(int32_t value) {
  _internal_set_custompaddingleft(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.customPaddingLeft)
}

// int32 customPaddingRight = 85;
inline void Convolution3DLayerParams::clear_custompaddingright() {
  custompaddingright_ = 0;
}
inline int32_t Convolution3DLayerParams::_internal_custompaddingright() const {
  return custompaddingright_;
}
inline int32_t Convolution3DLayerParams::custompaddingright() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.customPaddingRight)
  return _internal_custompaddingright();
}
inline void Convolution3DLayerParams::_internal_set_custompaddingright(int32_t value) {
  
  custompaddingright_ = value;
}
inline void Convolution3DLayerParams::set_custompaddingright(int32_t value) {
  _internal_set_custompaddingright(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.customPaddingRight)
}

// bool isDeconvolution = 86;
inline void Convolution3DLayerParams::clear_isdeconvolution() {
  isdeconvolution_ = false;
}
inline bool Convolution3DLayerParams::_internal_isdeconvolution() const {
  return isdeconvolution_;
}
inline bool Convolution3DLayerParams::isdeconvolution() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.isDeconvolution)
  return _internal_isdeconvolution();
}
inline void Convolution3DLayerParams::_internal_set_isdeconvolution(bool value) {
  
  isdeconvolution_ = value;
}
inline void Convolution3DLayerParams::set_isdeconvolution(bool value) {
  _internal_set_isdeconvolution(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.isDeconvolution)
}

// repeated uint64 outputShape = 87;
inline int Convolution3DLayerParams::_internal_outputshape_size() const {
  return outputshape_.size();
}
inline int Convolution3DLayerParams::outputshape_size() const {
  return _internal_outputshape_size();
}
inline void Convolution3DLayerParams::clear_outputshape() {
  outputshape_.Clear();
}
inline uint64_t Convolution3DLayerParams::_internal_outputshape(int index) const {
  return outputshape_.Get(index);
}
inline uint64_t Convolution3DLayerParams::outputshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Convolution3DLayerParams.outputShape)
  return _internal_outputshape(index);
}
inline void Convolution3DLayerParams::set_outputshape(int index, uint64_t value) {
  outputshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Convolution3DLayerParams.outputShape)
}
inline void Convolution3DLayerParams::_internal_add_outputshape(uint64_t value) {
  outputshape_.Add(value);
}
inline void Convolution3DLayerParams::add_outputshape(uint64_t value) {
  _internal_add_outputshape(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.Convolution3DLayerParams.outputShape)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
Convolution3DLayerParams::_internal_outputshape() const {
  return outputshape_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
Convolution3DLayerParams::outputshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.Convolution3DLayerParams.outputShape)
  return _internal_outputshape();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
Convolution3DLayerParams::_internal_mutable_outputshape() {
  return &outputshape_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
Convolution3DLayerParams::mutable_outputshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.Convolution3DLayerParams.outputShape)
  return _internal_mutable_outputshape();
}

// -------------------------------------------------------------------

// InnerProductLayerParams

// uint64 inputChannels = 1;
inline void InnerProductLayerParams::clear_inputchannels() {
  inputchannels_ = uint64_t{0u};
}
inline uint64_t InnerProductLayerParams::_internal_inputchannels() const {
  return inputchannels_;
}
inline uint64_t InnerProductLayerParams::inputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.inputChannels)
  return _internal_inputchannels();
}
inline void InnerProductLayerParams::_internal_set_inputchannels(uint64_t value) {
  
  inputchannels_ = value;
}
inline void InnerProductLayerParams::set_inputchannels(uint64_t value) {
  _internal_set_inputchannels(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.inputChannels)
}

// uint64 outputChannels = 2;
inline void InnerProductLayerParams::clear_outputchannels() {
  outputchannels_ = uint64_t{0u};
}
inline uint64_t InnerProductLayerParams::_internal_outputchannels() const {
  return outputchannels_;
}
inline uint64_t InnerProductLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.outputChannels)
  return _internal_outputchannels();
}
inline void InnerProductLayerParams::_internal_set_outputchannels(uint64_t value) {
  
  outputchannels_ = value;
}
inline void InnerProductLayerParams::set_outputchannels(uint64_t value) {
  _internal_set_outputchannels(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.outputChannels)
}

// bool hasBias = 10;
inline void InnerProductLayerParams::clear_hasbias() {
  hasbias_ = false;
}
inline bool InnerProductLayerParams::_internal_hasbias() const {
  return hasbias_;
}
inline bool InnerProductLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.hasBias)
  return _internal_hasbias();
}
inline void InnerProductLayerParams::_internal_set_hasbias(bool value) {
  
  hasbias_ = value;
}
inline void InnerProductLayerParams::set_hasbias(bool value) {
  _internal_set_hasbias(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 20;
inline bool InnerProductLayerParams::_internal_has_weights() const {
  return this != internal_default_instance() && weights_ != nullptr;
}
inline bool InnerProductLayerParams::has_weights() const {
  return _internal_has_weights();
}
inline void InnerProductLayerParams::clear_weights() {
  if (GetArenaForAllocation() == nullptr && weights_ != nullptr) {
    delete weights_;
  }
  weights_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& InnerProductLayerParams::_internal_weights() const {
  const ::CoreML::Specification::WeightParams* p = weights_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& InnerProductLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.weights)
  return _internal_weights();
}
inline void InnerProductLayerParams::unsafe_arena_set_allocated_weights(
    ::CoreML::Specification::WeightParams* weights) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(weights_);
  }
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.InnerProductLayerParams.weights)
}
inline ::CoreML::Specification::WeightParams* InnerProductLayerParams::release_weights() {
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* InnerProductLayerParams::unsafe_arena_release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.InnerProductLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* InnerProductLayerParams::_internal_mutable_weights() {
  
  if (weights_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    weights_ = p;
  }
  return weights_;
}
inline ::CoreML::Specification::WeightParams* InnerProductLayerParams::mutable_weights() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_weights();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.InnerProductLayerParams.weights)
  return _msg;
}
inline void InnerProductLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete weights_;
  }
  if (weights) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(weights);
    if (message_arena != submessage_arena) {
      weights = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, weights, submessage_arena);
    }
    
  } else {
    
  }
  weights_ = weights;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.InnerProductLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 21;
inline bool InnerProductLayerParams::_internal_has_bias() const {
  return this != internal_default_instance() && bias_ != nullptr;
}
inline bool InnerProductLayerParams::has_bias() const {
  return _internal_has_bias();
}
inline void InnerProductLayerParams::clear_bias() {
  if (GetArenaForAllocation() == nullptr && bias_ != nullptr) {
    delete bias_;
  }
  bias_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& InnerProductLayerParams::_internal_bias() const {
  const ::CoreML::Specification::WeightParams* p = bias_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& InnerProductLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.bias)
  return _internal_bias();
}
inline void InnerProductLayerParams::unsafe_arena_set_allocated_bias(
    ::CoreML::Specification::WeightParams* bias) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(bias_);
  }
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.InnerProductLayerParams.bias)
}
inline ::CoreML::Specification::WeightParams* InnerProductLayerParams::release_bias() {
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* InnerProductLayerParams::unsafe_arena_release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.InnerProductLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* InnerProductLayerParams::_internal_mutable_bias() {
  
  if (bias_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    bias_ = p;
  }
  return bias_;
}
inline ::CoreML::Specification::WeightParams* InnerProductLayerParams::mutable_bias() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_bias();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.InnerProductLayerParams.bias)
  return _msg;
}
inline void InnerProductLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete bias_;
  }
  if (bias) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(bias);
    if (message_arena != submessage_arena) {
      bias = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, bias, submessage_arena);
    }
    
  } else {
    
  }
  bias_ = bias;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.InnerProductLayerParams.bias)
}

// bool int8DynamicQuantize = 22;
inline void InnerProductLayerParams::clear_int8dynamicquantize() {
  int8dynamicquantize_ = false;
}
inline bool InnerProductLayerParams::_internal_int8dynamicquantize() const {
  return int8dynamicquantize_;
}
inline bool InnerProductLayerParams::int8dynamicquantize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.int8DynamicQuantize)
  return _internal_int8dynamicquantize();
}
inline void InnerProductLayerParams::_internal_set_int8dynamicquantize(bool value) {
  
  int8dynamicquantize_ = value;
}
inline void InnerProductLayerParams::set_int8dynamicquantize(bool value) {
  _internal_set_int8dynamicquantize(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.int8DynamicQuantize)
}

// -------------------------------------------------------------------

// EmbeddingLayerParams

// uint64 inputDim = 1;
inline void EmbeddingLayerParams::clear_inputdim() {
  inputdim_ = uint64_t{0u};
}
inline uint64_t EmbeddingLayerParams::_internal_inputdim() const {
  return inputdim_;
}
inline uint64_t EmbeddingLayerParams::inputdim() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.inputDim)
  return _internal_inputdim();
}
inline void EmbeddingLayerParams::_internal_set_inputdim(uint64_t value) {
  
  inputdim_ = value;
}
inline void EmbeddingLayerParams::set_inputdim(uint64_t value) {
  _internal_set_inputdim(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingLayerParams.inputDim)
}

// uint64 outputChannels = 2;
inline void EmbeddingLayerParams::clear_outputchannels() {
  outputchannels_ = uint64_t{0u};
}
inline uint64_t EmbeddingLayerParams::_internal_outputchannels() const {
  return outputchannels_;
}
inline uint64_t EmbeddingLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.outputChannels)
  return _internal_outputchannels();
}
inline void EmbeddingLayerParams::_internal_set_outputchannels(uint64_t value) {
  
  outputchannels_ = value;
}
inline void EmbeddingLayerParams::set_outputchannels(uint64_t value) {
  _internal_set_outputchannels(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingLayerParams.outputChannels)
}

// bool hasBias = 10;
inline void EmbeddingLayerParams::clear_hasbias() {
  hasbias_ = false;
}
inline bool EmbeddingLayerParams::_internal_hasbias() const {
  return hasbias_;
}
inline bool EmbeddingLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.hasBias)
  return _internal_hasbias();
}
inline void EmbeddingLayerParams::_internal_set_hasbias(bool value) {
  
  hasbias_ = value;
}
inline void EmbeddingLayerParams::set_hasbias(bool value) {
  _internal_set_hasbias(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 20;
inline bool EmbeddingLayerParams::_internal_has_weights() const {
  return this != internal_default_instance() && weights_ != nullptr;
}
inline bool EmbeddingLayerParams::has_weights() const {
  return _internal_has_weights();
}
inline void EmbeddingLayerParams::clear_weights() {
  if (GetArenaForAllocation() == nullptr && weights_ != nullptr) {
    delete weights_;
  }
  weights_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& EmbeddingLayerParams::_internal_weights() const {
  const ::CoreML::Specification::WeightParams* p = weights_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& EmbeddingLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.weights)
  return _internal_weights();
}
inline void EmbeddingLayerParams::unsafe_arena_set_allocated_weights(
    ::CoreML::Specification::WeightParams* weights) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(weights_);
  }
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.EmbeddingLayerParams.weights)
}
inline ::CoreML::Specification::WeightParams* EmbeddingLayerParams::release_weights() {
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* EmbeddingLayerParams::unsafe_arena_release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* EmbeddingLayerParams::_internal_mutable_weights() {
  
  if (weights_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    weights_ = p;
  }
  return weights_;
}
inline ::CoreML::Specification::WeightParams* EmbeddingLayerParams::mutable_weights() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_weights();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingLayerParams.weights)
  return _msg;
}
inline void EmbeddingLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete weights_;
  }
  if (weights) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(weights);
    if (message_arena != submessage_arena) {
      weights = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, weights, submessage_arena);
    }
    
  } else {
    
  }
  weights_ = weights;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 21;
inline bool EmbeddingLayerParams::_internal_has_bias() const {
  return this != internal_default_instance() && bias_ != nullptr;
}
inline bool EmbeddingLayerParams::has_bias() const {
  return _internal_has_bias();
}
inline void EmbeddingLayerParams::clear_bias() {
  if (GetArenaForAllocation() == nullptr && bias_ != nullptr) {
    delete bias_;
  }
  bias_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& EmbeddingLayerParams::_internal_bias() const {
  const ::CoreML::Specification::WeightParams* p = bias_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& EmbeddingLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.bias)
  return _internal_bias();
}
inline void EmbeddingLayerParams::unsafe_arena_set_allocated_bias(
    ::CoreML::Specification::WeightParams* bias) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(bias_);
  }
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.EmbeddingLayerParams.bias)
}
inline ::CoreML::Specification::WeightParams* EmbeddingLayerParams::release_bias() {
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* EmbeddingLayerParams::unsafe_arena_release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* EmbeddingLayerParams::_internal_mutable_bias() {
  
  if (bias_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    bias_ = p;
  }
  return bias_;
}
inline ::CoreML::Specification::WeightParams* EmbeddingLayerParams::mutable_bias() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_bias();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingLayerParams.bias)
  return _msg;
}
inline void EmbeddingLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete bias_;
  }
  if (bias) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(bias);
    if (message_arena != submessage_arena) {
      bias = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, bias, submessage_arena);
    }
    
  } else {
    
  }
  bias_ = bias;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingLayerParams.bias)
}

// -------------------------------------------------------------------

// EmbeddingNDLayerParams

// uint64 vocabSize = 1;
inline void EmbeddingNDLayerParams::clear_vocabsize() {
  vocabsize_ = uint64_t{0u};
}
inline uint64_t EmbeddingNDLayerParams::_internal_vocabsize() const {
  return vocabsize_;
}
inline uint64_t EmbeddingNDLayerParams::vocabsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingNDLayerParams.vocabSize)
  return _internal_vocabsize();
}
inline void EmbeddingNDLayerParams::_internal_set_vocabsize(uint64_t value) {
  
  vocabsize_ = value;
}
inline void EmbeddingNDLayerParams::set_vocabsize(uint64_t value) {
  _internal_set_vocabsize(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingNDLayerParams.vocabSize)
}

// uint64 embeddingSize = 2;
inline void EmbeddingNDLayerParams::clear_embeddingsize() {
  embeddingsize_ = uint64_t{0u};
}
inline uint64_t EmbeddingNDLayerParams::_internal_embeddingsize() const {
  return embeddingsize_;
}
inline uint64_t EmbeddingNDLayerParams::embeddingsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingNDLayerParams.embeddingSize)
  return _internal_embeddingsize();
}
inline void EmbeddingNDLayerParams::_internal_set_embeddingsize(uint64_t value) {
  
  embeddingsize_ = value;
}
inline void EmbeddingNDLayerParams::set_embeddingsize(uint64_t value) {
  _internal_set_embeddingsize(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingNDLayerParams.embeddingSize)
}

// bool hasBias = 3;
inline void EmbeddingNDLayerParams::clear_hasbias() {
  hasbias_ = false;
}
inline bool EmbeddingNDLayerParams::_internal_hasbias() const {
  return hasbias_;
}
inline bool EmbeddingNDLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingNDLayerParams.hasBias)
  return _internal_hasbias();
}
inline void EmbeddingNDLayerParams::_internal_set_hasbias(bool value) {
  
  hasbias_ = value;
}
inline void EmbeddingNDLayerParams::set_hasbias(bool value) {
  _internal_set_hasbias(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingNDLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 20;
inline bool EmbeddingNDLayerParams::_internal_has_weights() const {
  return this != internal_default_instance() && weights_ != nullptr;
}
inline bool EmbeddingNDLayerParams::has_weights() const {
  return _internal_has_weights();
}
inline void EmbeddingNDLayerParams::clear_weights() {
  if (GetArenaForAllocation() == nullptr && weights_ != nullptr) {
    delete weights_;
  }
  weights_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& EmbeddingNDLayerParams::_internal_weights() const {
  const ::CoreML::Specification::WeightParams* p = weights_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& EmbeddingNDLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingNDLayerParams.weights)
  return _internal_weights();
}
inline void EmbeddingNDLayerParams::unsafe_arena_set_allocated_weights(
    ::CoreML::Specification::WeightParams* weights) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(weights_);
  }
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.EmbeddingNDLayerParams.weights)
}
inline ::CoreML::Specification::WeightParams* EmbeddingNDLayerParams::release_weights() {
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* EmbeddingNDLayerParams::unsafe_arena_release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingNDLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* EmbeddingNDLayerParams::_internal_mutable_weights() {
  
  if (weights_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    weights_ = p;
  }
  return weights_;
}
inline ::CoreML::Specification::WeightParams* EmbeddingNDLayerParams::mutable_weights() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_weights();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingNDLayerParams.weights)
  return _msg;
}
inline void EmbeddingNDLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete weights_;
  }
  if (weights) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(weights);
    if (message_arena != submessage_arena) {
      weights = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, weights, submessage_arena);
    }
    
  } else {
    
  }
  weights_ = weights;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingNDLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 21;
inline bool EmbeddingNDLayerParams::_internal_has_bias() const {
  return this != internal_default_instance() && bias_ != nullptr;
}
inline bool EmbeddingNDLayerParams::has_bias() const {
  return _internal_has_bias();
}
inline void EmbeddingNDLayerParams::clear_bias() {
  if (GetArenaForAllocation() == nullptr && bias_ != nullptr) {
    delete bias_;
  }
  bias_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& EmbeddingNDLayerParams::_internal_bias() const {
  const ::CoreML::Specification::WeightParams* p = bias_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& EmbeddingNDLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingNDLayerParams.bias)
  return _internal_bias();
}
inline void EmbeddingNDLayerParams::unsafe_arena_set_allocated_bias(
    ::CoreML::Specification::WeightParams* bias) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(bias_);
  }
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.EmbeddingNDLayerParams.bias)
}
inline ::CoreML::Specification::WeightParams* EmbeddingNDLayerParams::release_bias() {
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* EmbeddingNDLayerParams::unsafe_arena_release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingNDLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* EmbeddingNDLayerParams::_internal_mutable_bias() {
  
  if (bias_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    bias_ = p;
  }
  return bias_;
}
inline ::CoreML::Specification::WeightParams* EmbeddingNDLayerParams::mutable_bias() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_bias();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingNDLayerParams.bias)
  return _msg;
}
inline void EmbeddingNDLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete bias_;
  }
  if (bias) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(bias);
    if (message_arena != submessage_arena) {
      bias = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, bias, submessage_arena);
    }
    
  } else {
    
  }
  bias_ = bias;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingNDLayerParams.bias)
}

// -------------------------------------------------------------------

// BatchnormLayerParams

// uint64 channels = 1;
inline void BatchnormLayerParams::clear_channels() {
  channels_ = uint64_t{0u};
}
inline uint64_t BatchnormLayerParams::_internal_channels() const {
  return channels_;
}
inline uint64_t BatchnormLayerParams::channels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.channels)
  return _internal_channels();
}
inline void BatchnormLayerParams::_internal_set_channels(uint64_t value) {
  
  channels_ = value;
}
inline void BatchnormLayerParams::set_channels(uint64_t value) {
  _internal_set_channels(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.channels)
}

// bool computeMeanVar = 5;
inline void BatchnormLayerParams::clear_computemeanvar() {
  computemeanvar_ = false;
}
inline bool BatchnormLayerParams::_internal_computemeanvar() const {
  return computemeanvar_;
}
inline bool BatchnormLayerParams::computemeanvar() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.computeMeanVar)
  return _internal_computemeanvar();
}
inline void BatchnormLayerParams::_internal_set_computemeanvar(bool value) {
  
  computemeanvar_ = value;
}
inline void BatchnormLayerParams::set_computemeanvar(bool value) {
  _internal_set_computemeanvar(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.computeMeanVar)
}

// bool instanceNormalization = 6;
inline void BatchnormLayerParams::clear_instancenormalization() {
  instancenormalization_ = false;
}
inline bool BatchnormLayerParams::_internal_instancenormalization() const {
  return instancenormalization_;
}
inline bool BatchnormLayerParams::instancenormalization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.instanceNormalization)
  return _internal_instancenormalization();
}
inline void BatchnormLayerParams::_internal_set_instancenormalization(bool value) {
  
  instancenormalization_ = value;
}
inline void BatchnormLayerParams::set_instancenormalization(bool value) {
  _internal_set_instancenormalization(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.instanceNormalization)
}

// float epsilon = 10;
inline void BatchnormLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
inline float BatchnormLayerParams::_internal_epsilon() const {
  return epsilon_;
}
inline float BatchnormLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.epsilon)
  return _internal_epsilon();
}
inline void BatchnormLayerParams::_internal_set_epsilon(float value) {
  
  epsilon_ = value;
}
inline void BatchnormLayerParams::set_epsilon(float value) {
  _internal_set_epsilon(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.epsilon)
}

// .CoreML.Specification.WeightParams gamma = 15;
inline bool BatchnormLayerParams::_internal_has_gamma() const {
  return this != internal_default_instance() && gamma_ != nullptr;
}
inline bool BatchnormLayerParams::has_gamma() const {
  return _internal_has_gamma();
}
inline void BatchnormLayerParams::clear_gamma() {
  if (GetArenaForAllocation() == nullptr && gamma_ != nullptr) {
    delete gamma_;
  }
  gamma_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& BatchnormLayerParams::_internal_gamma() const {
  const ::CoreML::Specification::WeightParams* p = gamma_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& BatchnormLayerParams::gamma() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.gamma)
  return _internal_gamma();
}
inline void BatchnormLayerParams::unsafe_arena_set_allocated_gamma(
    ::CoreML::Specification::WeightParams* gamma) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(gamma_);
  }
  gamma_ = gamma;
  if (gamma) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.BatchnormLayerParams.gamma)
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::release_gamma() {
  
  ::CoreML::Specification::WeightParams* temp = gamma_;
  gamma_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::unsafe_arena_release_gamma() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.gamma)
  
  ::CoreML::Specification::WeightParams* temp = gamma_;
  gamma_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::_internal_mutable_gamma() {
  
  if (gamma_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    gamma_ = p;
  }
  return gamma_;
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_gamma() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_gamma();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.gamma)
  return _msg;
}
inline void BatchnormLayerParams::set_allocated_gamma(::CoreML::Specification::WeightParams* gamma) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete gamma_;
  }
  if (gamma) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(gamma);
    if (message_arena != submessage_arena) {
      gamma = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, gamma, submessage_arena);
    }
    
  } else {
    
  }
  gamma_ = gamma;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.gamma)
}

// .CoreML.Specification.WeightParams beta = 16;
inline bool BatchnormLayerParams::_internal_has_beta() const {
  return this != internal_default_instance() && beta_ != nullptr;
}
inline bool BatchnormLayerParams::has_beta() const {
  return _internal_has_beta();
}
inline void BatchnormLayerParams::clear_beta() {
  if (GetArenaForAllocation() == nullptr && beta_ != nullptr) {
    delete beta_;
  }
  beta_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& BatchnormLayerParams::_internal_beta() const {
  const ::CoreML::Specification::WeightParams* p = beta_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& BatchnormLayerParams::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.beta)
  return _internal_beta();
}
inline void BatchnormLayerParams::unsafe_arena_set_allocated_beta(
    ::CoreML::Specification::WeightParams* beta) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(beta_);
  }
  beta_ = beta;
  if (beta) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.BatchnormLayerParams.beta)
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::release_beta() {
  
  ::CoreML::Specification::WeightParams* temp = beta_;
  beta_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::unsafe_arena_release_beta() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.beta)
  
  ::CoreML::Specification::WeightParams* temp = beta_;
  beta_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::_internal_mutable_beta() {
  
  if (beta_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    beta_ = p;
  }
  return beta_;
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_beta() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_beta();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.beta)
  return _msg;
}
inline void BatchnormLayerParams::set_allocated_beta(::CoreML::Specification::WeightParams* beta) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete beta_;
  }
  if (beta) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(beta);
    if (message_arena != submessage_arena) {
      beta = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, beta, submessage_arena);
    }
    
  } else {
    
  }
  beta_ = beta;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.beta)
}

// .CoreML.Specification.WeightParams mean = 17;
inline bool BatchnormLayerParams::_internal_has_mean() const {
  return this != internal_default_instance() && mean_ != nullptr;
}
inline bool BatchnormLayerParams::has_mean() const {
  return _internal_has_mean();
}
inline void BatchnormLayerParams::clear_mean() {
  if (GetArenaForAllocation() == nullptr && mean_ != nullptr) {
    delete mean_;
  }
  mean_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& BatchnormLayerParams::_internal_mean() const {
  const ::CoreML::Specification::WeightParams* p = mean_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& BatchnormLayerParams::mean() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.mean)
  return _internal_mean();
}
inline void BatchnormLayerParams::unsafe_arena_set_allocated_mean(
    ::CoreML::Specification::WeightParams* mean) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(mean_);
  }
  mean_ = mean;
  if (mean) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.BatchnormLayerParams.mean)
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::release_mean() {
  
  ::CoreML::Specification::WeightParams* temp = mean_;
  mean_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::unsafe_arena_release_mean() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.mean)
  
  ::CoreML::Specification::WeightParams* temp = mean_;
  mean_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::_internal_mutable_mean() {
  
  if (mean_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    mean_ = p;
  }
  return mean_;
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_mean() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_mean();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.mean)
  return _msg;
}
inline void BatchnormLayerParams::set_allocated_mean(::CoreML::Specification::WeightParams* mean) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete mean_;
  }
  if (mean) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(mean);
    if (message_arena != submessage_arena) {
      mean = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, mean, submessage_arena);
    }
    
  } else {
    
  }
  mean_ = mean;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.mean)
}

// .CoreML.Specification.WeightParams variance = 18;
inline bool BatchnormLayerParams::_internal_has_variance() const {
  return this != internal_default_instance() && variance_ != nullptr;
}
inline bool BatchnormLayerParams::has_variance() const {
  return _internal_has_variance();
}
inline void BatchnormLayerParams::clear_variance() {
  if (GetArenaForAllocation() == nullptr && variance_ != nullptr) {
    delete variance_;
  }
  variance_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& BatchnormLayerParams::_internal_variance() const {
  const ::CoreML::Specification::WeightParams* p = variance_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& BatchnormLayerParams::variance() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.variance)
  return _internal_variance();
}
inline void BatchnormLayerParams::unsafe_arena_set_allocated_variance(
    ::CoreML::Specification::WeightParams* variance) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(variance_);
  }
  variance_ = variance;
  if (variance) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.BatchnormLayerParams.variance)
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::release_variance() {
  
  ::CoreML::Specification::WeightParams* temp = variance_;
  variance_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::unsafe_arena_release_variance() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.variance)
  
  ::CoreML::Specification::WeightParams* temp = variance_;
  variance_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::_internal_mutable_variance() {
  
  if (variance_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    variance_ = p;
  }
  return variance_;
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_variance() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_variance();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.variance)
  return _msg;
}
inline void BatchnormLayerParams::set_allocated_variance(::CoreML::Specification::WeightParams* variance) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete variance_;
  }
  if (variance) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(variance);
    if (message_arena != submessage_arena) {
      variance = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, variance, submessage_arena);
    }
    
  } else {
    
  }
  variance_ = variance;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.variance)
}

// -------------------------------------------------------------------

// PoolingLayerParams_ValidCompletePadding

// repeated uint64 paddingAmounts = 10;
inline int PoolingLayerParams_ValidCompletePadding::_internal_paddingamounts_size() const {
  return paddingamounts_.size();
}
inline int PoolingLayerParams_ValidCompletePadding::paddingamounts_size() const {
  return _internal_paddingamounts_size();
}
inline void PoolingLayerParams_ValidCompletePadding::clear_paddingamounts() {
  paddingamounts_.Clear();
}
inline uint64_t PoolingLayerParams_ValidCompletePadding::_internal_paddingamounts(int index) const {
  return paddingamounts_.Get(index);
}
inline uint64_t PoolingLayerParams_ValidCompletePadding::paddingamounts(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
  return _internal_paddingamounts(index);
}
inline void PoolingLayerParams_ValidCompletePadding::set_paddingamounts(int index, uint64_t value) {
  paddingamounts_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
}
inline void PoolingLayerParams_ValidCompletePadding::_internal_add_paddingamounts(uint64_t value) {
  paddingamounts_.Add(value);
}
inline void PoolingLayerParams_ValidCompletePadding::add_paddingamounts(uint64_t value) {
  _internal_add_paddingamounts(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
PoolingLayerParams_ValidCompletePadding::_internal_paddingamounts() const {
  return paddingamounts_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
PoolingLayerParams_ValidCompletePadding::paddingamounts() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
  return _internal_paddingamounts();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
PoolingLayerParams_ValidCompletePadding::_internal_mutable_paddingamounts() {
  return &paddingamounts_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
PoolingLayerParams_ValidCompletePadding::mutable_paddingamounts() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
  return _internal_mutable_paddingamounts();
}

// -------------------------------------------------------------------

// PoolingLayerParams

// .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
inline void PoolingLayerParams::clear_type() {
  type_ = 0;
}
inline ::CoreML::Specification::PoolingLayerParams_PoolingType PoolingLayerParams::_internal_type() const {
  return static_cast< ::CoreML::Specification::PoolingLayerParams_PoolingType >(type_);
}
inline ::CoreML::Specification::PoolingLayerParams_PoolingType PoolingLayerParams::type() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.type)
  return _internal_type();
}
inline void PoolingLayerParams::_internal_set_type(::CoreML::Specification::PoolingLayerParams_PoolingType value) {
  
  type_ = value;
}
inline void PoolingLayerParams::set_type(::CoreML::Specification::PoolingLayerParams_PoolingType value) {
  _internal_set_type(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.type)
}

// repeated uint64 kernelSize = 10;
inline int PoolingLayerParams::_internal_kernelsize_size() const {
  return kernelsize_.size();
}
inline int PoolingLayerParams::kernelsize_size() const {
  return _internal_kernelsize_size();
}
inline void PoolingLayerParams::clear_kernelsize() {
  kernelsize_.Clear();
}
inline uint64_t PoolingLayerParams::_internal_kernelsize(int index) const {
  return kernelsize_.Get(index);
}
inline uint64_t PoolingLayerParams::kernelsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.kernelSize)
  return _internal_kernelsize(index);
}
inline void PoolingLayerParams::set_kernelsize(int index, uint64_t value) {
  kernelsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.kernelSize)
}
inline void PoolingLayerParams::_internal_add_kernelsize(uint64_t value) {
  kernelsize_.Add(value);
}
inline void PoolingLayerParams::add_kernelsize(uint64_t value) {
  _internal_add_kernelsize(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PoolingLayerParams.kernelSize)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
PoolingLayerParams::_internal_kernelsize() const {
  return kernelsize_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
PoolingLayerParams::kernelsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PoolingLayerParams.kernelSize)
  return _internal_kernelsize();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
PoolingLayerParams::_internal_mutable_kernelsize() {
  return &kernelsize_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
PoolingLayerParams::mutable_kernelsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PoolingLayerParams.kernelSize)
  return _internal_mutable_kernelsize();
}

// repeated uint64 stride = 20;
inline int PoolingLayerParams::_internal_stride_size() const {
  return stride_.size();
}
inline int PoolingLayerParams::stride_size() const {
  return _internal_stride_size();
}
inline void PoolingLayerParams::clear_stride() {
  stride_.Clear();
}
inline uint64_t PoolingLayerParams::_internal_stride(int index) const {
  return stride_.Get(index);
}
inline uint64_t PoolingLayerParams::stride(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.stride)
  return _internal_stride(index);
}
inline void PoolingLayerParams::set_stride(int index, uint64_t value) {
  stride_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.stride)
}
inline void PoolingLayerParams::_internal_add_stride(uint64_t value) {
  stride_.Add(value);
}
inline void PoolingLayerParams::add_stride(uint64_t value) {
  _internal_add_stride(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PoolingLayerParams.stride)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
PoolingLayerParams::_internal_stride() const {
  return stride_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
PoolingLayerParams::stride() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PoolingLayerParams.stride)
  return _internal_stride();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
PoolingLayerParams::_internal_mutable_stride() {
  return &stride_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
PoolingLayerParams::mutable_stride() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PoolingLayerParams.stride)
  return _internal_mutable_stride();
}

// .CoreML.Specification.ValidPadding valid = 30;
inline bool PoolingLayerParams::_internal_has_valid() const {
  return PoolingPaddingType_case() == kValid;
}
inline bool PoolingLayerParams::has_valid() const {
  return _internal_has_valid();
}
inline void PoolingLayerParams::set_has_valid() {
  _oneof_case_[0] = kValid;
}
inline void PoolingLayerParams::clear_valid() {
  if (_internal_has_valid()) {
    if (GetArenaForAllocation() == nullptr) {
      delete PoolingPaddingType_.valid_;
    }
    clear_has_PoolingPaddingType();
  }
}
inline ::CoreML::Specification::ValidPadding* PoolingLayerParams::release_valid() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PoolingLayerParams.valid)
  if (_internal_has_valid()) {
    clear_has_PoolingPaddingType();
      ::CoreML::Specification::ValidPadding* temp = PoolingPaddingType_.valid_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    PoolingPaddingType_.valid_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::ValidPadding& PoolingLayerParams::_internal_valid() const {
  return _internal_has_valid()
      ? *PoolingPaddingType_.valid_
      : reinterpret_cast< ::CoreML::Specification::ValidPadding&>(::CoreML::Specification::_ValidPadding_default_instance_);
}
inline const ::CoreML::Specification::ValidPadding& PoolingLayerParams::valid() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.valid)
  return _internal_valid();
}
inline ::CoreML::Specification::ValidPadding* PoolingLayerParams::unsafe_arena_release_valid() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.PoolingLayerParams.valid)
  if (_internal_has_valid()) {
    clear_has_PoolingPaddingType();
    ::CoreML::Specification::ValidPadding* temp = PoolingPaddingType_.valid_;
    PoolingPaddingType_.valid_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void PoolingLayerParams::unsafe_arena_set_allocated_valid(::CoreML::Specification::ValidPadding* valid) {
  clear_PoolingPaddingType();
  if (valid) {
    set_has_valid();
    PoolingPaddingType_.valid_ = valid;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.PoolingLayerParams.valid)
}
inline ::CoreML::Specification::ValidPadding* PoolingLayerParams::_internal_mutable_valid() {
  if (!_internal_has_valid()) {
    clear_PoolingPaddingType();
    set_has_valid();
    PoolingPaddingType_.valid_ = CreateMaybeMessage< ::CoreML::Specification::ValidPadding >(GetArenaForAllocation());
  }
  return PoolingPaddingType_.valid_;
}
inline ::CoreML::Specification::ValidPadding* PoolingLayerParams::mutable_valid() {
  ::CoreML::Specification::ValidPadding* _msg = _internal_mutable_valid();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PoolingLayerParams.valid)
  return _msg;
}

// .CoreML.Specification.SamePadding same = 31;
inline bool PoolingLayerParams::_internal_has_same() const {
  return PoolingPaddingType_case() == kSame;
}
inline bool PoolingLayerParams::has_same() const {
  return _internal_has_same();
}
inline void PoolingLayerParams::set_has_same() {
  _oneof_case_[0] = kSame;
}
inline void PoolingLayerParams::clear_same() {
  if (_internal_has_same()) {
    if (GetArenaForAllocation() == nullptr) {
      delete PoolingPaddingType_.same_;
    }
    clear_has_PoolingPaddingType();
  }
}
inline ::CoreML::Specification::SamePadding* PoolingLayerParams::release_same() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PoolingLayerParams.same)
  if (_internal_has_same()) {
    clear_has_PoolingPaddingType();
      ::CoreML::Specification::SamePadding* temp = PoolingPaddingType_.same_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    PoolingPaddingType_.same_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::SamePadding& PoolingLayerParams::_internal_same() const {
  return _internal_has_same()
      ? *PoolingPaddingType_.same_
      : reinterpret_cast< ::CoreML::Specification::SamePadding&>(::CoreML::Specification::_SamePadding_default_instance_);
}
inline const ::CoreML::Specification::SamePadding& PoolingLayerParams::same() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.same)
  return _internal_same();
}
inline ::CoreML::Specification::SamePadding* PoolingLayerParams::unsafe_arena_release_same() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.PoolingLayerParams.same)
  if (_internal_has_same()) {
    clear_has_PoolingPaddingType();
    ::CoreML::Specification::SamePadding* temp = PoolingPaddingType_.same_;
    PoolingPaddingType_.same_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void PoolingLayerParams::unsafe_arena_set_allocated_same(::CoreML::Specification::SamePadding* same) {
  clear_PoolingPaddingType();
  if (same) {
    set_has_same();
    PoolingPaddingType_.same_ = same;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.PoolingLayerParams.same)
}
inline ::CoreML::Specification::SamePadding* PoolingLayerParams::_internal_mutable_same() {
  if (!_internal_has_same()) {
    clear_PoolingPaddingType();
    set_has_same();
    PoolingPaddingType_.same_ = CreateMaybeMessage< ::CoreML::Specification::SamePadding >(GetArenaForAllocation());
  }
  return PoolingPaddingType_.same_;
}
inline ::CoreML::Specification::SamePadding* PoolingLayerParams::mutable_same() {
  ::CoreML::Specification::SamePadding* _msg = _internal_mutable_same();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PoolingLayerParams.same)
  return _msg;
}

// .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
inline bool PoolingLayerParams::_internal_has_includelastpixel() const {
  return PoolingPaddingType_case() == kIncludeLastPixel;
}
inline bool PoolingLayerParams::has_includelastpixel() const {
  return _internal_has_includelastpixel();
}
inline void PoolingLayerParams::set_has_includelastpixel() {
  _oneof_case_[0] = kIncludeLastPixel;
}
inline void PoolingLayerParams::clear_includelastpixel() {
  if (_internal_has_includelastpixel()) {
    if (GetArenaForAllocation() == nullptr) {
      delete PoolingPaddingType_.includelastpixel_;
    }
    clear_has_PoolingPaddingType();
  }
}
inline ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* PoolingLayerParams::release_includelastpixel() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  if (_internal_has_includelastpixel()) {
    clear_has_PoolingPaddingType();
      ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* temp = PoolingPaddingType_.includelastpixel_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    PoolingPaddingType_.includelastpixel_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding& PoolingLayerParams::_internal_includelastpixel() const {
  return _internal_has_includelastpixel()
      ? *PoolingPaddingType_.includelastpixel_
      : reinterpret_cast< ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding&>(::CoreML::Specification::_PoolingLayerParams_ValidCompletePadding_default_instance_);
}
inline const ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding& PoolingLayerParams::includelastpixel() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  return _internal_includelastpixel();
}
inline ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* PoolingLayerParams::unsafe_arena_release_includelastpixel() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  if (_internal_has_includelastpixel()) {
    clear_has_PoolingPaddingType();
    ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* temp = PoolingPaddingType_.includelastpixel_;
    PoolingPaddingType_.includelastpixel_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void PoolingLayerParams::unsafe_arena_set_allocated_includelastpixel(::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* includelastpixel) {
  clear_PoolingPaddingType();
  if (includelastpixel) {
    set_has_includelastpixel();
    PoolingPaddingType_.includelastpixel_ = includelastpixel;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.PoolingLayerParams.includeLastPixel)
}
inline ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* PoolingLayerParams::_internal_mutable_includelastpixel() {
  if (!_internal_has_includelastpixel()) {
    clear_PoolingPaddingType();
    set_has_includelastpixel();
    PoolingPaddingType_.includelastpixel_ = CreateMaybeMessage< ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding >(GetArenaForAllocation());
  }
  return PoolingPaddingType_.includelastpixel_;
}
inline ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* PoolingLayerParams::mutable_includelastpixel() {
  ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* _msg = _internal_mutable_includelastpixel();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  return _msg;
}

// bool avgPoolExcludePadding = 50;
inline void PoolingLayerParams::clear_avgpoolexcludepadding() {
  avgpoolexcludepadding_ = false;
}
inline bool PoolingLayerParams::_internal_avgpoolexcludepadding() const {
  return avgpoolexcludepadding_;
}
inline bool PoolingLayerParams::avgpoolexcludepadding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.avgPoolExcludePadding)
  return _internal_avgpoolexcludepadding();
}
inline void PoolingLayerParams::_internal_set_avgpoolexcludepadding(bool value) {
  
  avgpoolexcludepadding_ = value;
}
inline void PoolingLayerParams::set_avgpoolexcludepadding(bool value) {
  _internal_set_avgpoolexcludepadding(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.avgPoolExcludePadding)
}

// bool globalPooling = 60;
inline void PoolingLayerParams::clear_globalpooling() {
  globalpooling_ = false;
}
inline bool PoolingLayerParams::_internal_globalpooling() const {
  return globalpooling_;
}
inline bool PoolingLayerParams::globalpooling() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.globalPooling)
  return _internal_globalpooling();
}
inline void PoolingLayerParams::_internal_set_globalpooling(bool value) {
  
  globalpooling_ = value;
}
inline void PoolingLayerParams::set_globalpooling(bool value) {
  _internal_set_globalpooling(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.globalPooling)
}

inline bool PoolingLayerParams::has_PoolingPaddingType() const {
  return PoolingPaddingType_case() != POOLINGPADDINGTYPE_NOT_SET;
}
inline void PoolingLayerParams::clear_has_PoolingPaddingType() {
  _oneof_case_[0] = POOLINGPADDINGTYPE_NOT_SET;
}
inline PoolingLayerParams::PoolingPaddingTypeCase PoolingLayerParams::PoolingPaddingType_case() const {
  return PoolingLayerParams::PoolingPaddingTypeCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// Pooling3DLayerParams

// .CoreML.Specification.Pooling3DLayerParams.PoolingType3D type = 1;
inline void Pooling3DLayerParams::clear_type() {
  type_ = 0;
}
inline ::CoreML::Specification::Pooling3DLayerParams_PoolingType3D Pooling3DLayerParams::_internal_type() const {
  return static_cast< ::CoreML::Specification::Pooling3DLayerParams_PoolingType3D >(type_);
}
inline ::CoreML::Specification::Pooling3DLayerParams_PoolingType3D Pooling3DLayerParams::type() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Pooling3DLayerParams.type)
  return _internal_type();
}
inline void Pooling3DLayerParams::_internal_set_type(::CoreML::Specification::Pooling3DLayerParams_PoolingType3D value) {
  
  type_ = value;
}
inline void Pooling3DLayerParams::set_type(::CoreML::Specification::Pooling3DLayerParams_PoolingType3D value) {
  _internal_set_type(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Pooling3DLayerParams.type)
}

// int32 kernelDepth = 2;
inline void Pooling3DLayerParams::clear_kerneldepth() {
  kerneldepth_ = 0;
}
inline int32_t Pooling3DLayerParams::_internal_kerneldepth() const {
  return kerneldepth_;
}
inline int32_t Pooling3DLayerParams::kerneldepth() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Pooling3DLayerParams.kernelDepth)
  return _internal_kerneldepth();
}
inline void Pooling3DLayerParams::_internal_set_kerneldepth(int32_t value) {
  
  kerneldepth_ = value;
}
inline void Pooling3DLayerParams::set_kerneldepth(int32_t value) {
  _internal_set_kerneldepth(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Pooling3DLayerParams.kernelDepth)
}

// int32 kernelHeight = 3;
inline void Pooling3DLayerParams::clear_kernelheight() {
  kernelheight_ = 0;
}
inline int32_t Pooling3DLayerParams::_internal_kernelheight() const {
  return kernelheight_;
}
inline int32_t Pooling3DLayerParams::kernelheight() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Pooling3DLayerParams.kernelHeight)
  return _internal_kernelheight();
}
inline void Pooling3DLayerParams::_internal_set_kernelheight(int32_t value) {
  
  kernelheight_ = value;
}
inline void Pooling3DLayerParams::set_kernelheight(int32_t value) {
  _internal_set_kernelheight(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Pooling3DLayerParams.kernelHeight)
}

// int32 kernelWidth = 4;
inline void Pooling3DLayerParams::clear_kernelwidth() {
  kernelwidth_ = 0;
}
inline int32_t Pooling3DLayerParams::_internal_kernelwidth() const {
  return kernelwidth_;
}
inline int32_t Pooling3DLayerParams::kernelwidth() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Pooling3DLayerParams.kernelWidth)
  return _internal_kernelwidth();
}
inline void Pooling3DLayerParams::_internal_set_kernelwidth(int32_t value) {
  
  kernelwidth_ = value;
}
inline void Pooling3DLayerParams::set_kernelwidth(int32_t value) {
  _internal_set_kernelwidth(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Pooling3DLayerParams.kernelWidth)
}

// int32 strideDepth = 5;
inline void Pooling3DLayerParams::clear_stridedepth() {
  stridedepth_ = 0;
}
inline int32_t Pooling3DLayerParams::_internal_stridedepth() const {
  return stridedepth_;
}
inline int32_t Pooling3DLayerParams::stridedepth() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Pooling3DLayerParams.strideDepth)
  return _internal_stridedepth();
}
inline void Pooling3DLayerParams::_internal_set_stridedepth(int32_t value) {
  
  stridedepth_ = value;
}
inline void Pooling3DLayerParams::set_stridedepth(int32_t value) {
  _internal_set_stridedepth(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Pooling3DLayerParams.strideDepth)
}

// int32 strideHeight = 6;
inline void Pooling3DLayerParams::clear_strideheight() {
  strideheight_ = 0;
}
inline int32_t Pooling3DLayerParams::_internal_strideheight() const {
  return strideheight_;
}
inline int32_t Pooling3DLayerParams::strideheight() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Pooling3DLayerParams.strideHeight)
  return _internal_strideheight();
}
inline void Pooling3DLayerParams::_internal_set_strideheight(int32_t value) {
  
  strideheight_ = value;
}
inline void Pooling3DLayerParams::set_strideheight(int32_t value) {
  _internal_set_strideheight(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Pooling3DLayerParams.strideHeight)
}

// int32 strideWidth = 7;
inline void Pooling3DLayerParams::clear_stridewidth() {
  stridewidth_ = 0;
}
inline int32_t Pooling3DLayerParams::_internal_stridewidth() const {
  return stridewidth_;
}
inline int32_t Pooling3DLayerParams::stridewidth() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Pooling3DLayerParams.strideWidth)
  return _internal_stridewidth();
}
inline void Pooling3DLayerParams::_internal_set_stridewidth(int32_t value) {
  
  stridewidth_ = value;
}
inline void Pooling3DLayerParams::set_stridewidth(int32_t value) {
  _internal_set_stridewidth(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Pooling3DLayerParams.strideWidth)
}

// .CoreML.Specification.Pooling3DLayerParams.Pooling3DPaddingType paddingType = 15;
inline void Pooling3DLayerParams::clear_paddingtype() {
  paddingtype_ = 0;
}
inline ::CoreML::Specification::Pooling3DLayerParams_Pooling3DPaddingType Pooling3DLayerParams::_internal_paddingtype() const {
  return static_cast< ::CoreML::Specification::Pooling3DLayerParams_Pooling3DPaddingType >(paddingtype_);
}
inline ::CoreML::Specification::Pooling3DLayerParams_Pooling3DPaddingType Pooling3DLayerParams::paddingtype() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Pooling3DLayerParams.paddingType)
  return _internal_paddingtype();
}
inline void Pooling3DLayerParams::_internal_set_paddingtype(::CoreML::Specification::Pooling3DLayerParams_Pooling3DPaddingType value) {
  
  paddingtype_ = value;
}
inline void Pooling3DLayerParams::set_paddingtype(::CoreML::Specification::Pooling3DLayerParams_Pooling3DPaddingType value) {
  _internal_set_paddingtype(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Pooling3DLayerParams.paddingType)
}

// int32 customPaddingFront = 8;
inline void Pooling3DLayerParams::clear_custompaddingfront() {
  custompaddingfront_ = 0;
}
inline int32_t Pooling3DLayerParams::_internal_custompaddingfront() const {
  return custompaddingfront_;
}
inline int32_t Pooling3DLayerParams::custompaddingfront() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Pooling3DLayerParams.customPaddingFront)
  return _internal_custompaddingfront();
}
inline void Pooling3DLayerParams::_internal_set_custompaddingfront(int32_t value) {
  
  custompaddingfront_ = value;
}
inline void Pooling3DLayerParams::set_custompaddingfront(int32_t value) {
  _internal_set_custompaddingfront(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Pooling3DLayerParams.customPaddingFront)
}

// int32 customPaddingBack = 9;
inline void Pooling3DLayerParams::clear_custompaddingback() {
  custompaddingback_ = 0;
}
inline int32_t Pooling3DLayerParams::_internal_custompaddingback() const {
  return custompaddingback_;
}
inline int32_t Pooling3DLayerParams::custompaddingback() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Pooling3DLayerParams.customPaddingBack)
  return _internal_custompaddingback();
}
inline void Pooling3DLayerParams::_internal_set_custompaddingback(int32_t value) {
  
  custompaddingback_ = value;
}
inline void Pooling3DLayerParams::set_custompaddingback(int32_t value) {
  _internal_set_custompaddingback(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Pooling3DLayerParams.customPaddingBack)
}

// int32 customPaddingTop = 10;
inline void Pooling3DLayerParams::clear_custompaddingtop() {
  custompaddingtop_ = 0;
}
inline int32_t Pooling3DLayerParams::_internal_custompaddingtop() const {
  return custompaddingtop_;
}
inline int32_t Pooling3DLayerParams::custompaddingtop() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Pooling3DLayerParams.customPaddingTop)
  return _internal_custompaddingtop();
}
inline void Pooling3DLayerParams::_internal_set_custompaddingtop(int32_t value) {
  
  custompaddingtop_ = value;
}
inline void Pooling3DLayerParams::set_custompaddingtop(int32_t value) {
  _internal_set_custompaddingtop(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Pooling3DLayerParams.customPaddingTop)
}

// int32 customPaddingBottom = 11;
inline void Pooling3DLayerParams::clear_custompaddingbottom() {
  custompaddingbottom_ = 0;
}
inline int32_t Pooling3DLayerParams::_internal_custompaddingbottom() const {
  return custompaddingbottom_;
}
inline int32_t Pooling3DLayerParams::custompaddingbottom() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Pooling3DLayerParams.customPaddingBottom)
  return _internal_custompaddingbottom();
}
inline void Pooling3DLayerParams::_internal_set_custompaddingbottom(int32_t value) {
  
  custompaddingbottom_ = value;
}
inline void Pooling3DLayerParams::set_custompaddingbottom(int32_t value) {
  _internal_set_custompaddingbottom(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Pooling3DLayerParams.customPaddingBottom)
}

// int32 customPaddingLeft = 12;
inline void Pooling3DLayerParams::clear_custompaddingleft() {
  custompaddingleft_ = 0;
}
inline int32_t Pooling3DLayerParams::_internal_custompaddingleft() const {
  return custompaddingleft_;
}
inline int32_t Pooling3DLayerParams::custompaddingleft() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Pooling3DLayerParams.customPaddingLeft)
  return _internal_custompaddingleft();
}
inline void Pooling3DLayerParams::_internal_set_custompaddingleft(int32_t value) {
  
  custompaddingleft_ = value;
}
inline void Pooling3DLayerParams::set_custompaddingleft(int32_t value) {
  _internal_set_custompaddingleft(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Pooling3DLayerParams.customPaddingLeft)
}

// int32 customPaddingRight = 13;
inline void Pooling3DLayerParams::clear_custompaddingright() {
  custompaddingright_ = 0;
}
inline int32_t Pooling3DLayerParams::_internal_custompaddingright() const {
  return custompaddingright_;
}
inline int32_t Pooling3DLayerParams::custompaddingright() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Pooling3DLayerParams.customPaddingRight)
  return _internal_custompaddingright();
}
inline void Pooling3DLayerParams::_internal_set_custompaddingright(int32_t value) {
  
  custompaddingright_ = value;
}
inline void Pooling3DLayerParams::set_custompaddingright(int32_t value) {
  _internal_set_custompaddingright(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Pooling3DLayerParams.customPaddingRight)
}

// bool countExcludePadding = 14;
inline void Pooling3DLayerParams::clear_countexcludepadding() {
  countexcludepadding_ = false;
}
inline bool Pooling3DLayerParams::_internal_countexcludepadding() const {
  return countexcludepadding_;
}
inline bool Pooling3DLayerParams::countexcludepadding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Pooling3DLayerParams.countExcludePadding)
  return _internal_countexcludepadding();
}
inline void Pooling3DLayerParams::_internal_set_countexcludepadding(bool value) {
  
  countexcludepadding_ = value;
}
inline void Pooling3DLayerParams::set_countexcludepadding(bool value) {
  _internal_set_countexcludepadding(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Pooling3DLayerParams.countExcludePadding)
}

// -------------------------------------------------------------------

// GlobalPooling3DLayerParams

// .CoreML.Specification.GlobalPooling3DLayerParams.GlobalPoolingType3D type = 1;
inline void GlobalPooling3DLayerParams::clear_type() {
  type_ = 0;
}
inline ::CoreML::Specification::GlobalPooling3DLayerParams_GlobalPoolingType3D GlobalPooling3DLayerParams::_internal_type() const {
  return static_cast< ::CoreML::Specification::GlobalPooling3DLayerParams_GlobalPoolingType3D >(type_);
}
inline ::CoreML::Specification::GlobalPooling3DLayerParams_GlobalPoolingType3D GlobalPooling3DLayerParams::type() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GlobalPooling3DLayerParams.type)
  return _internal_type();
}
inline void GlobalPooling3DLayerParams::_internal_set_type(::CoreML::Specification::GlobalPooling3DLayerParams_GlobalPoolingType3D value) {
  
  type_ = value;
}
inline void GlobalPooling3DLayerParams::set_type(::CoreML::Specification::GlobalPooling3DLayerParams_GlobalPoolingType3D value) {
  _internal_set_type(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.GlobalPooling3DLayerParams.type)
}

// -------------------------------------------------------------------

// PaddingLayerParams_PaddingConstant

// float value = 1;
inline void PaddingLayerParams_PaddingConstant::clear_value() {
  value_ = 0;
}
inline float PaddingLayerParams_PaddingConstant::_internal_value() const {
  return value_;
}
inline float PaddingLayerParams_PaddingConstant::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.PaddingConstant.value)
  return _internal_value();
}
inline void PaddingLayerParams_PaddingConstant::_internal_set_value(float value) {
  
  value_ = value;
}
inline void PaddingLayerParams_PaddingConstant::set_value(float value) {
  _internal_set_value(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PaddingLayerParams.PaddingConstant.value)
}

// -------------------------------------------------------------------

// PaddingLayerParams_PaddingReflection

// -------------------------------------------------------------------

// PaddingLayerParams_PaddingReplication

// -------------------------------------------------------------------

// PaddingLayerParams

// .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
inline bool PaddingLayerParams::_internal_has_constant() const {
  return PaddingType_case() == kConstant;
}
inline bool PaddingLayerParams::has_constant() const {
  return _internal_has_constant();
}
inline void PaddingLayerParams::set_has_constant() {
  _oneof_case_[0] = kConstant;
}
inline void PaddingLayerParams::clear_constant() {
  if (_internal_has_constant()) {
    if (GetArenaForAllocation() == nullptr) {
      delete PaddingType_.constant_;
    }
    clear_has_PaddingType();
  }
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingConstant* PaddingLayerParams::release_constant() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.constant)
  if (_internal_has_constant()) {
    clear_has_PaddingType();
      ::CoreML::Specification::PaddingLayerParams_PaddingConstant* temp = PaddingType_.constant_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    PaddingType_.constant_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::PaddingLayerParams_PaddingConstant& PaddingLayerParams::_internal_constant() const {
  return _internal_has_constant()
      ? *PaddingType_.constant_
      : reinterpret_cast< ::CoreML::Specification::PaddingLayerParams_PaddingConstant&>(::CoreML::Specification::_PaddingLayerParams_PaddingConstant_default_instance_);
}
inline const ::CoreML::Specification::PaddingLayerParams_PaddingConstant& PaddingLayerParams::constant() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.constant)
  return _internal_constant();
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingConstant* PaddingLayerParams::unsafe_arena_release_constant() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.PaddingLayerParams.constant)
  if (_internal_has_constant()) {
    clear_has_PaddingType();
    ::CoreML::Specification::PaddingLayerParams_PaddingConstant* temp = PaddingType_.constant_;
    PaddingType_.constant_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void PaddingLayerParams::unsafe_arena_set_allocated_constant(::CoreML::Specification::PaddingLayerParams_PaddingConstant* constant) {
  clear_PaddingType();
  if (constant) {
    set_has_constant();
    PaddingType_.constant_ = constant;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.PaddingLayerParams.constant)
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingConstant* PaddingLayerParams::_internal_mutable_constant() {
  if (!_internal_has_constant()) {
    clear_PaddingType();
    set_has_constant();
    PaddingType_.constant_ = CreateMaybeMessage< ::CoreML::Specification::PaddingLayerParams_PaddingConstant >(GetArenaForAllocation());
  }
  return PaddingType_.constant_;
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingConstant* PaddingLayerParams::mutable_constant() {
  ::CoreML::Specification::PaddingLayerParams_PaddingConstant* _msg = _internal_mutable_constant();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.constant)
  return _msg;
}

// .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
inline bool PaddingLayerParams::_internal_has_reflection() const {
  return PaddingType_case() == kReflection;
}
inline bool PaddingLayerParams::has_reflection() const {
  return _internal_has_reflection();
}
inline void PaddingLayerParams::set_has_reflection() {
  _oneof_case_[0] = kReflection;
}
inline void PaddingLayerParams::clear_reflection() {
  if (_internal_has_reflection()) {
    if (GetArenaForAllocation() == nullptr) {
      delete PaddingType_.reflection_;
    }
    clear_has_PaddingType();
  }
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingReflection* PaddingLayerParams::release_reflection() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.reflection)
  if (_internal_has_reflection()) {
    clear_has_PaddingType();
      ::CoreML::Specification::PaddingLayerParams_PaddingReflection* temp = PaddingType_.reflection_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    PaddingType_.reflection_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::PaddingLayerParams_PaddingReflection& PaddingLayerParams::_internal_reflection() const {
  return _internal_has_reflection()
      ? *PaddingType_.reflection_
      : reinterpret_cast< ::CoreML::Specification::PaddingLayerParams_PaddingReflection&>(::CoreML::Specification::_PaddingLayerParams_PaddingReflection_default_instance_);
}
inline const ::CoreML::Specification::PaddingLayerParams_PaddingReflection& PaddingLayerParams::reflection() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.reflection)
  return _internal_reflection();
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingReflection* PaddingLayerParams::unsafe_arena_release_reflection() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.PaddingLayerParams.reflection)
  if (_internal_has_reflection()) {
    clear_has_PaddingType();
    ::CoreML::Specification::PaddingLayerParams_PaddingReflection* temp = PaddingType_.reflection_;
    PaddingType_.reflection_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void PaddingLayerParams::unsafe_arena_set_allocated_reflection(::CoreML::Specification::PaddingLayerParams_PaddingReflection* reflection) {
  clear_PaddingType();
  if (reflection) {
    set_has_reflection();
    PaddingType_.reflection_ = reflection;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.PaddingLayerParams.reflection)
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingReflection* PaddingLayerParams::_internal_mutable_reflection() {
  if (!_internal_has_reflection()) {
    clear_PaddingType();
    set_has_reflection();
    PaddingType_.reflection_ = CreateMaybeMessage< ::CoreML::Specification::PaddingLayerParams_PaddingReflection >(GetArenaForAllocation());
  }
  return PaddingType_.reflection_;
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingReflection* PaddingLayerParams::mutable_reflection() {
  ::CoreML::Specification::PaddingLayerParams_PaddingReflection* _msg = _internal_mutable_reflection();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.reflection)
  return _msg;
}

// .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
inline bool PaddingLayerParams::_internal_has_replication() const {
  return PaddingType_case() == kReplication;
}
inline bool PaddingLayerParams::has_replication() const {
  return _internal_has_replication();
}
inline void PaddingLayerParams::set_has_replication() {
  _oneof_case_[0] = kReplication;
}
inline void PaddingLayerParams::clear_replication() {
  if (_internal_has_replication()) {
    if (GetArenaForAllocation() == nullptr) {
      delete PaddingType_.replication_;
    }
    clear_has_PaddingType();
  }
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingReplication* PaddingLayerParams::release_replication() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.replication)
  if (_internal_has_replication()) {
    clear_has_PaddingType();
      ::CoreML::Specification::PaddingLayerParams_PaddingReplication* temp = PaddingType_.replication_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    PaddingType_.replication_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::PaddingLayerParams_PaddingReplication& PaddingLayerParams::_internal_replication() const {
  return _internal_has_replication()
      ? *PaddingType_.replication_
      : reinterpret_cast< ::CoreML::Specification::PaddingLayerParams_PaddingReplication&>(::CoreML::Specification::_PaddingLayerParams_PaddingReplication_default_instance_);
}
inline const ::CoreML::Specification::PaddingLayerParams_PaddingReplication& PaddingLayerParams::replication() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.replication)
  return _internal_replication();
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingReplication* PaddingLayerParams::unsafe_arena_release_replication() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.PaddingLayerParams.replication)
  if (_internal_has_replication()) {
    clear_has_PaddingType();
    ::CoreML::Specification::PaddingLayerParams_PaddingReplication* temp = PaddingType_.replication_;
    PaddingType_.replication_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void PaddingLayerParams::unsafe_arena_set_allocated_replication(::CoreML::Specification::PaddingLayerParams_PaddingReplication* replication) {
  clear_PaddingType();
  if (replication) {
    set_has_replication();
    PaddingType_.replication_ = replication;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.PaddingLayerParams.replication)
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingReplication* PaddingLayerParams::_internal_mutable_replication() {
  if (!_internal_has_replication()) {
    clear_PaddingType();
    set_has_replication();
    PaddingType_.replication_ = CreateMaybeMessage< ::CoreML::Specification::PaddingLayerParams_PaddingReplication >(GetArenaForAllocation());
  }
  return PaddingType_.replication_;
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingReplication* PaddingLayerParams::mutable_replication() {
  ::CoreML::Specification::PaddingLayerParams_PaddingReplication* _msg = _internal_mutable_replication();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.replication)
  return _msg;
}

// .CoreML.Specification.BorderAmounts paddingAmounts = 10;
inline bool PaddingLayerParams::_internal_has_paddingamounts() const {
  return this != internal_default_instance() && paddingamounts_ != nullptr;
}
inline bool PaddingLayerParams::has_paddingamounts() const {
  return _internal_has_paddingamounts();
}
inline void PaddingLayerParams::clear_paddingamounts() {
  if (GetArenaForAllocation() == nullptr && paddingamounts_ != nullptr) {
    delete paddingamounts_;
  }
  paddingamounts_ = nullptr;
}
inline const ::CoreML::Specification::BorderAmounts& PaddingLayerParams::_internal_paddingamounts() const {
  const ::CoreML::Specification::BorderAmounts* p = paddingamounts_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::BorderAmounts&>(
      ::CoreML::Specification::_BorderAmounts_default_instance_);
}
inline const ::CoreML::Specification::BorderAmounts& PaddingLayerParams::paddingamounts() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.paddingAmounts)
  return _internal_paddingamounts();
}
inline void PaddingLayerParams::unsafe_arena_set_allocated_paddingamounts(
    ::CoreML::Specification::BorderAmounts* paddingamounts) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(paddingamounts_);
  }
  paddingamounts_ = paddingamounts;
  if (paddingamounts) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.PaddingLayerParams.paddingAmounts)
}
inline ::CoreML::Specification::BorderAmounts* PaddingLayerParams::release_paddingamounts() {
  
  ::CoreML::Specification::BorderAmounts* temp = paddingamounts_;
  paddingamounts_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::BorderAmounts* PaddingLayerParams::unsafe_arena_release_paddingamounts() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.paddingAmounts)
  
  ::CoreML::Specification::BorderAmounts* temp = paddingamounts_;
  paddingamounts_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::BorderAmounts* PaddingLayerParams::_internal_mutable_paddingamounts() {
  
  if (paddingamounts_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::BorderAmounts>(GetArenaForAllocation());
    paddingamounts_ = p;
  }
  return paddingamounts_;
}
inline ::CoreML::Specification::BorderAmounts* PaddingLayerParams::mutable_paddingamounts() {
  ::CoreML::Specification::BorderAmounts* _msg = _internal_mutable_paddingamounts();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.paddingAmounts)
  return _msg;
}
inline void PaddingLayerParams::set_allocated_paddingamounts(::CoreML::Specification::BorderAmounts* paddingamounts) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete paddingamounts_;
  }
  if (paddingamounts) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::BorderAmounts>::GetOwningArena(paddingamounts);
    if (message_arena != submessage_arena) {
      paddingamounts = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, paddingamounts, submessage_arena);
    }
    
  } else {
    
  }
  paddingamounts_ = paddingamounts;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.paddingAmounts)
}

inline bool PaddingLayerParams::has_PaddingType() const {
  return PaddingType_case() != PADDINGTYPE_NOT_SET;
}
inline void PaddingLayerParams::clear_has_PaddingType() {
  _oneof_case_[0] = PADDINGTYPE_NOT_SET;
}
inline PaddingLayerParams::PaddingTypeCase PaddingLayerParams::PaddingType_case() const {
  return PaddingLayerParams::PaddingTypeCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// ConcatLayerParams

// bool sequenceConcat = 100;
inline void ConcatLayerParams::clear_sequenceconcat() {
  sequenceconcat_ = false;
}
inline bool ConcatLayerParams::_internal_sequenceconcat() const {
  return sequenceconcat_;
}
inline bool ConcatLayerParams::sequenceconcat() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConcatLayerParams.sequenceConcat)
  return _internal_sequenceconcat();
}
inline void ConcatLayerParams::_internal_set_sequenceconcat(bool value) {
  
  sequenceconcat_ = value;
}
inline void ConcatLayerParams::set_sequenceconcat(bool value) {
  _internal_set_sequenceconcat(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConcatLayerParams.sequenceConcat)
}

// -------------------------------------------------------------------

// LRNLayerParams

// float alpha = 1;
inline void LRNLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float LRNLayerParams::_internal_alpha() const {
  return alpha_;
}
inline float LRNLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.alpha)
  return _internal_alpha();
}
inline void LRNLayerParams::_internal_set_alpha(float value) {
  
  alpha_ = value;
}
inline void LRNLayerParams::set_alpha(float value) {
  _internal_set_alpha(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.alpha)
}

// float beta = 2;
inline void LRNLayerParams::clear_beta() {
  beta_ = 0;
}
inline float LRNLayerParams::_internal_beta() const {
  return beta_;
}
inline float LRNLayerParams::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.beta)
  return _internal_beta();
}
inline void LRNLayerParams::_internal_set_beta(float value) {
  
  beta_ = value;
}
inline void LRNLayerParams::set_beta(float value) {
  _internal_set_beta(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.beta)
}

// uint64 localSize = 3;
inline void LRNLayerParams::clear_localsize() {
  localsize_ = uint64_t{0u};
}
inline uint64_t LRNLayerParams::_internal_localsize() const {
  return localsize_;
}
inline uint64_t LRNLayerParams::localsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.localSize)
  return _internal_localsize();
}
inline void LRNLayerParams::_internal_set_localsize(uint64_t value) {
  
  localsize_ = value;
}
inline void LRNLayerParams::set_localsize(uint64_t value) {
  _internal_set_localsize(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.localSize)
}

// float k = 4;
inline void LRNLayerParams::clear_k() {
  k_ = 0;
}
inline float LRNLayerParams::_internal_k() const {
  return k_;
}
inline float LRNLayerParams::k() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.k)
  return _internal_k();
}
inline void LRNLayerParams::_internal_set_k(float value) {
  
  k_ = value;
}
inline void LRNLayerParams::set_k(float value) {
  _internal_set_k(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.k)
}

// -------------------------------------------------------------------

// SoftmaxLayerParams

// -------------------------------------------------------------------

// SplitLayerParams

// uint64 nOutputs = 1;
inline void SplitLayerParams::clear_noutputs() {
  noutputs_ = uint64_t{0u};
}
inline uint64_t SplitLayerParams::_internal_noutputs() const {
  return noutputs_;
}
inline uint64_t SplitLayerParams::noutputs() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SplitLayerParams.nOutputs)
  return _internal_noutputs();
}
inline void SplitLayerParams::_internal_set_noutputs(uint64_t value) {
  
  noutputs_ = value;
}
inline void SplitLayerParams::set_noutputs(uint64_t value) {
  _internal_set_noutputs(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SplitLayerParams.nOutputs)
}

// -------------------------------------------------------------------

// AddLayerParams

// float alpha = 1;
inline void AddLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float AddLayerParams::_internal_alpha() const {
  return alpha_;
}
inline float AddLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AddLayerParams.alpha)
  return _internal_alpha();
}
inline void AddLayerParams::_internal_set_alpha(float value) {
  
  alpha_ = value;
}
inline void AddLayerParams::set_alpha(float value) {
  _internal_set_alpha(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.AddLayerParams.alpha)
}

// -------------------------------------------------------------------

// MultiplyLayerParams

// float alpha = 1;
inline void MultiplyLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float MultiplyLayerParams::_internal_alpha() const {
  return alpha_;
}
inline float MultiplyLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MultiplyLayerParams.alpha)
  return _internal_alpha();
}
inline void MultiplyLayerParams::_internal_set_alpha(float value) {
  
  alpha_ = value;
}
inline void MultiplyLayerParams::set_alpha(float value) {
  _internal_set_alpha(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.MultiplyLayerParams.alpha)
}

// -------------------------------------------------------------------

// UnaryFunctionLayerParams

// .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
inline void UnaryFunctionLayerParams::clear_type() {
  type_ = 0;
}
inline ::CoreML::Specification::UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::_internal_type() const {
  return static_cast< ::CoreML::Specification::UnaryFunctionLayerParams_Operation >(type_);
}
inline ::CoreML::Specification::UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::type() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.type)
  return _internal_type();
}
inline void UnaryFunctionLayerParams::_internal_set_type(::CoreML::Specification::UnaryFunctionLayerParams_Operation value) {
  
  type_ = value;
}
inline void UnaryFunctionLayerParams::set_type(::CoreML::Specification::UnaryFunctionLayerParams_Operation value) {
  _internal_set_type(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.type)
}

// float alpha = 2;
inline void UnaryFunctionLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float UnaryFunctionLayerParams::_internal_alpha() const {
  return alpha_;
}
inline float UnaryFunctionLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.alpha)
  return _internal_alpha();
}
inline void UnaryFunctionLayerParams::_internal_set_alpha(float value) {
  
  alpha_ = value;
}
inline void UnaryFunctionLayerParams::set_alpha(float value) {
  _internal_set_alpha(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.alpha)
}

// float epsilon = 3;
inline void UnaryFunctionLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
inline float UnaryFunctionLayerParams::_internal_epsilon() const {
  return epsilon_;
}
inline float UnaryFunctionLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.epsilon)
  return _internal_epsilon();
}
inline void UnaryFunctionLayerParams::_internal_set_epsilon(float value) {
  
  epsilon_ = value;
}
inline void UnaryFunctionLayerParams::set_epsilon(float value) {
  _internal_set_epsilon(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.epsilon)
}

// float shift = 4;
inline void UnaryFunctionLayerParams::clear_shift() {
  shift_ = 0;
}
inline float UnaryFunctionLayerParams::_internal_shift() const {
  return shift_;
}
inline float UnaryFunctionLayerParams::shift() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.shift)
  return _internal_shift();
}
inline void UnaryFunctionLayerParams::_internal_set_shift(float value) {
  
  shift_ = value;
}
inline void UnaryFunctionLayerParams::set_shift(float value) {
  _internal_set_shift(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.shift)
}

// float scale = 5;
inline void UnaryFunctionLayerParams::clear_scale() {
  scale_ = 0;
}
inline float UnaryFunctionLayerParams::_internal_scale() const {
  return scale_;
}
inline float UnaryFunctionLayerParams::scale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.scale)
  return _internal_scale();
}
inline void UnaryFunctionLayerParams::_internal_set_scale(float value) {
  
  scale_ = value;
}
inline void UnaryFunctionLayerParams::set_scale(float value) {
  _internal_set_scale(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.scale)
}

// -------------------------------------------------------------------

// UpsampleLayerParams

// repeated uint64 scalingFactor = 1;
inline int UpsampleLayerParams::_internal_scalingfactor_size() const {
  return scalingfactor_.size();
}
inline int UpsampleLayerParams::scalingfactor_size() const {
  return _internal_scalingfactor_size();
}
inline void UpsampleLayerParams::clear_scalingfactor() {
  scalingfactor_.Clear();
}
inline uint64_t UpsampleLayerParams::_internal_scalingfactor(int index) const {
  return scalingfactor_.Get(index);
}
inline uint64_t UpsampleLayerParams::scalingfactor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UpsampleLayerParams.scalingFactor)
  return _internal_scalingfactor(index);
}
inline void UpsampleLayerParams::set_scalingfactor(int index, uint64_t value) {
  scalingfactor_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.UpsampleLayerParams.scalingFactor)
}
inline void UpsampleLayerParams::_internal_add_scalingfactor(uint64_t value) {
  scalingfactor_.Add(value);
}
inline void UpsampleLayerParams::add_scalingfactor(uint64_t value) {
  _internal_add_scalingfactor(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.UpsampleLayerParams.scalingFactor)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
UpsampleLayerParams::_internal_scalingfactor() const {
  return scalingfactor_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
UpsampleLayerParams::scalingfactor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.UpsampleLayerParams.scalingFactor)
  return _internal_scalingfactor();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
UpsampleLayerParams::_internal_mutable_scalingfactor() {
  return &scalingfactor_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
UpsampleLayerParams::mutable_scalingfactor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.UpsampleLayerParams.scalingFactor)
  return _internal_mutable_scalingfactor();
}

// repeated float fractionalScalingFactor = 7;
inline int UpsampleLayerParams::_internal_fractionalscalingfactor_size() const {
  return fractionalscalingfactor_.size();
}
inline int UpsampleLayerParams::fractionalscalingfactor_size() const {
  return _internal_fractionalscalingfactor_size();
}
inline void UpsampleLayerParams::clear_fractionalscalingfactor() {
  fractionalscalingfactor_.Clear();
}
inline float UpsampleLayerParams::_internal_fractionalscalingfactor(int index) const {
  return fractionalscalingfactor_.Get(index);
}
inline float UpsampleLayerParams::fractionalscalingfactor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UpsampleLayerParams.fractionalScalingFactor)
  return _internal_fractionalscalingfactor(index);
}
inline void UpsampleLayerParams::set_fractionalscalingfactor(int index, float value) {
  fractionalscalingfactor_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.UpsampleLayerParams.fractionalScalingFactor)
}
inline void UpsampleLayerParams::_internal_add_fractionalscalingfactor(float value) {
  fractionalscalingfactor_.Add(value);
}
inline void UpsampleLayerParams::add_fractionalscalingfactor(float value) {
  _internal_add_fractionalscalingfactor(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.UpsampleLayerParams.fractionalScalingFactor)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
UpsampleLayerParams::_internal_fractionalscalingfactor() const {
  return fractionalscalingfactor_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >&
UpsampleLayerParams::fractionalscalingfactor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.UpsampleLayerParams.fractionalScalingFactor)
  return _internal_fractionalscalingfactor();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
UpsampleLayerParams::_internal_mutable_fractionalscalingfactor() {
  return &fractionalscalingfactor_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< float >*
UpsampleLayerParams::mutable_fractionalscalingfactor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.UpsampleLayerParams.fractionalScalingFactor)
  return _internal_mutable_fractionalscalingfactor();
}

// .CoreML.Specification.UpsampleLayerParams.InterpolationMode mode = 5;
inline void UpsampleLayerParams::clear_mode() {
  mode_ = 0;
}
inline ::CoreML::Specification::UpsampleLayerParams_InterpolationMode UpsampleLayerParams::_internal_mode() const {
  return static_cast< ::CoreML::Specification::UpsampleLayerParams_InterpolationMode >(mode_);
}
inline ::CoreML::Specification::UpsampleLayerParams_InterpolationMode UpsampleLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UpsampleLayerParams.mode)
  return _internal_mode();
}
inline void UpsampleLayerParams::_internal_set_mode(::CoreML::Specification::UpsampleLayerParams_InterpolationMode value) {
  
  mode_ = value;
}
inline void UpsampleLayerParams::set_mode(::CoreML::Specification::UpsampleLayerParams_InterpolationMode value) {
  _internal_set_mode(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.UpsampleLayerParams.mode)
}

// .CoreML.Specification.UpsampleLayerParams.LinearUpsampleMode linearUpsampleMode = 6;
inline void UpsampleLayerParams::clear_linearupsamplemode() {
  linearupsamplemode_ = 0;
}
inline ::CoreML::Specification::UpsampleLayerParams_LinearUpsampleMode UpsampleLayerParams::_internal_linearupsamplemode() const {
  return static_cast< ::CoreML::Specification::UpsampleLayerParams_LinearUpsampleMode >(linearupsamplemode_);
}
inline ::CoreML::Specification::UpsampleLayerParams_LinearUpsampleMode UpsampleLayerParams::linearupsamplemode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UpsampleLayerParams.linearUpsampleMode)
  return _internal_linearupsamplemode();
}
inline void UpsampleLayerParams::_internal_set_linearupsamplemode(::CoreML::Specification::UpsampleLayerParams_LinearUpsampleMode value) {
  
  linearupsamplemode_ = value;
}
inline void UpsampleLayerParams::set_linearupsamplemode(::CoreML::Specification::UpsampleLayerParams_LinearUpsampleMode value) {
  _internal_set_linearupsamplemode(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.UpsampleLayerParams.linearUpsampleMode)
}

// -------------------------------------------------------------------

// ResizeBilinearLayerParams

// repeated uint64 targetSize = 1;
inline int ResizeBilinearLayerParams::_internal_targetsize_size() const {
  return targetsize_.size();
}
inline int ResizeBilinearLayerParams::targetsize_size() const {
  return _internal_targetsize_size();
}
inline void ResizeBilinearLayerParams::clear_targetsize() {
  targetsize_.Clear();
}
inline uint64_t ResizeBilinearLayerParams::_internal_targetsize(int index) const {
  return targetsize_.Get(index);
}
inline uint64_t ResizeBilinearLayerParams::targetsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ResizeBilinearLayerParams.targetSize)
  return _internal_targetsize(index);
}
inline void ResizeBilinearLayerParams::set_targetsize(int index, uint64_t value) {
  targetsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ResizeBilinearLayerParams.targetSize)
}
inline void ResizeBilinearLayerParams::_internal_add_targetsize(uint64_t value) {
  targetsize_.Add(value);
}
inline void ResizeBilinearLayerParams::add_targetsize(uint64_t value) {
  _internal_add_targetsize(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ResizeBilinearLayerParams.targetSize)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
ResizeBilinearLayerParams::_internal_targetsize() const {
  return targetsize_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
ResizeBilinearLayerParams::targetsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ResizeBilinearLayerParams.targetSize)
  return _internal_targetsize();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
ResizeBilinearLayerParams::_internal_mutable_targetsize() {
  return &targetsize_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
ResizeBilinearLayerParams::mutable_targetsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ResizeBilinearLayerParams.targetSize)
  return _internal_mutable_targetsize();
}

// .CoreML.Specification.SamplingMode mode = 2;
inline bool ResizeBilinearLayerParams::_internal_has_mode() const {
  return this != internal_default_instance() && mode_ != nullptr;
}
inline bool ResizeBilinearLayerParams::has_mode() const {
  return _internal_has_mode();
}
inline void ResizeBilinearLayerParams::clear_mode() {
  if (GetArenaForAllocation() == nullptr && mode_ != nullptr) {
    delete mode_;
  }
  mode_ = nullptr;
}
inline const ::CoreML::Specification::SamplingMode& ResizeBilinearLayerParams::_internal_mode() const {
  const ::CoreML::Specification::SamplingMode* p = mode_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::SamplingMode&>(
      ::CoreML::Specification::_SamplingMode_default_instance_);
}
inline const ::CoreML::Specification::SamplingMode& ResizeBilinearLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ResizeBilinearLayerParams.mode)
  return _internal_mode();
}
inline void ResizeBilinearLayerParams::unsafe_arena_set_allocated_mode(
    ::CoreML::Specification::SamplingMode* mode) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(mode_);
  }
  mode_ = mode;
  if (mode) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ResizeBilinearLayerParams.mode)
}
inline ::CoreML::Specification::SamplingMode* ResizeBilinearLayerParams::release_mode() {
  
  ::CoreML::Specification::SamplingMode* temp = mode_;
  mode_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::SamplingMode* ResizeBilinearLayerParams::unsafe_arena_release_mode() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ResizeBilinearLayerParams.mode)
  
  ::CoreML::Specification::SamplingMode* temp = mode_;
  mode_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::SamplingMode* ResizeBilinearLayerParams::_internal_mutable_mode() {
  
  if (mode_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::SamplingMode>(GetArenaForAllocation());
    mode_ = p;
  }
  return mode_;
}
inline ::CoreML::Specification::SamplingMode* ResizeBilinearLayerParams::mutable_mode() {
  ::CoreML::Specification::SamplingMode* _msg = _internal_mutable_mode();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ResizeBilinearLayerParams.mode)
  return _msg;
}
inline void ResizeBilinearLayerParams::set_allocated_mode(::CoreML::Specification::SamplingMode* mode) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete mode_;
  }
  if (mode) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::SamplingMode>::GetOwningArena(mode);
    if (message_arena != submessage_arena) {
      mode = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, mode, submessage_arena);
    }
    
  } else {
    
  }
  mode_ = mode;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ResizeBilinearLayerParams.mode)
}

// -------------------------------------------------------------------

// CropResizeLayerParams

// repeated uint64 targetSize = 1;
inline int CropResizeLayerParams::_internal_targetsize_size() const {
  return targetsize_.size();
}
inline int CropResizeLayerParams::targetsize_size() const {
  return _internal_targetsize_size();
}
inline void CropResizeLayerParams::clear_targetsize() {
  targetsize_.Clear();
}
inline uint64_t CropResizeLayerParams::_internal_targetsize(int index) const {
  return targetsize_.Get(index);
}
inline uint64_t CropResizeLayerParams::targetsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropResizeLayerParams.targetSize)
  return _internal_targetsize(index);
}
inline void CropResizeLayerParams::set_targetsize(int index, uint64_t value) {
  targetsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CropResizeLayerParams.targetSize)
}
inline void CropResizeLayerParams::_internal_add_targetsize(uint64_t value) {
  targetsize_.Add(value);
}
inline void CropResizeLayerParams::add_targetsize(uint64_t value) {
  _internal_add_targetsize(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.CropResizeLayerParams.targetSize)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
CropResizeLayerParams::_internal_targetsize() const {
  return targetsize_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
CropResizeLayerParams::targetsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.CropResizeLayerParams.targetSize)
  return _internal_targetsize();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
CropResizeLayerParams::_internal_mutable_targetsize() {
  return &targetsize_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
CropResizeLayerParams::mutable_targetsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.CropResizeLayerParams.targetSize)
  return _internal_mutable_targetsize();
}

// bool normalizedCoordinates = 2;
inline void CropResizeLayerParams::clear_normalizedcoordinates() {
  normalizedcoordinates_ = false;
}
inline bool CropResizeLayerParams::_internal_normalizedcoordinates() const {
  return normalizedcoordinates_;
}
inline bool CropResizeLayerParams::normalizedcoordinates() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropResizeLayerParams.normalizedCoordinates)
  return _internal_normalizedcoordinates();
}
inline void CropResizeLayerParams::_internal_set_normalizedcoordinates(bool value) {
  
  normalizedcoordinates_ = value;
}
inline void CropResizeLayerParams::set_normalizedcoordinates(bool value) {
  _internal_set_normalizedcoordinates(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CropResizeLayerParams.normalizedCoordinates)
}

// .CoreML.Specification.SamplingMode mode = 3;
inline bool CropResizeLayerParams::_internal_has_mode() const {
  return this != internal_default_instance() && mode_ != nullptr;
}
inline bool CropResizeLayerParams::has_mode() const {
  return _internal_has_mode();
}
inline void CropResizeLayerParams::clear_mode() {
  if (GetArenaForAllocation() == nullptr && mode_ != nullptr) {
    delete mode_;
  }
  mode_ = nullptr;
}
inline const ::CoreML::Specification::SamplingMode& CropResizeLayerParams::_internal_mode() const {
  const ::CoreML::Specification::SamplingMode* p = mode_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::SamplingMode&>(
      ::CoreML::Specification::_SamplingMode_default_instance_);
}
inline const ::CoreML::Specification::SamplingMode& CropResizeLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropResizeLayerParams.mode)
  return _internal_mode();
}
inline void CropResizeLayerParams::unsafe_arena_set_allocated_mode(
    ::CoreML::Specification::SamplingMode* mode) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(mode_);
  }
  mode_ = mode;
  if (mode) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.CropResizeLayerParams.mode)
}
inline ::CoreML::Specification::SamplingMode* CropResizeLayerParams::release_mode() {
  
  ::CoreML::Specification::SamplingMode* temp = mode_;
  mode_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::SamplingMode* CropResizeLayerParams::unsafe_arena_release_mode() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CropResizeLayerParams.mode)
  
  ::CoreML::Specification::SamplingMode* temp = mode_;
  mode_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::SamplingMode* CropResizeLayerParams::_internal_mutable_mode() {
  
  if (mode_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::SamplingMode>(GetArenaForAllocation());
    mode_ = p;
  }
  return mode_;
}
inline ::CoreML::Specification::SamplingMode* CropResizeLayerParams::mutable_mode() {
  ::CoreML::Specification::SamplingMode* _msg = _internal_mutable_mode();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CropResizeLayerParams.mode)
  return _msg;
}
inline void CropResizeLayerParams::set_allocated_mode(::CoreML::Specification::SamplingMode* mode) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete mode_;
  }
  if (mode) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::SamplingMode>::GetOwningArena(mode);
    if (message_arena != submessage_arena) {
      mode = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, mode, submessage_arena);
    }
    
  } else {
    
  }
  mode_ = mode;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CropResizeLayerParams.mode)
}

// .CoreML.Specification.BoxCoordinatesMode boxIndicesMode = 4;
inline bool CropResizeLayerParams::_internal_has_boxindicesmode() const {
  return this != internal_default_instance() && boxindicesmode_ != nullptr;
}
inline bool CropResizeLayerParams::has_boxindicesmode() const {
  return _internal_has_boxindicesmode();
}
inline void CropResizeLayerParams::clear_boxindicesmode() {
  if (GetArenaForAllocation() == nullptr && boxindicesmode_ != nullptr) {
    delete boxindicesmode_;
  }
  boxindicesmode_ = nullptr;
}
inline const ::CoreML::Specification::BoxCoordinatesMode& CropResizeLayerParams::_internal_boxindicesmode() const {
  const ::CoreML::Specification::BoxCoordinatesMode* p = boxindicesmode_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::BoxCoordinatesMode&>(
      ::CoreML::Specification::_BoxCoordinatesMode_default_instance_);
}
inline const ::CoreML::Specification::BoxCoordinatesMode& CropResizeLayerParams::boxindicesmode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropResizeLayerParams.boxIndicesMode)
  return _internal_boxindicesmode();
}
inline void CropResizeLayerParams::unsafe_arena_set_allocated_boxindicesmode(
    ::CoreML::Specification::BoxCoordinatesMode* boxindicesmode) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(boxindicesmode_);
  }
  boxindicesmode_ = boxindicesmode;
  if (boxindicesmode) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.CropResizeLayerParams.boxIndicesMode)
}
inline ::CoreML::Specification::BoxCoordinatesMode* CropResizeLayerParams::release_boxindicesmode() {
  
  ::CoreML::Specification::BoxCoordinatesMode* temp = boxindicesmode_;
  boxindicesmode_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::BoxCoordinatesMode* CropResizeLayerParams::unsafe_arena_release_boxindicesmode() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CropResizeLayerParams.boxIndicesMode)
  
  ::CoreML::Specification::BoxCoordinatesMode* temp = boxindicesmode_;
  boxindicesmode_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::BoxCoordinatesMode* CropResizeLayerParams::_internal_mutable_boxindicesmode() {
  
  if (boxindicesmode_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::BoxCoordinatesMode>(GetArenaForAllocation());
    boxindicesmode_ = p;
  }
  return boxindicesmode_;
}
inline ::CoreML::Specification::BoxCoordinatesMode* CropResizeLayerParams::mutable_boxindicesmode() {
  ::CoreML::Specification::BoxCoordinatesMode* _msg = _internal_mutable_boxindicesmode();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CropResizeLayerParams.boxIndicesMode)
  return _msg;
}
inline void CropResizeLayerParams::set_allocated_boxindicesmode(::CoreML::Specification::BoxCoordinatesMode* boxindicesmode) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete boxindicesmode_;
  }
  if (boxindicesmode) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::BoxCoordinatesMode>::GetOwningArena(boxindicesmode);
    if (message_arena != submessage_arena) {
      boxindicesmode = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, boxindicesmode, submessage_arena);
    }
    
  } else {
    
  }
  boxindicesmode_ = boxindicesmode;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CropResizeLayerParams.boxIndicesMode)
}

// float spatialScale = 5;
inline void CropResizeLayerParams::clear_spatialscale() {
  spatialscale_ = 0;
}
inline float CropResizeLayerParams::_internal_spatialscale() const {
  return spatialscale_;
}
inline float CropResizeLayerParams::spatialscale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropResizeLayerParams.spatialScale)
  return _internal_spatialscale();
}
inline void CropResizeLayerParams::_internal_set_spatialscale(float value) {
  
  spatialscale_ = value;
}
inline void CropResizeLayerParams::set_spatialscale(float value) {
  _internal_set_spatialscale(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CropResizeLayerParams.spatialScale)
}

// -------------------------------------------------------------------

// BiasLayerParams

// repeated uint64 shape = 1;
inline int BiasLayerParams::_internal_shape_size() const {
  return shape_.size();
}
inline int BiasLayerParams::shape_size() const {
  return _internal_shape_size();
}
inline void BiasLayerParams::clear_shape() {
  shape_.Clear();
}
inline uint64_t BiasLayerParams::_internal_shape(int index) const {
  return shape_.Get(index);
}
inline uint64_t BiasLayerParams::shape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiasLayerParams.shape)
  return _internal_shape(index);
}
inline void BiasLayerParams::set_shape(int index, uint64_t value) {
  shape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BiasLayerParams.shape)
}
inline void BiasLayerParams::_internal_add_shape(uint64_t value) {
  shape_.Add(value);
}
inline void BiasLayerParams::add_shape(uint64_t value) {
  _internal_add_shape(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiasLayerParams.shape)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
BiasLayerParams::_internal_shape() const {
  return shape_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
BiasLayerParams::shape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiasLayerParams.shape)
  return _internal_shape();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
BiasLayerParams::_internal_mutable_shape() {
  return &shape_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
BiasLayerParams::mutable_shape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiasLayerParams.shape)
  return _internal_mutable_shape();
}

// .CoreML.Specification.WeightParams bias = 2;
inline bool BiasLayerParams::_internal_has_bias() const {
  return this != internal_default_instance() && bias_ != nullptr;
}
inline bool BiasLayerParams::has_bias() const {
  return _internal_has_bias();
}
inline void BiasLayerParams::clear_bias() {
  if (GetArenaForAllocation() == nullptr && bias_ != nullptr) {
    delete bias_;
  }
  bias_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& BiasLayerParams::_internal_bias() const {
  const ::CoreML::Specification::WeightParams* p = bias_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& BiasLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiasLayerParams.bias)
  return _internal_bias();
}
inline void BiasLayerParams::unsafe_arena_set_allocated_bias(
    ::CoreML::Specification::WeightParams* bias) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(bias_);
  }
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.BiasLayerParams.bias)
}
inline ::CoreML::Specification::WeightParams* BiasLayerParams::release_bias() {
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* BiasLayerParams::unsafe_arena_release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BiasLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* BiasLayerParams::_internal_mutable_bias() {
  
  if (bias_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    bias_ = p;
  }
  return bias_;
}
inline ::CoreML::Specification::WeightParams* BiasLayerParams::mutable_bias() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_bias();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiasLayerParams.bias)
  return _msg;
}
inline void BiasLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete bias_;
  }
  if (bias) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(bias);
    if (message_arena != submessage_arena) {
      bias = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, bias, submessage_arena);
    }
    
  } else {
    
  }
  bias_ = bias;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BiasLayerParams.bias)
}

// -------------------------------------------------------------------

// ScaleLayerParams

// repeated uint64 shapeScale = 1;
inline int ScaleLayerParams::_internal_shapescale_size() const {
  return shapescale_.size();
}
inline int ScaleLayerParams::shapescale_size() const {
  return _internal_shapescale_size();
}
inline void ScaleLayerParams::clear_shapescale() {
  shapescale_.Clear();
}
inline uint64_t ScaleLayerParams::_internal_shapescale(int index) const {
  return shapescale_.Get(index);
}
inline uint64_t ScaleLayerParams::shapescale(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.shapeScale)
  return _internal_shapescale(index);
}
inline void ScaleLayerParams::set_shapescale(int index, uint64_t value) {
  shapescale_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScaleLayerParams.shapeScale)
}
inline void ScaleLayerParams::_internal_add_shapescale(uint64_t value) {
  shapescale_.Add(value);
}
inline void ScaleLayerParams::add_shapescale(uint64_t value) {
  _internal_add_shapescale(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ScaleLayerParams.shapeScale)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
ScaleLayerParams::_internal_shapescale() const {
  return shapescale_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
ScaleLayerParams::shapescale() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ScaleLayerParams.shapeScale)
  return _internal_shapescale();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
ScaleLayerParams::_internal_mutable_shapescale() {
  return &shapescale_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
ScaleLayerParams::mutable_shapescale() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ScaleLayerParams.shapeScale)
  return _internal_mutable_shapescale();
}

// .CoreML.Specification.WeightParams scale = 2;
inline bool ScaleLayerParams::_internal_has_scale() const {
  return this != internal_default_instance() && scale_ != nullptr;
}
inline bool ScaleLayerParams::has_scale() const {
  return _internal_has_scale();
}
inline void ScaleLayerParams::clear_scale() {
  if (GetArenaForAllocation() == nullptr && scale_ != nullptr) {
    delete scale_;
  }
  scale_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& ScaleLayerParams::_internal_scale() const {
  const ::CoreML::Specification::WeightParams* p = scale_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& ScaleLayerParams::scale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.scale)
  return _internal_scale();
}
inline void ScaleLayerParams::unsafe_arena_set_allocated_scale(
    ::CoreML::Specification::WeightParams* scale) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(scale_);
  }
  scale_ = scale;
  if (scale) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ScaleLayerParams.scale)
}
inline ::CoreML::Specification::WeightParams* ScaleLayerParams::release_scale() {
  
  ::CoreML::Specification::WeightParams* temp = scale_;
  scale_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* ScaleLayerParams::unsafe_arena_release_scale() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ScaleLayerParams.scale)
  
  ::CoreML::Specification::WeightParams* temp = scale_;
  scale_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* ScaleLayerParams::_internal_mutable_scale() {
  
  if (scale_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    scale_ = p;
  }
  return scale_;
}
inline ::CoreML::Specification::WeightParams* ScaleLayerParams::mutable_scale() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_scale();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ScaleLayerParams.scale)
  return _msg;
}
inline void ScaleLayerParams::set_allocated_scale(::CoreML::Specification::WeightParams* scale) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete scale_;
  }
  if (scale) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(scale);
    if (message_arena != submessage_arena) {
      scale = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, scale, submessage_arena);
    }
    
  } else {
    
  }
  scale_ = scale;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ScaleLayerParams.scale)
}

// bool hasBias = 3;
inline void ScaleLayerParams::clear_hasbias() {
  hasbias_ = false;
}
inline bool ScaleLayerParams::_internal_hasbias() const {
  return hasbias_;
}
inline bool ScaleLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.hasBias)
  return _internal_hasbias();
}
inline void ScaleLayerParams::_internal_set_hasbias(bool value) {
  
  hasbias_ = value;
}
inline void ScaleLayerParams::set_hasbias(bool value) {
  _internal_set_hasbias(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScaleLayerParams.hasBias)
}

// repeated uint64 shapeBias = 4;
inline int ScaleLayerParams::_internal_shapebias_size() const {
  return shapebias_.size();
}
inline int ScaleLayerParams::shapebias_size() const {
  return _internal_shapebias_size();
}
inline void ScaleLayerParams::clear_shapebias() {
  shapebias_.Clear();
}
inline uint64_t ScaleLayerParams::_internal_shapebias(int index) const {
  return shapebias_.Get(index);
}
inline uint64_t ScaleLayerParams::shapebias(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.shapeBias)
  return _internal_shapebias(index);
}
inline void ScaleLayerParams::set_shapebias(int index, uint64_t value) {
  shapebias_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScaleLayerParams.shapeBias)
}
inline void ScaleLayerParams::_internal_add_shapebias(uint64_t value) {
  shapebias_.Add(value);
}
inline void ScaleLayerParams::add_shapebias(uint64_t value) {
  _internal_add_shapebias(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ScaleLayerParams.shapeBias)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
ScaleLayerParams::_internal_shapebias() const {
  return shapebias_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
ScaleLayerParams::shapebias() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ScaleLayerParams.shapeBias)
  return _internal_shapebias();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
ScaleLayerParams::_internal_mutable_shapebias() {
  return &shapebias_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
ScaleLayerParams::mutable_shapebias() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ScaleLayerParams.shapeBias)
  return _internal_mutable_shapebias();
}

// .CoreML.Specification.WeightParams bias = 5;
inline bool ScaleLayerParams::_internal_has_bias() const {
  return this != internal_default_instance() && bias_ != nullptr;
}
inline bool ScaleLayerParams::has_bias() const {
  return _internal_has_bias();
}
inline void ScaleLayerParams::clear_bias() {
  if (GetArenaForAllocation() == nullptr && bias_ != nullptr) {
    delete bias_;
  }
  bias_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& ScaleLayerParams::_internal_bias() const {
  const ::CoreML::Specification::WeightParams* p = bias_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& ScaleLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.bias)
  return _internal_bias();
}
inline void ScaleLayerParams::unsafe_arena_set_allocated_bias(
    ::CoreML::Specification::WeightParams* bias) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(bias_);
  }
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.ScaleLayerParams.bias)
}
inline ::CoreML::Specification::WeightParams* ScaleLayerParams::release_bias() {
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* ScaleLayerParams::unsafe_arena_release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ScaleLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* ScaleLayerParams::_internal_mutable_bias() {
  
  if (bias_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    bias_ = p;
  }
  return bias_;
}
inline ::CoreML::Specification::WeightParams* ScaleLayerParams::mutable_bias() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_bias();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ScaleLayerParams.bias)
  return _msg;
}
inline void ScaleLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete bias_;
  }
  if (bias) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(bias);
    if (message_arena != submessage_arena) {
      bias = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, bias, submessage_arena);
    }
    
  } else {
    
  }
  bias_ = bias;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ScaleLayerParams.bias)
}

// -------------------------------------------------------------------

// LoadConstantLayerParams

// repeated uint64 shape = 1;
inline int LoadConstantLayerParams::_internal_shape_size() const {
  return shape_.size();
}
inline int LoadConstantLayerParams::shape_size() const {
  return _internal_shape_size();
}
inline void LoadConstantLayerParams::clear_shape() {
  shape_.Clear();
}
inline uint64_t LoadConstantLayerParams::_internal_shape(int index) const {
  return shape_.Get(index);
}
inline uint64_t LoadConstantLayerParams::shape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantLayerParams.shape)
  return _internal_shape(index);
}
inline void LoadConstantLayerParams::set_shape(int index, uint64_t value) {
  shape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LoadConstantLayerParams.shape)
}
inline void LoadConstantLayerParams::_internal_add_shape(uint64_t value) {
  shape_.Add(value);
}
inline void LoadConstantLayerParams::add_shape(uint64_t value) {
  _internal_add_shape(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LoadConstantLayerParams.shape)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
LoadConstantLayerParams::_internal_shape() const {
  return shape_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
LoadConstantLayerParams::shape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LoadConstantLayerParams.shape)
  return _internal_shape();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
LoadConstantLayerParams::_internal_mutable_shape() {
  return &shape_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
LoadConstantLayerParams::mutable_shape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LoadConstantLayerParams.shape)
  return _internal_mutable_shape();
}

// .CoreML.Specification.WeightParams data = 2;
inline bool LoadConstantLayerParams::_internal_has_data() const {
  return this != internal_default_instance() && data_ != nullptr;
}
inline bool LoadConstantLayerParams::has_data() const {
  return _internal_has_data();
}
inline void LoadConstantLayerParams::clear_data() {
  if (GetArenaForAllocation() == nullptr && data_ != nullptr) {
    delete data_;
  }
  data_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& LoadConstantLayerParams::_internal_data() const {
  const ::CoreML::Specification::WeightParams* p = data_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& LoadConstantLayerParams::data() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantLayerParams.data)
  return _internal_data();
}
inline void LoadConstantLayerParams::unsafe_arena_set_allocated_data(
    ::CoreML::Specification::WeightParams* data) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(data_);
  }
  data_ = data;
  if (data) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LoadConstantLayerParams.data)
}
inline ::CoreML::Specification::WeightParams* LoadConstantLayerParams::release_data() {
  
  ::CoreML::Specification::WeightParams* temp = data_;
  data_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* LoadConstantLayerParams::unsafe_arena_release_data() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoadConstantLayerParams.data)
  
  ::CoreML::Specification::WeightParams* temp = data_;
  data_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* LoadConstantLayerParams::_internal_mutable_data() {
  
  if (data_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    data_ = p;
  }
  return data_;
}
inline ::CoreML::Specification::WeightParams* LoadConstantLayerParams::mutable_data() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_data();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoadConstantLayerParams.data)
  return _msg;
}
inline void LoadConstantLayerParams::set_allocated_data(::CoreML::Specification::WeightParams* data) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete data_;
  }
  if (data) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(data);
    if (message_arena != submessage_arena) {
      data = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, data, submessage_arena);
    }
    
  } else {
    
  }
  data_ = data;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoadConstantLayerParams.data)
}

// -------------------------------------------------------------------

// L2NormalizeLayerParams

// float epsilon = 1;
inline void L2NormalizeLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
inline float L2NormalizeLayerParams::_internal_epsilon() const {
  return epsilon_;
}
inline float L2NormalizeLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.L2NormalizeLayerParams.epsilon)
  return _internal_epsilon();
}
inline void L2NormalizeLayerParams::_internal_set_epsilon(float value) {
  
  epsilon_ = value;
}
inline void L2NormalizeLayerParams::set_epsilon(float value) {
  _internal_set_epsilon(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.L2NormalizeLayerParams.epsilon)
}

// -------------------------------------------------------------------

// FlattenLayerParams

// .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
inline void FlattenLayerParams::clear_mode() {
  mode_ = 0;
}
inline ::CoreML::Specification::FlattenLayerParams_FlattenOrder FlattenLayerParams::_internal_mode() const {
  return static_cast< ::CoreML::Specification::FlattenLayerParams_FlattenOrder >(mode_);
}
inline ::CoreML::Specification::FlattenLayerParams_FlattenOrder FlattenLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FlattenLayerParams.mode)
  return _internal_mode();
}
inline void FlattenLayerParams::_internal_set_mode(::CoreML::Specification::FlattenLayerParams_FlattenOrder value) {
  
  mode_ = value;
}
inline void FlattenLayerParams::set_mode(::CoreML::Specification::FlattenLayerParams_FlattenOrder value) {
  _internal_set_mode(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.FlattenLayerParams.mode)
}

// -------------------------------------------------------------------

// ReshapeLayerParams

// repeated int64 targetShape = 1;
inline int ReshapeLayerParams::_internal_targetshape_size() const {
  return targetshape_.size();
}
inline int ReshapeLayerParams::targetshape_size() const {
  return _internal_targetshape_size();
}
inline void ReshapeLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
inline int64_t ReshapeLayerParams::_internal_targetshape(int index) const {
  return targetshape_.Get(index);
}
inline int64_t ReshapeLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReshapeLayerParams.targetShape)
  return _internal_targetshape(index);
}
inline void ReshapeLayerParams::set_targetshape(int index, int64_t value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReshapeLayerParams.targetShape)
}
inline void ReshapeLayerParams::_internal_add_targetshape(int64_t value) {
  targetshape_.Add(value);
}
inline void ReshapeLayerParams::add_targetshape(int64_t value) {
  _internal_add_targetshape(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReshapeLayerParams.targetShape)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReshapeLayerParams::_internal_targetshape() const {
  return targetshape_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReshapeLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReshapeLayerParams.targetShape)
  return _internal_targetshape();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReshapeLayerParams::_internal_mutable_targetshape() {
  return &targetshape_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReshapeLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReshapeLayerParams.targetShape)
  return _internal_mutable_targetshape();
}

// .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
inline void ReshapeLayerParams::clear_mode() {
  mode_ = 0;
}
inline ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::_internal_mode() const {
  return static_cast< ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder >(mode_);
}
inline ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReshapeLayerParams.mode)
  return _internal_mode();
}
inline void ReshapeLayerParams::_internal_set_mode(::CoreML::Specification::ReshapeLayerParams_ReshapeOrder value) {
  
  mode_ = value;
}
inline void ReshapeLayerParams::set_mode(::CoreML::Specification::ReshapeLayerParams_ReshapeOrder value) {
  _internal_set_mode(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReshapeLayerParams.mode)
}

// -------------------------------------------------------------------

// PermuteLayerParams

// repeated uint64 axis = 1;
inline int PermuteLayerParams::_internal_axis_size() const {
  return axis_.size();
}
inline int PermuteLayerParams::axis_size() const {
  return _internal_axis_size();
}
inline void PermuteLayerParams::clear_axis() {
  axis_.Clear();
}
inline uint64_t PermuteLayerParams::_internal_axis(int index) const {
  return axis_.Get(index);
}
inline uint64_t PermuteLayerParams::axis(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PermuteLayerParams.axis)
  return _internal_axis(index);
}
inline void PermuteLayerParams::set_axis(int index, uint64_t value) {
  axis_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PermuteLayerParams.axis)
}
inline void PermuteLayerParams::_internal_add_axis(uint64_t value) {
  axis_.Add(value);
}
inline void PermuteLayerParams::add_axis(uint64_t value) {
  _internal_add_axis(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PermuteLayerParams.axis)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
PermuteLayerParams::_internal_axis() const {
  return axis_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
PermuteLayerParams::axis() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PermuteLayerParams.axis)
  return _internal_axis();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
PermuteLayerParams::_internal_mutable_axis() {
  return &axis_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
PermuteLayerParams::mutable_axis() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PermuteLayerParams.axis)
  return _internal_mutable_axis();
}

// -------------------------------------------------------------------

// ReorganizeDataLayerParams

// .CoreML.Specification.ReorganizeDataLayerParams.ReorganizationType mode = 1;
inline void ReorganizeDataLayerParams::clear_mode() {
  mode_ = 0;
}
inline ::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams::_internal_mode() const {
  return static_cast< ::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType >(mode_);
}
inline ::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReorganizeDataLayerParams.mode)
  return _internal_mode();
}
inline void ReorganizeDataLayerParams::_internal_set_mode(::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType value) {
  
  mode_ = value;
}
inline void ReorganizeDataLayerParams::set_mode(::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType value) {
  _internal_set_mode(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReorganizeDataLayerParams.mode)
}

// uint64 blockSize = 2;
inline void ReorganizeDataLayerParams::clear_blocksize() {
  blocksize_ = uint64_t{0u};
}
inline uint64_t ReorganizeDataLayerParams::_internal_blocksize() const {
  return blocksize_;
}
inline uint64_t ReorganizeDataLayerParams::blocksize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReorganizeDataLayerParams.blockSize)
  return _internal_blocksize();
}
inline void ReorganizeDataLayerParams::_internal_set_blocksize(uint64_t value) {
  
  blocksize_ = value;
}
inline void ReorganizeDataLayerParams::set_blocksize(uint64_t value) {
  _internal_set_blocksize(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReorganizeDataLayerParams.blockSize)
}

// -------------------------------------------------------------------

// SliceLayerParams

// int64 startIndex = 1;
inline void SliceLayerParams::clear_startindex() {
  startindex_ = int64_t{0};
}
inline int64_t SliceLayerParams::_internal_startindex() const {
  return startindex_;
}
inline int64_t SliceLayerParams::startindex() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceLayerParams.startIndex)
  return _internal_startindex();
}
inline void SliceLayerParams::_internal_set_startindex(int64_t value) {
  
  startindex_ = value;
}
inline void SliceLayerParams::set_startindex(int64_t value) {
  _internal_set_startindex(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceLayerParams.startIndex)
}

// int64 endIndex = 2;
inline void SliceLayerParams::clear_endindex() {
  endindex_ = int64_t{0};
}
inline int64_t SliceLayerParams::_internal_endindex() const {
  return endindex_;
}
inline int64_t SliceLayerParams::endindex() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceLayerParams.endIndex)
  return _internal_endindex();
}
inline void SliceLayerParams::_internal_set_endindex(int64_t value) {
  
  endindex_ = value;
}
inline void SliceLayerParams::set_endindex(int64_t value) {
  _internal_set_endindex(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceLayerParams.endIndex)
}

// uint64 stride = 3;
inline void SliceLayerParams::clear_stride() {
  stride_ = uint64_t{0u};
}
inline uint64_t SliceLayerParams::_internal_stride() const {
  return stride_;
}
inline uint64_t SliceLayerParams::stride() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceLayerParams.stride)
  return _internal_stride();
}
inline void SliceLayerParams::_internal_set_stride(uint64_t value) {
  
  stride_ = value;
}
inline void SliceLayerParams::set_stride(uint64_t value) {
  _internal_set_stride(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceLayerParams.stride)
}

// .CoreML.Specification.SliceLayerParams.SliceAxis axis = 4;
inline void SliceLayerParams::clear_axis() {
  axis_ = 0;
}
inline ::CoreML::Specification::SliceLayerParams_SliceAxis SliceLayerParams::_internal_axis() const {
  return static_cast< ::CoreML::Specification::SliceLayerParams_SliceAxis >(axis_);
}
inline ::CoreML::Specification::SliceLayerParams_SliceAxis SliceLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceLayerParams.axis)
  return _internal_axis();
}
inline void SliceLayerParams::_internal_set_axis(::CoreML::Specification::SliceLayerParams_SliceAxis value) {
  
  axis_ = value;
}
inline void SliceLayerParams::set_axis(::CoreML::Specification::SliceLayerParams_SliceAxis value) {
  _internal_set_axis(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceLayerParams.axis)
}

// -------------------------------------------------------------------

// ReduceLayerParams

// .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
inline void ReduceLayerParams::clear_mode() {
  mode_ = 0;
}
inline ::CoreML::Specification::ReduceLayerParams_ReduceOperation ReduceLayerParams::_internal_mode() const {
  return static_cast< ::CoreML::Specification::ReduceLayerParams_ReduceOperation >(mode_);
}
inline ::CoreML::Specification::ReduceLayerParams_ReduceOperation ReduceLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLayerParams.mode)
  return _internal_mode();
}
inline void ReduceLayerParams::_internal_set_mode(::CoreML::Specification::ReduceLayerParams_ReduceOperation value) {
  
  mode_ = value;
}
inline void ReduceLayerParams::set_mode(::CoreML::Specification::ReduceLayerParams_ReduceOperation value) {
  _internal_set_mode(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLayerParams.mode)
}

// float epsilon = 2;
inline void ReduceLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
inline float ReduceLayerParams::_internal_epsilon() const {
  return epsilon_;
}
inline float ReduceLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLayerParams.epsilon)
  return _internal_epsilon();
}
inline void ReduceLayerParams::_internal_set_epsilon(float value) {
  
  epsilon_ = value;
}
inline void ReduceLayerParams::set_epsilon(float value) {
  _internal_set_epsilon(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLayerParams.epsilon)
}

// .CoreML.Specification.ReduceLayerParams.ReduceAxis axis = 3;
inline void ReduceLayerParams::clear_axis() {
  axis_ = 0;
}
inline ::CoreML::Specification::ReduceLayerParams_ReduceAxis ReduceLayerParams::_internal_axis() const {
  return static_cast< ::CoreML::Specification::ReduceLayerParams_ReduceAxis >(axis_);
}
inline ::CoreML::Specification::ReduceLayerParams_ReduceAxis ReduceLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLayerParams.axis)
  return _internal_axis();
}
inline void ReduceLayerParams::_internal_set_axis(::CoreML::Specification::ReduceLayerParams_ReduceAxis value) {
  
  axis_ = value;
}
inline void ReduceLayerParams::set_axis(::CoreML::Specification::ReduceLayerParams_ReduceAxis value) {
  _internal_set_axis(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLayerParams.axis)
}

// -------------------------------------------------------------------

// CropLayerParams

// .CoreML.Specification.BorderAmounts cropAmounts = 1;
inline bool CropLayerParams::_internal_has_cropamounts() const {
  return this != internal_default_instance() && cropamounts_ != nullptr;
}
inline bool CropLayerParams::has_cropamounts() const {
  return _internal_has_cropamounts();
}
inline void CropLayerParams::clear_cropamounts() {
  if (GetArenaForAllocation() == nullptr && cropamounts_ != nullptr) {
    delete cropamounts_;
  }
  cropamounts_ = nullptr;
}
inline const ::CoreML::Specification::BorderAmounts& CropLayerParams::_internal_cropamounts() const {
  const ::CoreML::Specification::BorderAmounts* p = cropamounts_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::BorderAmounts&>(
      ::CoreML::Specification::_BorderAmounts_default_instance_);
}
inline const ::CoreML::Specification::BorderAmounts& CropLayerParams::cropamounts() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropLayerParams.cropAmounts)
  return _internal_cropamounts();
}
inline void CropLayerParams::unsafe_arena_set_allocated_cropamounts(
    ::CoreML::Specification::BorderAmounts* cropamounts) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(cropamounts_);
  }
  cropamounts_ = cropamounts;
  if (cropamounts) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.CropLayerParams.cropAmounts)
}
inline ::CoreML::Specification::BorderAmounts* CropLayerParams::release_cropamounts() {
  
  ::CoreML::Specification::BorderAmounts* temp = cropamounts_;
  cropamounts_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::BorderAmounts* CropLayerParams::unsafe_arena_release_cropamounts() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CropLayerParams.cropAmounts)
  
  ::CoreML::Specification::BorderAmounts* temp = cropamounts_;
  cropamounts_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::BorderAmounts* CropLayerParams::_internal_mutable_cropamounts() {
  
  if (cropamounts_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::BorderAmounts>(GetArenaForAllocation());
    cropamounts_ = p;
  }
  return cropamounts_;
}
inline ::CoreML::Specification::BorderAmounts* CropLayerParams::mutable_cropamounts() {
  ::CoreML::Specification::BorderAmounts* _msg = _internal_mutable_cropamounts();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CropLayerParams.cropAmounts)
  return _msg;
}
inline void CropLayerParams::set_allocated_cropamounts(::CoreML::Specification::BorderAmounts* cropamounts) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete cropamounts_;
  }
  if (cropamounts) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::BorderAmounts>::GetOwningArena(cropamounts);
    if (message_arena != submessage_arena) {
      cropamounts = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, cropamounts, submessage_arena);
    }
    
  } else {
    
  }
  cropamounts_ = cropamounts;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CropLayerParams.cropAmounts)
}

// repeated uint64 offset = 5;
inline int CropLayerParams::_internal_offset_size() const {
  return offset_.size();
}
inline int CropLayerParams::offset_size() const {
  return _internal_offset_size();
}
inline void CropLayerParams::clear_offset() {
  offset_.Clear();
}
inline uint64_t CropLayerParams::_internal_offset(int index) const {
  return offset_.Get(index);
}
inline uint64_t CropLayerParams::offset(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropLayerParams.offset)
  return _internal_offset(index);
}
inline void CropLayerParams::set_offset(int index, uint64_t value) {
  offset_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CropLayerParams.offset)
}
inline void CropLayerParams::_internal_add_offset(uint64_t value) {
  offset_.Add(value);
}
inline void CropLayerParams::add_offset(uint64_t value) {
  _internal_add_offset(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.CropLayerParams.offset)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
CropLayerParams::_internal_offset() const {
  return offset_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
CropLayerParams::offset() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.CropLayerParams.offset)
  return _internal_offset();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
CropLayerParams::_internal_mutable_offset() {
  return &offset_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
CropLayerParams::mutable_offset() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.CropLayerParams.offset)
  return _internal_mutable_offset();
}

// -------------------------------------------------------------------

// AverageLayerParams

// -------------------------------------------------------------------

// MaxLayerParams

// -------------------------------------------------------------------

// MinLayerParams

// -------------------------------------------------------------------

// DotProductLayerParams

// bool cosineSimilarity = 1;
inline void DotProductLayerParams::clear_cosinesimilarity() {
  cosinesimilarity_ = false;
}
inline bool DotProductLayerParams::_internal_cosinesimilarity() const {
  return cosinesimilarity_;
}
inline bool DotProductLayerParams::cosinesimilarity() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.DotProductLayerParams.cosineSimilarity)
  return _internal_cosinesimilarity();
}
inline void DotProductLayerParams::_internal_set_cosinesimilarity(bool value) {
  
  cosinesimilarity_ = value;
}
inline void DotProductLayerParams::set_cosinesimilarity(bool value) {
  _internal_set_cosinesimilarity(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.DotProductLayerParams.cosineSimilarity)
}

// -------------------------------------------------------------------

// MeanVarianceNormalizeLayerParams

// bool acrossChannels = 1;
inline void MeanVarianceNormalizeLayerParams::clear_acrosschannels() {
  acrosschannels_ = false;
}
inline bool MeanVarianceNormalizeLayerParams::_internal_acrosschannels() const {
  return acrosschannels_;
}
inline bool MeanVarianceNormalizeLayerParams::acrosschannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanVarianceNormalizeLayerParams.acrossChannels)
  return _internal_acrosschannels();
}
inline void MeanVarianceNormalizeLayerParams::_internal_set_acrosschannels(bool value) {
  
  acrosschannels_ = value;
}
inline void MeanVarianceNormalizeLayerParams::set_acrosschannels(bool value) {
  _internal_set_acrosschannels(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanVarianceNormalizeLayerParams.acrossChannels)
}

// bool normalizeVariance = 2;
inline void MeanVarianceNormalizeLayerParams::clear_normalizevariance() {
  normalizevariance_ = false;
}
inline bool MeanVarianceNormalizeLayerParams::_internal_normalizevariance() const {
  return normalizevariance_;
}
inline bool MeanVarianceNormalizeLayerParams::normalizevariance() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanVarianceNormalizeLayerParams.normalizeVariance)
  return _internal_normalizevariance();
}
inline void MeanVarianceNormalizeLayerParams::_internal_set_normalizevariance(bool value) {
  
  normalizevariance_ = value;
}
inline void MeanVarianceNormalizeLayerParams::set_normalizevariance(bool value) {
  _internal_set_normalizevariance(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanVarianceNormalizeLayerParams.normalizeVariance)
}

// float epsilon = 3;
inline void MeanVarianceNormalizeLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
inline float MeanVarianceNormalizeLayerParams::_internal_epsilon() const {
  return epsilon_;
}
inline float MeanVarianceNormalizeLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanVarianceNormalizeLayerParams.epsilon)
  return _internal_epsilon();
}
inline void MeanVarianceNormalizeLayerParams::_internal_set_epsilon(float value) {
  
  epsilon_ = value;
}
inline void MeanVarianceNormalizeLayerParams::set_epsilon(float value) {
  _internal_set_epsilon(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanVarianceNormalizeLayerParams.epsilon)
}

// -------------------------------------------------------------------

// SequenceRepeatLayerParams

// uint64 nRepetitions = 1;
inline void SequenceRepeatLayerParams::clear_nrepetitions() {
  nrepetitions_ = uint64_t{0u};
}
inline uint64_t SequenceRepeatLayerParams::_internal_nrepetitions() const {
  return nrepetitions_;
}
inline uint64_t SequenceRepeatLayerParams::nrepetitions() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SequenceRepeatLayerParams.nRepetitions)
  return _internal_nrepetitions();
}
inline void SequenceRepeatLayerParams::_internal_set_nrepetitions(uint64_t value) {
  
  nrepetitions_ = value;
}
inline void SequenceRepeatLayerParams::set_nrepetitions(uint64_t value) {
  _internal_set_nrepetitions(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SequenceRepeatLayerParams.nRepetitions)
}

// -------------------------------------------------------------------

// SimpleRecurrentLayerParams

// uint64 inputVectorSize = 1;
inline void SimpleRecurrentLayerParams::clear_inputvectorsize() {
  inputvectorsize_ = uint64_t{0u};
}
inline uint64_t SimpleRecurrentLayerParams::_internal_inputvectorsize() const {
  return inputvectorsize_;
}
inline uint64_t SimpleRecurrentLayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.inputVectorSize)
  return _internal_inputvectorsize();
}
inline void SimpleRecurrentLayerParams::_internal_set_inputvectorsize(uint64_t value) {
  
  inputvectorsize_ = value;
}
inline void SimpleRecurrentLayerParams::set_inputvectorsize(uint64_t value) {
  _internal_set_inputvectorsize(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.inputVectorSize)
}

// uint64 outputVectorSize = 2;
inline void SimpleRecurrentLayerParams::clear_outputvectorsize() {
  outputvectorsize_ = uint64_t{0u};
}
inline uint64_t SimpleRecurrentLayerParams::_internal_outputvectorsize() const {
  return outputvectorsize_;
}
inline uint64_t SimpleRecurrentLayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.outputVectorSize)
  return _internal_outputvectorsize();
}
inline void SimpleRecurrentLayerParams::_internal_set_outputvectorsize(uint64_t value) {
  
  outputvectorsize_ = value;
}
inline void SimpleRecurrentLayerParams::set_outputvectorsize(uint64_t value) {
  _internal_set_outputvectorsize(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.outputVectorSize)
}

// .CoreML.Specification.ActivationParams activation = 10;
inline bool SimpleRecurrentLayerParams::_internal_has_activation() const {
  return this != internal_default_instance() && activation_ != nullptr;
}
inline bool SimpleRecurrentLayerParams::has_activation() const {
  return _internal_has_activation();
}
inline void SimpleRecurrentLayerParams::clear_activation() {
  if (GetArenaForAllocation() == nullptr && activation_ != nullptr) {
    delete activation_;
  }
  activation_ = nullptr;
}
inline const ::CoreML::Specification::ActivationParams& SimpleRecurrentLayerParams::_internal_activation() const {
  const ::CoreML::Specification::ActivationParams* p = activation_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::ActivationParams&>(
      ::CoreML::Specification::_ActivationParams_default_instance_);
}
inline const ::CoreML::Specification::ActivationParams& SimpleRecurrentLayerParams::activation() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.activation)
  return _internal_activation();
}
inline void SimpleRecurrentLayerParams::unsafe_arena_set_allocated_activation(
    ::CoreML::Specification::ActivationParams* activation) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(activation_);
  }
  activation_ = activation;
  if (activation) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.activation)
}
inline ::CoreML::Specification::ActivationParams* SimpleRecurrentLayerParams::release_activation() {
  
  ::CoreML::Specification::ActivationParams* temp = activation_;
  activation_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::ActivationParams* SimpleRecurrentLayerParams::unsafe_arena_release_activation() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.activation)
  
  ::CoreML::Specification::ActivationParams* temp = activation_;
  activation_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::ActivationParams* SimpleRecurrentLayerParams::_internal_mutable_activation() {
  
  if (activation_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::ActivationParams>(GetArenaForAllocation());
    activation_ = p;
  }
  return activation_;
}
inline ::CoreML::Specification::ActivationParams* SimpleRecurrentLayerParams::mutable_activation() {
  ::CoreML::Specification::ActivationParams* _msg = _internal_mutable_activation();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.activation)
  return _msg;
}
inline void SimpleRecurrentLayerParams::set_allocated_activation(::CoreML::Specification::ActivationParams* activation) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete activation_;
  }
  if (activation) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::ActivationParams>::GetOwningArena(activation);
    if (message_arena != submessage_arena) {
      activation = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, activation, submessage_arena);
    }
    
  } else {
    
  }
  activation_ = activation;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.activation)
}

// bool sequenceOutput = 15;
inline void SimpleRecurrentLayerParams::clear_sequenceoutput() {
  sequenceoutput_ = false;
}
inline bool SimpleRecurrentLayerParams::_internal_sequenceoutput() const {
  return sequenceoutput_;
}
inline bool SimpleRecurrentLayerParams::sequenceoutput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.sequenceOutput)
  return _internal_sequenceoutput();
}
inline void SimpleRecurrentLayerParams::_internal_set_sequenceoutput(bool value) {
  
  sequenceoutput_ = value;
}
inline void SimpleRecurrentLayerParams::set_sequenceoutput(bool value) {
  _internal_set_sequenceoutput(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.sequenceOutput)
}

// bool hasBiasVector = 20;
inline void SimpleRecurrentLayerParams::clear_hasbiasvector() {
  hasbiasvector_ = false;
}
inline bool SimpleRecurrentLayerParams::_internal_hasbiasvector() const {
  return hasbiasvector_;
}
inline bool SimpleRecurrentLayerParams::hasbiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.hasBiasVector)
  return _internal_hasbiasvector();
}
inline void SimpleRecurrentLayerParams::_internal_set_hasbiasvector(bool value) {
  
  hasbiasvector_ = value;
}
inline void SimpleRecurrentLayerParams::set_hasbiasvector(bool value) {
  _internal_set_hasbiasvector(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.hasBiasVector)
}

// .CoreML.Specification.WeightParams weightMatrix = 30;
inline bool SimpleRecurrentLayerParams::_internal_has_weightmatrix() const {
  return this != internal_default_instance() && weightmatrix_ != nullptr;
}
inline bool SimpleRecurrentLayerParams::has_weightmatrix() const {
  return _internal_has_weightmatrix();
}
inline void SimpleRecurrentLayerParams::clear_weightmatrix() {
  if (GetArenaForAllocation() == nullptr && weightmatrix_ != nullptr) {
    delete weightmatrix_;
  }
  weightmatrix_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::_internal_weightmatrix() const {
  const ::CoreML::Specification::WeightParams* p = weightmatrix_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::weightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
  return _internal_weightmatrix();
}
inline void SimpleRecurrentLayerParams::unsafe_arena_set_allocated_weightmatrix(
    ::CoreML::Specification::WeightParams* weightmatrix) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(weightmatrix_);
  }
  weightmatrix_ = weightmatrix;
  if (weightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::release_weightmatrix() {
  
  ::CoreML::Specification::WeightParams* temp = weightmatrix_;
  weightmatrix_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::unsafe_arena_release_weightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = weightmatrix_;
  weightmatrix_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::_internal_mutable_weightmatrix() {
  
  if (weightmatrix_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    weightmatrix_ = p;
  }
  return weightmatrix_;
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::mutable_weightmatrix() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_weightmatrix();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
  return _msg;
}
inline void SimpleRecurrentLayerParams::set_allocated_weightmatrix(::CoreML::Specification::WeightParams* weightmatrix) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete weightmatrix_;
  }
  if (weightmatrix) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(weightmatrix);
    if (message_arena != submessage_arena) {
      weightmatrix = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, weightmatrix, submessage_arena);
    }
    
  } else {
    
  }
  weightmatrix_ = weightmatrix;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
}

// .CoreML.Specification.WeightParams recursionMatrix = 31;
inline bool SimpleRecurrentLayerParams::_internal_has_recursionmatrix() const {
  return this != internal_default_instance() && recursionmatrix_ != nullptr;
}
inline bool SimpleRecurrentLayerParams::has_recursionmatrix() const {
  return _internal_has_recursionmatrix();
}
inline void SimpleRecurrentLayerParams::clear_recursionmatrix() {
  if (GetArenaForAllocation() == nullptr && recursionmatrix_ != nullptr) {
    delete recursionmatrix_;
  }
  recursionmatrix_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::_internal_recursionmatrix() const {
  const ::CoreML::Specification::WeightParams* p = recursionmatrix_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::recursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
  return _internal_recursionmatrix();
}
inline void SimpleRecurrentLayerParams::unsafe_arena_set_allocated_recursionmatrix(
    ::CoreML::Specification::WeightParams* recursionmatrix) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(recursionmatrix_);
  }
  recursionmatrix_ = recursionmatrix;
  if (recursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::release_recursionmatrix() {
  
  ::CoreML::Specification::WeightParams* temp = recursionmatrix_;
  recursionmatrix_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::unsafe_arena_release_recursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = recursionmatrix_;
  recursionmatrix_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::_internal_mutable_recursionmatrix() {
  
  if (recursionmatrix_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    recursionmatrix_ = p;
  }
  return recursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::mutable_recursionmatrix() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_recursionmatrix();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
  return _msg;
}
inline void SimpleRecurrentLayerParams::set_allocated_recursionmatrix(::CoreML::Specification::WeightParams* recursionmatrix) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete recursionmatrix_;
  }
  if (recursionmatrix) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(recursionmatrix);
    if (message_arena != submessage_arena) {
      recursionmatrix = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, recursionmatrix, submessage_arena);
    }
    
  } else {
    
  }
  recursionmatrix_ = recursionmatrix;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
}

// .CoreML.Specification.WeightParams biasVector = 32;
inline bool SimpleRecurrentLayerParams::_internal_has_biasvector() const {
  return this != internal_default_instance() && biasvector_ != nullptr;
}
inline bool SimpleRecurrentLayerParams::has_biasvector() const {
  return _internal_has_biasvector();
}
inline void SimpleRecurrentLayerParams::clear_biasvector() {
  if (GetArenaForAllocation() == nullptr && biasvector_ != nullptr) {
    delete biasvector_;
  }
  biasvector_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::_internal_biasvector() const {
  const ::CoreML::Specification::WeightParams* p = biasvector_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::biasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
  return _internal_biasvector();
}
inline void SimpleRecurrentLayerParams::unsafe_arena_set_allocated_biasvector(
    ::CoreML::Specification::WeightParams* biasvector) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(biasvector_);
  }
  biasvector_ = biasvector;
  if (biasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::release_biasvector() {
  
  ::CoreML::Specification::WeightParams* temp = biasvector_;
  biasvector_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::unsafe_arena_release_biasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
  
  ::CoreML::Specification::WeightParams* temp = biasvector_;
  biasvector_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::_internal_mutable_biasvector() {
  
  if (biasvector_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    biasvector_ = p;
  }
  return biasvector_;
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::mutable_biasvector() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_biasvector();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
  return _msg;
}
inline void SimpleRecurrentLayerParams::set_allocated_biasvector(::CoreML::Specification::WeightParams* biasvector) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete biasvector_;
  }
  if (biasvector) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(biasvector);
    if (message_arena != submessage_arena) {
      biasvector = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, biasvector, submessage_arena);
    }
    
  } else {
    
  }
  biasvector_ = biasvector;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
}

// bool reverseInput = 100;
inline void SimpleRecurrentLayerParams::clear_reverseinput() {
  reverseinput_ = false;
}
inline bool SimpleRecurrentLayerParams::_internal_reverseinput() const {
  return reverseinput_;
}
inline bool SimpleRecurrentLayerParams::reverseinput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.reverseInput)
  return _internal_reverseinput();
}
inline void SimpleRecurrentLayerParams::_internal_set_reverseinput(bool value) {
  
  reverseinput_ = value;
}
inline void SimpleRecurrentLayerParams::set_reverseinput(bool value) {
  _internal_set_reverseinput(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.reverseInput)
}

// -------------------------------------------------------------------

// GRULayerParams

// uint64 inputVectorSize = 1;
inline void GRULayerParams::clear_inputvectorsize() {
  inputvectorsize_ = uint64_t{0u};
}
inline uint64_t GRULayerParams::_internal_inputvectorsize() const {
  return inputvectorsize_;
}
inline uint64_t GRULayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.inputVectorSize)
  return _internal_inputvectorsize();
}
inline void GRULayerParams::_internal_set_inputvectorsize(uint64_t value) {
  
  inputvectorsize_ = value;
}
inline void GRULayerParams::set_inputvectorsize(uint64_t value) {
  _internal_set_inputvectorsize(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.inputVectorSize)
}

// uint64 outputVectorSize = 2;
inline void GRULayerParams::clear_outputvectorsize() {
  outputvectorsize_ = uint64_t{0u};
}
inline uint64_t GRULayerParams::_internal_outputvectorsize() const {
  return outputvectorsize_;
}
inline uint64_t GRULayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputVectorSize)
  return _internal_outputvectorsize();
}
inline void GRULayerParams::_internal_set_outputvectorsize(uint64_t value) {
  
  outputvectorsize_ = value;
}
inline void GRULayerParams::set_outputvectorsize(uint64_t value) {
  _internal_set_outputvectorsize(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.outputVectorSize)
}

// repeated .CoreML.Specification.ActivationParams activations = 10;
inline int GRULayerParams::_internal_activations_size() const {
  return activations_.size();
}
inline int GRULayerParams::activations_size() const {
  return _internal_activations_size();
}
inline void GRULayerParams::clear_activations() {
  activations_.Clear();
}
inline ::CoreML::Specification::ActivationParams* GRULayerParams::mutable_activations(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.activations)
  return activations_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
GRULayerParams::mutable_activations() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.GRULayerParams.activations)
  return &activations_;
}
inline const ::CoreML::Specification::ActivationParams& GRULayerParams::_internal_activations(int index) const {
  return activations_.Get(index);
}
inline const ::CoreML::Specification::ActivationParams& GRULayerParams::activations(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.activations)
  return _internal_activations(index);
}
inline ::CoreML::Specification::ActivationParams* GRULayerParams::_internal_add_activations() {
  return activations_.Add();
}
inline ::CoreML::Specification::ActivationParams* GRULayerParams::add_activations() {
  ::CoreML::Specification::ActivationParams* _add = _internal_add_activations();
  // @@protoc_insertion_point(field_add:CoreML.Specification.GRULayerParams.activations)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
GRULayerParams::activations() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.GRULayerParams.activations)
  return activations_;
}

// bool sequenceOutput = 15;
inline void GRULayerParams::clear_sequenceoutput() {
  sequenceoutput_ = false;
}
inline bool GRULayerParams::_internal_sequenceoutput() const {
  return sequenceoutput_;
}
inline bool GRULayerParams::sequenceoutput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.sequenceOutput)
  return _internal_sequenceoutput();
}
inline void GRULayerParams::_internal_set_sequenceoutput(bool value) {
  
  sequenceoutput_ = value;
}
inline void GRULayerParams::set_sequenceoutput(bool value) {
  _internal_set_sequenceoutput(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.sequenceOutput)
}

// bool hasBiasVectors = 20;
inline void GRULayerParams::clear_hasbiasvectors() {
  hasbiasvectors_ = false;
}
inline bool GRULayerParams::_internal_hasbiasvectors() const {
  return hasbiasvectors_;
}
inline bool GRULayerParams::hasbiasvectors() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.hasBiasVectors)
  return _internal_hasbiasvectors();
}
inline void GRULayerParams::_internal_set_hasbiasvectors(bool value) {
  
  hasbiasvectors_ = value;
}
inline void GRULayerParams::set_hasbiasvectors(bool value) {
  _internal_set_hasbiasvectors(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.hasBiasVectors)
}

// .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
inline bool GRULayerParams::_internal_has_updategateweightmatrix() const {
  return this != internal_default_instance() && updategateweightmatrix_ != nullptr;
}
inline bool GRULayerParams::has_updategateweightmatrix() const {
  return _internal_has_updategateweightmatrix();
}
inline void GRULayerParams::clear_updategateweightmatrix() {
  if (GetArenaForAllocation() == nullptr && updategateweightmatrix_ != nullptr) {
    delete updategateweightmatrix_;
  }
  updategateweightmatrix_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::_internal_updategateweightmatrix() const {
  const ::CoreML::Specification::WeightParams* p = updategateweightmatrix_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::updategateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
  return _internal_updategateweightmatrix();
}
inline void GRULayerParams::unsafe_arena_set_allocated_updategateweightmatrix(
    ::CoreML::Specification::WeightParams* updategateweightmatrix) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(updategateweightmatrix_);
  }
  updategateweightmatrix_ = updategateweightmatrix;
  if (updategateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_updategateweightmatrix() {
  
  ::CoreML::Specification::WeightParams* temp = updategateweightmatrix_;
  updategateweightmatrix_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::unsafe_arena_release_updategateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = updategateweightmatrix_;
  updategateweightmatrix_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::_internal_mutable_updategateweightmatrix() {
  
  if (updategateweightmatrix_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    updategateweightmatrix_ = p;
  }
  return updategateweightmatrix_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_updategateweightmatrix() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_updategateweightmatrix();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
  return _msg;
}
inline void GRULayerParams::set_allocated_updategateweightmatrix(::CoreML::Specification::WeightParams* updategateweightmatrix) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete updategateweightmatrix_;
  }
  if (updategateweightmatrix) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(updategateweightmatrix);
    if (message_arena != submessage_arena) {
      updategateweightmatrix = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, updategateweightmatrix, submessage_arena);
    }
    
  } else {
    
  }
  updategateweightmatrix_ = updategateweightmatrix;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
}

// .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
inline bool GRULayerParams::_internal_has_resetgateweightmatrix() const {
  return this != internal_default_instance() && resetgateweightmatrix_ != nullptr;
}
inline bool GRULayerParams::has_resetgateweightmatrix() const {
  return _internal_has_resetgateweightmatrix();
}
inline void GRULayerParams::clear_resetgateweightmatrix() {
  if (GetArenaForAllocation() == nullptr && resetgateweightmatrix_ != nullptr) {
    delete resetgateweightmatrix_;
  }
  resetgateweightmatrix_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::_internal_resetgateweightmatrix() const {
  const ::CoreML::Specification::WeightParams* p = resetgateweightmatrix_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::resetgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
  return _internal_resetgateweightmatrix();
}
inline void GRULayerParams::unsafe_arena_set_allocated_resetgateweightmatrix(
    ::CoreML::Specification::WeightParams* resetgateweightmatrix) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(resetgateweightmatrix_);
  }
  resetgateweightmatrix_ = resetgateweightmatrix;
  if (resetgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_resetgateweightmatrix() {
  
  ::CoreML::Specification::WeightParams* temp = resetgateweightmatrix_;
  resetgateweightmatrix_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::unsafe_arena_release_resetgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = resetgateweightmatrix_;
  resetgateweightmatrix_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::_internal_mutable_resetgateweightmatrix() {
  
  if (resetgateweightmatrix_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    resetgateweightmatrix_ = p;
  }
  return resetgateweightmatrix_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_resetgateweightmatrix() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_resetgateweightmatrix();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
  return _msg;
}
inline void GRULayerParams::set_allocated_resetgateweightmatrix(::CoreML::Specification::WeightParams* resetgateweightmatrix) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete resetgateweightmatrix_;
  }
  if (resetgateweightmatrix) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(resetgateweightmatrix);
    if (message_arena != submessage_arena) {
      resetgateweightmatrix = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, resetgateweightmatrix, submessage_arena);
    }
    
  } else {
    
  }
  resetgateweightmatrix_ = resetgateweightmatrix;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
}

// .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
inline bool GRULayerParams::_internal_has_outputgateweightmatrix() const {
  return this != internal_default_instance() && outputgateweightmatrix_ != nullptr;
}
inline bool GRULayerParams::has_outputgateweightmatrix() const {
  return _internal_has_outputgateweightmatrix();
}
inline void GRULayerParams::clear_outputgateweightmatrix() {
  if (GetArenaForAllocation() == nullptr && outputgateweightmatrix_ != nullptr) {
    delete outputgateweightmatrix_;
  }
  outputgateweightmatrix_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::_internal_outputgateweightmatrix() const {
  const ::CoreML::Specification::WeightParams* p = outputgateweightmatrix_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::outputgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
  return _internal_outputgateweightmatrix();
}
inline void GRULayerParams::unsafe_arena_set_allocated_outputgateweightmatrix(
    ::CoreML::Specification::WeightParams* outputgateweightmatrix) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(outputgateweightmatrix_);
  }
  outputgateweightmatrix_ = outputgateweightmatrix;
  if (outputgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_outputgateweightmatrix() {
  
  ::CoreML::Specification::WeightParams* temp = outputgateweightmatrix_;
  outputgateweightmatrix_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::unsafe_arena_release_outputgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgateweightmatrix_;
  outputgateweightmatrix_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::_internal_mutable_outputgateweightmatrix() {
  
  if (outputgateweightmatrix_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    outputgateweightmatrix_ = p;
  }
  return outputgateweightmatrix_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_outputgateweightmatrix() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_outputgateweightmatrix();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
  return _msg;
}
inline void GRULayerParams::set_allocated_outputgateweightmatrix(::CoreML::Specification::WeightParams* outputgateweightmatrix) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete outputgateweightmatrix_;
  }
  if (outputgateweightmatrix) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(outputgateweightmatrix);
    if (message_arena != submessage_arena) {
      outputgateweightmatrix = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, outputgateweightmatrix, submessage_arena);
    }
    
  } else {
    
  }
  outputgateweightmatrix_ = outputgateweightmatrix;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
}

// .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
inline bool GRULayerParams::_internal_has_updategaterecursionmatrix() const {
  return this != internal_default_instance() && updategaterecursionmatrix_ != nullptr;
}
inline bool GRULayerParams::has_updategaterecursionmatrix() const {
  return _internal_has_updategaterecursionmatrix();
}
inline void GRULayerParams::clear_updategaterecursionmatrix() {
  if (GetArenaForAllocation() == nullptr && updategaterecursionmatrix_ != nullptr) {
    delete updategaterecursionmatrix_;
  }
  updategaterecursionmatrix_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::_internal_updategaterecursionmatrix() const {
  const ::CoreML::Specification::WeightParams* p = updategaterecursionmatrix_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::updategaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
  return _internal_updategaterecursionmatrix();
}
inline void GRULayerParams::unsafe_arena_set_allocated_updategaterecursionmatrix(
    ::CoreML::Specification::WeightParams* updategaterecursionmatrix) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(updategaterecursionmatrix_);
  }
  updategaterecursionmatrix_ = updategaterecursionmatrix;
  if (updategaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_updategaterecursionmatrix() {
  
  ::CoreML::Specification::WeightParams* temp = updategaterecursionmatrix_;
  updategaterecursionmatrix_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::unsafe_arena_release_updategaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = updategaterecursionmatrix_;
  updategaterecursionmatrix_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::_internal_mutable_updategaterecursionmatrix() {
  
  if (updategaterecursionmatrix_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    updategaterecursionmatrix_ = p;
  }
  return updategaterecursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_updategaterecursionmatrix() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_updategaterecursionmatrix();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
  return _msg;
}
inline void GRULayerParams::set_allocated_updategaterecursionmatrix(::CoreML::Specification::WeightParams* updategaterecursionmatrix) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete updategaterecursionmatrix_;
  }
  if (updategaterecursionmatrix) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(updategaterecursionmatrix);
    if (message_arena != submessage_arena) {
      updategaterecursionmatrix = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, updategaterecursionmatrix, submessage_arena);
    }
    
  } else {
    
  }
  updategaterecursionmatrix_ = updategaterecursionmatrix;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
inline bool GRULayerParams::_internal_has_resetgaterecursionmatrix() const {
  return this != internal_default_instance() && resetgaterecursionmatrix_ != nullptr;
}
inline bool GRULayerParams::has_resetgaterecursionmatrix() const {
  return _internal_has_resetgaterecursionmatrix();
}
inline void GRULayerParams::clear_resetgaterecursionmatrix() {
  if (GetArenaForAllocation() == nullptr && resetgaterecursionmatrix_ != nullptr) {
    delete resetgaterecursionmatrix_;
  }
  resetgaterecursionmatrix_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::_internal_resetgaterecursionmatrix() const {
  const ::CoreML::Specification::WeightParams* p = resetgaterecursionmatrix_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::resetgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
  return _internal_resetgaterecursionmatrix();
}
inline void GRULayerParams::unsafe_arena_set_allocated_resetgaterecursionmatrix(
    ::CoreML::Specification::WeightParams* resetgaterecursionmatrix) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(resetgaterecursionmatrix_);
  }
  resetgaterecursionmatrix_ = resetgaterecursionmatrix;
  if (resetgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_resetgaterecursionmatrix() {
  
  ::CoreML::Specification::WeightParams* temp = resetgaterecursionmatrix_;
  resetgaterecursionmatrix_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::unsafe_arena_release_resetgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = resetgaterecursionmatrix_;
  resetgaterecursionmatrix_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::_internal_mutable_resetgaterecursionmatrix() {
  
  if (resetgaterecursionmatrix_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    resetgaterecursionmatrix_ = p;
  }
  return resetgaterecursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_resetgaterecursionmatrix() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_resetgaterecursionmatrix();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
  return _msg;
}
inline void GRULayerParams::set_allocated_resetgaterecursionmatrix(::CoreML::Specification::WeightParams* resetgaterecursionmatrix) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete resetgaterecursionmatrix_;
  }
  if (resetgaterecursionmatrix) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(resetgaterecursionmatrix);
    if (message_arena != submessage_arena) {
      resetgaterecursionmatrix = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, resetgaterecursionmatrix, submessage_arena);
    }
    
  } else {
    
  }
  resetgaterecursionmatrix_ = resetgaterecursionmatrix;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
inline bool GRULayerParams::_internal_has_outputgaterecursionmatrix() const {
  return this != internal_default_instance() && outputgaterecursionmatrix_ != nullptr;
}
inline bool GRULayerParams::has_outputgaterecursionmatrix() const {
  return _internal_has_outputgaterecursionmatrix();
}
inline void GRULayerParams::clear_outputgaterecursionmatrix() {
  if (GetArenaForAllocation() == nullptr && outputgaterecursionmatrix_ != nullptr) {
    delete outputgaterecursionmatrix_;
  }
  outputgaterecursionmatrix_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::_internal_outputgaterecursionmatrix() const {
  const ::CoreML::Specification::WeightParams* p = outputgaterecursionmatrix_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::outputgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
  return _internal_outputgaterecursionmatrix();
}
inline void GRULayerParams::unsafe_arena_set_allocated_outputgaterecursionmatrix(
    ::CoreML::Specification::WeightParams* outputgaterecursionmatrix) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(outputgaterecursionmatrix_);
  }
  outputgaterecursionmatrix_ = outputgaterecursionmatrix;
  if (outputgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_outputgaterecursionmatrix() {
  
  ::CoreML::Specification::WeightParams* temp = outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::unsafe_arena_release_outputgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::_internal_mutable_outputgaterecursionmatrix() {
  
  if (outputgaterecursionmatrix_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    outputgaterecursionmatrix_ = p;
  }
  return outputgaterecursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_outputgaterecursionmatrix() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_outputgaterecursionmatrix();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
  return _msg;
}
inline void GRULayerParams::set_allocated_outputgaterecursionmatrix(::CoreML::Specification::WeightParams* outputgaterecursionmatrix) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete outputgaterecursionmatrix_;
  }
  if (outputgaterecursionmatrix) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(outputgaterecursionmatrix);
    if (message_arena != submessage_arena) {
      outputgaterecursionmatrix = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, outputgaterecursionmatrix, submessage_arena);
    }
    
  } else {
    
  }
  outputgaterecursionmatrix_ = outputgaterecursionmatrix;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams updateGateBiasVector = 70;
inline bool GRULayerParams::_internal_has_updategatebiasvector() const {
  return this != internal_default_instance() && updategatebiasvector_ != nullptr;
}
inline bool GRULayerParams::has_updategatebiasvector() const {
  return _internal_has_updategatebiasvector();
}
inline void GRULayerParams::clear_updategatebiasvector() {
  if (GetArenaForAllocation() == nullptr && updategatebiasvector_ != nullptr) {
    delete updategatebiasvector_;
  }
  updategatebiasvector_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::_internal_updategatebiasvector() const {
  const ::CoreML::Specification::WeightParams* p = updategatebiasvector_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::updategatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.updateGateBiasVector)
  return _internal_updategatebiasvector();
}
inline void GRULayerParams::unsafe_arena_set_allocated_updategatebiasvector(
    ::CoreML::Specification::WeightParams* updategatebiasvector) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(updategatebiasvector_);
  }
  updategatebiasvector_ = updategatebiasvector;
  if (updategatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.GRULayerParams.updateGateBiasVector)
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_updategatebiasvector() {
  
  ::CoreML::Specification::WeightParams* temp = updategatebiasvector_;
  updategatebiasvector_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::unsafe_arena_release_updategatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.updateGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = updategatebiasvector_;
  updategatebiasvector_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::_internal_mutable_updategatebiasvector() {
  
  if (updategatebiasvector_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    updategatebiasvector_ = p;
  }
  return updategatebiasvector_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_updategatebiasvector() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_updategatebiasvector();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.updateGateBiasVector)
  return _msg;
}
inline void GRULayerParams::set_allocated_updategatebiasvector(::CoreML::Specification::WeightParams* updategatebiasvector) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete updategatebiasvector_;
  }
  if (updategatebiasvector) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(updategatebiasvector);
    if (message_arena != submessage_arena) {
      updategatebiasvector = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, updategatebiasvector, submessage_arena);
    }
    
  } else {
    
  }
  updategatebiasvector_ = updategatebiasvector;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.updateGateBiasVector)
}

// .CoreML.Specification.WeightParams resetGateBiasVector = 71;
inline bool GRULayerParams::_internal_has_resetgatebiasvector() const {
  return this != internal_default_instance() && resetgatebiasvector_ != nullptr;
}
inline bool GRULayerParams::has_resetgatebiasvector() const {
  return _internal_has_resetgatebiasvector();
}
inline void GRULayerParams::clear_resetgatebiasvector() {
  if (GetArenaForAllocation() == nullptr && resetgatebiasvector_ != nullptr) {
    delete resetgatebiasvector_;
  }
  resetgatebiasvector_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::_internal_resetgatebiasvector() const {
  const ::CoreML::Specification::WeightParams* p = resetgatebiasvector_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::resetgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.resetGateBiasVector)
  return _internal_resetgatebiasvector();
}
inline void GRULayerParams::unsafe_arena_set_allocated_resetgatebiasvector(
    ::CoreML::Specification::WeightParams* resetgatebiasvector) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(resetgatebiasvector_);
  }
  resetgatebiasvector_ = resetgatebiasvector;
  if (resetgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.GRULayerParams.resetGateBiasVector)
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_resetgatebiasvector() {
  
  ::CoreML::Specification::WeightParams* temp = resetgatebiasvector_;
  resetgatebiasvector_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::unsafe_arena_release_resetgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.resetGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = resetgatebiasvector_;
  resetgatebiasvector_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::_internal_mutable_resetgatebiasvector() {
  
  if (resetgatebiasvector_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    resetgatebiasvector_ = p;
  }
  return resetgatebiasvector_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_resetgatebiasvector() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_resetgatebiasvector();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.resetGateBiasVector)
  return _msg;
}
inline void GRULayerParams::set_allocated_resetgatebiasvector(::CoreML::Specification::WeightParams* resetgatebiasvector) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete resetgatebiasvector_;
  }
  if (resetgatebiasvector) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(resetgatebiasvector);
    if (message_arena != submessage_arena) {
      resetgatebiasvector = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, resetgatebiasvector, submessage_arena);
    }
    
  } else {
    
  }
  resetgatebiasvector_ = resetgatebiasvector;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.resetGateBiasVector)
}

// .CoreML.Specification.WeightParams outputGateBiasVector = 72;
inline bool GRULayerParams::_internal_has_outputgatebiasvector() const {
  return this != internal_default_instance() && outputgatebiasvector_ != nullptr;
}
inline bool GRULayerParams::has_outputgatebiasvector() const {
  return _internal_has_outputgatebiasvector();
}
inline void GRULayerParams::clear_outputgatebiasvector() {
  if (GetArenaForAllocation() == nullptr && outputgatebiasvector_ != nullptr) {
    delete outputgatebiasvector_;
  }
  outputgatebiasvector_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::_internal_outputgatebiasvector() const {
  const ::CoreML::Specification::WeightParams* p = outputgatebiasvector_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::outputgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputGateBiasVector)
  return _internal_outputgatebiasvector();
}
inline void GRULayerParams::unsafe_arena_set_allocated_outputgatebiasvector(
    ::CoreML::Specification::WeightParams* outputgatebiasvector) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(outputgatebiasvector_);
  }
  outputgatebiasvector_ = outputgatebiasvector;
  if (outputgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.GRULayerParams.outputGateBiasVector)
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_outputgatebiasvector() {
  
  ::CoreML::Specification::WeightParams* temp = outputgatebiasvector_;
  outputgatebiasvector_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::unsafe_arena_release_outputgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.outputGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = outputgatebiasvector_;
  outputgatebiasvector_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::_internal_mutable_outputgatebiasvector() {
  
  if (outputgatebiasvector_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    outputgatebiasvector_ = p;
  }
  return outputgatebiasvector_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_outputgatebiasvector() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_outputgatebiasvector();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.outputGateBiasVector)
  return _msg;
}
inline void GRULayerParams::set_allocated_outputgatebiasvector(::CoreML::Specification::WeightParams* outputgatebiasvector) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete outputgatebiasvector_;
  }
  if (outputgatebiasvector) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(outputgatebiasvector);
    if (message_arena != submessage_arena) {
      outputgatebiasvector = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, outputgatebiasvector, submessage_arena);
    }
    
  } else {
    
  }
  outputgatebiasvector_ = outputgatebiasvector;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.outputGateBiasVector)
}

// bool reverseInput = 100;
inline void GRULayerParams::clear_reverseinput() {
  reverseinput_ = false;
}
inline bool GRULayerParams::_internal_reverseinput() const {
  return reverseinput_;
}
inline bool GRULayerParams::reverseinput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.reverseInput)
  return _internal_reverseinput();
}
inline void GRULayerParams::_internal_set_reverseinput(bool value) {
  
  reverseinput_ = value;
}
inline void GRULayerParams::set_reverseinput(bool value) {
  _internal_set_reverseinput(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.reverseInput)
}

// -------------------------------------------------------------------

// LSTMParams

// bool sequenceOutput = 10;
inline void LSTMParams::clear_sequenceoutput() {
  sequenceoutput_ = false;
}
inline bool LSTMParams::_internal_sequenceoutput() const {
  return sequenceoutput_;
}
inline bool LSTMParams::sequenceoutput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.sequenceOutput)
  return _internal_sequenceoutput();
}
inline void LSTMParams::_internal_set_sequenceoutput(bool value) {
  
  sequenceoutput_ = value;
}
inline void LSTMParams::set_sequenceoutput(bool value) {
  _internal_set_sequenceoutput(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.sequenceOutput)
}

// bool hasBiasVectors = 20;
inline void LSTMParams::clear_hasbiasvectors() {
  hasbiasvectors_ = false;
}
inline bool LSTMParams::_internal_hasbiasvectors() const {
  return hasbiasvectors_;
}
inline bool LSTMParams::hasbiasvectors() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.hasBiasVectors)
  return _internal_hasbiasvectors();
}
inline void LSTMParams::_internal_set_hasbiasvectors(bool value) {
  
  hasbiasvectors_ = value;
}
inline void LSTMParams::set_hasbiasvectors(bool value) {
  _internal_set_hasbiasvectors(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.hasBiasVectors)
}

// bool forgetBias = 30;
inline void LSTMParams::clear_forgetbias() {
  forgetbias_ = false;
}
inline bool LSTMParams::_internal_forgetbias() const {
  return forgetbias_;
}
inline bool LSTMParams::forgetbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.forgetBias)
  return _internal_forgetbias();
}
inline void LSTMParams::_internal_set_forgetbias(bool value) {
  
  forgetbias_ = value;
}
inline void LSTMParams::set_forgetbias(bool value) {
  _internal_set_forgetbias(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.forgetBias)
}

// bool hasPeepholeVectors = 40;
inline void LSTMParams::clear_haspeepholevectors() {
  haspeepholevectors_ = false;
}
inline bool LSTMParams::_internal_haspeepholevectors() const {
  return haspeepholevectors_;
}
inline bool LSTMParams::haspeepholevectors() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.hasPeepholeVectors)
  return _internal_haspeepholevectors();
}
inline void LSTMParams::_internal_set_haspeepholevectors(bool value) {
  
  haspeepholevectors_ = value;
}
inline void LSTMParams::set_haspeepholevectors(bool value) {
  _internal_set_haspeepholevectors(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.hasPeepholeVectors)
}

// bool coupledInputAndForgetGate = 50;
inline void LSTMParams::clear_coupledinputandforgetgate() {
  coupledinputandforgetgate_ = false;
}
inline bool LSTMParams::_internal_coupledinputandforgetgate() const {
  return coupledinputandforgetgate_;
}
inline bool LSTMParams::coupledinputandforgetgate() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.coupledInputAndForgetGate)
  return _internal_coupledinputandforgetgate();
}
inline void LSTMParams::_internal_set_coupledinputandforgetgate(bool value) {
  
  coupledinputandforgetgate_ = value;
}
inline void LSTMParams::set_coupledinputandforgetgate(bool value) {
  _internal_set_coupledinputandforgetgate(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.coupledInputAndForgetGate)
}

// float cellClipThreshold = 60;
inline void LSTMParams::clear_cellclipthreshold() {
  cellclipthreshold_ = 0;
}
inline float LSTMParams::_internal_cellclipthreshold() const {
  return cellclipthreshold_;
}
inline float LSTMParams::cellclipthreshold() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.cellClipThreshold)
  return _internal_cellclipthreshold();
}
inline void LSTMParams::_internal_set_cellclipthreshold(float value) {
  
  cellclipthreshold_ = value;
}
inline void LSTMParams::set_cellclipthreshold(float value) {
  _internal_set_cellclipthreshold(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.cellClipThreshold)
}

// -------------------------------------------------------------------

// LSTMWeightParams

// .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
inline bool LSTMWeightParams::_internal_has_inputgateweightmatrix() const {
  return this != internal_default_instance() && inputgateweightmatrix_ != nullptr;
}
inline bool LSTMWeightParams::has_inputgateweightmatrix() const {
  return _internal_has_inputgateweightmatrix();
}
inline void LSTMWeightParams::clear_inputgateweightmatrix() {
  if (GetArenaForAllocation() == nullptr && inputgateweightmatrix_ != nullptr) {
    delete inputgateweightmatrix_;
  }
  inputgateweightmatrix_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::_internal_inputgateweightmatrix() const {
  const ::CoreML::Specification::WeightParams* p = inputgateweightmatrix_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
  return _internal_inputgateweightmatrix();
}
inline void LSTMWeightParams::unsafe_arena_set_allocated_inputgateweightmatrix(
    ::CoreML::Specification::WeightParams* inputgateweightmatrix) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(inputgateweightmatrix_);
  }
  inputgateweightmatrix_ = inputgateweightmatrix;
  if (inputgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgateweightmatrix() {
  
  ::CoreML::Specification::WeightParams* temp = inputgateweightmatrix_;
  inputgateweightmatrix_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::unsafe_arena_release_inputgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = inputgateweightmatrix_;
  inputgateweightmatrix_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::_internal_mutable_inputgateweightmatrix() {
  
  if (inputgateweightmatrix_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    inputgateweightmatrix_ = p;
  }
  return inputgateweightmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgateweightmatrix() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_inputgateweightmatrix();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
  return _msg;
}
inline void LSTMWeightParams::set_allocated_inputgateweightmatrix(::CoreML::Specification::WeightParams* inputgateweightmatrix) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete inputgateweightmatrix_;
  }
  if (inputgateweightmatrix) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(inputgateweightmatrix);
    if (message_arena != submessage_arena) {
      inputgateweightmatrix = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, inputgateweightmatrix, submessage_arena);
    }
    
  } else {
    
  }
  inputgateweightmatrix_ = inputgateweightmatrix;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
}

// .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
inline bool LSTMWeightParams::_internal_has_forgetgateweightmatrix() const {
  return this != internal_default_instance() && forgetgateweightmatrix_ != nullptr;
}
inline bool LSTMWeightParams::has_forgetgateweightmatrix() const {
  return _internal_has_forgetgateweightmatrix();
}
inline void LSTMWeightParams::clear_forgetgateweightmatrix() {
  if (GetArenaForAllocation() == nullptr && forgetgateweightmatrix_ != nullptr) {
    delete forgetgateweightmatrix_;
  }
  forgetgateweightmatrix_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::_internal_forgetgateweightmatrix() const {
  const ::CoreML::Specification::WeightParams* p = forgetgateweightmatrix_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
  return _internal_forgetgateweightmatrix();
}
inline void LSTMWeightParams::unsafe_arena_set_allocated_forgetgateweightmatrix(
    ::CoreML::Specification::WeightParams* forgetgateweightmatrix) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(forgetgateweightmatrix_);
  }
  forgetgateweightmatrix_ = forgetgateweightmatrix;
  if (forgetgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgateweightmatrix() {
  
  ::CoreML::Specification::WeightParams* temp = forgetgateweightmatrix_;
  forgetgateweightmatrix_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::unsafe_arena_release_forgetgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = forgetgateweightmatrix_;
  forgetgateweightmatrix_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::_internal_mutable_forgetgateweightmatrix() {
  
  if (forgetgateweightmatrix_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    forgetgateweightmatrix_ = p;
  }
  return forgetgateweightmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgateweightmatrix() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_forgetgateweightmatrix();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
  return _msg;
}
inline void LSTMWeightParams::set_allocated_forgetgateweightmatrix(::CoreML::Specification::WeightParams* forgetgateweightmatrix) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete forgetgateweightmatrix_;
  }
  if (forgetgateweightmatrix) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(forgetgateweightmatrix);
    if (message_arena != submessage_arena) {
      forgetgateweightmatrix = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, forgetgateweightmatrix, submessage_arena);
    }
    
  } else {
    
  }
  forgetgateweightmatrix_ = forgetgateweightmatrix;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
}

// .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
inline bool LSTMWeightParams::_internal_has_blockinputweightmatrix() const {
  return this != internal_default_instance() && blockinputweightmatrix_ != nullptr;
}
inline bool LSTMWeightParams::has_blockinputweightmatrix() const {
  return _internal_has_blockinputweightmatrix();
}
inline void LSTMWeightParams::clear_blockinputweightmatrix() {
  if (GetArenaForAllocation() == nullptr && blockinputweightmatrix_ != nullptr) {
    delete blockinputweightmatrix_;
  }
  blockinputweightmatrix_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::_internal_blockinputweightmatrix() const {
  const ::CoreML::Specification::WeightParams* p = blockinputweightmatrix_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::blockinputweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
  return _internal_blockinputweightmatrix();
}
inline void LSTMWeightParams::unsafe_arena_set_allocated_blockinputweightmatrix(
    ::CoreML::Specification::WeightParams* blockinputweightmatrix) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(blockinputweightmatrix_);
  }
  blockinputweightmatrix_ = blockinputweightmatrix;
  if (blockinputweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_blockinputweightmatrix() {
  
  ::CoreML::Specification::WeightParams* temp = blockinputweightmatrix_;
  blockinputweightmatrix_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::unsafe_arena_release_blockinputweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = blockinputweightmatrix_;
  blockinputweightmatrix_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::_internal_mutable_blockinputweightmatrix() {
  
  if (blockinputweightmatrix_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    blockinputweightmatrix_ = p;
  }
  return blockinputweightmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_blockinputweightmatrix() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_blockinputweightmatrix();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
  return _msg;
}
inline void LSTMWeightParams::set_allocated_blockinputweightmatrix(::CoreML::Specification::WeightParams* blockinputweightmatrix) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete blockinputweightmatrix_;
  }
  if (blockinputweightmatrix) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(blockinputweightmatrix);
    if (message_arena != submessage_arena) {
      blockinputweightmatrix = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, blockinputweightmatrix, submessage_arena);
    }
    
  } else {
    
  }
  blockinputweightmatrix_ = blockinputweightmatrix;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
}

// .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
inline bool LSTMWeightParams::_internal_has_outputgateweightmatrix() const {
  return this != internal_default_instance() && outputgateweightmatrix_ != nullptr;
}
inline bool LSTMWeightParams::has_outputgateweightmatrix() const {
  return _internal_has_outputgateweightmatrix();
}
inline void LSTMWeightParams::clear_outputgateweightmatrix() {
  if (GetArenaForAllocation() == nullptr && outputgateweightmatrix_ != nullptr) {
    delete outputgateweightmatrix_;
  }
  outputgateweightmatrix_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::_internal_outputgateweightmatrix() const {
  const ::CoreML::Specification::WeightParams* p = outputgateweightmatrix_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
  return _internal_outputgateweightmatrix();
}
inline void LSTMWeightParams::unsafe_arena_set_allocated_outputgateweightmatrix(
    ::CoreML::Specification::WeightParams* outputgateweightmatrix) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(outputgateweightmatrix_);
  }
  outputgateweightmatrix_ = outputgateweightmatrix;
  if (outputgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgateweightmatrix() {
  
  ::CoreML::Specification::WeightParams* temp = outputgateweightmatrix_;
  outputgateweightmatrix_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::unsafe_arena_release_outputgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgateweightmatrix_;
  outputgateweightmatrix_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::_internal_mutable_outputgateweightmatrix() {
  
  if (outputgateweightmatrix_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    outputgateweightmatrix_ = p;
  }
  return outputgateweightmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgateweightmatrix() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_outputgateweightmatrix();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
  return _msg;
}
inline void LSTMWeightParams::set_allocated_outputgateweightmatrix(::CoreML::Specification::WeightParams* outputgateweightmatrix) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete outputgateweightmatrix_;
  }
  if (outputgateweightmatrix) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(outputgateweightmatrix);
    if (message_arena != submessage_arena) {
      outputgateweightmatrix = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, outputgateweightmatrix, submessage_arena);
    }
    
  } else {
    
  }
  outputgateweightmatrix_ = outputgateweightmatrix;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
}

// .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
inline bool LSTMWeightParams::_internal_has_inputgaterecursionmatrix() const {
  return this != internal_default_instance() && inputgaterecursionmatrix_ != nullptr;
}
inline bool LSTMWeightParams::has_inputgaterecursionmatrix() const {
  return _internal_has_inputgaterecursionmatrix();
}
inline void LSTMWeightParams::clear_inputgaterecursionmatrix() {
  if (GetArenaForAllocation() == nullptr && inputgaterecursionmatrix_ != nullptr) {
    delete inputgaterecursionmatrix_;
  }
  inputgaterecursionmatrix_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::_internal_inputgaterecursionmatrix() const {
  const ::CoreML::Specification::WeightParams* p = inputgaterecursionmatrix_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
  return _internal_inputgaterecursionmatrix();
}
inline void LSTMWeightParams::unsafe_arena_set_allocated_inputgaterecursionmatrix(
    ::CoreML::Specification::WeightParams* inputgaterecursionmatrix) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(inputgaterecursionmatrix_);
  }
  inputgaterecursionmatrix_ = inputgaterecursionmatrix;
  if (inputgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgaterecursionmatrix() {
  
  ::CoreML::Specification::WeightParams* temp = inputgaterecursionmatrix_;
  inputgaterecursionmatrix_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::unsafe_arena_release_inputgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = inputgaterecursionmatrix_;
  inputgaterecursionmatrix_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::_internal_mutable_inputgaterecursionmatrix() {
  
  if (inputgaterecursionmatrix_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    inputgaterecursionmatrix_ = p;
  }
  return inputgaterecursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgaterecursionmatrix() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_inputgaterecursionmatrix();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
  return _msg;
}
inline void LSTMWeightParams::set_allocated_inputgaterecursionmatrix(::CoreML::Specification::WeightParams* inputgaterecursionmatrix) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete inputgaterecursionmatrix_;
  }
  if (inputgaterecursionmatrix) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(inputgaterecursionmatrix);
    if (message_arena != submessage_arena) {
      inputgaterecursionmatrix = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, inputgaterecursionmatrix, submessage_arena);
    }
    
  } else {
    
  }
  inputgaterecursionmatrix_ = inputgaterecursionmatrix;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
inline bool LSTMWeightParams::_internal_has_forgetgaterecursionmatrix() const {
  return this != internal_default_instance() && forgetgaterecursionmatrix_ != nullptr;
}
inline bool LSTMWeightParams::has_forgetgaterecursionmatrix() const {
  return _internal_has_forgetgaterecursionmatrix();
}
inline void LSTMWeightParams::clear_forgetgaterecursionmatrix() {
  if (GetArenaForAllocation() == nullptr && forgetgaterecursionmatrix_ != nullptr) {
    delete forgetgaterecursionmatrix_;
  }
  forgetgaterecursionmatrix_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::_internal_forgetgaterecursionmatrix() const {
  const ::CoreML::Specification::WeightParams* p = forgetgaterecursionmatrix_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
  return _internal_forgetgaterecursionmatrix();
}
inline void LSTMWeightParams::unsafe_arena_set_allocated_forgetgaterecursionmatrix(
    ::CoreML::Specification::WeightParams* forgetgaterecursionmatrix) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(forgetgaterecursionmatrix_);
  }
  forgetgaterecursionmatrix_ = forgetgaterecursionmatrix;
  if (forgetgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgaterecursionmatrix() {
  
  ::CoreML::Specification::WeightParams* temp = forgetgaterecursionmatrix_;
  forgetgaterecursionmatrix_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::unsafe_arena_release_forgetgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = forgetgaterecursionmatrix_;
  forgetgaterecursionmatrix_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::_internal_mutable_forgetgaterecursionmatrix() {
  
  if (forgetgaterecursionmatrix_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    forgetgaterecursionmatrix_ = p;
  }
  return forgetgaterecursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgaterecursionmatrix() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_forgetgaterecursionmatrix();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
  return _msg;
}
inline void LSTMWeightParams::set_allocated_forgetgaterecursionmatrix(::CoreML::Specification::WeightParams* forgetgaterecursionmatrix) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete forgetgaterecursionmatrix_;
  }
  if (forgetgaterecursionmatrix) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(forgetgaterecursionmatrix);
    if (message_arena != submessage_arena) {
      forgetgaterecursionmatrix = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, forgetgaterecursionmatrix, submessage_arena);
    }
    
  } else {
    
  }
  forgetgaterecursionmatrix_ = forgetgaterecursionmatrix;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
inline bool LSTMWeightParams::_internal_has_blockinputrecursionmatrix() const {
  return this != internal_default_instance() && blockinputrecursionmatrix_ != nullptr;
}
inline bool LSTMWeightParams::has_blockinputrecursionmatrix() const {
  return _internal_has_blockinputrecursionmatrix();
}
inline void LSTMWeightParams::clear_blockinputrecursionmatrix() {
  if (GetArenaForAllocation() == nullptr && blockinputrecursionmatrix_ != nullptr) {
    delete blockinputrecursionmatrix_;
  }
  blockinputrecursionmatrix_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::_internal_blockinputrecursionmatrix() const {
  const ::CoreML::Specification::WeightParams* p = blockinputrecursionmatrix_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::blockinputrecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
  return _internal_blockinputrecursionmatrix();
}
inline void LSTMWeightParams::unsafe_arena_set_allocated_blockinputrecursionmatrix(
    ::CoreML::Specification::WeightParams* blockinputrecursionmatrix) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(blockinputrecursionmatrix_);
  }
  blockinputrecursionmatrix_ = blockinputrecursionmatrix;
  if (blockinputrecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_blockinputrecursionmatrix() {
  
  ::CoreML::Specification::WeightParams* temp = blockinputrecursionmatrix_;
  blockinputrecursionmatrix_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::unsafe_arena_release_blockinputrecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = blockinputrecursionmatrix_;
  blockinputrecursionmatrix_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::_internal_mutable_blockinputrecursionmatrix() {
  
  if (blockinputrecursionmatrix_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    blockinputrecursionmatrix_ = p;
  }
  return blockinputrecursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_blockinputrecursionmatrix() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_blockinputrecursionmatrix();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
  return _msg;
}
inline void LSTMWeightParams::set_allocated_blockinputrecursionmatrix(::CoreML::Specification::WeightParams* blockinputrecursionmatrix) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete blockinputrecursionmatrix_;
  }
  if (blockinputrecursionmatrix) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(blockinputrecursionmatrix);
    if (message_arena != submessage_arena) {
      blockinputrecursionmatrix = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, blockinputrecursionmatrix, submessage_arena);
    }
    
  } else {
    
  }
  blockinputrecursionmatrix_ = blockinputrecursionmatrix;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
}

// .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
inline bool LSTMWeightParams::_internal_has_outputgaterecursionmatrix() const {
  return this != internal_default_instance() && outputgaterecursionmatrix_ != nullptr;
}
inline bool LSTMWeightParams::has_outputgaterecursionmatrix() const {
  return _internal_has_outputgaterecursionmatrix();
}
inline void LSTMWeightParams::clear_outputgaterecursionmatrix() {
  if (GetArenaForAllocation() == nullptr && outputgaterecursionmatrix_ != nullptr) {
    delete outputgaterecursionmatrix_;
  }
  outputgaterecursionmatrix_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::_internal_outputgaterecursionmatrix() const {
  const ::CoreML::Specification::WeightParams* p = outputgaterecursionmatrix_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
  return _internal_outputgaterecursionmatrix();
}
inline void LSTMWeightParams::unsafe_arena_set_allocated_outputgaterecursionmatrix(
    ::CoreML::Specification::WeightParams* outputgaterecursionmatrix) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(outputgaterecursionmatrix_);
  }
  outputgaterecursionmatrix_ = outputgaterecursionmatrix;
  if (outputgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgaterecursionmatrix() {
  
  ::CoreML::Specification::WeightParams* temp = outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::unsafe_arena_release_outputgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::_internal_mutable_outputgaterecursionmatrix() {
  
  if (outputgaterecursionmatrix_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    outputgaterecursionmatrix_ = p;
  }
  return outputgaterecursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgaterecursionmatrix() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_outputgaterecursionmatrix();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
  return _msg;
}
inline void LSTMWeightParams::set_allocated_outputgaterecursionmatrix(::CoreML::Specification::WeightParams* outputgaterecursionmatrix) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete outputgaterecursionmatrix_;
  }
  if (outputgaterecursionmatrix) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(outputgaterecursionmatrix);
    if (message_arena != submessage_arena) {
      outputgaterecursionmatrix = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, outputgaterecursionmatrix, submessage_arena);
    }
    
  } else {
    
  }
  outputgaterecursionmatrix_ = outputgaterecursionmatrix;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams inputGateBiasVector = 40;
inline bool LSTMWeightParams::_internal_has_inputgatebiasvector() const {
  return this != internal_default_instance() && inputgatebiasvector_ != nullptr;
}
inline bool LSTMWeightParams::has_inputgatebiasvector() const {
  return _internal_has_inputgatebiasvector();
}
inline void LSTMWeightParams::clear_inputgatebiasvector() {
  if (GetArenaForAllocation() == nullptr && inputgatebiasvector_ != nullptr) {
    delete inputgatebiasvector_;
  }
  inputgatebiasvector_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::_internal_inputgatebiasvector() const {
  const ::CoreML::Specification::WeightParams* p = inputgatebiasvector_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
  return _internal_inputgatebiasvector();
}
inline void LSTMWeightParams::unsafe_arena_set_allocated_inputgatebiasvector(
    ::CoreML::Specification::WeightParams* inputgatebiasvector) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(inputgatebiasvector_);
  }
  inputgatebiasvector_ = inputgatebiasvector;
  if (inputgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgatebiasvector() {
  
  ::CoreML::Specification::WeightParams* temp = inputgatebiasvector_;
  inputgatebiasvector_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::unsafe_arena_release_inputgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = inputgatebiasvector_;
  inputgatebiasvector_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::_internal_mutable_inputgatebiasvector() {
  
  if (inputgatebiasvector_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    inputgatebiasvector_ = p;
  }
  return inputgatebiasvector_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgatebiasvector() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_inputgatebiasvector();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
  return _msg;
}
inline void LSTMWeightParams::set_allocated_inputgatebiasvector(::CoreML::Specification::WeightParams* inputgatebiasvector) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete inputgatebiasvector_;
  }
  if (inputgatebiasvector) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(inputgatebiasvector);
    if (message_arena != submessage_arena) {
      inputgatebiasvector = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, inputgatebiasvector, submessage_arena);
    }
    
  } else {
    
  }
  inputgatebiasvector_ = inputgatebiasvector;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
}

// .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
inline bool LSTMWeightParams::_internal_has_forgetgatebiasvector() const {
  return this != internal_default_instance() && forgetgatebiasvector_ != nullptr;
}
inline bool LSTMWeightParams::has_forgetgatebiasvector() const {
  return _internal_has_forgetgatebiasvector();
}
inline void LSTMWeightParams::clear_forgetgatebiasvector() {
  if (GetArenaForAllocation() == nullptr && forgetgatebiasvector_ != nullptr) {
    delete forgetgatebiasvector_;
  }
  forgetgatebiasvector_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::_internal_forgetgatebiasvector() const {
  const ::CoreML::Specification::WeightParams* p = forgetgatebiasvector_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
  return _internal_forgetgatebiasvector();
}
inline void LSTMWeightParams::unsafe_arena_set_allocated_forgetgatebiasvector(
    ::CoreML::Specification::WeightParams* forgetgatebiasvector) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(forgetgatebiasvector_);
  }
  forgetgatebiasvector_ = forgetgatebiasvector;
  if (forgetgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgatebiasvector() {
  
  ::CoreML::Specification::WeightParams* temp = forgetgatebiasvector_;
  forgetgatebiasvector_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::unsafe_arena_release_forgetgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = forgetgatebiasvector_;
  forgetgatebiasvector_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::_internal_mutable_forgetgatebiasvector() {
  
  if (forgetgatebiasvector_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    forgetgatebiasvector_ = p;
  }
  return forgetgatebiasvector_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgatebiasvector() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_forgetgatebiasvector();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
  return _msg;
}
inline void LSTMWeightParams::set_allocated_forgetgatebiasvector(::CoreML::Specification::WeightParams* forgetgatebiasvector) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete forgetgatebiasvector_;
  }
  if (forgetgatebiasvector) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(forgetgatebiasvector);
    if (message_arena != submessage_arena) {
      forgetgatebiasvector = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, forgetgatebiasvector, submessage_arena);
    }
    
  } else {
    
  }
  forgetgatebiasvector_ = forgetgatebiasvector;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
}

// .CoreML.Specification.WeightParams blockInputBiasVector = 42;
inline bool LSTMWeightParams::_internal_has_blockinputbiasvector() const {
  return this != internal_default_instance() && blockinputbiasvector_ != nullptr;
}
inline bool LSTMWeightParams::has_blockinputbiasvector() const {
  return _internal_has_blockinputbiasvector();
}
inline void LSTMWeightParams::clear_blockinputbiasvector() {
  if (GetArenaForAllocation() == nullptr && blockinputbiasvector_ != nullptr) {
    delete blockinputbiasvector_;
  }
  blockinputbiasvector_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::_internal_blockinputbiasvector() const {
  const ::CoreML::Specification::WeightParams* p = blockinputbiasvector_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::blockinputbiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
  return _internal_blockinputbiasvector();
}
inline void LSTMWeightParams::unsafe_arena_set_allocated_blockinputbiasvector(
    ::CoreML::Specification::WeightParams* blockinputbiasvector) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(blockinputbiasvector_);
  }
  blockinputbiasvector_ = blockinputbiasvector;
  if (blockinputbiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_blockinputbiasvector() {
  
  ::CoreML::Specification::WeightParams* temp = blockinputbiasvector_;
  blockinputbiasvector_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::unsafe_arena_release_blockinputbiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = blockinputbiasvector_;
  blockinputbiasvector_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::_internal_mutable_blockinputbiasvector() {
  
  if (blockinputbiasvector_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    blockinputbiasvector_ = p;
  }
  return blockinputbiasvector_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_blockinputbiasvector() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_blockinputbiasvector();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
  return _msg;
}
inline void LSTMWeightParams::set_allocated_blockinputbiasvector(::CoreML::Specification::WeightParams* blockinputbiasvector) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete blockinputbiasvector_;
  }
  if (blockinputbiasvector) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(blockinputbiasvector);
    if (message_arena != submessage_arena) {
      blockinputbiasvector = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, blockinputbiasvector, submessage_arena);
    }
    
  } else {
    
  }
  blockinputbiasvector_ = blockinputbiasvector;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
}

// .CoreML.Specification.WeightParams outputGateBiasVector = 43;
inline bool LSTMWeightParams::_internal_has_outputgatebiasvector() const {
  return this != internal_default_instance() && outputgatebiasvector_ != nullptr;
}
inline bool LSTMWeightParams::has_outputgatebiasvector() const {
  return _internal_has_outputgatebiasvector();
}
inline void LSTMWeightParams::clear_outputgatebiasvector() {
  if (GetArenaForAllocation() == nullptr && outputgatebiasvector_ != nullptr) {
    delete outputgatebiasvector_;
  }
  outputgatebiasvector_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::_internal_outputgatebiasvector() const {
  const ::CoreML::Specification::WeightParams* p = outputgatebiasvector_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
  return _internal_outputgatebiasvector();
}
inline void LSTMWeightParams::unsafe_arena_set_allocated_outputgatebiasvector(
    ::CoreML::Specification::WeightParams* outputgatebiasvector) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(outputgatebiasvector_);
  }
  outputgatebiasvector_ = outputgatebiasvector;
  if (outputgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgatebiasvector() {
  
  ::CoreML::Specification::WeightParams* temp = outputgatebiasvector_;
  outputgatebiasvector_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::unsafe_arena_release_outputgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = outputgatebiasvector_;
  outputgatebiasvector_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::_internal_mutable_outputgatebiasvector() {
  
  if (outputgatebiasvector_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    outputgatebiasvector_ = p;
  }
  return outputgatebiasvector_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgatebiasvector() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_outputgatebiasvector();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
  return _msg;
}
inline void LSTMWeightParams::set_allocated_outputgatebiasvector(::CoreML::Specification::WeightParams* outputgatebiasvector) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete outputgatebiasvector_;
  }
  if (outputgatebiasvector) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(outputgatebiasvector);
    if (message_arena != submessage_arena) {
      outputgatebiasvector = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, outputgatebiasvector, submessage_arena);
    }
    
  } else {
    
  }
  outputgatebiasvector_ = outputgatebiasvector;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
}

// .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
inline bool LSTMWeightParams::_internal_has_inputgatepeepholevector() const {
  return this != internal_default_instance() && inputgatepeepholevector_ != nullptr;
}
inline bool LSTMWeightParams::has_inputgatepeepholevector() const {
  return _internal_has_inputgatepeepholevector();
}
inline void LSTMWeightParams::clear_inputgatepeepholevector() {
  if (GetArenaForAllocation() == nullptr && inputgatepeepholevector_ != nullptr) {
    delete inputgatepeepholevector_;
  }
  inputgatepeepholevector_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::_internal_inputgatepeepholevector() const {
  const ::CoreML::Specification::WeightParams* p = inputgatepeepholevector_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgatepeepholevector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
  return _internal_inputgatepeepholevector();
}
inline void LSTMWeightParams::unsafe_arena_set_allocated_inputgatepeepholevector(
    ::CoreML::Specification::WeightParams* inputgatepeepholevector) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(inputgatepeepholevector_);
  }
  inputgatepeepholevector_ = inputgatepeepholevector;
  if (inputgatepeepholevector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgatepeepholevector() {
  
  ::CoreML::Specification::WeightParams* temp = inputgatepeepholevector_;
  inputgatepeepholevector_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::unsafe_arena_release_inputgatepeepholevector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
  
  ::CoreML::Specification::WeightParams* temp = inputgatepeepholevector_;
  inputgatepeepholevector_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::_internal_mutable_inputgatepeepholevector() {
  
  if (inputgatepeepholevector_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    inputgatepeepholevector_ = p;
  }
  return inputgatepeepholevector_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgatepeepholevector() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_inputgatepeepholevector();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
  return _msg;
}
inline void LSTMWeightParams::set_allocated_inputgatepeepholevector(::CoreML::Specification::WeightParams* inputgatepeepholevector) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete inputgatepeepholevector_;
  }
  if (inputgatepeepholevector) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(inputgatepeepholevector);
    if (message_arena != submessage_arena) {
      inputgatepeepholevector = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, inputgatepeepholevector, submessage_arena);
    }
    
  } else {
    
  }
  inputgatepeepholevector_ = inputgatepeepholevector;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
}

// .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
inline bool LSTMWeightParams::_internal_has_forgetgatepeepholevector() const {
  return this != internal_default_instance() && forgetgatepeepholevector_ != nullptr;
}
inline bool LSTMWeightParams::has_forgetgatepeepholevector() const {
  return _internal_has_forgetgatepeepholevector();
}
inline void LSTMWeightParams::clear_forgetgatepeepholevector() {
  if (GetArenaForAllocation() == nullptr && forgetgatepeepholevector_ != nullptr) {
    delete forgetgatepeepholevector_;
  }
  forgetgatepeepholevector_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::_internal_forgetgatepeepholevector() const {
  const ::CoreML::Specification::WeightParams* p = forgetgatepeepholevector_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgatepeepholevector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
  return _internal_forgetgatepeepholevector();
}
inline void LSTMWeightParams::unsafe_arena_set_allocated_forgetgatepeepholevector(
    ::CoreML::Specification::WeightParams* forgetgatepeepholevector) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(forgetgatepeepholevector_);
  }
  forgetgatepeepholevector_ = forgetgatepeepholevector;
  if (forgetgatepeepholevector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgatepeepholevector() {
  
  ::CoreML::Specification::WeightParams* temp = forgetgatepeepholevector_;
  forgetgatepeepholevector_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::unsafe_arena_release_forgetgatepeepholevector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
  
  ::CoreML::Specification::WeightParams* temp = forgetgatepeepholevector_;
  forgetgatepeepholevector_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::_internal_mutable_forgetgatepeepholevector() {
  
  if (forgetgatepeepholevector_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    forgetgatepeepholevector_ = p;
  }
  return forgetgatepeepholevector_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgatepeepholevector() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_forgetgatepeepholevector();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
  return _msg;
}
inline void LSTMWeightParams::set_allocated_forgetgatepeepholevector(::CoreML::Specification::WeightParams* forgetgatepeepholevector) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete forgetgatepeepholevector_;
  }
  if (forgetgatepeepholevector) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(forgetgatepeepholevector);
    if (message_arena != submessage_arena) {
      forgetgatepeepholevector = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, forgetgatepeepholevector, submessage_arena);
    }
    
  } else {
    
  }
  forgetgatepeepholevector_ = forgetgatepeepholevector;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
}

// .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
inline bool LSTMWeightParams::_internal_has_outputgatepeepholevector() const {
  return this != internal_default_instance() && outputgatepeepholevector_ != nullptr;
}
inline bool LSTMWeightParams::has_outputgatepeepholevector() const {
  return _internal_has_outputgatepeepholevector();
}
inline void LSTMWeightParams::clear_outputgatepeepholevector() {
  if (GetArenaForAllocation() == nullptr && outputgatepeepholevector_ != nullptr) {
    delete outputgatepeepholevector_;
  }
  outputgatepeepholevector_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::_internal_outputgatepeepholevector() const {
  const ::CoreML::Specification::WeightParams* p = outputgatepeepholevector_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgatepeepholevector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
  return _internal_outputgatepeepholevector();
}
inline void LSTMWeightParams::unsafe_arena_set_allocated_outputgatepeepholevector(
    ::CoreML::Specification::WeightParams* outputgatepeepholevector) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(outputgatepeepholevector_);
  }
  outputgatepeepholevector_ = outputgatepeepholevector;
  if (outputgatepeepholevector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgatepeepholevector() {
  
  ::CoreML::Specification::WeightParams* temp = outputgatepeepholevector_;
  outputgatepeepholevector_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::unsafe_arena_release_outputgatepeepholevector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
  
  ::CoreML::Specification::WeightParams* temp = outputgatepeepholevector_;
  outputgatepeepholevector_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::_internal_mutable_outputgatepeepholevector() {
  
  if (outputgatepeepholevector_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    outputgatepeepholevector_ = p;
  }
  return outputgatepeepholevector_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgatepeepholevector() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_outputgatepeepholevector();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
  return _msg;
}
inline void LSTMWeightParams::set_allocated_outputgatepeepholevector(::CoreML::Specification::WeightParams* outputgatepeepholevector) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete outputgatepeepholevector_;
  }
  if (outputgatepeepholevector) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(outputgatepeepholevector);
    if (message_arena != submessage_arena) {
      outputgatepeepholevector = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, outputgatepeepholevector, submessage_arena);
    }
    
  } else {
    
  }
  outputgatepeepholevector_ = outputgatepeepholevector;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
}

// -------------------------------------------------------------------

// UniDirectionalLSTMLayerParams

// uint64 inputVectorSize = 1;
inline void UniDirectionalLSTMLayerParams::clear_inputvectorsize() {
  inputvectorsize_ = uint64_t{0u};
}
inline uint64_t UniDirectionalLSTMLayerParams::_internal_inputvectorsize() const {
  return inputvectorsize_;
}
inline uint64_t UniDirectionalLSTMLayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.inputVectorSize)
  return _internal_inputvectorsize();
}
inline void UniDirectionalLSTMLayerParams::_internal_set_inputvectorsize(uint64_t value) {
  
  inputvectorsize_ = value;
}
inline void UniDirectionalLSTMLayerParams::set_inputvectorsize(uint64_t value) {
  _internal_set_inputvectorsize(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.UniDirectionalLSTMLayerParams.inputVectorSize)
}

// uint64 outputVectorSize = 2;
inline void UniDirectionalLSTMLayerParams::clear_outputvectorsize() {
  outputvectorsize_ = uint64_t{0u};
}
inline uint64_t UniDirectionalLSTMLayerParams::_internal_outputvectorsize() const {
  return outputvectorsize_;
}
inline uint64_t UniDirectionalLSTMLayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.outputVectorSize)
  return _internal_outputvectorsize();
}
inline void UniDirectionalLSTMLayerParams::_internal_set_outputvectorsize(uint64_t value) {
  
  outputvectorsize_ = value;
}
inline void UniDirectionalLSTMLayerParams::set_outputvectorsize(uint64_t value) {
  _internal_set_outputvectorsize(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.UniDirectionalLSTMLayerParams.outputVectorSize)
}

// repeated .CoreML.Specification.ActivationParams activations = 10;
inline int UniDirectionalLSTMLayerParams::_internal_activations_size() const {
  return activations_.size();
}
inline int UniDirectionalLSTMLayerParams::activations_size() const {
  return _internal_activations_size();
}
inline void UniDirectionalLSTMLayerParams::clear_activations() {
  activations_.Clear();
}
inline ::CoreML::Specification::ActivationParams* UniDirectionalLSTMLayerParams::mutable_activations(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
UniDirectionalLSTMLayerParams::mutable_activations() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return &activations_;
}
inline const ::CoreML::Specification::ActivationParams& UniDirectionalLSTMLayerParams::_internal_activations(int index) const {
  return activations_.Get(index);
}
inline const ::CoreML::Specification::ActivationParams& UniDirectionalLSTMLayerParams::activations(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return _internal_activations(index);
}
inline ::CoreML::Specification::ActivationParams* UniDirectionalLSTMLayerParams::_internal_add_activations() {
  return activations_.Add();
}
inline ::CoreML::Specification::ActivationParams* UniDirectionalLSTMLayerParams::add_activations() {
  ::CoreML::Specification::ActivationParams* _add = _internal_add_activations();
  // @@protoc_insertion_point(field_add:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
UniDirectionalLSTMLayerParams::activations() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_;
}

// .CoreML.Specification.LSTMParams params = 15;
inline bool UniDirectionalLSTMLayerParams::_internal_has_params() const {
  return this != internal_default_instance() && params_ != nullptr;
}
inline bool UniDirectionalLSTMLayerParams::has_params() const {
  return _internal_has_params();
}
inline void UniDirectionalLSTMLayerParams::clear_params() {
  if (GetArenaForAllocation() == nullptr && params_ != nullptr) {
    delete params_;
  }
  params_ = nullptr;
}
inline const ::CoreML::Specification::LSTMParams& UniDirectionalLSTMLayerParams::_internal_params() const {
  const ::CoreML::Specification::LSTMParams* p = params_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::LSTMParams&>(
      ::CoreML::Specification::_LSTMParams_default_instance_);
}
inline const ::CoreML::Specification::LSTMParams& UniDirectionalLSTMLayerParams::params() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
  return _internal_params();
}
inline void UniDirectionalLSTMLayerParams::unsafe_arena_set_allocated_params(
    ::CoreML::Specification::LSTMParams* params) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(params_);
  }
  params_ = params;
  if (params) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
}
inline ::CoreML::Specification::LSTMParams* UniDirectionalLSTMLayerParams::release_params() {
  
  ::CoreML::Specification::LSTMParams* temp = params_;
  params_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::LSTMParams* UniDirectionalLSTMLayerParams::unsafe_arena_release_params() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
  
  ::CoreML::Specification::LSTMParams* temp = params_;
  params_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::LSTMParams* UniDirectionalLSTMLayerParams::_internal_mutable_params() {
  
  if (params_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::LSTMParams>(GetArenaForAllocation());
    params_ = p;
  }
  return params_;
}
inline ::CoreML::Specification::LSTMParams* UniDirectionalLSTMLayerParams::mutable_params() {
  ::CoreML::Specification::LSTMParams* _msg = _internal_mutable_params();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
  return _msg;
}
inline void UniDirectionalLSTMLayerParams::set_allocated_params(::CoreML::Specification::LSTMParams* params) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete params_;
  }
  if (params) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::LSTMParams>::GetOwningArena(params);
    if (message_arena != submessage_arena) {
      params = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, params, submessage_arena);
    }
    
  } else {
    
  }
  params_ = params;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
}

// .CoreML.Specification.LSTMWeightParams weightParams = 20;
inline bool UniDirectionalLSTMLayerParams::_internal_has_weightparams() const {
  return this != internal_default_instance() && weightparams_ != nullptr;
}
inline bool UniDirectionalLSTMLayerParams::has_weightparams() const {
  return _internal_has_weightparams();
}
inline void UniDirectionalLSTMLayerParams::clear_weightparams() {
  if (GetArenaForAllocation() == nullptr && weightparams_ != nullptr) {
    delete weightparams_;
  }
  weightparams_ = nullptr;
}
inline const ::CoreML::Specification::LSTMWeightParams& UniDirectionalLSTMLayerParams::_internal_weightparams() const {
  const ::CoreML::Specification::LSTMWeightParams* p = weightparams_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::LSTMWeightParams&>(
      ::CoreML::Specification::_LSTMWeightParams_default_instance_);
}
inline const ::CoreML::Specification::LSTMWeightParams& UniDirectionalLSTMLayerParams::weightparams() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
  return _internal_weightparams();
}
inline void UniDirectionalLSTMLayerParams::unsafe_arena_set_allocated_weightparams(
    ::CoreML::Specification::LSTMWeightParams* weightparams) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(weightparams_);
  }
  weightparams_ = weightparams;
  if (weightparams) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
}
inline ::CoreML::Specification::LSTMWeightParams* UniDirectionalLSTMLayerParams::release_weightparams() {
  
  ::CoreML::Specification::LSTMWeightParams* temp = weightparams_;
  weightparams_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::LSTMWeightParams* UniDirectionalLSTMLayerParams::unsafe_arena_release_weightparams() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
  
  ::CoreML::Specification::LSTMWeightParams* temp = weightparams_;
  weightparams_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::LSTMWeightParams* UniDirectionalLSTMLayerParams::_internal_mutable_weightparams() {
  
  if (weightparams_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::LSTMWeightParams>(GetArenaForAllocation());
    weightparams_ = p;
  }
  return weightparams_;
}
inline ::CoreML::Specification::LSTMWeightParams* UniDirectionalLSTMLayerParams::mutable_weightparams() {
  ::CoreML::Specification::LSTMWeightParams* _msg = _internal_mutable_weightparams();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
  return _msg;
}
inline void UniDirectionalLSTMLayerParams::set_allocated_weightparams(::CoreML::Specification::LSTMWeightParams* weightparams) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete weightparams_;
  }
  if (weightparams) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::LSTMWeightParams>::GetOwningArena(weightparams);
    if (message_arena != submessage_arena) {
      weightparams = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, weightparams, submessage_arena);
    }
    
  } else {
    
  }
  weightparams_ = weightparams;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
}

// bool reverseInput = 100;
inline void UniDirectionalLSTMLayerParams::clear_reverseinput() {
  reverseinput_ = false;
}
inline bool UniDirectionalLSTMLayerParams::_internal_reverseinput() const {
  return reverseinput_;
}
inline bool UniDirectionalLSTMLayerParams::reverseinput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.reverseInput)
  return _internal_reverseinput();
}
inline void UniDirectionalLSTMLayerParams::_internal_set_reverseinput(bool value) {
  
  reverseinput_ = value;
}
inline void UniDirectionalLSTMLayerParams::set_reverseinput(bool value) {
  _internal_set_reverseinput(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.UniDirectionalLSTMLayerParams.reverseInput)
}

// -------------------------------------------------------------------

// BiDirectionalLSTMLayerParams

// uint64 inputVectorSize = 1;
inline void BiDirectionalLSTMLayerParams::clear_inputvectorsize() {
  inputvectorsize_ = uint64_t{0u};
}
inline uint64_t BiDirectionalLSTMLayerParams::_internal_inputvectorsize() const {
  return inputvectorsize_;
}
inline uint64_t BiDirectionalLSTMLayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.inputVectorSize)
  return _internal_inputvectorsize();
}
inline void BiDirectionalLSTMLayerParams::_internal_set_inputvectorsize(uint64_t value) {
  
  inputvectorsize_ = value;
}
inline void BiDirectionalLSTMLayerParams::set_inputvectorsize(uint64_t value) {
  _internal_set_inputvectorsize(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BiDirectionalLSTMLayerParams.inputVectorSize)
}

// uint64 outputVectorSize = 2;
inline void BiDirectionalLSTMLayerParams::clear_outputvectorsize() {
  outputvectorsize_ = uint64_t{0u};
}
inline uint64_t BiDirectionalLSTMLayerParams::_internal_outputvectorsize() const {
  return outputvectorsize_;
}
inline uint64_t BiDirectionalLSTMLayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.outputVectorSize)
  return _internal_outputvectorsize();
}
inline void BiDirectionalLSTMLayerParams::_internal_set_outputvectorsize(uint64_t value) {
  
  outputvectorsize_ = value;
}
inline void BiDirectionalLSTMLayerParams::set_outputvectorsize(uint64_t value) {
  _internal_set_outputvectorsize(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BiDirectionalLSTMLayerParams.outputVectorSize)
}

// repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
inline int BiDirectionalLSTMLayerParams::_internal_activationsforwardlstm_size() const {
  return activationsforwardlstm_.size();
}
inline int BiDirectionalLSTMLayerParams::activationsforwardlstm_size() const {
  return _internal_activationsforwardlstm_size();
}
inline void BiDirectionalLSTMLayerParams::clear_activationsforwardlstm() {
  activationsforwardlstm_.Clear();
}
inline ::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::mutable_activationsforwardlstm(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
BiDirectionalLSTMLayerParams::mutable_activationsforwardlstm() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return &activationsforwardlstm_;
}
inline const ::CoreML::Specification::ActivationParams& BiDirectionalLSTMLayerParams::_internal_activationsforwardlstm(int index) const {
  return activationsforwardlstm_.Get(index);
}
inline const ::CoreML::Specification::ActivationParams& BiDirectionalLSTMLayerParams::activationsforwardlstm(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return _internal_activationsforwardlstm(index);
}
inline ::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::_internal_add_activationsforwardlstm() {
  return activationsforwardlstm_.Add();
}
inline ::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::add_activationsforwardlstm() {
  ::CoreML::Specification::ActivationParams* _add = _internal_add_activationsforwardlstm();
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
BiDirectionalLSTMLayerParams::activationsforwardlstm() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_;
}

// repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
inline int BiDirectionalLSTMLayerParams::_internal_activationsbackwardlstm_size() const {
  return activationsbackwardlstm_.size();
}
inline int BiDirectionalLSTMLayerParams::activationsbackwardlstm_size() const {
  return _internal_activationsbackwardlstm_size();
}
inline void BiDirectionalLSTMLayerParams::clear_activationsbackwardlstm() {
  activationsbackwardlstm_.Clear();
}
inline ::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::mutable_activationsbackwardlstm(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
BiDirectionalLSTMLayerParams::mutable_activationsbackwardlstm() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return &activationsbackwardlstm_;
}
inline const ::CoreML::Specification::ActivationParams& BiDirectionalLSTMLayerParams::_internal_activationsbackwardlstm(int index) const {
  return activationsbackwardlstm_.Get(index);
}
inline const ::CoreML::Specification::ActivationParams& BiDirectionalLSTMLayerParams::activationsbackwardlstm(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return _internal_activationsbackwardlstm(index);
}
inline ::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::_internal_add_activationsbackwardlstm() {
  return activationsbackwardlstm_.Add();
}
inline ::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::add_activationsbackwardlstm() {
  ::CoreML::Specification::ActivationParams* _add = _internal_add_activationsbackwardlstm();
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
BiDirectionalLSTMLayerParams::activationsbackwardlstm() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_;
}

// .CoreML.Specification.LSTMParams params = 15;
inline bool BiDirectionalLSTMLayerParams::_internal_has_params() const {
  return this != internal_default_instance() && params_ != nullptr;
}
inline bool BiDirectionalLSTMLayerParams::has_params() const {
  return _internal_has_params();
}
inline void BiDirectionalLSTMLayerParams::clear_params() {
  if (GetArenaForAllocation() == nullptr && params_ != nullptr) {
    delete params_;
  }
  params_ = nullptr;
}
inline const ::CoreML::Specification::LSTMParams& BiDirectionalLSTMLayerParams::_internal_params() const {
  const ::CoreML::Specification::LSTMParams* p = params_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::LSTMParams&>(
      ::CoreML::Specification::_LSTMParams_default_instance_);
}
inline const ::CoreML::Specification::LSTMParams& BiDirectionalLSTMLayerParams::params() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
  return _internal_params();
}
inline void BiDirectionalLSTMLayerParams::unsafe_arena_set_allocated_params(
    ::CoreML::Specification::LSTMParams* params) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(params_);
  }
  params_ = params;
  if (params) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
}
inline ::CoreML::Specification::LSTMParams* BiDirectionalLSTMLayerParams::release_params() {
  
  ::CoreML::Specification::LSTMParams* temp = params_;
  params_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::LSTMParams* BiDirectionalLSTMLayerParams::unsafe_arena_release_params() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
  
  ::CoreML::Specification::LSTMParams* temp = params_;
  params_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::LSTMParams* BiDirectionalLSTMLayerParams::_internal_mutable_params() {
  
  if (params_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::LSTMParams>(GetArenaForAllocation());
    params_ = p;
  }
  return params_;
}
inline ::CoreML::Specification::LSTMParams* BiDirectionalLSTMLayerParams::mutable_params() {
  ::CoreML::Specification::LSTMParams* _msg = _internal_mutable_params();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
  return _msg;
}
inline void BiDirectionalLSTMLayerParams::set_allocated_params(::CoreML::Specification::LSTMParams* params) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete params_;
  }
  if (params) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::LSTMParams>::GetOwningArena(params);
    if (message_arena != submessage_arena) {
      params = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, params, submessage_arena);
    }
    
  } else {
    
  }
  params_ = params;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
}

// repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
inline int BiDirectionalLSTMLayerParams::_internal_weightparams_size() const {
  return weightparams_.size();
}
inline int BiDirectionalLSTMLayerParams::weightparams_size() const {
  return _internal_weightparams_size();
}
inline void BiDirectionalLSTMLayerParams::clear_weightparams() {
  weightparams_.Clear();
}
inline ::CoreML::Specification::LSTMWeightParams* BiDirectionalLSTMLayerParams::mutable_weightparams(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams >*
BiDirectionalLSTMLayerParams::mutable_weightparams() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return &weightparams_;
}
inline const ::CoreML::Specification::LSTMWeightParams& BiDirectionalLSTMLayerParams::_internal_weightparams(int index) const {
  return weightparams_.Get(index);
}
inline const ::CoreML::Specification::LSTMWeightParams& BiDirectionalLSTMLayerParams::weightparams(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return _internal_weightparams(index);
}
inline ::CoreML::Specification::LSTMWeightParams* BiDirectionalLSTMLayerParams::_internal_add_weightparams() {
  return weightparams_.Add();
}
inline ::CoreML::Specification::LSTMWeightParams* BiDirectionalLSTMLayerParams::add_weightparams() {
  ::CoreML::Specification::LSTMWeightParams* _add = _internal_add_weightparams();
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams >&
BiDirectionalLSTMLayerParams::weightparams() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_;
}

// -------------------------------------------------------------------

// CustomLayerParams_CustomLayerParamValue

// double doubleValue = 10;
inline bool CustomLayerParams_CustomLayerParamValue::_internal_has_doublevalue() const {
  return value_case() == kDoubleValue;
}
inline bool CustomLayerParams_CustomLayerParamValue::has_doublevalue() const {
  return _internal_has_doublevalue();
}
inline void CustomLayerParams_CustomLayerParamValue::set_has_doublevalue() {
  _oneof_case_[0] = kDoubleValue;
}
inline void CustomLayerParams_CustomLayerParamValue::clear_doublevalue() {
  if (_internal_has_doublevalue()) {
    value_.doublevalue_ = 0;
    clear_has_value();
  }
}
inline double CustomLayerParams_CustomLayerParamValue::_internal_doublevalue() const {
  if (_internal_has_doublevalue()) {
    return value_.doublevalue_;
  }
  return 0;
}
inline void CustomLayerParams_CustomLayerParamValue::_internal_set_doublevalue(double value) {
  if (!_internal_has_doublevalue()) {
    clear_value();
    set_has_doublevalue();
  }
  value_.doublevalue_ = value;
}
inline double CustomLayerParams_CustomLayerParamValue::doublevalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.doubleValue)
  return _internal_doublevalue();
}
inline void CustomLayerParams_CustomLayerParamValue::set_doublevalue(double value) {
  _internal_set_doublevalue(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.doubleValue)
}

// string stringValue = 20;
inline bool CustomLayerParams_CustomLayerParamValue::_internal_has_stringvalue() const {
  return value_case() == kStringValue;
}
inline bool CustomLayerParams_CustomLayerParamValue::has_stringvalue() const {
  return _internal_has_stringvalue();
}
inline void CustomLayerParams_CustomLayerParamValue::set_has_stringvalue() {
  _oneof_case_[0] = kStringValue;
}
inline void CustomLayerParams_CustomLayerParamValue::clear_stringvalue() {
  if (_internal_has_stringvalue()) {
    value_.stringvalue_.Destroy(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
    clear_has_value();
  }
}
inline const std::string& CustomLayerParams_CustomLayerParamValue::stringvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  return _internal_stringvalue();
}
template <typename ArgT0, typename... ArgT>
inline void CustomLayerParams_CustomLayerParamValue::set_stringvalue(ArgT0&& arg0, ArgT... args) {
  if (!_internal_has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
  }
  value_.stringvalue_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}
inline std::string* CustomLayerParams_CustomLayerParamValue::mutable_stringvalue() {
  std::string* _s = _internal_mutable_stringvalue();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  return _s;
}
inline const std::string& CustomLayerParams_CustomLayerParamValue::_internal_stringvalue() const {
  if (_internal_has_stringvalue()) {
    return value_.stringvalue_.Get();
  }
  return ::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited();
}
inline void CustomLayerParams_CustomLayerParamValue::_internal_set_stringvalue(const std::string& value) {
  if (!_internal_has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
  }
  value_.stringvalue_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* CustomLayerParams_CustomLayerParamValue::_internal_mutable_stringvalue() {
  if (!_internal_has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
  }
  return value_.stringvalue_.Mutable(
      ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* CustomLayerParams_CustomLayerParamValue::release_stringvalue() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  if (_internal_has_stringvalue()) {
    clear_has_value();
    return value_.stringvalue_.ReleaseNonDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
  } else {
    return nullptr;
  }
}
inline void CustomLayerParams_CustomLayerParamValue::set_allocated_stringvalue(std::string* stringvalue) {
  if (has_value()) {
    clear_value();
  }
  if (stringvalue != nullptr) {
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(stringvalue);
    ::PROTOBUF_NAMESPACE_ID::Arena* arena = GetArenaForAllocation();
    if (arena != nullptr) {
      arena->Own(stringvalue);
    }
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}

// int32 intValue = 30;
inline bool CustomLayerParams_CustomLayerParamValue::_internal_has_intvalue() const {
  return value_case() == kIntValue;
}
inline bool CustomLayerParams_CustomLayerParamValue::has_intvalue() const {
  return _internal_has_intvalue();
}
inline void CustomLayerParams_CustomLayerParamValue::set_has_intvalue() {
  _oneof_case_[0] = kIntValue;
}
inline void CustomLayerParams_CustomLayerParamValue::clear_intvalue() {
  if (_internal_has_intvalue()) {
    value_.intvalue_ = 0;
    clear_has_value();
  }
}
inline int32_t CustomLayerParams_CustomLayerParamValue::_internal_intvalue() const {
  if (_internal_has_intvalue()) {
    return value_.intvalue_;
  }
  return 0;
}
inline void CustomLayerParams_CustomLayerParamValue::_internal_set_intvalue(int32_t value) {
  if (!_internal_has_intvalue()) {
    clear_value();
    set_has_intvalue();
  }
  value_.intvalue_ = value;
}
inline int32_t CustomLayerParams_CustomLayerParamValue::intvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.intValue)
  return _internal_intvalue();
}
inline void CustomLayerParams_CustomLayerParamValue::set_intvalue(int32_t value) {
  _internal_set_intvalue(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.intValue)
}

// int64 longValue = 40;
inline bool CustomLayerParams_CustomLayerParamValue::_internal_has_longvalue() const {
  return value_case() == kLongValue;
}
inline bool CustomLayerParams_CustomLayerParamValue::has_longvalue() const {
  return _internal_has_longvalue();
}
inline void CustomLayerParams_CustomLayerParamValue::set_has_longvalue() {
  _oneof_case_[0] = kLongValue;
}
inline void CustomLayerParams_CustomLayerParamValue::clear_longvalue() {
  if (_internal_has_longvalue()) {
    value_.longvalue_ = int64_t{0};
    clear_has_value();
  }
}
inline int64_t CustomLayerParams_CustomLayerParamValue::_internal_longvalue() const {
  if (_internal_has_longvalue()) {
    return value_.longvalue_;
  }
  return int64_t{0};
}
inline void CustomLayerParams_CustomLayerParamValue::_internal_set_longvalue(int64_t value) {
  if (!_internal_has_longvalue()) {
    clear_value();
    set_has_longvalue();
  }
  value_.longvalue_ = value;
}
inline int64_t CustomLayerParams_CustomLayerParamValue::longvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.longValue)
  return _internal_longvalue();
}
inline void CustomLayerParams_CustomLayerParamValue::set_longvalue(int64_t value) {
  _internal_set_longvalue(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.longValue)
}

// bool boolValue = 50;
inline bool CustomLayerParams_CustomLayerParamValue::_internal_has_boolvalue() const {
  return value_case() == kBoolValue;
}
inline bool CustomLayerParams_CustomLayerParamValue::has_boolvalue() const {
  return _internal_has_boolvalue();
}
inline void CustomLayerParams_CustomLayerParamValue::set_has_boolvalue() {
  _oneof_case_[0] = kBoolValue;
}
inline void CustomLayerParams_CustomLayerParamValue::clear_boolvalue() {
  if (_internal_has_boolvalue()) {
    value_.boolvalue_ = false;
    clear_has_value();
  }
}
inline bool CustomLayerParams_CustomLayerParamValue::_internal_boolvalue() const {
  if (_internal_has_boolvalue()) {
    return value_.boolvalue_;
  }
  return false;
}
inline void CustomLayerParams_CustomLayerParamValue::_internal_set_boolvalue(bool value) {
  if (!_internal_has_boolvalue()) {
    clear_value();
    set_has_boolvalue();
  }
  value_.boolvalue_ = value;
}
inline bool CustomLayerParams_CustomLayerParamValue::boolvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.boolValue)
  return _internal_boolvalue();
}
inline void CustomLayerParams_CustomLayerParamValue::set_boolvalue(bool value) {
  _internal_set_boolvalue(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.boolValue)
}

inline bool CustomLayerParams_CustomLayerParamValue::has_value() const {
  return value_case() != VALUE_NOT_SET;
}
inline void CustomLayerParams_CustomLayerParamValue::clear_has_value() {
  _oneof_case_[0] = VALUE_NOT_SET;
}
inline CustomLayerParams_CustomLayerParamValue::ValueCase CustomLayerParams_CustomLayerParamValue::value_case() const {
  return CustomLayerParams_CustomLayerParamValue::ValueCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// -------------------------------------------------------------------

// CustomLayerParams

// string className = 10;
inline void CustomLayerParams::clear_classname() {
  classname_.ClearToEmpty();
}
inline const std::string& CustomLayerParams::classname() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.className)
  return _internal_classname();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void CustomLayerParams::set_classname(ArgT0&& arg0, ArgT... args) {
 
 classname_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.className)
}
inline std::string* CustomLayerParams::mutable_classname() {
  std::string* _s = _internal_mutable_classname();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CustomLayerParams.className)
  return _s;
}
inline const std::string& CustomLayerParams::_internal_classname() const {
  return classname_.Get();
}
inline void CustomLayerParams::_internal_set_classname(const std::string& value) {
  
  classname_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* CustomLayerParams::_internal_mutable_classname() {
  
  return classname_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* CustomLayerParams::release_classname() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CustomLayerParams.className)
  return classname_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void CustomLayerParams::set_allocated_classname(std::string* classname) {
  if (classname != nullptr) {
    
  } else {
    
  }
  classname_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), classname,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (classname_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    classname_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CustomLayerParams.className)
}

// repeated .CoreML.Specification.WeightParams weights = 20;
inline int CustomLayerParams::_internal_weights_size() const {
  return weights_.size();
}
inline int CustomLayerParams::weights_size() const {
  return _internal_weights_size();
}
inline void CustomLayerParams::clear_weights() {
  weights_.Clear();
}
inline ::CoreML::Specification::WeightParams* CustomLayerParams::mutable_weights(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CustomLayerParams.weights)
  return weights_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::WeightParams >*
CustomLayerParams::mutable_weights() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.CustomLayerParams.weights)
  return &weights_;
}
inline const ::CoreML::Specification::WeightParams& CustomLayerParams::_internal_weights(int index) const {
  return weights_.Get(index);
}
inline const ::CoreML::Specification::WeightParams& CustomLayerParams::weights(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.weights)
  return _internal_weights(index);
}
inline ::CoreML::Specification::WeightParams* CustomLayerParams::_internal_add_weights() {
  return weights_.Add();
}
inline ::CoreML::Specification::WeightParams* CustomLayerParams::add_weights() {
  ::CoreML::Specification::WeightParams* _add = _internal_add_weights();
  // @@protoc_insertion_point(field_add:CoreML.Specification.CustomLayerParams.weights)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::WeightParams >&
CustomLayerParams::weights() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.CustomLayerParams.weights)
  return weights_;
}

// map<string, .CoreML.Specification.CustomLayerParams.CustomLayerParamValue> parameters = 30;
inline int CustomLayerParams::_internal_parameters_size() const {
  return parameters_.size();
}
inline int CustomLayerParams::parameters_size() const {
  return _internal_parameters_size();
}
inline void CustomLayerParams::clear_parameters() {
  parameters_.Clear();
}
inline const ::PROTOBUF_NAMESPACE_ID::Map< std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >&
CustomLayerParams::_internal_parameters() const {
  return parameters_.GetMap();
}
inline const ::PROTOBUF_NAMESPACE_ID::Map< std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >&
CustomLayerParams::parameters() const {
  // @@protoc_insertion_point(field_map:CoreML.Specification.CustomLayerParams.parameters)
  return _internal_parameters();
}
inline ::PROTOBUF_NAMESPACE_ID::Map< std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >*
CustomLayerParams::_internal_mutable_parameters() {
  return parameters_.MutableMap();
}
inline ::PROTOBUF_NAMESPACE_ID::Map< std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >*
CustomLayerParams::mutable_parameters() {
  // @@protoc_insertion_point(field_mutable_map:CoreML.Specification.CustomLayerParams.parameters)
  return _internal_mutable_parameters();
}

// string description = 40;
inline void CustomLayerParams::clear_description() {
  description_.ClearToEmpty();
}
inline const std::string& CustomLayerParams::description() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.description)
  return _internal_description();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void CustomLayerParams::set_description(ArgT0&& arg0, ArgT... args) {
 
 description_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.description)
}
inline std::string* CustomLayerParams::mutable_description() {
  std::string* _s = _internal_mutable_description();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CustomLayerParams.description)
  return _s;
}
inline const std::string& CustomLayerParams::_internal_description() const {
  return description_.Get();
}
inline void CustomLayerParams::_internal_set_description(const std::string& value) {
  
  description_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* CustomLayerParams::_internal_mutable_description() {
  
  return description_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* CustomLayerParams::release_description() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CustomLayerParams.description)
  return description_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void CustomLayerParams::set_allocated_description(std::string* description) {
  if (description != nullptr) {
    
  } else {
    
  }
  description_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), description,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (description_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    description_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CustomLayerParams.description)
}

// -------------------------------------------------------------------

// TransposeLayerParams

// repeated uint64 axes = 1;
inline int TransposeLayerParams::_internal_axes_size() const {
  return axes_.size();
}
inline int TransposeLayerParams::axes_size() const {
  return _internal_axes_size();
}
inline void TransposeLayerParams::clear_axes() {
  axes_.Clear();
}
inline uint64_t TransposeLayerParams::_internal_axes(int index) const {
  return axes_.Get(index);
}
inline uint64_t TransposeLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.TransposeLayerParams.axes)
  return _internal_axes(index);
}
inline void TransposeLayerParams::set_axes(int index, uint64_t value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.TransposeLayerParams.axes)
}
inline void TransposeLayerParams::_internal_add_axes(uint64_t value) {
  axes_.Add(value);
}
inline void TransposeLayerParams::add_axes(uint64_t value) {
  _internal_add_axes(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.TransposeLayerParams.axes)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
TransposeLayerParams::_internal_axes() const {
  return axes_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
TransposeLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.TransposeLayerParams.axes)
  return _internal_axes();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
TransposeLayerParams::_internal_mutable_axes() {
  return &axes_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
TransposeLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.TransposeLayerParams.axes)
  return _internal_mutable_axes();
}

// -------------------------------------------------------------------

// BatchedMatMulLayerParams

// bool transposeA = 1;
inline void BatchedMatMulLayerParams::clear_transposea() {
  transposea_ = false;
}
inline bool BatchedMatMulLayerParams::_internal_transposea() const {
  return transposea_;
}
inline bool BatchedMatMulLayerParams::transposea() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.transposeA)
  return _internal_transposea();
}
inline void BatchedMatMulLayerParams::_internal_set_transposea(bool value) {
  
  transposea_ = value;
}
inline void BatchedMatMulLayerParams::set_transposea(bool value) {
  _internal_set_transposea(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchedMatMulLayerParams.transposeA)
}

// bool transposeB = 2;
inline void BatchedMatMulLayerParams::clear_transposeb() {
  transposeb_ = false;
}
inline bool BatchedMatMulLayerParams::_internal_transposeb() const {
  return transposeb_;
}
inline bool BatchedMatMulLayerParams::transposeb() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.transposeB)
  return _internal_transposeb();
}
inline void BatchedMatMulLayerParams::_internal_set_transposeb(bool value) {
  
  transposeb_ = value;
}
inline void BatchedMatMulLayerParams::set_transposeb(bool value) {
  _internal_set_transposeb(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchedMatMulLayerParams.transposeB)
}

// uint64 weightMatrixFirstDimension = 5;
inline void BatchedMatMulLayerParams::clear_weightmatrixfirstdimension() {
  weightmatrixfirstdimension_ = uint64_t{0u};
}
inline uint64_t BatchedMatMulLayerParams::_internal_weightmatrixfirstdimension() const {
  return weightmatrixfirstdimension_;
}
inline uint64_t BatchedMatMulLayerParams::weightmatrixfirstdimension() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.weightMatrixFirstDimension)
  return _internal_weightmatrixfirstdimension();
}
inline void BatchedMatMulLayerParams::_internal_set_weightmatrixfirstdimension(uint64_t value) {
  
  weightmatrixfirstdimension_ = value;
}
inline void BatchedMatMulLayerParams::set_weightmatrixfirstdimension(uint64_t value) {
  _internal_set_weightmatrixfirstdimension(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchedMatMulLayerParams.weightMatrixFirstDimension)
}

// uint64 weightMatrixSecondDimension = 6;
inline void BatchedMatMulLayerParams::clear_weightmatrixseconddimension() {
  weightmatrixseconddimension_ = uint64_t{0u};
}
inline uint64_t BatchedMatMulLayerParams::_internal_weightmatrixseconddimension() const {
  return weightmatrixseconddimension_;
}
inline uint64_t BatchedMatMulLayerParams::weightmatrixseconddimension() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.weightMatrixSecondDimension)
  return _internal_weightmatrixseconddimension();
}
inline void BatchedMatMulLayerParams::_internal_set_weightmatrixseconddimension(uint64_t value) {
  
  weightmatrixseconddimension_ = value;
}
inline void BatchedMatMulLayerParams::set_weightmatrixseconddimension(uint64_t value) {
  _internal_set_weightmatrixseconddimension(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchedMatMulLayerParams.weightMatrixSecondDimension)
}

// bool hasBias = 7;
inline void BatchedMatMulLayerParams::clear_hasbias() {
  hasbias_ = false;
}
inline bool BatchedMatMulLayerParams::_internal_hasbias() const {
  return hasbias_;
}
inline bool BatchedMatMulLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.hasBias)
  return _internal_hasbias();
}
inline void BatchedMatMulLayerParams::_internal_set_hasbias(bool value) {
  
  hasbias_ = value;
}
inline void BatchedMatMulLayerParams::set_hasbias(bool value) {
  _internal_set_hasbias(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchedMatMulLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 8;
inline bool BatchedMatMulLayerParams::_internal_has_weights() const {
  return this != internal_default_instance() && weights_ != nullptr;
}
inline bool BatchedMatMulLayerParams::has_weights() const {
  return _internal_has_weights();
}
inline void BatchedMatMulLayerParams::clear_weights() {
  if (GetArenaForAllocation() == nullptr && weights_ != nullptr) {
    delete weights_;
  }
  weights_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& BatchedMatMulLayerParams::_internal_weights() const {
  const ::CoreML::Specification::WeightParams* p = weights_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& BatchedMatMulLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.weights)
  return _internal_weights();
}
inline void BatchedMatMulLayerParams::unsafe_arena_set_allocated_weights(
    ::CoreML::Specification::WeightParams* weights) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(weights_);
  }
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.BatchedMatMulLayerParams.weights)
}
inline ::CoreML::Specification::WeightParams* BatchedMatMulLayerParams::release_weights() {
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* BatchedMatMulLayerParams::unsafe_arena_release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchedMatMulLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* BatchedMatMulLayerParams::_internal_mutable_weights() {
  
  if (weights_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    weights_ = p;
  }
  return weights_;
}
inline ::CoreML::Specification::WeightParams* BatchedMatMulLayerParams::mutable_weights() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_weights();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchedMatMulLayerParams.weights)
  return _msg;
}
inline void BatchedMatMulLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete weights_;
  }
  if (weights) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(weights);
    if (message_arena != submessage_arena) {
      weights = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, weights, submessage_arena);
    }
    
  } else {
    
  }
  weights_ = weights;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchedMatMulLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 9;
inline bool BatchedMatMulLayerParams::_internal_has_bias() const {
  return this != internal_default_instance() && bias_ != nullptr;
}
inline bool BatchedMatMulLayerParams::has_bias() const {
  return _internal_has_bias();
}
inline void BatchedMatMulLayerParams::clear_bias() {
  if (GetArenaForAllocation() == nullptr && bias_ != nullptr) {
    delete bias_;
  }
  bias_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& BatchedMatMulLayerParams::_internal_bias() const {
  const ::CoreML::Specification::WeightParams* p = bias_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& BatchedMatMulLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.bias)
  return _internal_bias();
}
inline void BatchedMatMulLayerParams::unsafe_arena_set_allocated_bias(
    ::CoreML::Specification::WeightParams* bias) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(bias_);
  }
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.BatchedMatMulLayerParams.bias)
}
inline ::CoreML::Specification::WeightParams* BatchedMatMulLayerParams::release_bias() {
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* BatchedMatMulLayerParams::unsafe_arena_release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchedMatMulLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* BatchedMatMulLayerParams::_internal_mutable_bias() {
  
  if (bias_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    bias_ = p;
  }
  return bias_;
}
inline ::CoreML::Specification::WeightParams* BatchedMatMulLayerParams::mutable_bias() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_bias();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchedMatMulLayerParams.bias)
  return _msg;
}
inline void BatchedMatMulLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete bias_;
  }
  if (bias) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(bias);
    if (message_arena != submessage_arena) {
      bias = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, bias, submessage_arena);
    }
    
  } else {
    
  }
  bias_ = bias;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchedMatMulLayerParams.bias)
}

// bool int8DynamicQuantize = 10;
inline void BatchedMatMulLayerParams::clear_int8dynamicquantize() {
  int8dynamicquantize_ = false;
}
inline bool BatchedMatMulLayerParams::_internal_int8dynamicquantize() const {
  return int8dynamicquantize_;
}
inline bool BatchedMatMulLayerParams::int8dynamicquantize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.int8DynamicQuantize)
  return _internal_int8dynamicquantize();
}
inline void BatchedMatMulLayerParams::_internal_set_int8dynamicquantize(bool value) {
  
  int8dynamicquantize_ = value;
}
inline void BatchedMatMulLayerParams::set_int8dynamicquantize(bool value) {
  _internal_set_int8dynamicquantize(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchedMatMulLayerParams.int8DynamicQuantize)
}

// -------------------------------------------------------------------

// ConcatNDLayerParams

// int64 axis = 1;
inline void ConcatNDLayerParams::clear_axis() {
  axis_ = int64_t{0};
}
inline int64_t ConcatNDLayerParams::_internal_axis() const {
  return axis_;
}
inline int64_t ConcatNDLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConcatNDLayerParams.axis)
  return _internal_axis();
}
inline void ConcatNDLayerParams::_internal_set_axis(int64_t value) {
  
  axis_ = value;
}
inline void ConcatNDLayerParams::set_axis(int64_t value) {
  _internal_set_axis(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConcatNDLayerParams.axis)
}

// bool interleave = 2;
inline void ConcatNDLayerParams::clear_interleave() {
  interleave_ = false;
}
inline bool ConcatNDLayerParams::_internal_interleave() const {
  return interleave_;
}
inline bool ConcatNDLayerParams::interleave() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConcatNDLayerParams.interleave)
  return _internal_interleave();
}
inline void ConcatNDLayerParams::_internal_set_interleave(bool value) {
  
  interleave_ = value;
}
inline void ConcatNDLayerParams::set_interleave(bool value) {
  _internal_set_interleave(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConcatNDLayerParams.interleave)
}

// -------------------------------------------------------------------

// SoftmaxNDLayerParams

// int64 axis = 1;
inline void SoftmaxNDLayerParams::clear_axis() {
  axis_ = int64_t{0};
}
inline int64_t SoftmaxNDLayerParams::_internal_axis() const {
  return axis_;
}
inline int64_t SoftmaxNDLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SoftmaxNDLayerParams.axis)
  return _internal_axis();
}
inline void SoftmaxNDLayerParams::_internal_set_axis(int64_t value) {
  
  axis_ = value;
}
inline void SoftmaxNDLayerParams::set_axis(int64_t value) {
  _internal_set_axis(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SoftmaxNDLayerParams.axis)
}

// -------------------------------------------------------------------

// ReverseLayerParams

// repeated bool reverseDim = 1;
inline int ReverseLayerParams::_internal_reversedim_size() const {
  return reversedim_.size();
}
inline int ReverseLayerParams::reversedim_size() const {
  return _internal_reversedim_size();
}
inline void ReverseLayerParams::clear_reversedim() {
  reversedim_.Clear();
}
inline bool ReverseLayerParams::_internal_reversedim(int index) const {
  return reversedim_.Get(index);
}
inline bool ReverseLayerParams::reversedim(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReverseLayerParams.reverseDim)
  return _internal_reversedim(index);
}
inline void ReverseLayerParams::set_reversedim(int index, bool value) {
  reversedim_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReverseLayerParams.reverseDim)
}
inline void ReverseLayerParams::_internal_add_reversedim(bool value) {
  reversedim_.Add(value);
}
inline void ReverseLayerParams::add_reversedim(bool value) {
  _internal_add_reversedim(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReverseLayerParams.reverseDim)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
ReverseLayerParams::_internal_reversedim() const {
  return reversedim_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
ReverseLayerParams::reversedim() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReverseLayerParams.reverseDim)
  return _internal_reversedim();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
ReverseLayerParams::_internal_mutable_reversedim() {
  return &reversedim_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
ReverseLayerParams::mutable_reversedim() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReverseLayerParams.reverseDim)
  return _internal_mutable_reversedim();
}

// -------------------------------------------------------------------

// ReverseSeqLayerParams

// int64 batchAxis = 1;
inline void ReverseSeqLayerParams::clear_batchaxis() {
  batchaxis_ = int64_t{0};
}
inline int64_t ReverseSeqLayerParams::_internal_batchaxis() const {
  return batchaxis_;
}
inline int64_t ReverseSeqLayerParams::batchaxis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReverseSeqLayerParams.batchAxis)
  return _internal_batchaxis();
}
inline void ReverseSeqLayerParams::_internal_set_batchaxis(int64_t value) {
  
  batchaxis_ = value;
}
inline void ReverseSeqLayerParams::set_batchaxis(int64_t value) {
  _internal_set_batchaxis(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReverseSeqLayerParams.batchAxis)
}

// int64 sequenceAxis = 2;
inline void ReverseSeqLayerParams::clear_sequenceaxis() {
  sequenceaxis_ = int64_t{0};
}
inline int64_t ReverseSeqLayerParams::_internal_sequenceaxis() const {
  return sequenceaxis_;
}
inline int64_t ReverseSeqLayerParams::sequenceaxis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReverseSeqLayerParams.sequenceAxis)
  return _internal_sequenceaxis();
}
inline void ReverseSeqLayerParams::_internal_set_sequenceaxis(int64_t value) {
  
  sequenceaxis_ = value;
}
inline void ReverseSeqLayerParams::set_sequenceaxis(int64_t value) {
  _internal_set_sequenceaxis(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReverseSeqLayerParams.sequenceAxis)
}

// -------------------------------------------------------------------

// LoadConstantNDLayerParams

// repeated uint64 shape = 1;
inline int LoadConstantNDLayerParams::_internal_shape_size() const {
  return shape_.size();
}
inline int LoadConstantNDLayerParams::shape_size() const {
  return _internal_shape_size();
}
inline void LoadConstantNDLayerParams::clear_shape() {
  shape_.Clear();
}
inline uint64_t LoadConstantNDLayerParams::_internal_shape(int index) const {
  return shape_.Get(index);
}
inline uint64_t LoadConstantNDLayerParams::shape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantNDLayerParams.shape)
  return _internal_shape(index);
}
inline void LoadConstantNDLayerParams::set_shape(int index, uint64_t value) {
  shape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LoadConstantNDLayerParams.shape)
}
inline void LoadConstantNDLayerParams::_internal_add_shape(uint64_t value) {
  shape_.Add(value);
}
inline void LoadConstantNDLayerParams::add_shape(uint64_t value) {
  _internal_add_shape(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LoadConstantNDLayerParams.shape)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
LoadConstantNDLayerParams::_internal_shape() const {
  return shape_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
LoadConstantNDLayerParams::shape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LoadConstantNDLayerParams.shape)
  return _internal_shape();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
LoadConstantNDLayerParams::_internal_mutable_shape() {
  return &shape_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
LoadConstantNDLayerParams::mutable_shape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LoadConstantNDLayerParams.shape)
  return _internal_mutable_shape();
}

// .CoreML.Specification.WeightParams data = 2;
inline bool LoadConstantNDLayerParams::_internal_has_data() const {
  return this != internal_default_instance() && data_ != nullptr;
}
inline bool LoadConstantNDLayerParams::has_data() const {
  return _internal_has_data();
}
inline void LoadConstantNDLayerParams::clear_data() {
  if (GetArenaForAllocation() == nullptr && data_ != nullptr) {
    delete data_;
  }
  data_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& LoadConstantNDLayerParams::_internal_data() const {
  const ::CoreML::Specification::WeightParams* p = data_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& LoadConstantNDLayerParams::data() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantNDLayerParams.data)
  return _internal_data();
}
inline void LoadConstantNDLayerParams::unsafe_arena_set_allocated_data(
    ::CoreML::Specification::WeightParams* data) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(data_);
  }
  data_ = data;
  if (data) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LoadConstantNDLayerParams.data)
}
inline ::CoreML::Specification::WeightParams* LoadConstantNDLayerParams::release_data() {
  
  ::CoreML::Specification::WeightParams* temp = data_;
  data_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* LoadConstantNDLayerParams::unsafe_arena_release_data() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoadConstantNDLayerParams.data)
  
  ::CoreML::Specification::WeightParams* temp = data_;
  data_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* LoadConstantNDLayerParams::_internal_mutable_data() {
  
  if (data_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    data_ = p;
  }
  return data_;
}
inline ::CoreML::Specification::WeightParams* LoadConstantNDLayerParams::mutable_data() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_data();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoadConstantNDLayerParams.data)
  return _msg;
}
inline void LoadConstantNDLayerParams::set_allocated_data(::CoreML::Specification::WeightParams* data) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete data_;
  }
  if (data) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(data);
    if (message_arena != submessage_arena) {
      data = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, data, submessage_arena);
    }
    
  } else {
    
  }
  data_ = data;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoadConstantNDLayerParams.data)
}

// -------------------------------------------------------------------

// FillLikeLayerParams

// float value = 1;
inline void FillLikeLayerParams::clear_value() {
  value_ = 0;
}
inline float FillLikeLayerParams::_internal_value() const {
  return value_;
}
inline float FillLikeLayerParams::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FillLikeLayerParams.value)
  return _internal_value();
}
inline void FillLikeLayerParams::_internal_set_value(float value) {
  
  value_ = value;
}
inline void FillLikeLayerParams::set_value(float value) {
  _internal_set_value(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.FillLikeLayerParams.value)
}

// -------------------------------------------------------------------

// FillStaticLayerParams

// float value = 1;
inline void FillStaticLayerParams::clear_value() {
  value_ = 0;
}
inline float FillStaticLayerParams::_internal_value() const {
  return value_;
}
inline float FillStaticLayerParams::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FillStaticLayerParams.value)
  return _internal_value();
}
inline void FillStaticLayerParams::_internal_set_value(float value) {
  
  value_ = value;
}
inline void FillStaticLayerParams::set_value(float value) {
  _internal_set_value(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.FillStaticLayerParams.value)
}

// repeated uint64 targetShape = 2;
inline int FillStaticLayerParams::_internal_targetshape_size() const {
  return targetshape_.size();
}
inline int FillStaticLayerParams::targetshape_size() const {
  return _internal_targetshape_size();
}
inline void FillStaticLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
inline uint64_t FillStaticLayerParams::_internal_targetshape(int index) const {
  return targetshape_.Get(index);
}
inline uint64_t FillStaticLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FillStaticLayerParams.targetShape)
  return _internal_targetshape(index);
}
inline void FillStaticLayerParams::set_targetshape(int index, uint64_t value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.FillStaticLayerParams.targetShape)
}
inline void FillStaticLayerParams::_internal_add_targetshape(uint64_t value) {
  targetshape_.Add(value);
}
inline void FillStaticLayerParams::add_targetshape(uint64_t value) {
  _internal_add_targetshape(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.FillStaticLayerParams.targetShape)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
FillStaticLayerParams::_internal_targetshape() const {
  return targetshape_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
FillStaticLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.FillStaticLayerParams.targetShape)
  return _internal_targetshape();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
FillStaticLayerParams::_internal_mutable_targetshape() {
  return &targetshape_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
FillStaticLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.FillStaticLayerParams.targetShape)
  return _internal_mutable_targetshape();
}

// -------------------------------------------------------------------

// FillDynamicLayerParams

// float value = 1;
inline void FillDynamicLayerParams::clear_value() {
  value_ = 0;
}
inline float FillDynamicLayerParams::_internal_value() const {
  return value_;
}
inline float FillDynamicLayerParams::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FillDynamicLayerParams.value)
  return _internal_value();
}
inline void FillDynamicLayerParams::_internal_set_value(float value) {
  
  value_ = value;
}
inline void FillDynamicLayerParams::set_value(float value) {
  _internal_set_value(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.FillDynamicLayerParams.value)
}

// -------------------------------------------------------------------

// WhereBroadcastableLayerParams

// -------------------------------------------------------------------

// SinLayerParams

// -------------------------------------------------------------------

// CosLayerParams

// -------------------------------------------------------------------

// TanLayerParams

// -------------------------------------------------------------------

// AsinLayerParams

// -------------------------------------------------------------------

// AcosLayerParams

// -------------------------------------------------------------------

// AtanLayerParams

// -------------------------------------------------------------------

// SinhLayerParams

// -------------------------------------------------------------------

// CoshLayerParams

// -------------------------------------------------------------------

// TanhLayerParams

// -------------------------------------------------------------------

// AsinhLayerParams

// -------------------------------------------------------------------

// AcoshLayerParams

// -------------------------------------------------------------------

// AtanhLayerParams

// -------------------------------------------------------------------

// PowBroadcastableLayerParams

// -------------------------------------------------------------------

// Exp2LayerParams

// -------------------------------------------------------------------

// WhereNonZeroLayerParams

// -------------------------------------------------------------------

// MatrixBandPartLayerParams

// int64 numLower = 1;
inline void MatrixBandPartLayerParams::clear_numlower() {
  numlower_ = int64_t{0};
}
inline int64_t MatrixBandPartLayerParams::_internal_numlower() const {
  return numlower_;
}
inline int64_t MatrixBandPartLayerParams::numlower() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MatrixBandPartLayerParams.numLower)
  return _internal_numlower();
}
inline void MatrixBandPartLayerParams::_internal_set_numlower(int64_t value) {
  
  numlower_ = value;
}
inline void MatrixBandPartLayerParams::set_numlower(int64_t value) {
  _internal_set_numlower(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.MatrixBandPartLayerParams.numLower)
}

// int64 numUpper = 2;
inline void MatrixBandPartLayerParams::clear_numupper() {
  numupper_ = int64_t{0};
}
inline int64_t MatrixBandPartLayerParams::_internal_numupper() const {
  return numupper_;
}
inline int64_t MatrixBandPartLayerParams::numupper() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MatrixBandPartLayerParams.numUpper)
  return _internal_numupper();
}
inline void MatrixBandPartLayerParams::_internal_set_numupper(int64_t value) {
  
  numupper_ = value;
}
inline void MatrixBandPartLayerParams::set_numupper(int64_t value) {
  _internal_set_numupper(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.MatrixBandPartLayerParams.numUpper)
}

// -------------------------------------------------------------------

// UpperTriangularLayerParams

// int64 k = 1;
inline void UpperTriangularLayerParams::clear_k() {
  k_ = int64_t{0};
}
inline int64_t UpperTriangularLayerParams::_internal_k() const {
  return k_;
}
inline int64_t UpperTriangularLayerParams::k() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UpperTriangularLayerParams.k)
  return _internal_k();
}
inline void UpperTriangularLayerParams::_internal_set_k(int64_t value) {
  
  k_ = value;
}
inline void UpperTriangularLayerParams::set_k(int64_t value) {
  _internal_set_k(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.UpperTriangularLayerParams.k)
}

// -------------------------------------------------------------------

// LowerTriangularLayerParams

// int64 k = 1;
inline void LowerTriangularLayerParams::clear_k() {
  k_ = int64_t{0};
}
inline int64_t LowerTriangularLayerParams::_internal_k() const {
  return k_;
}
inline int64_t LowerTriangularLayerParams::k() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LowerTriangularLayerParams.k)
  return _internal_k();
}
inline void LowerTriangularLayerParams::_internal_set_k(int64_t value) {
  
  k_ = value;
}
inline void LowerTriangularLayerParams::set_k(int64_t value) {
  _internal_set_k(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LowerTriangularLayerParams.k)
}

// -------------------------------------------------------------------

// BroadcastToLikeLayerParams

// -------------------------------------------------------------------

// BroadcastToStaticLayerParams

// repeated uint64 targetShape = 1;
inline int BroadcastToStaticLayerParams::_internal_targetshape_size() const {
  return targetshape_.size();
}
inline int BroadcastToStaticLayerParams::targetshape_size() const {
  return _internal_targetshape_size();
}
inline void BroadcastToStaticLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
inline uint64_t BroadcastToStaticLayerParams::_internal_targetshape(int index) const {
  return targetshape_.Get(index);
}
inline uint64_t BroadcastToStaticLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BroadcastToStaticLayerParams.targetShape)
  return _internal_targetshape(index);
}
inline void BroadcastToStaticLayerParams::set_targetshape(int index, uint64_t value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BroadcastToStaticLayerParams.targetShape)
}
inline void BroadcastToStaticLayerParams::_internal_add_targetshape(uint64_t value) {
  targetshape_.Add(value);
}
inline void BroadcastToStaticLayerParams::add_targetshape(uint64_t value) {
  _internal_add_targetshape(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.BroadcastToStaticLayerParams.targetShape)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
BroadcastToStaticLayerParams::_internal_targetshape() const {
  return targetshape_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
BroadcastToStaticLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BroadcastToStaticLayerParams.targetShape)
  return _internal_targetshape();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
BroadcastToStaticLayerParams::_internal_mutable_targetshape() {
  return &targetshape_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
BroadcastToStaticLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BroadcastToStaticLayerParams.targetShape)
  return _internal_mutable_targetshape();
}

// -------------------------------------------------------------------

// BroadcastToDynamicLayerParams

// -------------------------------------------------------------------

// AddBroadcastableLayerParams

// -------------------------------------------------------------------

// MaxBroadcastableLayerParams

// -------------------------------------------------------------------

// MinBroadcastableLayerParams

// -------------------------------------------------------------------

// ModBroadcastableLayerParams

// -------------------------------------------------------------------

// FloorDivBroadcastableLayerParams

// -------------------------------------------------------------------

// SubtractBroadcastableLayerParams

// -------------------------------------------------------------------

// MultiplyBroadcastableLayerParams

// -------------------------------------------------------------------

// DivideBroadcastableLayerParams

// -------------------------------------------------------------------

// GatherLayerParams

// int64 axis = 1;
inline void GatherLayerParams::clear_axis() {
  axis_ = int64_t{0};
}
inline int64_t GatherLayerParams::_internal_axis() const {
  return axis_;
}
inline int64_t GatherLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GatherLayerParams.axis)
  return _internal_axis();
}
inline void GatherLayerParams::_internal_set_axis(int64_t value) {
  
  axis_ = value;
}
inline void GatherLayerParams::set_axis(int64_t value) {
  _internal_set_axis(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.GatherLayerParams.axis)
}

// -------------------------------------------------------------------

// ScatterLayerParams

// int64 axis = 1;
inline void ScatterLayerParams::clear_axis() {
  axis_ = int64_t{0};
}
inline int64_t ScatterLayerParams::_internal_axis() const {
  return axis_;
}
inline int64_t ScatterLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScatterLayerParams.axis)
  return _internal_axis();
}
inline void ScatterLayerParams::_internal_set_axis(int64_t value) {
  
  axis_ = value;
}
inline void ScatterLayerParams::set_axis(int64_t value) {
  _internal_set_axis(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScatterLayerParams.axis)
}

// .CoreML.Specification.ScatterMode mode = 2;
inline void ScatterLayerParams::clear_mode() {
  mode_ = 0;
}
inline ::CoreML::Specification::ScatterMode ScatterLayerParams::_internal_mode() const {
  return static_cast< ::CoreML::Specification::ScatterMode >(mode_);
}
inline ::CoreML::Specification::ScatterMode ScatterLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScatterLayerParams.mode)
  return _internal_mode();
}
inline void ScatterLayerParams::_internal_set_mode(::CoreML::Specification::ScatterMode value) {
  
  mode_ = value;
}
inline void ScatterLayerParams::set_mode(::CoreML::Specification::ScatterMode value) {
  _internal_set_mode(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScatterLayerParams.mode)
}

// -------------------------------------------------------------------

// GatherNDLayerParams

// -------------------------------------------------------------------

// ScatterNDLayerParams

// .CoreML.Specification.ScatterMode mode = 1;
inline void ScatterNDLayerParams::clear_mode() {
  mode_ = 0;
}
inline ::CoreML::Specification::ScatterMode ScatterNDLayerParams::_internal_mode() const {
  return static_cast< ::CoreML::Specification::ScatterMode >(mode_);
}
inline ::CoreML::Specification::ScatterMode ScatterNDLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScatterNDLayerParams.mode)
  return _internal_mode();
}
inline void ScatterNDLayerParams::_internal_set_mode(::CoreML::Specification::ScatterMode value) {
  
  mode_ = value;
}
inline void ScatterNDLayerParams::set_mode(::CoreML::Specification::ScatterMode value) {
  _internal_set_mode(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScatterNDLayerParams.mode)
}

// -------------------------------------------------------------------

// GatherAlongAxisLayerParams

// int64 axis = 1;
inline void GatherAlongAxisLayerParams::clear_axis() {
  axis_ = int64_t{0};
}
inline int64_t GatherAlongAxisLayerParams::_internal_axis() const {
  return axis_;
}
inline int64_t GatherAlongAxisLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GatherAlongAxisLayerParams.axis)
  return _internal_axis();
}
inline void GatherAlongAxisLayerParams::_internal_set_axis(int64_t value) {
  
  axis_ = value;
}
inline void GatherAlongAxisLayerParams::set_axis(int64_t value) {
  _internal_set_axis(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.GatherAlongAxisLayerParams.axis)
}

// -------------------------------------------------------------------

// ScatterAlongAxisLayerParams

// int64 axis = 1;
inline void ScatterAlongAxisLayerParams::clear_axis() {
  axis_ = int64_t{0};
}
inline int64_t ScatterAlongAxisLayerParams::_internal_axis() const {
  return axis_;
}
inline int64_t ScatterAlongAxisLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScatterAlongAxisLayerParams.axis)
  return _internal_axis();
}
inline void ScatterAlongAxisLayerParams::_internal_set_axis(int64_t value) {
  
  axis_ = value;
}
inline void ScatterAlongAxisLayerParams::set_axis(int64_t value) {
  _internal_set_axis(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScatterAlongAxisLayerParams.axis)
}

// .CoreML.Specification.ScatterMode mode = 2;
inline void ScatterAlongAxisLayerParams::clear_mode() {
  mode_ = 0;
}
inline ::CoreML::Specification::ScatterMode ScatterAlongAxisLayerParams::_internal_mode() const {
  return static_cast< ::CoreML::Specification::ScatterMode >(mode_);
}
inline ::CoreML::Specification::ScatterMode ScatterAlongAxisLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScatterAlongAxisLayerParams.mode)
  return _internal_mode();
}
inline void ScatterAlongAxisLayerParams::_internal_set_mode(::CoreML::Specification::ScatterMode value) {
  
  mode_ = value;
}
inline void ScatterAlongAxisLayerParams::set_mode(::CoreML::Specification::ScatterMode value) {
  _internal_set_mode(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScatterAlongAxisLayerParams.mode)
}

// -------------------------------------------------------------------

// StackLayerParams

// int64 axis = 1;
inline void StackLayerParams::clear_axis() {
  axis_ = int64_t{0};
}
inline int64_t StackLayerParams::_internal_axis() const {
  return axis_;
}
inline int64_t StackLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.StackLayerParams.axis)
  return _internal_axis();
}
inline void StackLayerParams::_internal_set_axis(int64_t value) {
  
  axis_ = value;
}
inline void StackLayerParams::set_axis(int64_t value) {
  _internal_set_axis(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.StackLayerParams.axis)
}

// -------------------------------------------------------------------

// RankPreservingReshapeLayerParams

// repeated int64 targetShape = 1;
inline int RankPreservingReshapeLayerParams::_internal_targetshape_size() const {
  return targetshape_.size();
}
inline int RankPreservingReshapeLayerParams::targetshape_size() const {
  return _internal_targetshape_size();
}
inline void RankPreservingReshapeLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
inline int64_t RankPreservingReshapeLayerParams::_internal_targetshape(int index) const {
  return targetshape_.Get(index);
}
inline int64_t RankPreservingReshapeLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RankPreservingReshapeLayerParams.targetShape)
  return _internal_targetshape(index);
}
inline void RankPreservingReshapeLayerParams::set_targetshape(int index, int64_t value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RankPreservingReshapeLayerParams.targetShape)
}
inline void RankPreservingReshapeLayerParams::_internal_add_targetshape(int64_t value) {
  targetshape_.Add(value);
}
inline void RankPreservingReshapeLayerParams::add_targetshape(int64_t value) {
  _internal_add_targetshape(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.RankPreservingReshapeLayerParams.targetShape)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
RankPreservingReshapeLayerParams::_internal_targetshape() const {
  return targetshape_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
RankPreservingReshapeLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.RankPreservingReshapeLayerParams.targetShape)
  return _internal_targetshape();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
RankPreservingReshapeLayerParams::_internal_mutable_targetshape() {
  return &targetshape_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
RankPreservingReshapeLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.RankPreservingReshapeLayerParams.targetShape)
  return _internal_mutable_targetshape();
}

// -------------------------------------------------------------------

// ConstantPaddingLayerParams

// float value = 1;
inline void ConstantPaddingLayerParams::clear_value() {
  value_ = 0;
}
inline float ConstantPaddingLayerParams::_internal_value() const {
  return value_;
}
inline float ConstantPaddingLayerParams::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConstantPaddingLayerParams.value)
  return _internal_value();
}
inline void ConstantPaddingLayerParams::_internal_set_value(float value) {
  
  value_ = value;
}
inline void ConstantPaddingLayerParams::set_value(float value) {
  _internal_set_value(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConstantPaddingLayerParams.value)
}

// repeated uint64 padAmounts = 2;
inline int ConstantPaddingLayerParams::_internal_padamounts_size() const {
  return padamounts_.size();
}
inline int ConstantPaddingLayerParams::padamounts_size() const {
  return _internal_padamounts_size();
}
inline void ConstantPaddingLayerParams::clear_padamounts() {
  padamounts_.Clear();
}
inline uint64_t ConstantPaddingLayerParams::_internal_padamounts(int index) const {
  return padamounts_.Get(index);
}
inline uint64_t ConstantPaddingLayerParams::padamounts(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConstantPaddingLayerParams.padAmounts)
  return _internal_padamounts(index);
}
inline void ConstantPaddingLayerParams::set_padamounts(int index, uint64_t value) {
  padamounts_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConstantPaddingLayerParams.padAmounts)
}
inline void ConstantPaddingLayerParams::_internal_add_padamounts(uint64_t value) {
  padamounts_.Add(value);
}
inline void ConstantPaddingLayerParams::add_padamounts(uint64_t value) {
  _internal_add_padamounts(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConstantPaddingLayerParams.padAmounts)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
ConstantPaddingLayerParams::_internal_padamounts() const {
  return padamounts_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
ConstantPaddingLayerParams::padamounts() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConstantPaddingLayerParams.padAmounts)
  return _internal_padamounts();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
ConstantPaddingLayerParams::_internal_mutable_padamounts() {
  return &padamounts_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
ConstantPaddingLayerParams::mutable_padamounts() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConstantPaddingLayerParams.padAmounts)
  return _internal_mutable_padamounts();
}

// bool padToGivenOutputSizeMode = 3;
inline void ConstantPaddingLayerParams::clear_padtogivenoutputsizemode() {
  padtogivenoutputsizemode_ = false;
}
inline bool ConstantPaddingLayerParams::_internal_padtogivenoutputsizemode() const {
  return padtogivenoutputsizemode_;
}
inline bool ConstantPaddingLayerParams::padtogivenoutputsizemode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConstantPaddingLayerParams.padToGivenOutputSizeMode)
  return _internal_padtogivenoutputsizemode();
}
inline void ConstantPaddingLayerParams::_internal_set_padtogivenoutputsizemode(bool value) {
  
  padtogivenoutputsizemode_ = value;
}
inline void ConstantPaddingLayerParams::set_padtogivenoutputsizemode(bool value) {
  _internal_set_padtogivenoutputsizemode(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConstantPaddingLayerParams.padToGivenOutputSizeMode)
}

// -------------------------------------------------------------------

// RandomNormalLikeLayerParams

// int64 seed = 1;
inline void RandomNormalLikeLayerParams::clear_seed() {
  seed_ = int64_t{0};
}
inline int64_t RandomNormalLikeLayerParams::_internal_seed() const {
  return seed_;
}
inline int64_t RandomNormalLikeLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalLikeLayerParams.seed)
  return _internal_seed();
}
inline void RandomNormalLikeLayerParams::_internal_set_seed(int64_t value) {
  
  seed_ = value;
}
inline void RandomNormalLikeLayerParams::set_seed(int64_t value) {
  _internal_set_seed(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalLikeLayerParams.seed)
}

// float mean = 2;
inline void RandomNormalLikeLayerParams::clear_mean() {
  mean_ = 0;
}
inline float RandomNormalLikeLayerParams::_internal_mean() const {
  return mean_;
}
inline float RandomNormalLikeLayerParams::mean() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalLikeLayerParams.mean)
  return _internal_mean();
}
inline void RandomNormalLikeLayerParams::_internal_set_mean(float value) {
  
  mean_ = value;
}
inline void RandomNormalLikeLayerParams::set_mean(float value) {
  _internal_set_mean(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalLikeLayerParams.mean)
}

// float stdDev = 3;
inline void RandomNormalLikeLayerParams::clear_stddev() {
  stddev_ = 0;
}
inline float RandomNormalLikeLayerParams::_internal_stddev() const {
  return stddev_;
}
inline float RandomNormalLikeLayerParams::stddev() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalLikeLayerParams.stdDev)
  return _internal_stddev();
}
inline void RandomNormalLikeLayerParams::_internal_set_stddev(float value) {
  
  stddev_ = value;
}
inline void RandomNormalLikeLayerParams::set_stddev(float value) {
  _internal_set_stddev(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalLikeLayerParams.stdDev)
}

// -------------------------------------------------------------------

// RandomNormalStaticLayerParams

// int64 seed = 1;
inline void RandomNormalStaticLayerParams::clear_seed() {
  seed_ = int64_t{0};
}
inline int64_t RandomNormalStaticLayerParams::_internal_seed() const {
  return seed_;
}
inline int64_t RandomNormalStaticLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalStaticLayerParams.seed)
  return _internal_seed();
}
inline void RandomNormalStaticLayerParams::_internal_set_seed(int64_t value) {
  
  seed_ = value;
}
inline void RandomNormalStaticLayerParams::set_seed(int64_t value) {
  _internal_set_seed(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalStaticLayerParams.seed)
}

// float mean = 2;
inline void RandomNormalStaticLayerParams::clear_mean() {
  mean_ = 0;
}
inline float RandomNormalStaticLayerParams::_internal_mean() const {
  return mean_;
}
inline float RandomNormalStaticLayerParams::mean() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalStaticLayerParams.mean)
  return _internal_mean();
}
inline void RandomNormalStaticLayerParams::_internal_set_mean(float value) {
  
  mean_ = value;
}
inline void RandomNormalStaticLayerParams::set_mean(float value) {
  _internal_set_mean(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalStaticLayerParams.mean)
}

// float stdDev = 3;
inline void RandomNormalStaticLayerParams::clear_stddev() {
  stddev_ = 0;
}
inline float RandomNormalStaticLayerParams::_internal_stddev() const {
  return stddev_;
}
inline float RandomNormalStaticLayerParams::stddev() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalStaticLayerParams.stdDev)
  return _internal_stddev();
}
inline void RandomNormalStaticLayerParams::_internal_set_stddev(float value) {
  
  stddev_ = value;
}
inline void RandomNormalStaticLayerParams::set_stddev(float value) {
  _internal_set_stddev(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalStaticLayerParams.stdDev)
}

// repeated uint64 outputShape = 4;
inline int RandomNormalStaticLayerParams::_internal_outputshape_size() const {
  return outputshape_.size();
}
inline int RandomNormalStaticLayerParams::outputshape_size() const {
  return _internal_outputshape_size();
}
inline void RandomNormalStaticLayerParams::clear_outputshape() {
  outputshape_.Clear();
}
inline uint64_t RandomNormalStaticLayerParams::_internal_outputshape(int index) const {
  return outputshape_.Get(index);
}
inline uint64_t RandomNormalStaticLayerParams::outputshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalStaticLayerParams.outputShape)
  return _internal_outputshape(index);
}
inline void RandomNormalStaticLayerParams::set_outputshape(int index, uint64_t value) {
  outputshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalStaticLayerParams.outputShape)
}
inline void RandomNormalStaticLayerParams::_internal_add_outputshape(uint64_t value) {
  outputshape_.Add(value);
}
inline void RandomNormalStaticLayerParams::add_outputshape(uint64_t value) {
  _internal_add_outputshape(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.RandomNormalStaticLayerParams.outputShape)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
RandomNormalStaticLayerParams::_internal_outputshape() const {
  return outputshape_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
RandomNormalStaticLayerParams::outputshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.RandomNormalStaticLayerParams.outputShape)
  return _internal_outputshape();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
RandomNormalStaticLayerParams::_internal_mutable_outputshape() {
  return &outputshape_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
RandomNormalStaticLayerParams::mutable_outputshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.RandomNormalStaticLayerParams.outputShape)
  return _internal_mutable_outputshape();
}

// -------------------------------------------------------------------

// RandomNormalDynamicLayerParams

// int64 seed = 1;
inline void RandomNormalDynamicLayerParams::clear_seed() {
  seed_ = int64_t{0};
}
inline int64_t RandomNormalDynamicLayerParams::_internal_seed() const {
  return seed_;
}
inline int64_t RandomNormalDynamicLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalDynamicLayerParams.seed)
  return _internal_seed();
}
inline void RandomNormalDynamicLayerParams::_internal_set_seed(int64_t value) {
  
  seed_ = value;
}
inline void RandomNormalDynamicLayerParams::set_seed(int64_t value) {
  _internal_set_seed(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalDynamicLayerParams.seed)
}

// float mean = 2;
inline void RandomNormalDynamicLayerParams::clear_mean() {
  mean_ = 0;
}
inline float RandomNormalDynamicLayerParams::_internal_mean() const {
  return mean_;
}
inline float RandomNormalDynamicLayerParams::mean() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalDynamicLayerParams.mean)
  return _internal_mean();
}
inline void RandomNormalDynamicLayerParams::_internal_set_mean(float value) {
  
  mean_ = value;
}
inline void RandomNormalDynamicLayerParams::set_mean(float value) {
  _internal_set_mean(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalDynamicLayerParams.mean)
}

// float stdDev = 3;
inline void RandomNormalDynamicLayerParams::clear_stddev() {
  stddev_ = 0;
}
inline float RandomNormalDynamicLayerParams::_internal_stddev() const {
  return stddev_;
}
inline float RandomNormalDynamicLayerParams::stddev() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalDynamicLayerParams.stdDev)
  return _internal_stddev();
}
inline void RandomNormalDynamicLayerParams::_internal_set_stddev(float value) {
  
  stddev_ = value;
}
inline void RandomNormalDynamicLayerParams::set_stddev(float value) {
  _internal_set_stddev(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalDynamicLayerParams.stdDev)
}

// -------------------------------------------------------------------

// RandomUniformLikeLayerParams

// int64 seed = 1;
inline void RandomUniformLikeLayerParams::clear_seed() {
  seed_ = int64_t{0};
}
inline int64_t RandomUniformLikeLayerParams::_internal_seed() const {
  return seed_;
}
inline int64_t RandomUniformLikeLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformLikeLayerParams.seed)
  return _internal_seed();
}
inline void RandomUniformLikeLayerParams::_internal_set_seed(int64_t value) {
  
  seed_ = value;
}
inline void RandomUniformLikeLayerParams::set_seed(int64_t value) {
  _internal_set_seed(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformLikeLayerParams.seed)
}

// float minVal = 2;
inline void RandomUniformLikeLayerParams::clear_minval() {
  minval_ = 0;
}
inline float RandomUniformLikeLayerParams::_internal_minval() const {
  return minval_;
}
inline float RandomUniformLikeLayerParams::minval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformLikeLayerParams.minVal)
  return _internal_minval();
}
inline void RandomUniformLikeLayerParams::_internal_set_minval(float value) {
  
  minval_ = value;
}
inline void RandomUniformLikeLayerParams::set_minval(float value) {
  _internal_set_minval(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformLikeLayerParams.minVal)
}

// float maxVal = 3;
inline void RandomUniformLikeLayerParams::clear_maxval() {
  maxval_ = 0;
}
inline float RandomUniformLikeLayerParams::_internal_maxval() const {
  return maxval_;
}
inline float RandomUniformLikeLayerParams::maxval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformLikeLayerParams.maxVal)
  return _internal_maxval();
}
inline void RandomUniformLikeLayerParams::_internal_set_maxval(float value) {
  
  maxval_ = value;
}
inline void RandomUniformLikeLayerParams::set_maxval(float value) {
  _internal_set_maxval(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformLikeLayerParams.maxVal)
}

// -------------------------------------------------------------------

// RandomUniformStaticLayerParams

// int64 seed = 1;
inline void RandomUniformStaticLayerParams::clear_seed() {
  seed_ = int64_t{0};
}
inline int64_t RandomUniformStaticLayerParams::_internal_seed() const {
  return seed_;
}
inline int64_t RandomUniformStaticLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformStaticLayerParams.seed)
  return _internal_seed();
}
inline void RandomUniformStaticLayerParams::_internal_set_seed(int64_t value) {
  
  seed_ = value;
}
inline void RandomUniformStaticLayerParams::set_seed(int64_t value) {
  _internal_set_seed(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformStaticLayerParams.seed)
}

// float minVal = 2;
inline void RandomUniformStaticLayerParams::clear_minval() {
  minval_ = 0;
}
inline float RandomUniformStaticLayerParams::_internal_minval() const {
  return minval_;
}
inline float RandomUniformStaticLayerParams::minval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformStaticLayerParams.minVal)
  return _internal_minval();
}
inline void RandomUniformStaticLayerParams::_internal_set_minval(float value) {
  
  minval_ = value;
}
inline void RandomUniformStaticLayerParams::set_minval(float value) {
  _internal_set_minval(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformStaticLayerParams.minVal)
}

// float maxVal = 3;
inline void RandomUniformStaticLayerParams::clear_maxval() {
  maxval_ = 0;
}
inline float RandomUniformStaticLayerParams::_internal_maxval() const {
  return maxval_;
}
inline float RandomUniformStaticLayerParams::maxval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformStaticLayerParams.maxVal)
  return _internal_maxval();
}
inline void RandomUniformStaticLayerParams::_internal_set_maxval(float value) {
  
  maxval_ = value;
}
inline void RandomUniformStaticLayerParams::set_maxval(float value) {
  _internal_set_maxval(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformStaticLayerParams.maxVal)
}

// repeated uint64 outputShape = 4;
inline int RandomUniformStaticLayerParams::_internal_outputshape_size() const {
  return outputshape_.size();
}
inline int RandomUniformStaticLayerParams::outputshape_size() const {
  return _internal_outputshape_size();
}
inline void RandomUniformStaticLayerParams::clear_outputshape() {
  outputshape_.Clear();
}
inline uint64_t RandomUniformStaticLayerParams::_internal_outputshape(int index) const {
  return outputshape_.Get(index);
}
inline uint64_t RandomUniformStaticLayerParams::outputshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformStaticLayerParams.outputShape)
  return _internal_outputshape(index);
}
inline void RandomUniformStaticLayerParams::set_outputshape(int index, uint64_t value) {
  outputshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformStaticLayerParams.outputShape)
}
inline void RandomUniformStaticLayerParams::_internal_add_outputshape(uint64_t value) {
  outputshape_.Add(value);
}
inline void RandomUniformStaticLayerParams::add_outputshape(uint64_t value) {
  _internal_add_outputshape(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.RandomUniformStaticLayerParams.outputShape)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
RandomUniformStaticLayerParams::_internal_outputshape() const {
  return outputshape_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
RandomUniformStaticLayerParams::outputshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.RandomUniformStaticLayerParams.outputShape)
  return _internal_outputshape();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
RandomUniformStaticLayerParams::_internal_mutable_outputshape() {
  return &outputshape_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
RandomUniformStaticLayerParams::mutable_outputshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.RandomUniformStaticLayerParams.outputShape)
  return _internal_mutable_outputshape();
}

// -------------------------------------------------------------------

// RandomUniformDynamicLayerParams

// int64 seed = 1;
inline void RandomUniformDynamicLayerParams::clear_seed() {
  seed_ = int64_t{0};
}
inline int64_t RandomUniformDynamicLayerParams::_internal_seed() const {
  return seed_;
}
inline int64_t RandomUniformDynamicLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformDynamicLayerParams.seed)
  return _internal_seed();
}
inline void RandomUniformDynamicLayerParams::_internal_set_seed(int64_t value) {
  
  seed_ = value;
}
inline void RandomUniformDynamicLayerParams::set_seed(int64_t value) {
  _internal_set_seed(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformDynamicLayerParams.seed)
}

// float minVal = 2;
inline void RandomUniformDynamicLayerParams::clear_minval() {
  minval_ = 0;
}
inline float RandomUniformDynamicLayerParams::_internal_minval() const {
  return minval_;
}
inline float RandomUniformDynamicLayerParams::minval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformDynamicLayerParams.minVal)
  return _internal_minval();
}
inline void RandomUniformDynamicLayerParams::_internal_set_minval(float value) {
  
  minval_ = value;
}
inline void RandomUniformDynamicLayerParams::set_minval(float value) {
  _internal_set_minval(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformDynamicLayerParams.minVal)
}

// float maxVal = 3;
inline void RandomUniformDynamicLayerParams::clear_maxval() {
  maxval_ = 0;
}
inline float RandomUniformDynamicLayerParams::_internal_maxval() const {
  return maxval_;
}
inline float RandomUniformDynamicLayerParams::maxval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformDynamicLayerParams.maxVal)
  return _internal_maxval();
}
inline void RandomUniformDynamicLayerParams::_internal_set_maxval(float value) {
  
  maxval_ = value;
}
inline void RandomUniformDynamicLayerParams::set_maxval(float value) {
  _internal_set_maxval(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformDynamicLayerParams.maxVal)
}

// -------------------------------------------------------------------

// RandomBernoulliLikeLayerParams

// int64 seed = 1;
inline void RandomBernoulliLikeLayerParams::clear_seed() {
  seed_ = int64_t{0};
}
inline int64_t RandomBernoulliLikeLayerParams::_internal_seed() const {
  return seed_;
}
inline int64_t RandomBernoulliLikeLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomBernoulliLikeLayerParams.seed)
  return _internal_seed();
}
inline void RandomBernoulliLikeLayerParams::_internal_set_seed(int64_t value) {
  
  seed_ = value;
}
inline void RandomBernoulliLikeLayerParams::set_seed(int64_t value) {
  _internal_set_seed(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomBernoulliLikeLayerParams.seed)
}

// float prob = 2;
inline void RandomBernoulliLikeLayerParams::clear_prob() {
  prob_ = 0;
}
inline float RandomBernoulliLikeLayerParams::_internal_prob() const {
  return prob_;
}
inline float RandomBernoulliLikeLayerParams::prob() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomBernoulliLikeLayerParams.prob)
  return _internal_prob();
}
inline void RandomBernoulliLikeLayerParams::_internal_set_prob(float value) {
  
  prob_ = value;
}
inline void RandomBernoulliLikeLayerParams::set_prob(float value) {
  _internal_set_prob(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomBernoulliLikeLayerParams.prob)
}

// -------------------------------------------------------------------

// RandomBernoulliStaticLayerParams

// int64 seed = 1;
inline void RandomBernoulliStaticLayerParams::clear_seed() {
  seed_ = int64_t{0};
}
inline int64_t RandomBernoulliStaticLayerParams::_internal_seed() const {
  return seed_;
}
inline int64_t RandomBernoulliStaticLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomBernoulliStaticLayerParams.seed)
  return _internal_seed();
}
inline void RandomBernoulliStaticLayerParams::_internal_set_seed(int64_t value) {
  
  seed_ = value;
}
inline void RandomBernoulliStaticLayerParams::set_seed(int64_t value) {
  _internal_set_seed(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomBernoulliStaticLayerParams.seed)
}

// float prob = 2;
inline void RandomBernoulliStaticLayerParams::clear_prob() {
  prob_ = 0;
}
inline float RandomBernoulliStaticLayerParams::_internal_prob() const {
  return prob_;
}
inline float RandomBernoulliStaticLayerParams::prob() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomBernoulliStaticLayerParams.prob)
  return _internal_prob();
}
inline void RandomBernoulliStaticLayerParams::_internal_set_prob(float value) {
  
  prob_ = value;
}
inline void RandomBernoulliStaticLayerParams::set_prob(float value) {
  _internal_set_prob(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomBernoulliStaticLayerParams.prob)
}

// repeated uint64 outputShape = 3;
inline int RandomBernoulliStaticLayerParams::_internal_outputshape_size() const {
  return outputshape_.size();
}
inline int RandomBernoulliStaticLayerParams::outputshape_size() const {
  return _internal_outputshape_size();
}
inline void RandomBernoulliStaticLayerParams::clear_outputshape() {
  outputshape_.Clear();
}
inline uint64_t RandomBernoulliStaticLayerParams::_internal_outputshape(int index) const {
  return outputshape_.Get(index);
}
inline uint64_t RandomBernoulliStaticLayerParams::outputshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomBernoulliStaticLayerParams.outputShape)
  return _internal_outputshape(index);
}
inline void RandomBernoulliStaticLayerParams::set_outputshape(int index, uint64_t value) {
  outputshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomBernoulliStaticLayerParams.outputShape)
}
inline void RandomBernoulliStaticLayerParams::_internal_add_outputshape(uint64_t value) {
  outputshape_.Add(value);
}
inline void RandomBernoulliStaticLayerParams::add_outputshape(uint64_t value) {
  _internal_add_outputshape(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.RandomBernoulliStaticLayerParams.outputShape)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
RandomBernoulliStaticLayerParams::_internal_outputshape() const {
  return outputshape_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
RandomBernoulliStaticLayerParams::outputshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.RandomBernoulliStaticLayerParams.outputShape)
  return _internal_outputshape();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
RandomBernoulliStaticLayerParams::_internal_mutable_outputshape() {
  return &outputshape_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
RandomBernoulliStaticLayerParams::mutable_outputshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.RandomBernoulliStaticLayerParams.outputShape)
  return _internal_mutable_outputshape();
}

// -------------------------------------------------------------------

// RandomBernoulliDynamicLayerParams

// int64 seed = 1;
inline void RandomBernoulliDynamicLayerParams::clear_seed() {
  seed_ = int64_t{0};
}
inline int64_t RandomBernoulliDynamicLayerParams::_internal_seed() const {
  return seed_;
}
inline int64_t RandomBernoulliDynamicLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomBernoulliDynamicLayerParams.seed)
  return _internal_seed();
}
inline void RandomBernoulliDynamicLayerParams::_internal_set_seed(int64_t value) {
  
  seed_ = value;
}
inline void RandomBernoulliDynamicLayerParams::set_seed(int64_t value) {
  _internal_set_seed(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomBernoulliDynamicLayerParams.seed)
}

// float prob = 2;
inline void RandomBernoulliDynamicLayerParams::clear_prob() {
  prob_ = 0;
}
inline float RandomBernoulliDynamicLayerParams::_internal_prob() const {
  return prob_;
}
inline float RandomBernoulliDynamicLayerParams::prob() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomBernoulliDynamicLayerParams.prob)
  return _internal_prob();
}
inline void RandomBernoulliDynamicLayerParams::_internal_set_prob(float value) {
  
  prob_ = value;
}
inline void RandomBernoulliDynamicLayerParams::set_prob(float value) {
  _internal_set_prob(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomBernoulliDynamicLayerParams.prob)
}

// -------------------------------------------------------------------

// CategoricalDistributionLayerParams

// int64 seed = 1;
inline void CategoricalDistributionLayerParams::clear_seed() {
  seed_ = int64_t{0};
}
inline int64_t CategoricalDistributionLayerParams::_internal_seed() const {
  return seed_;
}
inline int64_t CategoricalDistributionLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CategoricalDistributionLayerParams.seed)
  return _internal_seed();
}
inline void CategoricalDistributionLayerParams::_internal_set_seed(int64_t value) {
  
  seed_ = value;
}
inline void CategoricalDistributionLayerParams::set_seed(int64_t value) {
  _internal_set_seed(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CategoricalDistributionLayerParams.seed)
}

// int64 numSamples = 2;
inline void CategoricalDistributionLayerParams::clear_numsamples() {
  numsamples_ = int64_t{0};
}
inline int64_t CategoricalDistributionLayerParams::_internal_numsamples() const {
  return numsamples_;
}
inline int64_t CategoricalDistributionLayerParams::numsamples() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CategoricalDistributionLayerParams.numSamples)
  return _internal_numsamples();
}
inline void CategoricalDistributionLayerParams::_internal_set_numsamples(int64_t value) {
  
  numsamples_ = value;
}
inline void CategoricalDistributionLayerParams::set_numsamples(int64_t value) {
  _internal_set_numsamples(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CategoricalDistributionLayerParams.numSamples)
}

// bool isLogits = 3;
inline void CategoricalDistributionLayerParams::clear_islogits() {
  islogits_ = false;
}
inline bool CategoricalDistributionLayerParams::_internal_islogits() const {
  return islogits_;
}
inline bool CategoricalDistributionLayerParams::islogits() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CategoricalDistributionLayerParams.isLogits)
  return _internal_islogits();
}
inline void CategoricalDistributionLayerParams::_internal_set_islogits(bool value) {
  
  islogits_ = value;
}
inline void CategoricalDistributionLayerParams::set_islogits(bool value) {
  _internal_set_islogits(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CategoricalDistributionLayerParams.isLogits)
}

// float eps = 4;
inline void CategoricalDistributionLayerParams::clear_eps() {
  eps_ = 0;
}
inline float CategoricalDistributionLayerParams::_internal_eps() const {
  return eps_;
}
inline float CategoricalDistributionLayerParams::eps() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CategoricalDistributionLayerParams.eps)
  return _internal_eps();
}
inline void CategoricalDistributionLayerParams::_internal_set_eps(float value) {
  
  eps_ = value;
}
inline void CategoricalDistributionLayerParams::set_eps(float value) {
  _internal_set_eps(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CategoricalDistributionLayerParams.eps)
}

// float temperature = 5;
inline void CategoricalDistributionLayerParams::clear_temperature() {
  temperature_ = 0;
}
inline float CategoricalDistributionLayerParams::_internal_temperature() const {
  return temperature_;
}
inline float CategoricalDistributionLayerParams::temperature() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CategoricalDistributionLayerParams.temperature)
  return _internal_temperature();
}
inline void CategoricalDistributionLayerParams::_internal_set_temperature(float value) {
  
  temperature_ = value;
}
inline void CategoricalDistributionLayerParams::set_temperature(float value) {
  _internal_set_temperature(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CategoricalDistributionLayerParams.temperature)
}

// -------------------------------------------------------------------

// ReduceL1LayerParams

// repeated int64 axes = 1;
inline int ReduceL1LayerParams::_internal_axes_size() const {
  return axes_.size();
}
inline int ReduceL1LayerParams::axes_size() const {
  return _internal_axes_size();
}
inline void ReduceL1LayerParams::clear_axes() {
  axes_.Clear();
}
inline int64_t ReduceL1LayerParams::_internal_axes(int index) const {
  return axes_.Get(index);
}
inline int64_t ReduceL1LayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceL1LayerParams.axes)
  return _internal_axes(index);
}
inline void ReduceL1LayerParams::set_axes(int index, int64_t value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceL1LayerParams.axes)
}
inline void ReduceL1LayerParams::_internal_add_axes(int64_t value) {
  axes_.Add(value);
}
inline void ReduceL1LayerParams::add_axes(int64_t value) {
  _internal_add_axes(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceL1LayerParams.axes)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReduceL1LayerParams::_internal_axes() const {
  return axes_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReduceL1LayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceL1LayerParams.axes)
  return _internal_axes();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReduceL1LayerParams::_internal_mutable_axes() {
  return &axes_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReduceL1LayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceL1LayerParams.axes)
  return _internal_mutable_axes();
}

// bool keepDims = 2;
inline void ReduceL1LayerParams::clear_keepdims() {
  keepdims_ = false;
}
inline bool ReduceL1LayerParams::_internal_keepdims() const {
  return keepdims_;
}
inline bool ReduceL1LayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceL1LayerParams.keepDims)
  return _internal_keepdims();
}
inline void ReduceL1LayerParams::_internal_set_keepdims(bool value) {
  
  keepdims_ = value;
}
inline void ReduceL1LayerParams::set_keepdims(bool value) {
  _internal_set_keepdims(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceL1LayerParams.keepDims)
}

// bool reduceAll = 3;
inline void ReduceL1LayerParams::clear_reduceall() {
  reduceall_ = false;
}
inline bool ReduceL1LayerParams::_internal_reduceall() const {
  return reduceall_;
}
inline bool ReduceL1LayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceL1LayerParams.reduceAll)
  return _internal_reduceall();
}
inline void ReduceL1LayerParams::_internal_set_reduceall(bool value) {
  
  reduceall_ = value;
}
inline void ReduceL1LayerParams::set_reduceall(bool value) {
  _internal_set_reduceall(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceL1LayerParams.reduceAll)
}

// -------------------------------------------------------------------

// ReduceL2LayerParams

// repeated int64 axes = 1;
inline int ReduceL2LayerParams::_internal_axes_size() const {
  return axes_.size();
}
inline int ReduceL2LayerParams::axes_size() const {
  return _internal_axes_size();
}
inline void ReduceL2LayerParams::clear_axes() {
  axes_.Clear();
}
inline int64_t ReduceL2LayerParams::_internal_axes(int index) const {
  return axes_.Get(index);
}
inline int64_t ReduceL2LayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceL2LayerParams.axes)
  return _internal_axes(index);
}
inline void ReduceL2LayerParams::set_axes(int index, int64_t value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceL2LayerParams.axes)
}
inline void ReduceL2LayerParams::_internal_add_axes(int64_t value) {
  axes_.Add(value);
}
inline void ReduceL2LayerParams::add_axes(int64_t value) {
  _internal_add_axes(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceL2LayerParams.axes)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReduceL2LayerParams::_internal_axes() const {
  return axes_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReduceL2LayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceL2LayerParams.axes)
  return _internal_axes();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReduceL2LayerParams::_internal_mutable_axes() {
  return &axes_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReduceL2LayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceL2LayerParams.axes)
  return _internal_mutable_axes();
}

// bool keepDims = 2;
inline void ReduceL2LayerParams::clear_keepdims() {
  keepdims_ = false;
}
inline bool ReduceL2LayerParams::_internal_keepdims() const {
  return keepdims_;
}
inline bool ReduceL2LayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceL2LayerParams.keepDims)
  return _internal_keepdims();
}
inline void ReduceL2LayerParams::_internal_set_keepdims(bool value) {
  
  keepdims_ = value;
}
inline void ReduceL2LayerParams::set_keepdims(bool value) {
  _internal_set_keepdims(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceL2LayerParams.keepDims)
}

// bool reduceAll = 3;
inline void ReduceL2LayerParams::clear_reduceall() {
  reduceall_ = false;
}
inline bool ReduceL2LayerParams::_internal_reduceall() const {
  return reduceall_;
}
inline bool ReduceL2LayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceL2LayerParams.reduceAll)
  return _internal_reduceall();
}
inline void ReduceL2LayerParams::_internal_set_reduceall(bool value) {
  
  reduceall_ = value;
}
inline void ReduceL2LayerParams::set_reduceall(bool value) {
  _internal_set_reduceall(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceL2LayerParams.reduceAll)
}

// -------------------------------------------------------------------

// ReduceMaxLayerParams

// repeated int64 axes = 1;
inline int ReduceMaxLayerParams::_internal_axes_size() const {
  return axes_.size();
}
inline int ReduceMaxLayerParams::axes_size() const {
  return _internal_axes_size();
}
inline void ReduceMaxLayerParams::clear_axes() {
  axes_.Clear();
}
inline int64_t ReduceMaxLayerParams::_internal_axes(int index) const {
  return axes_.Get(index);
}
inline int64_t ReduceMaxLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMaxLayerParams.axes)
  return _internal_axes(index);
}
inline void ReduceMaxLayerParams::set_axes(int index, int64_t value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMaxLayerParams.axes)
}
inline void ReduceMaxLayerParams::_internal_add_axes(int64_t value) {
  axes_.Add(value);
}
inline void ReduceMaxLayerParams::add_axes(int64_t value) {
  _internal_add_axes(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceMaxLayerParams.axes)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReduceMaxLayerParams::_internal_axes() const {
  return axes_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReduceMaxLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceMaxLayerParams.axes)
  return _internal_axes();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReduceMaxLayerParams::_internal_mutable_axes() {
  return &axes_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReduceMaxLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceMaxLayerParams.axes)
  return _internal_mutable_axes();
}

// bool keepDims = 2;
inline void ReduceMaxLayerParams::clear_keepdims() {
  keepdims_ = false;
}
inline bool ReduceMaxLayerParams::_internal_keepdims() const {
  return keepdims_;
}
inline bool ReduceMaxLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMaxLayerParams.keepDims)
  return _internal_keepdims();
}
inline void ReduceMaxLayerParams::_internal_set_keepdims(bool value) {
  
  keepdims_ = value;
}
inline void ReduceMaxLayerParams::set_keepdims(bool value) {
  _internal_set_keepdims(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMaxLayerParams.keepDims)
}

// bool reduceAll = 3;
inline void ReduceMaxLayerParams::clear_reduceall() {
  reduceall_ = false;
}
inline bool ReduceMaxLayerParams::_internal_reduceall() const {
  return reduceall_;
}
inline bool ReduceMaxLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMaxLayerParams.reduceAll)
  return _internal_reduceall();
}
inline void ReduceMaxLayerParams::_internal_set_reduceall(bool value) {
  
  reduceall_ = value;
}
inline void ReduceMaxLayerParams::set_reduceall(bool value) {
  _internal_set_reduceall(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMaxLayerParams.reduceAll)
}

// -------------------------------------------------------------------

// ReduceMinLayerParams

// repeated int64 axes = 1;
inline int ReduceMinLayerParams::_internal_axes_size() const {
  return axes_.size();
}
inline int ReduceMinLayerParams::axes_size() const {
  return _internal_axes_size();
}
inline void ReduceMinLayerParams::clear_axes() {
  axes_.Clear();
}
inline int64_t ReduceMinLayerParams::_internal_axes(int index) const {
  return axes_.Get(index);
}
inline int64_t ReduceMinLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMinLayerParams.axes)
  return _internal_axes(index);
}
inline void ReduceMinLayerParams::set_axes(int index, int64_t value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMinLayerParams.axes)
}
inline void ReduceMinLayerParams::_internal_add_axes(int64_t value) {
  axes_.Add(value);
}
inline void ReduceMinLayerParams::add_axes(int64_t value) {
  _internal_add_axes(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceMinLayerParams.axes)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReduceMinLayerParams::_internal_axes() const {
  return axes_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReduceMinLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceMinLayerParams.axes)
  return _internal_axes();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReduceMinLayerParams::_internal_mutable_axes() {
  return &axes_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReduceMinLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceMinLayerParams.axes)
  return _internal_mutable_axes();
}

// bool keepDims = 2;
inline void ReduceMinLayerParams::clear_keepdims() {
  keepdims_ = false;
}
inline bool ReduceMinLayerParams::_internal_keepdims() const {
  return keepdims_;
}
inline bool ReduceMinLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMinLayerParams.keepDims)
  return _internal_keepdims();
}
inline void ReduceMinLayerParams::_internal_set_keepdims(bool value) {
  
  keepdims_ = value;
}
inline void ReduceMinLayerParams::set_keepdims(bool value) {
  _internal_set_keepdims(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMinLayerParams.keepDims)
}

// bool reduceAll = 3;
inline void ReduceMinLayerParams::clear_reduceall() {
  reduceall_ = false;
}
inline bool ReduceMinLayerParams::_internal_reduceall() const {
  return reduceall_;
}
inline bool ReduceMinLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMinLayerParams.reduceAll)
  return _internal_reduceall();
}
inline void ReduceMinLayerParams::_internal_set_reduceall(bool value) {
  
  reduceall_ = value;
}
inline void ReduceMinLayerParams::set_reduceall(bool value) {
  _internal_set_reduceall(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMinLayerParams.reduceAll)
}

// -------------------------------------------------------------------

// ReduceSumLayerParams

// repeated int64 axes = 1;
inline int ReduceSumLayerParams::_internal_axes_size() const {
  return axes_.size();
}
inline int ReduceSumLayerParams::axes_size() const {
  return _internal_axes_size();
}
inline void ReduceSumLayerParams::clear_axes() {
  axes_.Clear();
}
inline int64_t ReduceSumLayerParams::_internal_axes(int index) const {
  return axes_.Get(index);
}
inline int64_t ReduceSumLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceSumLayerParams.axes)
  return _internal_axes(index);
}
inline void ReduceSumLayerParams::set_axes(int index, int64_t value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceSumLayerParams.axes)
}
inline void ReduceSumLayerParams::_internal_add_axes(int64_t value) {
  axes_.Add(value);
}
inline void ReduceSumLayerParams::add_axes(int64_t value) {
  _internal_add_axes(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceSumLayerParams.axes)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReduceSumLayerParams::_internal_axes() const {
  return axes_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReduceSumLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceSumLayerParams.axes)
  return _internal_axes();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReduceSumLayerParams::_internal_mutable_axes() {
  return &axes_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReduceSumLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceSumLayerParams.axes)
  return _internal_mutable_axes();
}

// bool keepDims = 2;
inline void ReduceSumLayerParams::clear_keepdims() {
  keepdims_ = false;
}
inline bool ReduceSumLayerParams::_internal_keepdims() const {
  return keepdims_;
}
inline bool ReduceSumLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceSumLayerParams.keepDims)
  return _internal_keepdims();
}
inline void ReduceSumLayerParams::_internal_set_keepdims(bool value) {
  
  keepdims_ = value;
}
inline void ReduceSumLayerParams::set_keepdims(bool value) {
  _internal_set_keepdims(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceSumLayerParams.keepDims)
}

// bool reduceAll = 3;
inline void ReduceSumLayerParams::clear_reduceall() {
  reduceall_ = false;
}
inline bool ReduceSumLayerParams::_internal_reduceall() const {
  return reduceall_;
}
inline bool ReduceSumLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceSumLayerParams.reduceAll)
  return _internal_reduceall();
}
inline void ReduceSumLayerParams::_internal_set_reduceall(bool value) {
  
  reduceall_ = value;
}
inline void ReduceSumLayerParams::set_reduceall(bool value) {
  _internal_set_reduceall(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceSumLayerParams.reduceAll)
}

// -------------------------------------------------------------------

// ReduceProdLayerParams

// repeated int64 axes = 1;
inline int ReduceProdLayerParams::_internal_axes_size() const {
  return axes_.size();
}
inline int ReduceProdLayerParams::axes_size() const {
  return _internal_axes_size();
}
inline void ReduceProdLayerParams::clear_axes() {
  axes_.Clear();
}
inline int64_t ReduceProdLayerParams::_internal_axes(int index) const {
  return axes_.Get(index);
}
inline int64_t ReduceProdLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceProdLayerParams.axes)
  return _internal_axes(index);
}
inline void ReduceProdLayerParams::set_axes(int index, int64_t value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceProdLayerParams.axes)
}
inline void ReduceProdLayerParams::_internal_add_axes(int64_t value) {
  axes_.Add(value);
}
inline void ReduceProdLayerParams::add_axes(int64_t value) {
  _internal_add_axes(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceProdLayerParams.axes)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReduceProdLayerParams::_internal_axes() const {
  return axes_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReduceProdLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceProdLayerParams.axes)
  return _internal_axes();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReduceProdLayerParams::_internal_mutable_axes() {
  return &axes_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReduceProdLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceProdLayerParams.axes)
  return _internal_mutable_axes();
}

// bool keepDims = 2;
inline void ReduceProdLayerParams::clear_keepdims() {
  keepdims_ = false;
}
inline bool ReduceProdLayerParams::_internal_keepdims() const {
  return keepdims_;
}
inline bool ReduceProdLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceProdLayerParams.keepDims)
  return _internal_keepdims();
}
inline void ReduceProdLayerParams::_internal_set_keepdims(bool value) {
  
  keepdims_ = value;
}
inline void ReduceProdLayerParams::set_keepdims(bool value) {
  _internal_set_keepdims(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceProdLayerParams.keepDims)
}

// bool reduceAll = 3;
inline void ReduceProdLayerParams::clear_reduceall() {
  reduceall_ = false;
}
inline bool ReduceProdLayerParams::_internal_reduceall() const {
  return reduceall_;
}
inline bool ReduceProdLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceProdLayerParams.reduceAll)
  return _internal_reduceall();
}
inline void ReduceProdLayerParams::_internal_set_reduceall(bool value) {
  
  reduceall_ = value;
}
inline void ReduceProdLayerParams::set_reduceall(bool value) {
  _internal_set_reduceall(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceProdLayerParams.reduceAll)
}

// -------------------------------------------------------------------

// ReduceMeanLayerParams

// repeated int64 axes = 1;
inline int ReduceMeanLayerParams::_internal_axes_size() const {
  return axes_.size();
}
inline int ReduceMeanLayerParams::axes_size() const {
  return _internal_axes_size();
}
inline void ReduceMeanLayerParams::clear_axes() {
  axes_.Clear();
}
inline int64_t ReduceMeanLayerParams::_internal_axes(int index) const {
  return axes_.Get(index);
}
inline int64_t ReduceMeanLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMeanLayerParams.axes)
  return _internal_axes(index);
}
inline void ReduceMeanLayerParams::set_axes(int index, int64_t value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMeanLayerParams.axes)
}
inline void ReduceMeanLayerParams::_internal_add_axes(int64_t value) {
  axes_.Add(value);
}
inline void ReduceMeanLayerParams::add_axes(int64_t value) {
  _internal_add_axes(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceMeanLayerParams.axes)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReduceMeanLayerParams::_internal_axes() const {
  return axes_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReduceMeanLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceMeanLayerParams.axes)
  return _internal_axes();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReduceMeanLayerParams::_internal_mutable_axes() {
  return &axes_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReduceMeanLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceMeanLayerParams.axes)
  return _internal_mutable_axes();
}

// bool keepDims = 2;
inline void ReduceMeanLayerParams::clear_keepdims() {
  keepdims_ = false;
}
inline bool ReduceMeanLayerParams::_internal_keepdims() const {
  return keepdims_;
}
inline bool ReduceMeanLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMeanLayerParams.keepDims)
  return _internal_keepdims();
}
inline void ReduceMeanLayerParams::_internal_set_keepdims(bool value) {
  
  keepdims_ = value;
}
inline void ReduceMeanLayerParams::set_keepdims(bool value) {
  _internal_set_keepdims(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMeanLayerParams.keepDims)
}

// bool reduceAll = 3;
inline void ReduceMeanLayerParams::clear_reduceall() {
  reduceall_ = false;
}
inline bool ReduceMeanLayerParams::_internal_reduceall() const {
  return reduceall_;
}
inline bool ReduceMeanLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMeanLayerParams.reduceAll)
  return _internal_reduceall();
}
inline void ReduceMeanLayerParams::_internal_set_reduceall(bool value) {
  
  reduceall_ = value;
}
inline void ReduceMeanLayerParams::set_reduceall(bool value) {
  _internal_set_reduceall(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMeanLayerParams.reduceAll)
}

// -------------------------------------------------------------------

// ReduceLogSumLayerParams

// repeated int64 axes = 1;
inline int ReduceLogSumLayerParams::_internal_axes_size() const {
  return axes_.size();
}
inline int ReduceLogSumLayerParams::axes_size() const {
  return _internal_axes_size();
}
inline void ReduceLogSumLayerParams::clear_axes() {
  axes_.Clear();
}
inline int64_t ReduceLogSumLayerParams::_internal_axes(int index) const {
  return axes_.Get(index);
}
inline int64_t ReduceLogSumLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLogSumLayerParams.axes)
  return _internal_axes(index);
}
inline void ReduceLogSumLayerParams::set_axes(int index, int64_t value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLogSumLayerParams.axes)
}
inline void ReduceLogSumLayerParams::_internal_add_axes(int64_t value) {
  axes_.Add(value);
}
inline void ReduceLogSumLayerParams::add_axes(int64_t value) {
  _internal_add_axes(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceLogSumLayerParams.axes)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReduceLogSumLayerParams::_internal_axes() const {
  return axes_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReduceLogSumLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceLogSumLayerParams.axes)
  return _internal_axes();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReduceLogSumLayerParams::_internal_mutable_axes() {
  return &axes_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReduceLogSumLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceLogSumLayerParams.axes)
  return _internal_mutable_axes();
}

// bool keepDims = 2;
inline void ReduceLogSumLayerParams::clear_keepdims() {
  keepdims_ = false;
}
inline bool ReduceLogSumLayerParams::_internal_keepdims() const {
  return keepdims_;
}
inline bool ReduceLogSumLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLogSumLayerParams.keepDims)
  return _internal_keepdims();
}
inline void ReduceLogSumLayerParams::_internal_set_keepdims(bool value) {
  
  keepdims_ = value;
}
inline void ReduceLogSumLayerParams::set_keepdims(bool value) {
  _internal_set_keepdims(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLogSumLayerParams.keepDims)
}

// bool reduceAll = 3;
inline void ReduceLogSumLayerParams::clear_reduceall() {
  reduceall_ = false;
}
inline bool ReduceLogSumLayerParams::_internal_reduceall() const {
  return reduceall_;
}
inline bool ReduceLogSumLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLogSumLayerParams.reduceAll)
  return _internal_reduceall();
}
inline void ReduceLogSumLayerParams::_internal_set_reduceall(bool value) {
  
  reduceall_ = value;
}
inline void ReduceLogSumLayerParams::set_reduceall(bool value) {
  _internal_set_reduceall(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLogSumLayerParams.reduceAll)
}

// -------------------------------------------------------------------

// ReduceSumSquareLayerParams

// repeated int64 axes = 1;
inline int ReduceSumSquareLayerParams::_internal_axes_size() const {
  return axes_.size();
}
inline int ReduceSumSquareLayerParams::axes_size() const {
  return _internal_axes_size();
}
inline void ReduceSumSquareLayerParams::clear_axes() {
  axes_.Clear();
}
inline int64_t ReduceSumSquareLayerParams::_internal_axes(int index) const {
  return axes_.Get(index);
}
inline int64_t ReduceSumSquareLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceSumSquareLayerParams.axes)
  return _internal_axes(index);
}
inline void ReduceSumSquareLayerParams::set_axes(int index, int64_t value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceSumSquareLayerParams.axes)
}
inline void ReduceSumSquareLayerParams::_internal_add_axes(int64_t value) {
  axes_.Add(value);
}
inline void ReduceSumSquareLayerParams::add_axes(int64_t value) {
  _internal_add_axes(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceSumSquareLayerParams.axes)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReduceSumSquareLayerParams::_internal_axes() const {
  return axes_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReduceSumSquareLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceSumSquareLayerParams.axes)
  return _internal_axes();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReduceSumSquareLayerParams::_internal_mutable_axes() {
  return &axes_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReduceSumSquareLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceSumSquareLayerParams.axes)
  return _internal_mutable_axes();
}

// bool keepDims = 2;
inline void ReduceSumSquareLayerParams::clear_keepdims() {
  keepdims_ = false;
}
inline bool ReduceSumSquareLayerParams::_internal_keepdims() const {
  return keepdims_;
}
inline bool ReduceSumSquareLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceSumSquareLayerParams.keepDims)
  return _internal_keepdims();
}
inline void ReduceSumSquareLayerParams::_internal_set_keepdims(bool value) {
  
  keepdims_ = value;
}
inline void ReduceSumSquareLayerParams::set_keepdims(bool value) {
  _internal_set_keepdims(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceSumSquareLayerParams.keepDims)
}

// bool reduceAll = 3;
inline void ReduceSumSquareLayerParams::clear_reduceall() {
  reduceall_ = false;
}
inline bool ReduceSumSquareLayerParams::_internal_reduceall() const {
  return reduceall_;
}
inline bool ReduceSumSquareLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceSumSquareLayerParams.reduceAll)
  return _internal_reduceall();
}
inline void ReduceSumSquareLayerParams::_internal_set_reduceall(bool value) {
  
  reduceall_ = value;
}
inline void ReduceSumSquareLayerParams::set_reduceall(bool value) {
  _internal_set_reduceall(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceSumSquareLayerParams.reduceAll)
}

// -------------------------------------------------------------------

// ReduceLogSumExpLayerParams

// repeated int64 axes = 1;
inline int ReduceLogSumExpLayerParams::_internal_axes_size() const {
  return axes_.size();
}
inline int ReduceLogSumExpLayerParams::axes_size() const {
  return _internal_axes_size();
}
inline void ReduceLogSumExpLayerParams::clear_axes() {
  axes_.Clear();
}
inline int64_t ReduceLogSumExpLayerParams::_internal_axes(int index) const {
  return axes_.Get(index);
}
inline int64_t ReduceLogSumExpLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLogSumExpLayerParams.axes)
  return _internal_axes(index);
}
inline void ReduceLogSumExpLayerParams::set_axes(int index, int64_t value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLogSumExpLayerParams.axes)
}
inline void ReduceLogSumExpLayerParams::_internal_add_axes(int64_t value) {
  axes_.Add(value);
}
inline void ReduceLogSumExpLayerParams::add_axes(int64_t value) {
  _internal_add_axes(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceLogSumExpLayerParams.axes)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReduceLogSumExpLayerParams::_internal_axes() const {
  return axes_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReduceLogSumExpLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceLogSumExpLayerParams.axes)
  return _internal_axes();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReduceLogSumExpLayerParams::_internal_mutable_axes() {
  return &axes_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReduceLogSumExpLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceLogSumExpLayerParams.axes)
  return _internal_mutable_axes();
}

// bool keepDims = 2;
inline void ReduceLogSumExpLayerParams::clear_keepdims() {
  keepdims_ = false;
}
inline bool ReduceLogSumExpLayerParams::_internal_keepdims() const {
  return keepdims_;
}
inline bool ReduceLogSumExpLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLogSumExpLayerParams.keepDims)
  return _internal_keepdims();
}
inline void ReduceLogSumExpLayerParams::_internal_set_keepdims(bool value) {
  
  keepdims_ = value;
}
inline void ReduceLogSumExpLayerParams::set_keepdims(bool value) {
  _internal_set_keepdims(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLogSumExpLayerParams.keepDims)
}

// bool reduceAll = 3;
inline void ReduceLogSumExpLayerParams::clear_reduceall() {
  reduceall_ = false;
}
inline bool ReduceLogSumExpLayerParams::_internal_reduceall() const {
  return reduceall_;
}
inline bool ReduceLogSumExpLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLogSumExpLayerParams.reduceAll)
  return _internal_reduceall();
}
inline void ReduceLogSumExpLayerParams::_internal_set_reduceall(bool value) {
  
  reduceall_ = value;
}
inline void ReduceLogSumExpLayerParams::set_reduceall(bool value) {
  _internal_set_reduceall(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLogSumExpLayerParams.reduceAll)
}

// -------------------------------------------------------------------

// ExpandDimsLayerParams

// repeated int64 axes = 1;
inline int ExpandDimsLayerParams::_internal_axes_size() const {
  return axes_.size();
}
inline int ExpandDimsLayerParams::axes_size() const {
  return _internal_axes_size();
}
inline void ExpandDimsLayerParams::clear_axes() {
  axes_.Clear();
}
inline int64_t ExpandDimsLayerParams::_internal_axes(int index) const {
  return axes_.Get(index);
}
inline int64_t ExpandDimsLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ExpandDimsLayerParams.axes)
  return _internal_axes(index);
}
inline void ExpandDimsLayerParams::set_axes(int index, int64_t value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ExpandDimsLayerParams.axes)
}
inline void ExpandDimsLayerParams::_internal_add_axes(int64_t value) {
  axes_.Add(value);
}
inline void ExpandDimsLayerParams::add_axes(int64_t value) {
  _internal_add_axes(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ExpandDimsLayerParams.axes)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ExpandDimsLayerParams::_internal_axes() const {
  return axes_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ExpandDimsLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ExpandDimsLayerParams.axes)
  return _internal_axes();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ExpandDimsLayerParams::_internal_mutable_axes() {
  return &axes_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ExpandDimsLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ExpandDimsLayerParams.axes)
  return _internal_mutable_axes();
}

// -------------------------------------------------------------------

// FlattenTo2DLayerParams

// int64 axis = 1;
inline void FlattenTo2DLayerParams::clear_axis() {
  axis_ = int64_t{0};
}
inline int64_t FlattenTo2DLayerParams::_internal_axis() const {
  return axis_;
}
inline int64_t FlattenTo2DLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FlattenTo2DLayerParams.axis)
  return _internal_axis();
}
inline void FlattenTo2DLayerParams::_internal_set_axis(int64_t value) {
  
  axis_ = value;
}
inline void FlattenTo2DLayerParams::set_axis(int64_t value) {
  _internal_set_axis(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.FlattenTo2DLayerParams.axis)
}

// -------------------------------------------------------------------

// ReshapeStaticLayerParams

// repeated int64 targetShape = 1;
inline int ReshapeStaticLayerParams::_internal_targetshape_size() const {
  return targetshape_.size();
}
inline int ReshapeStaticLayerParams::targetshape_size() const {
  return _internal_targetshape_size();
}
inline void ReshapeStaticLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
inline int64_t ReshapeStaticLayerParams::_internal_targetshape(int index) const {
  return targetshape_.Get(index);
}
inline int64_t ReshapeStaticLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReshapeStaticLayerParams.targetShape)
  return _internal_targetshape(index);
}
inline void ReshapeStaticLayerParams::set_targetshape(int index, int64_t value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReshapeStaticLayerParams.targetShape)
}
inline void ReshapeStaticLayerParams::_internal_add_targetshape(int64_t value) {
  targetshape_.Add(value);
}
inline void ReshapeStaticLayerParams::add_targetshape(int64_t value) {
  _internal_add_targetshape(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReshapeStaticLayerParams.targetShape)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReshapeStaticLayerParams::_internal_targetshape() const {
  return targetshape_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ReshapeStaticLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReshapeStaticLayerParams.targetShape)
  return _internal_targetshape();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReshapeStaticLayerParams::_internal_mutable_targetshape() {
  return &targetshape_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ReshapeStaticLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReshapeStaticLayerParams.targetShape)
  return _internal_mutable_targetshape();
}

// -------------------------------------------------------------------

// ReshapeLikeLayerParams

// -------------------------------------------------------------------

// ReshapeDynamicLayerParams

// -------------------------------------------------------------------

// SqueezeLayerParams

// repeated int64 axes = 1;
inline int SqueezeLayerParams::_internal_axes_size() const {
  return axes_.size();
}
inline int SqueezeLayerParams::axes_size() const {
  return _internal_axes_size();
}
inline void SqueezeLayerParams::clear_axes() {
  axes_.Clear();
}
inline int64_t SqueezeLayerParams::_internal_axes(int index) const {
  return axes_.Get(index);
}
inline int64_t SqueezeLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SqueezeLayerParams.axes)
  return _internal_axes(index);
}
inline void SqueezeLayerParams::set_axes(int index, int64_t value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SqueezeLayerParams.axes)
}
inline void SqueezeLayerParams::_internal_add_axes(int64_t value) {
  axes_.Add(value);
}
inline void SqueezeLayerParams::add_axes(int64_t value) {
  _internal_add_axes(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SqueezeLayerParams.axes)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
SqueezeLayerParams::_internal_axes() const {
  return axes_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
SqueezeLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SqueezeLayerParams.axes)
  return _internal_axes();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
SqueezeLayerParams::_internal_mutable_axes() {
  return &axes_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
SqueezeLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SqueezeLayerParams.axes)
  return _internal_mutable_axes();
}

// bool squeezeAll = 2;
inline void SqueezeLayerParams::clear_squeezeall() {
  squeezeall_ = false;
}
inline bool SqueezeLayerParams::_internal_squeezeall() const {
  return squeezeall_;
}
inline bool SqueezeLayerParams::squeezeall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SqueezeLayerParams.squeezeAll)
  return _internal_squeezeall();
}
inline void SqueezeLayerParams::_internal_set_squeezeall(bool value) {
  
  squeezeall_ = value;
}
inline void SqueezeLayerParams::set_squeezeall(bool value) {
  _internal_set_squeezeall(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SqueezeLayerParams.squeezeAll)
}

// -------------------------------------------------------------------

// TopKLayerParams

// int64 axis = 1;
inline void TopKLayerParams::clear_axis() {
  axis_ = int64_t{0};
}
inline int64_t TopKLayerParams::_internal_axis() const {
  return axis_;
}
inline int64_t TopKLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.TopKLayerParams.axis)
  return _internal_axis();
}
inline void TopKLayerParams::_internal_set_axis(int64_t value) {
  
  axis_ = value;
}
inline void TopKLayerParams::set_axis(int64_t value) {
  _internal_set_axis(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.TopKLayerParams.axis)
}

// uint64 K = 2;
inline void TopKLayerParams::clear_k() {
  k_ = uint64_t{0u};
}
inline uint64_t TopKLayerParams::_internal_k() const {
  return k_;
}
inline uint64_t TopKLayerParams::k() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.TopKLayerParams.K)
  return _internal_k();
}
inline void TopKLayerParams::_internal_set_k(uint64_t value) {
  
  k_ = value;
}
inline void TopKLayerParams::set_k(uint64_t value) {
  _internal_set_k(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.TopKLayerParams.K)
}

// bool useBottomK = 3;
inline void TopKLayerParams::clear_usebottomk() {
  usebottomk_ = false;
}
inline bool TopKLayerParams::_internal_usebottomk() const {
  return usebottomk_;
}
inline bool TopKLayerParams::usebottomk() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.TopKLayerParams.useBottomK)
  return _internal_usebottomk();
}
inline void TopKLayerParams::_internal_set_usebottomk(bool value) {
  
  usebottomk_ = value;
}
inline void TopKLayerParams::set_usebottomk(bool value) {
  _internal_set_usebottomk(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.TopKLayerParams.useBottomK)
}

// -------------------------------------------------------------------

// ArgMaxLayerParams

// int64 axis = 1;
inline void ArgMaxLayerParams::clear_axis() {
  axis_ = int64_t{0};
}
inline int64_t ArgMaxLayerParams::_internal_axis() const {
  return axis_;
}
inline int64_t ArgMaxLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ArgMaxLayerParams.axis)
  return _internal_axis();
}
inline void ArgMaxLayerParams::_internal_set_axis(int64_t value) {
  
  axis_ = value;
}
inline void ArgMaxLayerParams::set_axis(int64_t value) {
  _internal_set_axis(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ArgMaxLayerParams.axis)
}

// bool removeDim = 2;
inline void ArgMaxLayerParams::clear_removedim() {
  removedim_ = false;
}
inline bool ArgMaxLayerParams::_internal_removedim() const {
  return removedim_;
}
inline bool ArgMaxLayerParams::removedim() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ArgMaxLayerParams.removeDim)
  return _internal_removedim();
}
inline void ArgMaxLayerParams::_internal_set_removedim(bool value) {
  
  removedim_ = value;
}
inline void ArgMaxLayerParams::set_removedim(bool value) {
  _internal_set_removedim(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ArgMaxLayerParams.removeDim)
}

// -------------------------------------------------------------------

// ArgMinLayerParams

// int64 axis = 1;
inline void ArgMinLayerParams::clear_axis() {
  axis_ = int64_t{0};
}
inline int64_t ArgMinLayerParams::_internal_axis() const {
  return axis_;
}
inline int64_t ArgMinLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ArgMinLayerParams.axis)
  return _internal_axis();
}
inline void ArgMinLayerParams::_internal_set_axis(int64_t value) {
  
  axis_ = value;
}
inline void ArgMinLayerParams::set_axis(int64_t value) {
  _internal_set_axis(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ArgMinLayerParams.axis)
}

// bool removeDim = 2;
inline void ArgMinLayerParams::clear_removedim() {
  removedim_ = false;
}
inline bool ArgMinLayerParams::_internal_removedim() const {
  return removedim_;
}
inline bool ArgMinLayerParams::removedim() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ArgMinLayerParams.removeDim)
  return _internal_removedim();
}
inline void ArgMinLayerParams::_internal_set_removedim(bool value) {
  
  removedim_ = value;
}
inline void ArgMinLayerParams::set_removedim(bool value) {
  _internal_set_removedim(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ArgMinLayerParams.removeDim)
}

// -------------------------------------------------------------------

// SplitNDLayerParams

// int64 axis = 1;
inline void SplitNDLayerParams::clear_axis() {
  axis_ = int64_t{0};
}
inline int64_t SplitNDLayerParams::_internal_axis() const {
  return axis_;
}
inline int64_t SplitNDLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SplitNDLayerParams.axis)
  return _internal_axis();
}
inline void SplitNDLayerParams::_internal_set_axis(int64_t value) {
  
  axis_ = value;
}
inline void SplitNDLayerParams::set_axis(int64_t value) {
  _internal_set_axis(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SplitNDLayerParams.axis)
}

// uint64 numSplits = 2;
inline void SplitNDLayerParams::clear_numsplits() {
  numsplits_ = uint64_t{0u};
}
inline uint64_t SplitNDLayerParams::_internal_numsplits() const {
  return numsplits_;
}
inline uint64_t SplitNDLayerParams::numsplits() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SplitNDLayerParams.numSplits)
  return _internal_numsplits();
}
inline void SplitNDLayerParams::_internal_set_numsplits(uint64_t value) {
  
  numsplits_ = value;
}
inline void SplitNDLayerParams::set_numsplits(uint64_t value) {
  _internal_set_numsplits(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SplitNDLayerParams.numSplits)
}

// repeated uint64 splitSizes = 3;
inline int SplitNDLayerParams::_internal_splitsizes_size() const {
  return splitsizes_.size();
}
inline int SplitNDLayerParams::splitsizes_size() const {
  return _internal_splitsizes_size();
}
inline void SplitNDLayerParams::clear_splitsizes() {
  splitsizes_.Clear();
}
inline uint64_t SplitNDLayerParams::_internal_splitsizes(int index) const {
  return splitsizes_.Get(index);
}
inline uint64_t SplitNDLayerParams::splitsizes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SplitNDLayerParams.splitSizes)
  return _internal_splitsizes(index);
}
inline void SplitNDLayerParams::set_splitsizes(int index, uint64_t value) {
  splitsizes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SplitNDLayerParams.splitSizes)
}
inline void SplitNDLayerParams::_internal_add_splitsizes(uint64_t value) {
  splitsizes_.Add(value);
}
inline void SplitNDLayerParams::add_splitsizes(uint64_t value) {
  _internal_add_splitsizes(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SplitNDLayerParams.splitSizes)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
SplitNDLayerParams::_internal_splitsizes() const {
  return splitsizes_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
SplitNDLayerParams::splitsizes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SplitNDLayerParams.splitSizes)
  return _internal_splitsizes();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
SplitNDLayerParams::_internal_mutable_splitsizes() {
  return &splitsizes_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
SplitNDLayerParams::mutable_splitsizes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SplitNDLayerParams.splitSizes)
  return _internal_mutable_splitsizes();
}

// -------------------------------------------------------------------

// CeilLayerParams

// -------------------------------------------------------------------

// RoundLayerParams

// -------------------------------------------------------------------

// FloorLayerParams

// -------------------------------------------------------------------

// SignLayerParams

// -------------------------------------------------------------------

// ClipLayerParams

// float minVal = 1;
inline void ClipLayerParams::clear_minval() {
  minval_ = 0;
}
inline float ClipLayerParams::_internal_minval() const {
  return minval_;
}
inline float ClipLayerParams::minval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ClipLayerParams.minVal)
  return _internal_minval();
}
inline void ClipLayerParams::_internal_set_minval(float value) {
  
  minval_ = value;
}
inline void ClipLayerParams::set_minval(float value) {
  _internal_set_minval(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ClipLayerParams.minVal)
}

// float maxVal = 2;
inline void ClipLayerParams::clear_maxval() {
  maxval_ = 0;
}
inline float ClipLayerParams::_internal_maxval() const {
  return maxval_;
}
inline float ClipLayerParams::maxval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ClipLayerParams.maxVal)
  return _internal_maxval();
}
inline void ClipLayerParams::_internal_set_maxval(float value) {
  
  maxval_ = value;
}
inline void ClipLayerParams::set_maxval(float value) {
  _internal_set_maxval(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ClipLayerParams.maxVal)
}

// -------------------------------------------------------------------

// SliceStaticLayerParams

// repeated int64 beginIds = 1;
inline int SliceStaticLayerParams::_internal_beginids_size() const {
  return beginids_.size();
}
inline int SliceStaticLayerParams::beginids_size() const {
  return _internal_beginids_size();
}
inline void SliceStaticLayerParams::clear_beginids() {
  beginids_.Clear();
}
inline int64_t SliceStaticLayerParams::_internal_beginids(int index) const {
  return beginids_.Get(index);
}
inline int64_t SliceStaticLayerParams::beginids(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceStaticLayerParams.beginIds)
  return _internal_beginids(index);
}
inline void SliceStaticLayerParams::set_beginids(int index, int64_t value) {
  beginids_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceStaticLayerParams.beginIds)
}
inline void SliceStaticLayerParams::_internal_add_beginids(int64_t value) {
  beginids_.Add(value);
}
inline void SliceStaticLayerParams::add_beginids(int64_t value) {
  _internal_add_beginids(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceStaticLayerParams.beginIds)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
SliceStaticLayerParams::_internal_beginids() const {
  return beginids_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
SliceStaticLayerParams::beginids() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceStaticLayerParams.beginIds)
  return _internal_beginids();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
SliceStaticLayerParams::_internal_mutable_beginids() {
  return &beginids_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
SliceStaticLayerParams::mutable_beginids() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceStaticLayerParams.beginIds)
  return _internal_mutable_beginids();
}

// repeated bool beginMasks = 2;
inline int SliceStaticLayerParams::_internal_beginmasks_size() const {
  return beginmasks_.size();
}
inline int SliceStaticLayerParams::beginmasks_size() const {
  return _internal_beginmasks_size();
}
inline void SliceStaticLayerParams::clear_beginmasks() {
  beginmasks_.Clear();
}
inline bool SliceStaticLayerParams::_internal_beginmasks(int index) const {
  return beginmasks_.Get(index);
}
inline bool SliceStaticLayerParams::beginmasks(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceStaticLayerParams.beginMasks)
  return _internal_beginmasks(index);
}
inline void SliceStaticLayerParams::set_beginmasks(int index, bool value) {
  beginmasks_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceStaticLayerParams.beginMasks)
}
inline void SliceStaticLayerParams::_internal_add_beginmasks(bool value) {
  beginmasks_.Add(value);
}
inline void SliceStaticLayerParams::add_beginmasks(bool value) {
  _internal_add_beginmasks(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceStaticLayerParams.beginMasks)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
SliceStaticLayerParams::_internal_beginmasks() const {
  return beginmasks_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
SliceStaticLayerParams::beginmasks() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceStaticLayerParams.beginMasks)
  return _internal_beginmasks();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
SliceStaticLayerParams::_internal_mutable_beginmasks() {
  return &beginmasks_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
SliceStaticLayerParams::mutable_beginmasks() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceStaticLayerParams.beginMasks)
  return _internal_mutable_beginmasks();
}

// repeated int64 endIds = 3;
inline int SliceStaticLayerParams::_internal_endids_size() const {
  return endids_.size();
}
inline int SliceStaticLayerParams::endids_size() const {
  return _internal_endids_size();
}
inline void SliceStaticLayerParams::clear_endids() {
  endids_.Clear();
}
inline int64_t SliceStaticLayerParams::_internal_endids(int index) const {
  return endids_.Get(index);
}
inline int64_t SliceStaticLayerParams::endids(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceStaticLayerParams.endIds)
  return _internal_endids(index);
}
inline void SliceStaticLayerParams::set_endids(int index, int64_t value) {
  endids_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceStaticLayerParams.endIds)
}
inline void SliceStaticLayerParams::_internal_add_endids(int64_t value) {
  endids_.Add(value);
}
inline void SliceStaticLayerParams::add_endids(int64_t value) {
  _internal_add_endids(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceStaticLayerParams.endIds)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
SliceStaticLayerParams::_internal_endids() const {
  return endids_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
SliceStaticLayerParams::endids() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceStaticLayerParams.endIds)
  return _internal_endids();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
SliceStaticLayerParams::_internal_mutable_endids() {
  return &endids_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
SliceStaticLayerParams::mutable_endids() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceStaticLayerParams.endIds)
  return _internal_mutable_endids();
}

// repeated bool endMasks = 4;
inline int SliceStaticLayerParams::_internal_endmasks_size() const {
  return endmasks_.size();
}
inline int SliceStaticLayerParams::endmasks_size() const {
  return _internal_endmasks_size();
}
inline void SliceStaticLayerParams::clear_endmasks() {
  endmasks_.Clear();
}
inline bool SliceStaticLayerParams::_internal_endmasks(int index) const {
  return endmasks_.Get(index);
}
inline bool SliceStaticLayerParams::endmasks(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceStaticLayerParams.endMasks)
  return _internal_endmasks(index);
}
inline void SliceStaticLayerParams::set_endmasks(int index, bool value) {
  endmasks_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceStaticLayerParams.endMasks)
}
inline void SliceStaticLayerParams::_internal_add_endmasks(bool value) {
  endmasks_.Add(value);
}
inline void SliceStaticLayerParams::add_endmasks(bool value) {
  _internal_add_endmasks(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceStaticLayerParams.endMasks)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
SliceStaticLayerParams::_internal_endmasks() const {
  return endmasks_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
SliceStaticLayerParams::endmasks() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceStaticLayerParams.endMasks)
  return _internal_endmasks();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
SliceStaticLayerParams::_internal_mutable_endmasks() {
  return &endmasks_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
SliceStaticLayerParams::mutable_endmasks() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceStaticLayerParams.endMasks)
  return _internal_mutable_endmasks();
}

// repeated int64 strides = 5;
inline int SliceStaticLayerParams::_internal_strides_size() const {
  return strides_.size();
}
inline int SliceStaticLayerParams::strides_size() const {
  return _internal_strides_size();
}
inline void SliceStaticLayerParams::clear_strides() {
  strides_.Clear();
}
inline int64_t SliceStaticLayerParams::_internal_strides(int index) const {
  return strides_.Get(index);
}
inline int64_t SliceStaticLayerParams::strides(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceStaticLayerParams.strides)
  return _internal_strides(index);
}
inline void SliceStaticLayerParams::set_strides(int index, int64_t value) {
  strides_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceStaticLayerParams.strides)
}
inline void SliceStaticLayerParams::_internal_add_strides(int64_t value) {
  strides_.Add(value);
}
inline void SliceStaticLayerParams::add_strides(int64_t value) {
  _internal_add_strides(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceStaticLayerParams.strides)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
SliceStaticLayerParams::_internal_strides() const {
  return strides_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
SliceStaticLayerParams::strides() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceStaticLayerParams.strides)
  return _internal_strides();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
SliceStaticLayerParams::_internal_mutable_strides() {
  return &strides_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
SliceStaticLayerParams::mutable_strides() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceStaticLayerParams.strides)
  return _internal_mutable_strides();
}

// repeated bool squeezeMasks = 6;
inline int SliceStaticLayerParams::_internal_squeezemasks_size() const {
  return squeezemasks_.size();
}
inline int SliceStaticLayerParams::squeezemasks_size() const {
  return _internal_squeezemasks_size();
}
inline void SliceStaticLayerParams::clear_squeezemasks() {
  squeezemasks_.Clear();
}
inline bool SliceStaticLayerParams::_internal_squeezemasks(int index) const {
  return squeezemasks_.Get(index);
}
inline bool SliceStaticLayerParams::squeezemasks(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceStaticLayerParams.squeezeMasks)
  return _internal_squeezemasks(index);
}
inline void SliceStaticLayerParams::set_squeezemasks(int index, bool value) {
  squeezemasks_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceStaticLayerParams.squeezeMasks)
}
inline void SliceStaticLayerParams::_internal_add_squeezemasks(bool value) {
  squeezemasks_.Add(value);
}
inline void SliceStaticLayerParams::add_squeezemasks(bool value) {
  _internal_add_squeezemasks(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceStaticLayerParams.squeezeMasks)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
SliceStaticLayerParams::_internal_squeezemasks() const {
  return squeezemasks_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
SliceStaticLayerParams::squeezemasks() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceStaticLayerParams.squeezeMasks)
  return _internal_squeezemasks();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
SliceStaticLayerParams::_internal_mutable_squeezemasks() {
  return &squeezemasks_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
SliceStaticLayerParams::mutable_squeezemasks() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceStaticLayerParams.squeezeMasks)
  return _internal_mutable_squeezemasks();
}

// -------------------------------------------------------------------

// SliceDynamicLayerParams

// repeated bool beginMasks = 2;
inline int SliceDynamicLayerParams::_internal_beginmasks_size() const {
  return beginmasks_.size();
}
inline int SliceDynamicLayerParams::beginmasks_size() const {
  return _internal_beginmasks_size();
}
inline void SliceDynamicLayerParams::clear_beginmasks() {
  beginmasks_.Clear();
}
inline bool SliceDynamicLayerParams::_internal_beginmasks(int index) const {
  return beginmasks_.Get(index);
}
inline bool SliceDynamicLayerParams::beginmasks(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceDynamicLayerParams.beginMasks)
  return _internal_beginmasks(index);
}
inline void SliceDynamicLayerParams::set_beginmasks(int index, bool value) {
  beginmasks_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceDynamicLayerParams.beginMasks)
}
inline void SliceDynamicLayerParams::_internal_add_beginmasks(bool value) {
  beginmasks_.Add(value);
}
inline void SliceDynamicLayerParams::add_beginmasks(bool value) {
  _internal_add_beginmasks(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceDynamicLayerParams.beginMasks)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
SliceDynamicLayerParams::_internal_beginmasks() const {
  return beginmasks_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
SliceDynamicLayerParams::beginmasks() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceDynamicLayerParams.beginMasks)
  return _internal_beginmasks();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
SliceDynamicLayerParams::_internal_mutable_beginmasks() {
  return &beginmasks_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
SliceDynamicLayerParams::mutable_beginmasks() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceDynamicLayerParams.beginMasks)
  return _internal_mutable_beginmasks();
}

// repeated int64 endIds = 3;
inline int SliceDynamicLayerParams::_internal_endids_size() const {
  return endids_.size();
}
inline int SliceDynamicLayerParams::endids_size() const {
  return _internal_endids_size();
}
inline void SliceDynamicLayerParams::clear_endids() {
  endids_.Clear();
}
inline int64_t SliceDynamicLayerParams::_internal_endids(int index) const {
  return endids_.Get(index);
}
inline int64_t SliceDynamicLayerParams::endids(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceDynamicLayerParams.endIds)
  return _internal_endids(index);
}
inline void SliceDynamicLayerParams::set_endids(int index, int64_t value) {
  endids_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceDynamicLayerParams.endIds)
}
inline void SliceDynamicLayerParams::_internal_add_endids(int64_t value) {
  endids_.Add(value);
}
inline void SliceDynamicLayerParams::add_endids(int64_t value) {
  _internal_add_endids(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceDynamicLayerParams.endIds)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
SliceDynamicLayerParams::_internal_endids() const {
  return endids_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
SliceDynamicLayerParams::endids() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceDynamicLayerParams.endIds)
  return _internal_endids();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
SliceDynamicLayerParams::_internal_mutable_endids() {
  return &endids_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
SliceDynamicLayerParams::mutable_endids() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceDynamicLayerParams.endIds)
  return _internal_mutable_endids();
}

// repeated bool endMasks = 4;
inline int SliceDynamicLayerParams::_internal_endmasks_size() const {
  return endmasks_.size();
}
inline int SliceDynamicLayerParams::endmasks_size() const {
  return _internal_endmasks_size();
}
inline void SliceDynamicLayerParams::clear_endmasks() {
  endmasks_.Clear();
}
inline bool SliceDynamicLayerParams::_internal_endmasks(int index) const {
  return endmasks_.Get(index);
}
inline bool SliceDynamicLayerParams::endmasks(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceDynamicLayerParams.endMasks)
  return _internal_endmasks(index);
}
inline void SliceDynamicLayerParams::set_endmasks(int index, bool value) {
  endmasks_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceDynamicLayerParams.endMasks)
}
inline void SliceDynamicLayerParams::_internal_add_endmasks(bool value) {
  endmasks_.Add(value);
}
inline void SliceDynamicLayerParams::add_endmasks(bool value) {
  _internal_add_endmasks(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceDynamicLayerParams.endMasks)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
SliceDynamicLayerParams::_internal_endmasks() const {
  return endmasks_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
SliceDynamicLayerParams::endmasks() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceDynamicLayerParams.endMasks)
  return _internal_endmasks();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
SliceDynamicLayerParams::_internal_mutable_endmasks() {
  return &endmasks_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
SliceDynamicLayerParams::mutable_endmasks() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceDynamicLayerParams.endMasks)
  return _internal_mutable_endmasks();
}

// repeated int64 strides = 5;
inline int SliceDynamicLayerParams::_internal_strides_size() const {
  return strides_.size();
}
inline int SliceDynamicLayerParams::strides_size() const {
  return _internal_strides_size();
}
inline void SliceDynamicLayerParams::clear_strides() {
  strides_.Clear();
}
inline int64_t SliceDynamicLayerParams::_internal_strides(int index) const {
  return strides_.Get(index);
}
inline int64_t SliceDynamicLayerParams::strides(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceDynamicLayerParams.strides)
  return _internal_strides(index);
}
inline void SliceDynamicLayerParams::set_strides(int index, int64_t value) {
  strides_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceDynamicLayerParams.strides)
}
inline void SliceDynamicLayerParams::_internal_add_strides(int64_t value) {
  strides_.Add(value);
}
inline void SliceDynamicLayerParams::add_strides(int64_t value) {
  _internal_add_strides(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceDynamicLayerParams.strides)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
SliceDynamicLayerParams::_internal_strides() const {
  return strides_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
SliceDynamicLayerParams::strides() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceDynamicLayerParams.strides)
  return _internal_strides();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
SliceDynamicLayerParams::_internal_mutable_strides() {
  return &strides_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
SliceDynamicLayerParams::mutable_strides() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceDynamicLayerParams.strides)
  return _internal_mutable_strides();
}

// repeated bool squeezeMasks = 6;
inline int SliceDynamicLayerParams::_internal_squeezemasks_size() const {
  return squeezemasks_.size();
}
inline int SliceDynamicLayerParams::squeezemasks_size() const {
  return _internal_squeezemasks_size();
}
inline void SliceDynamicLayerParams::clear_squeezemasks() {
  squeezemasks_.Clear();
}
inline bool SliceDynamicLayerParams::_internal_squeezemasks(int index) const {
  return squeezemasks_.Get(index);
}
inline bool SliceDynamicLayerParams::squeezemasks(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceDynamicLayerParams.squeezeMasks)
  return _internal_squeezemasks(index);
}
inline void SliceDynamicLayerParams::set_squeezemasks(int index, bool value) {
  squeezemasks_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceDynamicLayerParams.squeezeMasks)
}
inline void SliceDynamicLayerParams::_internal_add_squeezemasks(bool value) {
  squeezemasks_.Add(value);
}
inline void SliceDynamicLayerParams::add_squeezemasks(bool value) {
  _internal_add_squeezemasks(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceDynamicLayerParams.squeezeMasks)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
SliceDynamicLayerParams::_internal_squeezemasks() const {
  return squeezemasks_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
SliceDynamicLayerParams::squeezemasks() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceDynamicLayerParams.squeezeMasks)
  return _internal_squeezemasks();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
SliceDynamicLayerParams::_internal_mutable_squeezemasks() {
  return &squeezemasks_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
SliceDynamicLayerParams::mutable_squeezemasks() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceDynamicLayerParams.squeezeMasks)
  return _internal_mutable_squeezemasks();
}

// -------------------------------------------------------------------

// TileLayerParams

// repeated uint64 reps = 1;
inline int TileLayerParams::_internal_reps_size() const {
  return reps_.size();
}
inline int TileLayerParams::reps_size() const {
  return _internal_reps_size();
}
inline void TileLayerParams::clear_reps() {
  reps_.Clear();
}
inline uint64_t TileLayerParams::_internal_reps(int index) const {
  return reps_.Get(index);
}
inline uint64_t TileLayerParams::reps(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.TileLayerParams.reps)
  return _internal_reps(index);
}
inline void TileLayerParams::set_reps(int index, uint64_t value) {
  reps_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.TileLayerParams.reps)
}
inline void TileLayerParams::_internal_add_reps(uint64_t value) {
  reps_.Add(value);
}
inline void TileLayerParams::add_reps(uint64_t value) {
  _internal_add_reps(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.TileLayerParams.reps)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
TileLayerParams::_internal_reps() const {
  return reps_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >&
TileLayerParams::reps() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.TileLayerParams.reps)
  return _internal_reps();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
TileLayerParams::_internal_mutable_reps() {
  return &reps_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< uint64_t >*
TileLayerParams::mutable_reps() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.TileLayerParams.reps)
  return _internal_mutable_reps();
}

// -------------------------------------------------------------------

// GetShapeLayerParams

// -------------------------------------------------------------------

// ErfLayerParams

// -------------------------------------------------------------------

// GeluLayerParams

// .CoreML.Specification.GeluLayerParams.GeluMode mode = 1;
inline void GeluLayerParams::clear_mode() {
  mode_ = 0;
}
inline ::CoreML::Specification::GeluLayerParams_GeluMode GeluLayerParams::_internal_mode() const {
  return static_cast< ::CoreML::Specification::GeluLayerParams_GeluMode >(mode_);
}
inline ::CoreML::Specification::GeluLayerParams_GeluMode GeluLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GeluLayerParams.mode)
  return _internal_mode();
}
inline void GeluLayerParams::_internal_set_mode(::CoreML::Specification::GeluLayerParams_GeluMode value) {
  
  mode_ = value;
}
inline void GeluLayerParams::set_mode(::CoreML::Specification::GeluLayerParams_GeluMode value) {
  _internal_set_mode(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.GeluLayerParams.mode)
}

// -------------------------------------------------------------------

// RangeStaticLayerParams

// float endValue = 1;
inline void RangeStaticLayerParams::clear_endvalue() {
  endvalue_ = 0;
}
inline float RangeStaticLayerParams::_internal_endvalue() const {
  return endvalue_;
}
inline float RangeStaticLayerParams::endvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RangeStaticLayerParams.endValue)
  return _internal_endvalue();
}
inline void RangeStaticLayerParams::_internal_set_endvalue(float value) {
  
  endvalue_ = value;
}
inline void RangeStaticLayerParams::set_endvalue(float value) {
  _internal_set_endvalue(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RangeStaticLayerParams.endValue)
}

// float startValue = 2;
inline void RangeStaticLayerParams::clear_startvalue() {
  startvalue_ = 0;
}
inline float RangeStaticLayerParams::_internal_startvalue() const {
  return startvalue_;
}
inline float RangeStaticLayerParams::startvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RangeStaticLayerParams.startValue)
  return _internal_startvalue();
}
inline void RangeStaticLayerParams::_internal_set_startvalue(float value) {
  
  startvalue_ = value;
}
inline void RangeStaticLayerParams::set_startvalue(float value) {
  _internal_set_startvalue(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RangeStaticLayerParams.startValue)
}

// float stepSizeValue = 3;
inline void RangeStaticLayerParams::clear_stepsizevalue() {
  stepsizevalue_ = 0;
}
inline float RangeStaticLayerParams::_internal_stepsizevalue() const {
  return stepsizevalue_;
}
inline float RangeStaticLayerParams::stepsizevalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RangeStaticLayerParams.stepSizeValue)
  return _internal_stepsizevalue();
}
inline void RangeStaticLayerParams::_internal_set_stepsizevalue(float value) {
  
  stepsizevalue_ = value;
}
inline void RangeStaticLayerParams::set_stepsizevalue(float value) {
  _internal_set_stepsizevalue(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RangeStaticLayerParams.stepSizeValue)
}

// -------------------------------------------------------------------

// RangeDynamicLayerParams

// float startValue = 2;
inline void RangeDynamicLayerParams::clear_startvalue() {
  startvalue_ = 0;
}
inline float RangeDynamicLayerParams::_internal_startvalue() const {
  return startvalue_;
}
inline float RangeDynamicLayerParams::startvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RangeDynamicLayerParams.startValue)
  return _internal_startvalue();
}
inline void RangeDynamicLayerParams::_internal_set_startvalue(float value) {
  
  startvalue_ = value;
}
inline void RangeDynamicLayerParams::set_startvalue(float value) {
  _internal_set_startvalue(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RangeDynamicLayerParams.startValue)
}

// float stepSizeValue = 3;
inline void RangeDynamicLayerParams::clear_stepsizevalue() {
  stepsizevalue_ = 0;
}
inline float RangeDynamicLayerParams::_internal_stepsizevalue() const {
  return stepsizevalue_;
}
inline float RangeDynamicLayerParams::stepsizevalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RangeDynamicLayerParams.stepSizeValue)
  return _internal_stepsizevalue();
}
inline void RangeDynamicLayerParams::_internal_set_stepsizevalue(float value) {
  
  stepsizevalue_ = value;
}
inline void RangeDynamicLayerParams::set_stepsizevalue(float value) {
  _internal_set_stepsizevalue(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RangeDynamicLayerParams.stepSizeValue)
}

// -------------------------------------------------------------------

// SlidingWindowsLayerParams

// int64 axis = 1;
inline void SlidingWindowsLayerParams::clear_axis() {
  axis_ = int64_t{0};
}
inline int64_t SlidingWindowsLayerParams::_internal_axis() const {
  return axis_;
}
inline int64_t SlidingWindowsLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SlidingWindowsLayerParams.axis)
  return _internal_axis();
}
inline void SlidingWindowsLayerParams::_internal_set_axis(int64_t value) {
  
  axis_ = value;
}
inline void SlidingWindowsLayerParams::set_axis(int64_t value) {
  _internal_set_axis(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SlidingWindowsLayerParams.axis)
}

// uint64 windowSize = 2;
inline void SlidingWindowsLayerParams::clear_windowsize() {
  windowsize_ = uint64_t{0u};
}
inline uint64_t SlidingWindowsLayerParams::_internal_windowsize() const {
  return windowsize_;
}
inline uint64_t SlidingWindowsLayerParams::windowsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SlidingWindowsLayerParams.windowSize)
  return _internal_windowsize();
}
inline void SlidingWindowsLayerParams::_internal_set_windowsize(uint64_t value) {
  
  windowsize_ = value;
}
inline void SlidingWindowsLayerParams::set_windowsize(uint64_t value) {
  _internal_set_windowsize(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SlidingWindowsLayerParams.windowSize)
}

// uint64 step = 3;
inline void SlidingWindowsLayerParams::clear_step() {
  step_ = uint64_t{0u};
}
inline uint64_t SlidingWindowsLayerParams::_internal_step() const {
  return step_;
}
inline uint64_t SlidingWindowsLayerParams::step() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SlidingWindowsLayerParams.step)
  return _internal_step();
}
inline void SlidingWindowsLayerParams::_internal_set_step(uint64_t value) {
  
  step_ = value;
}
inline void SlidingWindowsLayerParams::set_step(uint64_t value) {
  _internal_set_step(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SlidingWindowsLayerParams.step)
}

// -------------------------------------------------------------------

// LayerNormalizationLayerParams

// repeated int64 normalizedShape = 1;
inline int LayerNormalizationLayerParams::_internal_normalizedshape_size() const {
  return normalizedshape_.size();
}
inline int LayerNormalizationLayerParams::normalizedshape_size() const {
  return _internal_normalizedshape_size();
}
inline void LayerNormalizationLayerParams::clear_normalizedshape() {
  normalizedshape_.Clear();
}
inline int64_t LayerNormalizationLayerParams::_internal_normalizedshape(int index) const {
  return normalizedshape_.Get(index);
}
inline int64_t LayerNormalizationLayerParams::normalizedshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LayerNormalizationLayerParams.normalizedShape)
  return _internal_normalizedshape(index);
}
inline void LayerNormalizationLayerParams::set_normalizedshape(int index, int64_t value) {
  normalizedshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LayerNormalizationLayerParams.normalizedShape)
}
inline void LayerNormalizationLayerParams::_internal_add_normalizedshape(int64_t value) {
  normalizedshape_.Add(value);
}
inline void LayerNormalizationLayerParams::add_normalizedshape(int64_t value) {
  _internal_add_normalizedshape(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LayerNormalizationLayerParams.normalizedShape)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
LayerNormalizationLayerParams::_internal_normalizedshape() const {
  return normalizedshape_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
LayerNormalizationLayerParams::normalizedshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LayerNormalizationLayerParams.normalizedShape)
  return _internal_normalizedshape();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
LayerNormalizationLayerParams::_internal_mutable_normalizedshape() {
  return &normalizedshape_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
LayerNormalizationLayerParams::mutable_normalizedshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LayerNormalizationLayerParams.normalizedShape)
  return _internal_mutable_normalizedshape();
}

// float eps = 2;
inline void LayerNormalizationLayerParams::clear_eps() {
  eps_ = 0;
}
inline float LayerNormalizationLayerParams::_internal_eps() const {
  return eps_;
}
inline float LayerNormalizationLayerParams::eps() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LayerNormalizationLayerParams.eps)
  return _internal_eps();
}
inline void LayerNormalizationLayerParams::_internal_set_eps(float value) {
  
  eps_ = value;
}
inline void LayerNormalizationLayerParams::set_eps(float value) {
  _internal_set_eps(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LayerNormalizationLayerParams.eps)
}

// .CoreML.Specification.WeightParams gamma = 3;
inline bool LayerNormalizationLayerParams::_internal_has_gamma() const {
  return this != internal_default_instance() && gamma_ != nullptr;
}
inline bool LayerNormalizationLayerParams::has_gamma() const {
  return _internal_has_gamma();
}
inline void LayerNormalizationLayerParams::clear_gamma() {
  if (GetArenaForAllocation() == nullptr && gamma_ != nullptr) {
    delete gamma_;
  }
  gamma_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& LayerNormalizationLayerParams::_internal_gamma() const {
  const ::CoreML::Specification::WeightParams* p = gamma_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& LayerNormalizationLayerParams::gamma() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LayerNormalizationLayerParams.gamma)
  return _internal_gamma();
}
inline void LayerNormalizationLayerParams::unsafe_arena_set_allocated_gamma(
    ::CoreML::Specification::WeightParams* gamma) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(gamma_);
  }
  gamma_ = gamma;
  if (gamma) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LayerNormalizationLayerParams.gamma)
}
inline ::CoreML::Specification::WeightParams* LayerNormalizationLayerParams::release_gamma() {
  
  ::CoreML::Specification::WeightParams* temp = gamma_;
  gamma_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* LayerNormalizationLayerParams::unsafe_arena_release_gamma() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LayerNormalizationLayerParams.gamma)
  
  ::CoreML::Specification::WeightParams* temp = gamma_;
  gamma_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* LayerNormalizationLayerParams::_internal_mutable_gamma() {
  
  if (gamma_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    gamma_ = p;
  }
  return gamma_;
}
inline ::CoreML::Specification::WeightParams* LayerNormalizationLayerParams::mutable_gamma() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_gamma();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LayerNormalizationLayerParams.gamma)
  return _msg;
}
inline void LayerNormalizationLayerParams::set_allocated_gamma(::CoreML::Specification::WeightParams* gamma) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete gamma_;
  }
  if (gamma) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(gamma);
    if (message_arena != submessage_arena) {
      gamma = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, gamma, submessage_arena);
    }
    
  } else {
    
  }
  gamma_ = gamma;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LayerNormalizationLayerParams.gamma)
}

// .CoreML.Specification.WeightParams beta = 4;
inline bool LayerNormalizationLayerParams::_internal_has_beta() const {
  return this != internal_default_instance() && beta_ != nullptr;
}
inline bool LayerNormalizationLayerParams::has_beta() const {
  return _internal_has_beta();
}
inline void LayerNormalizationLayerParams::clear_beta() {
  if (GetArenaForAllocation() == nullptr && beta_ != nullptr) {
    delete beta_;
  }
  beta_ = nullptr;
}
inline const ::CoreML::Specification::WeightParams& LayerNormalizationLayerParams::_internal_beta() const {
  const ::CoreML::Specification::WeightParams* p = beta_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::WeightParams&>(
      ::CoreML::Specification::_WeightParams_default_instance_);
}
inline const ::CoreML::Specification::WeightParams& LayerNormalizationLayerParams::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LayerNormalizationLayerParams.beta)
  return _internal_beta();
}
inline void LayerNormalizationLayerParams::unsafe_arena_set_allocated_beta(
    ::CoreML::Specification::WeightParams* beta) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(beta_);
  }
  beta_ = beta;
  if (beta) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LayerNormalizationLayerParams.beta)
}
inline ::CoreML::Specification::WeightParams* LayerNormalizationLayerParams::release_beta() {
  
  ::CoreML::Specification::WeightParams* temp = beta_;
  beta_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::WeightParams* LayerNormalizationLayerParams::unsafe_arena_release_beta() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LayerNormalizationLayerParams.beta)
  
  ::CoreML::Specification::WeightParams* temp = beta_;
  beta_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::WeightParams* LayerNormalizationLayerParams::_internal_mutable_beta() {
  
  if (beta_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::WeightParams>(GetArenaForAllocation());
    beta_ = p;
  }
  return beta_;
}
inline ::CoreML::Specification::WeightParams* LayerNormalizationLayerParams::mutable_beta() {
  ::CoreML::Specification::WeightParams* _msg = _internal_mutable_beta();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LayerNormalizationLayerParams.beta)
  return _msg;
}
inline void LayerNormalizationLayerParams::set_allocated_beta(::CoreML::Specification::WeightParams* beta) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete beta_;
  }
  if (beta) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::WeightParams>::GetOwningArena(beta);
    if (message_arena != submessage_arena) {
      beta = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, beta, submessage_arena);
    }
    
  } else {
    
  }
  beta_ = beta;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LayerNormalizationLayerParams.beta)
}

// -------------------------------------------------------------------

// NonMaximumSuppressionLayerParams

// float iouThreshold = 1;
inline void NonMaximumSuppressionLayerParams::clear_iouthreshold() {
  iouthreshold_ = 0;
}
inline float NonMaximumSuppressionLayerParams::_internal_iouthreshold() const {
  return iouthreshold_;
}
inline float NonMaximumSuppressionLayerParams::iouthreshold() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NonMaximumSuppressionLayerParams.iouThreshold)
  return _internal_iouthreshold();
}
inline void NonMaximumSuppressionLayerParams::_internal_set_iouthreshold(float value) {
  
  iouthreshold_ = value;
}
inline void NonMaximumSuppressionLayerParams::set_iouthreshold(float value) {
  _internal_set_iouthreshold(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NonMaximumSuppressionLayerParams.iouThreshold)
}

// float scoreThreshold = 2;
inline void NonMaximumSuppressionLayerParams::clear_scorethreshold() {
  scorethreshold_ = 0;
}
inline float NonMaximumSuppressionLayerParams::_internal_scorethreshold() const {
  return scorethreshold_;
}
inline float NonMaximumSuppressionLayerParams::scorethreshold() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NonMaximumSuppressionLayerParams.scoreThreshold)
  return _internal_scorethreshold();
}
inline void NonMaximumSuppressionLayerParams::_internal_set_scorethreshold(float value) {
  
  scorethreshold_ = value;
}
inline void NonMaximumSuppressionLayerParams::set_scorethreshold(float value) {
  _internal_set_scorethreshold(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NonMaximumSuppressionLayerParams.scoreThreshold)
}

// uint64 maxBoxes = 3;
inline void NonMaximumSuppressionLayerParams::clear_maxboxes() {
  maxboxes_ = uint64_t{0u};
}
inline uint64_t NonMaximumSuppressionLayerParams::_internal_maxboxes() const {
  return maxboxes_;
}
inline uint64_t NonMaximumSuppressionLayerParams::maxboxes() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NonMaximumSuppressionLayerParams.maxBoxes)
  return _internal_maxboxes();
}
inline void NonMaximumSuppressionLayerParams::_internal_set_maxboxes(uint64_t value) {
  
  maxboxes_ = value;
}
inline void NonMaximumSuppressionLayerParams::set_maxboxes(uint64_t value) {
  _internal_set_maxboxes(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NonMaximumSuppressionLayerParams.maxBoxes)
}

// bool perClassSuppression = 4;
inline void NonMaximumSuppressionLayerParams::clear_perclasssuppression() {
  perclasssuppression_ = false;
}
inline bool NonMaximumSuppressionLayerParams::_internal_perclasssuppression() const {
  return perclasssuppression_;
}
inline bool NonMaximumSuppressionLayerParams::perclasssuppression() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NonMaximumSuppressionLayerParams.perClassSuppression)
  return _internal_perclasssuppression();
}
inline void NonMaximumSuppressionLayerParams::_internal_set_perclasssuppression(bool value) {
  
  perclasssuppression_ = value;
}
inline void NonMaximumSuppressionLayerParams::set_perclasssuppression(bool value) {
  _internal_set_perclasssuppression(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NonMaximumSuppressionLayerParams.perClassSuppression)
}

// -------------------------------------------------------------------

// ClampedReLULayerParams

// float alpha = 1;
inline void ClampedReLULayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float ClampedReLULayerParams::_internal_alpha() const {
  return alpha_;
}
inline float ClampedReLULayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ClampedReLULayerParams.alpha)
  return _internal_alpha();
}
inline void ClampedReLULayerParams::_internal_set_alpha(float value) {
  
  alpha_ = value;
}
inline void ClampedReLULayerParams::set_alpha(float value) {
  _internal_set_alpha(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ClampedReLULayerParams.alpha)
}

// float beta = 2;
inline void ClampedReLULayerParams::clear_beta() {
  beta_ = 0;
}
inline float ClampedReLULayerParams::_internal_beta() const {
  return beta_;
}
inline float ClampedReLULayerParams::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ClampedReLULayerParams.beta)
  return _internal_beta();
}
inline void ClampedReLULayerParams::_internal_set_beta(float value) {
  
  beta_ = value;
}
inline void ClampedReLULayerParams::set_beta(float value) {
  _internal_set_beta(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ClampedReLULayerParams.beta)
}

// -------------------------------------------------------------------

// ArgSortLayerParams

// int64 axis = 1;
inline void ArgSortLayerParams::clear_axis() {
  axis_ = int64_t{0};
}
inline int64_t ArgSortLayerParams::_internal_axis() const {
  return axis_;
}
inline int64_t ArgSortLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ArgSortLayerParams.axis)
  return _internal_axis();
}
inline void ArgSortLayerParams::_internal_set_axis(int64_t value) {
  
  axis_ = value;
}
inline void ArgSortLayerParams::set_axis(int64_t value) {
  _internal_set_axis(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ArgSortLayerParams.axis)
}

// bool descending = 2;
inline void ArgSortLayerParams::clear_descending() {
  descending_ = false;
}
inline bool ArgSortLayerParams::_internal_descending() const {
  return descending_;
}
inline bool ArgSortLayerParams::descending() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ArgSortLayerParams.descending)
  return _internal_descending();
}
inline void ArgSortLayerParams::_internal_set_descending(bool value) {
  
  descending_ = value;
}
inline void ArgSortLayerParams::set_descending(bool value) {
  _internal_set_descending(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ArgSortLayerParams.descending)
}

// -------------------------------------------------------------------

// SliceBySizeLayerParams

// int64 size = 2;
inline void SliceBySizeLayerParams::clear_size() {
  size_ = int64_t{0};
}
inline int64_t SliceBySizeLayerParams::_internal_size() const {
  return size_;
}
inline int64_t SliceBySizeLayerParams::size() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceBySizeLayerParams.size)
  return _internal_size();
}
inline void SliceBySizeLayerParams::_internal_set_size(int64_t value) {
  
  size_ = value;
}
inline void SliceBySizeLayerParams::set_size(int64_t value) {
  _internal_set_size(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceBySizeLayerParams.size)
}

// int64 axis = 3;
inline void SliceBySizeLayerParams::clear_axis() {
  axis_ = int64_t{0};
}
inline int64_t SliceBySizeLayerParams::_internal_axis() const {
  return axis_;
}
inline int64_t SliceBySizeLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceBySizeLayerParams.axis)
  return _internal_axis();
}
inline void SliceBySizeLayerParams::_internal_set_axis(int64_t value) {
  
  axis_ = value;
}
inline void SliceBySizeLayerParams::set_axis(int64_t value) {
  _internal_set_axis(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceBySizeLayerParams.axis)
}

// -------------------------------------------------------------------

// NeuralNetworkClassifier

// repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
inline int NeuralNetworkClassifier::_internal_layers_size() const {
  return layers_.size();
}
inline int NeuralNetworkClassifier::layers_size() const {
  return _internal_layers_size();
}
inline void NeuralNetworkClassifier::clear_layers() {
  layers_.Clear();
}
inline ::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkClassifier::mutable_layers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
NeuralNetworkClassifier::mutable_layers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkClassifier.layers)
  return &layers_;
}
inline const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetworkClassifier::_internal_layers(int index) const {
  return layers_.Get(index);
}
inline const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetworkClassifier::layers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.layers)
  return _internal_layers(index);
}
inline ::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkClassifier::_internal_add_layers() {
  return layers_.Add();
}
inline ::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkClassifier::add_layers() {
  ::CoreML::Specification::NeuralNetworkLayer* _add = _internal_add_layers();
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkClassifier.layers)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
NeuralNetworkClassifier::layers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_;
}

// repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
inline int NeuralNetworkClassifier::_internal_preprocessing_size() const {
  return preprocessing_.size();
}
inline int NeuralNetworkClassifier::preprocessing_size() const {
  return _internal_preprocessing_size();
}
inline void NeuralNetworkClassifier::clear_preprocessing() {
  preprocessing_.Clear();
}
inline ::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkClassifier::mutable_preprocessing(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
NeuralNetworkClassifier::mutable_preprocessing() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return &preprocessing_;
}
inline const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetworkClassifier::_internal_preprocessing(int index) const {
  return preprocessing_.Get(index);
}
inline const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetworkClassifier::preprocessing(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return _internal_preprocessing(index);
}
inline ::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkClassifier::_internal_add_preprocessing() {
  return preprocessing_.Add();
}
inline ::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkClassifier::add_preprocessing() {
  ::CoreML::Specification::NeuralNetworkPreprocessing* _add = _internal_add_preprocessing();
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
NeuralNetworkClassifier::preprocessing() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_;
}

// .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
inline void NeuralNetworkClassifier::clear_arrayinputshapemapping() {
  arrayinputshapemapping_ = 0;
}
inline ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping NeuralNetworkClassifier::_internal_arrayinputshapemapping() const {
  return static_cast< ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping >(arrayinputshapemapping_);
}
inline ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping NeuralNetworkClassifier::arrayinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.arrayInputShapeMapping)
  return _internal_arrayinputshapemapping();
}
inline void NeuralNetworkClassifier::_internal_set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value) {
  
  arrayinputshapemapping_ = value;
}
inline void NeuralNetworkClassifier::set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value) {
  _internal_set_arrayinputshapemapping(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkClassifier.arrayInputShapeMapping)
}

// .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
inline void NeuralNetworkClassifier::clear_imageinputshapemapping() {
  imageinputshapemapping_ = 0;
}
inline ::CoreML::Specification::NeuralNetworkImageShapeMapping NeuralNetworkClassifier::_internal_imageinputshapemapping() const {
  return static_cast< ::CoreML::Specification::NeuralNetworkImageShapeMapping >(imageinputshapemapping_);
}
inline ::CoreML::Specification::NeuralNetworkImageShapeMapping NeuralNetworkClassifier::imageinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.imageInputShapeMapping)
  return _internal_imageinputshapemapping();
}
inline void NeuralNetworkClassifier::_internal_set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value) {
  
  imageinputshapemapping_ = value;
}
inline void NeuralNetworkClassifier::set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value) {
  _internal_set_imageinputshapemapping(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkClassifier.imageInputShapeMapping)
}

// .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
inline bool NeuralNetworkClassifier::_internal_has_updateparams() const {
  return this != internal_default_instance() && updateparams_ != nullptr;
}
inline bool NeuralNetworkClassifier::has_updateparams() const {
  return _internal_has_updateparams();
}
inline void NeuralNetworkClassifier::clear_updateparams() {
  if (GetArenaForAllocation() == nullptr && updateparams_ != nullptr) {
    delete updateparams_;
  }
  updateparams_ = nullptr;
}
inline const ::CoreML::Specification::NetworkUpdateParameters& NeuralNetworkClassifier::_internal_updateparams() const {
  const ::CoreML::Specification::NetworkUpdateParameters* p = updateparams_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::NetworkUpdateParameters&>(
      ::CoreML::Specification::_NetworkUpdateParameters_default_instance_);
}
inline const ::CoreML::Specification::NetworkUpdateParameters& NeuralNetworkClassifier::updateparams() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.updateParams)
  return _internal_updateparams();
}
inline void NeuralNetworkClassifier::unsafe_arena_set_allocated_updateparams(
    ::CoreML::Specification::NetworkUpdateParameters* updateparams) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(updateparams_);
  }
  updateparams_ = updateparams;
  if (updateparams) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkClassifier.updateParams)
}
inline ::CoreML::Specification::NetworkUpdateParameters* NeuralNetworkClassifier::release_updateparams() {
  
  ::CoreML::Specification::NetworkUpdateParameters* temp = updateparams_;
  updateparams_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::NetworkUpdateParameters* NeuralNetworkClassifier::unsafe_arena_release_updateparams() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.updateParams)
  
  ::CoreML::Specification::NetworkUpdateParameters* temp = updateparams_;
  updateparams_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::NetworkUpdateParameters* NeuralNetworkClassifier::_internal_mutable_updateparams() {
  
  if (updateparams_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::NetworkUpdateParameters>(GetArenaForAllocation());
    updateparams_ = p;
  }
  return updateparams_;
}
inline ::CoreML::Specification::NetworkUpdateParameters* NeuralNetworkClassifier::mutable_updateparams() {
  ::CoreML::Specification::NetworkUpdateParameters* _msg = _internal_mutable_updateparams();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.updateParams)
  return _msg;
}
inline void NeuralNetworkClassifier::set_allocated_updateparams(::CoreML::Specification::NetworkUpdateParameters* updateparams) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete updateparams_;
  }
  if (updateparams) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::NetworkUpdateParameters>::GetOwningArena(updateparams);
    if (message_arena != submessage_arena) {
      updateparams = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, updateparams, submessage_arena);
    }
    
  } else {
    
  }
  updateparams_ = updateparams;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.updateParams)
}

// .CoreML.Specification.StringVector stringClassLabels = 100;
inline bool NeuralNetworkClassifier::_internal_has_stringclasslabels() const {
  return ClassLabels_case() == kStringClassLabels;
}
inline bool NeuralNetworkClassifier::has_stringclasslabels() const {
  return _internal_has_stringclasslabels();
}
inline void NeuralNetworkClassifier::set_has_stringclasslabels() {
  _oneof_case_[0] = kStringClassLabels;
}
inline ::CoreML::Specification::StringVector* NeuralNetworkClassifier::release_stringclasslabels() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  if (_internal_has_stringclasslabels()) {
    clear_has_ClassLabels();
      ::CoreML::Specification::StringVector* temp = ClassLabels_.stringclasslabels_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    ClassLabels_.stringclasslabels_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::StringVector& NeuralNetworkClassifier::_internal_stringclasslabels() const {
  return _internal_has_stringclasslabels()
      ? *ClassLabels_.stringclasslabels_
      : reinterpret_cast< ::CoreML::Specification::StringVector&>(::CoreML::Specification::_StringVector_default_instance_);
}
inline const ::CoreML::Specification::StringVector& NeuralNetworkClassifier::stringclasslabels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  return _internal_stringclasslabels();
}
inline ::CoreML::Specification::StringVector* NeuralNetworkClassifier::unsafe_arena_release_stringclasslabels() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  if (_internal_has_stringclasslabels()) {
    clear_has_ClassLabels();
    ::CoreML::Specification::StringVector* temp = ClassLabels_.stringclasslabels_;
    ClassLabels_.stringclasslabels_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkClassifier::unsafe_arena_set_allocated_stringclasslabels(::CoreML::Specification::StringVector* stringclasslabels) {
  clear_ClassLabels();
  if (stringclasslabels) {
    set_has_stringclasslabels();
    ClassLabels_.stringclasslabels_ = stringclasslabels;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
}
inline ::CoreML::Specification::StringVector* NeuralNetworkClassifier::_internal_mutable_stringclasslabels() {
  if (!_internal_has_stringclasslabels()) {
    clear_ClassLabels();
    set_has_stringclasslabels();
    ClassLabels_.stringclasslabels_ = CreateMaybeMessage< ::CoreML::Specification::StringVector >(GetArenaForAllocation());
  }
  return ClassLabels_.stringclasslabels_;
}
inline ::CoreML::Specification::StringVector* NeuralNetworkClassifier::mutable_stringclasslabels() {
  ::CoreML::Specification::StringVector* _msg = _internal_mutable_stringclasslabels();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  return _msg;
}

// .CoreML.Specification.Int64Vector int64ClassLabels = 101;
inline bool NeuralNetworkClassifier::_internal_has_int64classlabels() const {
  return ClassLabels_case() == kInt64ClassLabels;
}
inline bool NeuralNetworkClassifier::has_int64classlabels() const {
  return _internal_has_int64classlabels();
}
inline void NeuralNetworkClassifier::set_has_int64classlabels() {
  _oneof_case_[0] = kInt64ClassLabels;
}
inline ::CoreML::Specification::Int64Vector* NeuralNetworkClassifier::release_int64classlabels() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  if (_internal_has_int64classlabels()) {
    clear_has_ClassLabels();
      ::CoreML::Specification::Int64Vector* temp = ClassLabels_.int64classlabels_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    ClassLabels_.int64classlabels_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::Int64Vector& NeuralNetworkClassifier::_internal_int64classlabels() const {
  return _internal_has_int64classlabels()
      ? *ClassLabels_.int64classlabels_
      : reinterpret_cast< ::CoreML::Specification::Int64Vector&>(::CoreML::Specification::_Int64Vector_default_instance_);
}
inline const ::CoreML::Specification::Int64Vector& NeuralNetworkClassifier::int64classlabels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  return _internal_int64classlabels();
}
inline ::CoreML::Specification::Int64Vector* NeuralNetworkClassifier::unsafe_arena_release_int64classlabels() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  if (_internal_has_int64classlabels()) {
    clear_has_ClassLabels();
    ::CoreML::Specification::Int64Vector* temp = ClassLabels_.int64classlabels_;
    ClassLabels_.int64classlabels_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void NeuralNetworkClassifier::unsafe_arena_set_allocated_int64classlabels(::CoreML::Specification::Int64Vector* int64classlabels) {
  clear_ClassLabels();
  if (int64classlabels) {
    set_has_int64classlabels();
    ClassLabels_.int64classlabels_ = int64classlabels;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
}
inline ::CoreML::Specification::Int64Vector* NeuralNetworkClassifier::_internal_mutable_int64classlabels() {
  if (!_internal_has_int64classlabels()) {
    clear_ClassLabels();
    set_has_int64classlabels();
    ClassLabels_.int64classlabels_ = CreateMaybeMessage< ::CoreML::Specification::Int64Vector >(GetArenaForAllocation());
  }
  return ClassLabels_.int64classlabels_;
}
inline ::CoreML::Specification::Int64Vector* NeuralNetworkClassifier::mutable_int64classlabels() {
  ::CoreML::Specification::Int64Vector* _msg = _internal_mutable_int64classlabels();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  return _msg;
}

// string labelProbabilityLayerName = 200;
inline void NeuralNetworkClassifier::clear_labelprobabilitylayername() {
  labelprobabilitylayername_.ClearToEmpty();
}
inline const std::string& NeuralNetworkClassifier::labelprobabilitylayername() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
  return _internal_labelprobabilitylayername();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void NeuralNetworkClassifier::set_labelprobabilitylayername(ArgT0&& arg0, ArgT... args) {
 
 labelprobabilitylayername_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
inline std::string* NeuralNetworkClassifier::mutable_labelprobabilitylayername() {
  std::string* _s = _internal_mutable_labelprobabilitylayername();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
  return _s;
}
inline const std::string& NeuralNetworkClassifier::_internal_labelprobabilitylayername() const {
  return labelprobabilitylayername_.Get();
}
inline void NeuralNetworkClassifier::_internal_set_labelprobabilitylayername(const std::string& value) {
  
  labelprobabilitylayername_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* NeuralNetworkClassifier::_internal_mutable_labelprobabilitylayername() {
  
  return labelprobabilitylayername_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* NeuralNetworkClassifier::release_labelprobabilitylayername() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
  return labelprobabilitylayername_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void NeuralNetworkClassifier::set_allocated_labelprobabilitylayername(std::string* labelprobabilitylayername) {
  if (labelprobabilitylayername != nullptr) {
    
  } else {
    
  }
  labelprobabilitylayername_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), labelprobabilitylayername,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (labelprobabilitylayername_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    labelprobabilitylayername_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}

inline bool NeuralNetworkClassifier::has_ClassLabels() const {
  return ClassLabels_case() != CLASSLABELS_NOT_SET;
}
inline void NeuralNetworkClassifier::clear_has_ClassLabels() {
  _oneof_case_[0] = CLASSLABELS_NOT_SET;
}
inline NeuralNetworkClassifier::ClassLabelsCase NeuralNetworkClassifier::ClassLabels_case() const {
  return NeuralNetworkClassifier::ClassLabelsCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// OneHotLayerParams

// uint64 oneHotVectorSize = 1;
inline void OneHotLayerParams::clear_onehotvectorsize() {
  onehotvectorsize_ = uint64_t{0u};
}
inline uint64_t OneHotLayerParams::_internal_onehotvectorsize() const {
  return onehotvectorsize_;
}
inline uint64_t OneHotLayerParams::onehotvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.OneHotLayerParams.oneHotVectorSize)
  return _internal_onehotvectorsize();
}
inline void OneHotLayerParams::_internal_set_onehotvectorsize(uint64_t value) {
  
  onehotvectorsize_ = value;
}
inline void OneHotLayerParams::set_onehotvectorsize(uint64_t value) {
  _internal_set_onehotvectorsize(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.OneHotLayerParams.oneHotVectorSize)
}

// int64 axis = 2;
inline void OneHotLayerParams::clear_axis() {
  axis_ = int64_t{0};
}
inline int64_t OneHotLayerParams::_internal_axis() const {
  return axis_;
}
inline int64_t OneHotLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.OneHotLayerParams.axis)
  return _internal_axis();
}
inline void OneHotLayerParams::_internal_set_axis(int64_t value) {
  
  axis_ = value;
}
inline void OneHotLayerParams::set_axis(int64_t value) {
  _internal_set_axis(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.OneHotLayerParams.axis)
}

// float onValue = 3;
inline void OneHotLayerParams::clear_onvalue() {
  onvalue_ = 0;
}
inline float OneHotLayerParams::_internal_onvalue() const {
  return onvalue_;
}
inline float OneHotLayerParams::onvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.OneHotLayerParams.onValue)
  return _internal_onvalue();
}
inline void OneHotLayerParams::_internal_set_onvalue(float value) {
  
  onvalue_ = value;
}
inline void OneHotLayerParams::set_onvalue(float value) {
  _internal_set_onvalue(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.OneHotLayerParams.onValue)
}

// float offValue = 4;
inline void OneHotLayerParams::clear_offvalue() {
  offvalue_ = 0;
}
inline float OneHotLayerParams::_internal_offvalue() const {
  return offvalue_;
}
inline float OneHotLayerParams::offvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.OneHotLayerParams.offValue)
  return _internal_offvalue();
}
inline void OneHotLayerParams::_internal_set_offvalue(float value) {
  
  offvalue_ = value;
}
inline void OneHotLayerParams::set_offvalue(float value) {
  _internal_set_offvalue(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.OneHotLayerParams.offValue)
}

// -------------------------------------------------------------------

// CumSumLayerParams

// int64 axis = 1;
inline void CumSumLayerParams::clear_axis() {
  axis_ = int64_t{0};
}
inline int64_t CumSumLayerParams::_internal_axis() const {
  return axis_;
}
inline int64_t CumSumLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CumSumLayerParams.axis)
  return _internal_axis();
}
inline void CumSumLayerParams::_internal_set_axis(int64_t value) {
  
  axis_ = value;
}
inline void CumSumLayerParams::set_axis(int64_t value) {
  _internal_set_axis(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CumSumLayerParams.axis)
}

// bool excludeFinalSum = 2;
inline void CumSumLayerParams::clear_excludefinalsum() {
  excludefinalsum_ = false;
}
inline bool CumSumLayerParams::_internal_excludefinalsum() const {
  return excludefinalsum_;
}
inline bool CumSumLayerParams::excludefinalsum() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CumSumLayerParams.excludeFinalSum)
  return _internal_excludefinalsum();
}
inline void CumSumLayerParams::_internal_set_excludefinalsum(bool value) {
  
  excludefinalsum_ = value;
}
inline void CumSumLayerParams::set_excludefinalsum(bool value) {
  _internal_set_excludefinalsum(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CumSumLayerParams.excludeFinalSum)
}

// bool reverse = 3;
inline void CumSumLayerParams::clear_reverse() {
  reverse_ = false;
}
inline bool CumSumLayerParams::_internal_reverse() const {
  return reverse_;
}
inline bool CumSumLayerParams::reverse() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CumSumLayerParams.reverse)
  return _internal_reverse();
}
inline void CumSumLayerParams::_internal_set_reverse(bool value) {
  
  reverse_ = value;
}
inline void CumSumLayerParams::set_reverse(bool value) {
  _internal_set_reverse(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CumSumLayerParams.reverse)
}

// -------------------------------------------------------------------

// NeuralNetworkRegressor

// repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
inline int NeuralNetworkRegressor::_internal_layers_size() const {
  return layers_.size();
}
inline int NeuralNetworkRegressor::layers_size() const {
  return _internal_layers_size();
}
inline void NeuralNetworkRegressor::clear_layers() {
  layers_.Clear();
}
inline ::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkRegressor::mutable_layers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
NeuralNetworkRegressor::mutable_layers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkRegressor.layers)
  return &layers_;
}
inline const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetworkRegressor::_internal_layers(int index) const {
  return layers_.Get(index);
}
inline const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetworkRegressor::layers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.layers)
  return _internal_layers(index);
}
inline ::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkRegressor::_internal_add_layers() {
  return layers_.Add();
}
inline ::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkRegressor::add_layers() {
  ::CoreML::Specification::NeuralNetworkLayer* _add = _internal_add_layers();
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkRegressor.layers)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
NeuralNetworkRegressor::layers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_;
}

// repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
inline int NeuralNetworkRegressor::_internal_preprocessing_size() const {
  return preprocessing_.size();
}
inline int NeuralNetworkRegressor::preprocessing_size() const {
  return _internal_preprocessing_size();
}
inline void NeuralNetworkRegressor::clear_preprocessing() {
  preprocessing_.Clear();
}
inline ::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkRegressor::mutable_preprocessing(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
NeuralNetworkRegressor::mutable_preprocessing() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return &preprocessing_;
}
inline const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetworkRegressor::_internal_preprocessing(int index) const {
  return preprocessing_.Get(index);
}
inline const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetworkRegressor::preprocessing(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return _internal_preprocessing(index);
}
inline ::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkRegressor::_internal_add_preprocessing() {
  return preprocessing_.Add();
}
inline ::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkRegressor::add_preprocessing() {
  ::CoreML::Specification::NeuralNetworkPreprocessing* _add = _internal_add_preprocessing();
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
NeuralNetworkRegressor::preprocessing() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_;
}

// .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
inline void NeuralNetworkRegressor::clear_arrayinputshapemapping() {
  arrayinputshapemapping_ = 0;
}
inline ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping NeuralNetworkRegressor::_internal_arrayinputshapemapping() const {
  return static_cast< ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping >(arrayinputshapemapping_);
}
inline ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping NeuralNetworkRegressor::arrayinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.arrayInputShapeMapping)
  return _internal_arrayinputshapemapping();
}
inline void NeuralNetworkRegressor::_internal_set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value) {
  
  arrayinputshapemapping_ = value;
}
inline void NeuralNetworkRegressor::set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value) {
  _internal_set_arrayinputshapemapping(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkRegressor.arrayInputShapeMapping)
}

// .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
inline void NeuralNetworkRegressor::clear_imageinputshapemapping() {
  imageinputshapemapping_ = 0;
}
inline ::CoreML::Specification::NeuralNetworkImageShapeMapping NeuralNetworkRegressor::_internal_imageinputshapemapping() const {
  return static_cast< ::CoreML::Specification::NeuralNetworkImageShapeMapping >(imageinputshapemapping_);
}
inline ::CoreML::Specification::NeuralNetworkImageShapeMapping NeuralNetworkRegressor::imageinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.imageInputShapeMapping)
  return _internal_imageinputshapemapping();
}
inline void NeuralNetworkRegressor::_internal_set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value) {
  
  imageinputshapemapping_ = value;
}
inline void NeuralNetworkRegressor::set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value) {
  _internal_set_imageinputshapemapping(value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkRegressor.imageInputShapeMapping)
}

// .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
inline bool NeuralNetworkRegressor::_internal_has_updateparams() const {
  return this != internal_default_instance() && updateparams_ != nullptr;
}
inline bool NeuralNetworkRegressor::has_updateparams() const {
  return _internal_has_updateparams();
}
inline void NeuralNetworkRegressor::clear_updateparams() {
  if (GetArenaForAllocation() == nullptr && updateparams_ != nullptr) {
    delete updateparams_;
  }
  updateparams_ = nullptr;
}
inline const ::CoreML::Specification::NetworkUpdateParameters& NeuralNetworkRegressor::_internal_updateparams() const {
  const ::CoreML::Specification::NetworkUpdateParameters* p = updateparams_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::NetworkUpdateParameters&>(
      ::CoreML::Specification::_NetworkUpdateParameters_default_instance_);
}
inline const ::CoreML::Specification::NetworkUpdateParameters& NeuralNetworkRegressor::updateparams() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.updateParams)
  return _internal_updateparams();
}
inline void NeuralNetworkRegressor::unsafe_arena_set_allocated_updateparams(
    ::CoreML::Specification::NetworkUpdateParameters* updateparams) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(updateparams_);
  }
  updateparams_ = updateparams;
  if (updateparams) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NeuralNetworkRegressor.updateParams)
}
inline ::CoreML::Specification::NetworkUpdateParameters* NeuralNetworkRegressor::release_updateparams() {
  
  ::CoreML::Specification::NetworkUpdateParameters* temp = updateparams_;
  updateparams_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::NetworkUpdateParameters* NeuralNetworkRegressor::unsafe_arena_release_updateparams() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkRegressor.updateParams)
  
  ::CoreML::Specification::NetworkUpdateParameters* temp = updateparams_;
  updateparams_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::NetworkUpdateParameters* NeuralNetworkRegressor::_internal_mutable_updateparams() {
  
  if (updateparams_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::NetworkUpdateParameters>(GetArenaForAllocation());
    updateparams_ = p;
  }
  return updateparams_;
}
inline ::CoreML::Specification::NetworkUpdateParameters* NeuralNetworkRegressor::mutable_updateparams() {
  ::CoreML::Specification::NetworkUpdateParameters* _msg = _internal_mutable_updateparams();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkRegressor.updateParams)
  return _msg;
}
inline void NeuralNetworkRegressor::set_allocated_updateparams(::CoreML::Specification::NetworkUpdateParameters* updateparams) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete updateparams_;
  }
  if (updateparams) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::NetworkUpdateParameters>::GetOwningArena(updateparams);
    if (message_arena != submessage_arena) {
      updateparams = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, updateparams, submessage_arena);
    }
    
  } else {
    
  }
  updateparams_ = updateparams;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkRegressor.updateParams)
}

// -------------------------------------------------------------------

// NetworkUpdateParameters

// repeated .CoreML.Specification.LossLayer lossLayers = 1;
inline int NetworkUpdateParameters::_internal_losslayers_size() const {
  return losslayers_.size();
}
inline int NetworkUpdateParameters::losslayers_size() const {
  return _internal_losslayers_size();
}
inline void NetworkUpdateParameters::clear_losslayers() {
  losslayers_.Clear();
}
inline ::CoreML::Specification::LossLayer* NetworkUpdateParameters::mutable_losslayers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NetworkUpdateParameters.lossLayers)
  return losslayers_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::LossLayer >*
NetworkUpdateParameters::mutable_losslayers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NetworkUpdateParameters.lossLayers)
  return &losslayers_;
}
inline const ::CoreML::Specification::LossLayer& NetworkUpdateParameters::_internal_losslayers(int index) const {
  return losslayers_.Get(index);
}
inline const ::CoreML::Specification::LossLayer& NetworkUpdateParameters::losslayers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NetworkUpdateParameters.lossLayers)
  return _internal_losslayers(index);
}
inline ::CoreML::Specification::LossLayer* NetworkUpdateParameters::_internal_add_losslayers() {
  return losslayers_.Add();
}
inline ::CoreML::Specification::LossLayer* NetworkUpdateParameters::add_losslayers() {
  ::CoreML::Specification::LossLayer* _add = _internal_add_losslayers();
  // @@protoc_insertion_point(field_add:CoreML.Specification.NetworkUpdateParameters.lossLayers)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::CoreML::Specification::LossLayer >&
NetworkUpdateParameters::losslayers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NetworkUpdateParameters.lossLayers)
  return losslayers_;
}

// .CoreML.Specification.Optimizer optimizer = 2;
inline bool NetworkUpdateParameters::_internal_has_optimizer() const {
  return this != internal_default_instance() && optimizer_ != nullptr;
}
inline bool NetworkUpdateParameters::has_optimizer() const {
  return _internal_has_optimizer();
}
inline void NetworkUpdateParameters::clear_optimizer() {
  if (GetArenaForAllocation() == nullptr && optimizer_ != nullptr) {
    delete optimizer_;
  }
  optimizer_ = nullptr;
}
inline const ::CoreML::Specification::Optimizer& NetworkUpdateParameters::_internal_optimizer() const {
  const ::CoreML::Specification::Optimizer* p = optimizer_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::Optimizer&>(
      ::CoreML::Specification::_Optimizer_default_instance_);
}
inline const ::CoreML::Specification::Optimizer& NetworkUpdateParameters::optimizer() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NetworkUpdateParameters.optimizer)
  return _internal_optimizer();
}
inline void NetworkUpdateParameters::unsafe_arena_set_allocated_optimizer(
    ::CoreML::Specification::Optimizer* optimizer) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(optimizer_);
  }
  optimizer_ = optimizer;
  if (optimizer) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NetworkUpdateParameters.optimizer)
}
inline ::CoreML::Specification::Optimizer* NetworkUpdateParameters::release_optimizer() {
  
  ::CoreML::Specification::Optimizer* temp = optimizer_;
  optimizer_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::Optimizer* NetworkUpdateParameters::unsafe_arena_release_optimizer() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NetworkUpdateParameters.optimizer)
  
  ::CoreML::Specification::Optimizer* temp = optimizer_;
  optimizer_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::Optimizer* NetworkUpdateParameters::_internal_mutable_optimizer() {
  
  if (optimizer_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::Optimizer>(GetArenaForAllocation());
    optimizer_ = p;
  }
  return optimizer_;
}
inline ::CoreML::Specification::Optimizer* NetworkUpdateParameters::mutable_optimizer() {
  ::CoreML::Specification::Optimizer* _msg = _internal_mutable_optimizer();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NetworkUpdateParameters.optimizer)
  return _msg;
}
inline void NetworkUpdateParameters::set_allocated_optimizer(::CoreML::Specification::Optimizer* optimizer) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete optimizer_;
  }
  if (optimizer) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::CoreML::Specification::Optimizer>::GetOwningArena(optimizer);
    if (message_arena != submessage_arena) {
      optimizer = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, optimizer, submessage_arena);
    }
    
  } else {
    
  }
  optimizer_ = optimizer;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NetworkUpdateParameters.optimizer)
}

// .CoreML.Specification.Int64Parameter epochs = 3;
inline bool NetworkUpdateParameters::_internal_has_epochs() const {
  return this != internal_default_instance() && epochs_ != nullptr;
}
inline bool NetworkUpdateParameters::has_epochs() const {
  return _internal_has_epochs();
}
inline const ::CoreML::Specification::Int64Parameter& NetworkUpdateParameters::_internal_epochs() const {
  const ::CoreML::Specification::Int64Parameter* p = epochs_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::Int64Parameter&>(
      ::CoreML::Specification::_Int64Parameter_default_instance_);
}
inline const ::CoreML::Specification::Int64Parameter& NetworkUpdateParameters::epochs() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NetworkUpdateParameters.epochs)
  return _internal_epochs();
}
inline void NetworkUpdateParameters::unsafe_arena_set_allocated_epochs(
    ::CoreML::Specification::Int64Parameter* epochs) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(epochs_);
  }
  epochs_ = epochs;
  if (epochs) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NetworkUpdateParameters.epochs)
}
inline ::CoreML::Specification::Int64Parameter* NetworkUpdateParameters::release_epochs() {
  
  ::CoreML::Specification::Int64Parameter* temp = epochs_;
  epochs_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::Int64Parameter* NetworkUpdateParameters::unsafe_arena_release_epochs() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NetworkUpdateParameters.epochs)
  
  ::CoreML::Specification::Int64Parameter* temp = epochs_;
  epochs_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::Int64Parameter* NetworkUpdateParameters::_internal_mutable_epochs() {
  
  if (epochs_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::Int64Parameter>(GetArenaForAllocation());
    epochs_ = p;
  }
  return epochs_;
}
inline ::CoreML::Specification::Int64Parameter* NetworkUpdateParameters::mutable_epochs() {
  ::CoreML::Specification::Int64Parameter* _msg = _internal_mutable_epochs();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NetworkUpdateParameters.epochs)
  return _msg;
}
inline void NetworkUpdateParameters::set_allocated_epochs(::CoreML::Specification::Int64Parameter* epochs) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(epochs_);
  }
  if (epochs) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<
            ::PROTOBUF_NAMESPACE_ID::MessageLite>::GetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(epochs));
    if (message_arena != submessage_arena) {
      epochs = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, epochs, submessage_arena);
    }
    
  } else {
    
  }
  epochs_ = epochs;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NetworkUpdateParameters.epochs)
}

// .CoreML.Specification.BoolParameter shuffle = 10;
inline bool NetworkUpdateParameters::_internal_has_shuffle() const {
  return this != internal_default_instance() && shuffle_ != nullptr;
}
inline bool NetworkUpdateParameters::has_shuffle() const {
  return _internal_has_shuffle();
}
inline const ::CoreML::Specification::BoolParameter& NetworkUpdateParameters::_internal_shuffle() const {
  const ::CoreML::Specification::BoolParameter* p = shuffle_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::BoolParameter&>(
      ::CoreML::Specification::_BoolParameter_default_instance_);
}
inline const ::CoreML::Specification::BoolParameter& NetworkUpdateParameters::shuffle() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NetworkUpdateParameters.shuffle)
  return _internal_shuffle();
}
inline void NetworkUpdateParameters::unsafe_arena_set_allocated_shuffle(
    ::CoreML::Specification::BoolParameter* shuffle) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(shuffle_);
  }
  shuffle_ = shuffle;
  if (shuffle) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NetworkUpdateParameters.shuffle)
}
inline ::CoreML::Specification::BoolParameter* NetworkUpdateParameters::release_shuffle() {
  
  ::CoreML::Specification::BoolParameter* temp = shuffle_;
  shuffle_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::BoolParameter* NetworkUpdateParameters::unsafe_arena_release_shuffle() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NetworkUpdateParameters.shuffle)
  
  ::CoreML::Specification::BoolParameter* temp = shuffle_;
  shuffle_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::BoolParameter* NetworkUpdateParameters::_internal_mutable_shuffle() {
  
  if (shuffle_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::BoolParameter>(GetArenaForAllocation());
    shuffle_ = p;
  }
  return shuffle_;
}
inline ::CoreML::Specification::BoolParameter* NetworkUpdateParameters::mutable_shuffle() {
  ::CoreML::Specification::BoolParameter* _msg = _internal_mutable_shuffle();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NetworkUpdateParameters.shuffle)
  return _msg;
}
inline void NetworkUpdateParameters::set_allocated_shuffle(::CoreML::Specification::BoolParameter* shuffle) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(shuffle_);
  }
  if (shuffle) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<
            ::PROTOBUF_NAMESPACE_ID::MessageLite>::GetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(shuffle));
    if (message_arena != submessage_arena) {
      shuffle = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, shuffle, submessage_arena);
    }
    
  } else {
    
  }
  shuffle_ = shuffle;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NetworkUpdateParameters.shuffle)
}

// .CoreML.Specification.Int64Parameter seed = 20;
inline bool NetworkUpdateParameters::_internal_has_seed() const {
  return this != internal_default_instance() && seed_ != nullptr;
}
inline bool NetworkUpdateParameters::has_seed() const {
  return _internal_has_seed();
}
inline const ::CoreML::Specification::Int64Parameter& NetworkUpdateParameters::_internal_seed() const {
  const ::CoreML::Specification::Int64Parameter* p = seed_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::Int64Parameter&>(
      ::CoreML::Specification::_Int64Parameter_default_instance_);
}
inline const ::CoreML::Specification::Int64Parameter& NetworkUpdateParameters::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NetworkUpdateParameters.seed)
  return _internal_seed();
}
inline void NetworkUpdateParameters::unsafe_arena_set_allocated_seed(
    ::CoreML::Specification::Int64Parameter* seed) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(seed_);
  }
  seed_ = seed;
  if (seed) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.NetworkUpdateParameters.seed)
}
inline ::CoreML::Specification::Int64Parameter* NetworkUpdateParameters::release_seed() {
  
  ::CoreML::Specification::Int64Parameter* temp = seed_;
  seed_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::Int64Parameter* NetworkUpdateParameters::unsafe_arena_release_seed() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NetworkUpdateParameters.seed)
  
  ::CoreML::Specification::Int64Parameter* temp = seed_;
  seed_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::Int64Parameter* NetworkUpdateParameters::_internal_mutable_seed() {
  
  if (seed_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::Int64Parameter>(GetArenaForAllocation());
    seed_ = p;
  }
  return seed_;
}
inline ::CoreML::Specification::Int64Parameter* NetworkUpdateParameters::mutable_seed() {
  ::CoreML::Specification::Int64Parameter* _msg = _internal_mutable_seed();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NetworkUpdateParameters.seed)
  return _msg;
}
inline void NetworkUpdateParameters::set_allocated_seed(::CoreML::Specification::Int64Parameter* seed) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(seed_);
  }
  if (seed) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<
            ::PROTOBUF_NAMESPACE_ID::MessageLite>::GetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(seed));
    if (message_arena != submessage_arena) {
      seed = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, seed, submessage_arena);
    }
    
  } else {
    
  }
  seed_ = seed;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NetworkUpdateParameters.seed)
}

// -------------------------------------------------------------------

// LossLayer

// string name = 1;
inline void LossLayer::clear_name() {
  name_.ClearToEmpty();
}
inline const std::string& LossLayer::name() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LossLayer.name)
  return _internal_name();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void LossLayer::set_name(ArgT0&& arg0, ArgT... args) {
 
 name_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:CoreML.Specification.LossLayer.name)
}
inline std::string* LossLayer::mutable_name() {
  std::string* _s = _internal_mutable_name();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LossLayer.name)
  return _s;
}
inline const std::string& LossLayer::_internal_name() const {
  return name_.Get();
}
inline void LossLayer::_internal_set_name(const std::string& value) {
  
  name_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* LossLayer::_internal_mutable_name() {
  
  return name_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* LossLayer::release_name() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LossLayer.name)
  return name_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void LossLayer::set_allocated_name(std::string* name) {
  if (name != nullptr) {
    
  } else {
    
  }
  name_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), name,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (name_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    name_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LossLayer.name)
}

// .CoreML.Specification.CategoricalCrossEntropyLossLayer categoricalCrossEntropyLossLayer = 10;
inline bool LossLayer::_internal_has_categoricalcrossentropylosslayer() const {
  return LossLayerType_case() == kCategoricalCrossEntropyLossLayer;
}
inline bool LossLayer::has_categoricalcrossentropylosslayer() const {
  return _internal_has_categoricalcrossentropylosslayer();
}
inline void LossLayer::set_has_categoricalcrossentropylosslayer() {
  _oneof_case_[0] = kCategoricalCrossEntropyLossLayer;
}
inline void LossLayer::clear_categoricalcrossentropylosslayer() {
  if (_internal_has_categoricalcrossentropylosslayer()) {
    if (GetArenaForAllocation() == nullptr) {
      delete LossLayerType_.categoricalcrossentropylosslayer_;
    }
    clear_has_LossLayerType();
  }
}
inline ::CoreML::Specification::CategoricalCrossEntropyLossLayer* LossLayer::release_categoricalcrossentropylosslayer() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LossLayer.categoricalCrossEntropyLossLayer)
  if (_internal_has_categoricalcrossentropylosslayer()) {
    clear_has_LossLayerType();
      ::CoreML::Specification::CategoricalCrossEntropyLossLayer* temp = LossLayerType_.categoricalcrossentropylosslayer_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    LossLayerType_.categoricalcrossentropylosslayer_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::CategoricalCrossEntropyLossLayer& LossLayer::_internal_categoricalcrossentropylosslayer() const {
  return _internal_has_categoricalcrossentropylosslayer()
      ? *LossLayerType_.categoricalcrossentropylosslayer_
      : reinterpret_cast< ::CoreML::Specification::CategoricalCrossEntropyLossLayer&>(::CoreML::Specification::_CategoricalCrossEntropyLossLayer_default_instance_);
}
inline const ::CoreML::Specification::CategoricalCrossEntropyLossLayer& LossLayer::categoricalcrossentropylosslayer() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LossLayer.categoricalCrossEntropyLossLayer)
  return _internal_categoricalcrossentropylosslayer();
}
inline ::CoreML::Specification::CategoricalCrossEntropyLossLayer* LossLayer::unsafe_arena_release_categoricalcrossentropylosslayer() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.LossLayer.categoricalCrossEntropyLossLayer)
  if (_internal_has_categoricalcrossentropylosslayer()) {
    clear_has_LossLayerType();
    ::CoreML::Specification::CategoricalCrossEntropyLossLayer* temp = LossLayerType_.categoricalcrossentropylosslayer_;
    LossLayerType_.categoricalcrossentropylosslayer_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void LossLayer::unsafe_arena_set_allocated_categoricalcrossentropylosslayer(::CoreML::Specification::CategoricalCrossEntropyLossLayer* categoricalcrossentropylosslayer) {
  clear_LossLayerType();
  if (categoricalcrossentropylosslayer) {
    set_has_categoricalcrossentropylosslayer();
    LossLayerType_.categoricalcrossentropylosslayer_ = categoricalcrossentropylosslayer;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LossLayer.categoricalCrossEntropyLossLayer)
}
inline ::CoreML::Specification::CategoricalCrossEntropyLossLayer* LossLayer::_internal_mutable_categoricalcrossentropylosslayer() {
  if (!_internal_has_categoricalcrossentropylosslayer()) {
    clear_LossLayerType();
    set_has_categoricalcrossentropylosslayer();
    LossLayerType_.categoricalcrossentropylosslayer_ = CreateMaybeMessage< ::CoreML::Specification::CategoricalCrossEntropyLossLayer >(GetArenaForAllocation());
  }
  return LossLayerType_.categoricalcrossentropylosslayer_;
}
inline ::CoreML::Specification::CategoricalCrossEntropyLossLayer* LossLayer::mutable_categoricalcrossentropylosslayer() {
  ::CoreML::Specification::CategoricalCrossEntropyLossLayer* _msg = _internal_mutable_categoricalcrossentropylosslayer();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LossLayer.categoricalCrossEntropyLossLayer)
  return _msg;
}

// .CoreML.Specification.MeanSquaredErrorLossLayer meanSquaredErrorLossLayer = 11;
inline bool LossLayer::_internal_has_meansquarederrorlosslayer() const {
  return LossLayerType_case() == kMeanSquaredErrorLossLayer;
}
inline bool LossLayer::has_meansquarederrorlosslayer() const {
  return _internal_has_meansquarederrorlosslayer();
}
inline void LossLayer::set_has_meansquarederrorlosslayer() {
  _oneof_case_[0] = kMeanSquaredErrorLossLayer;
}
inline void LossLayer::clear_meansquarederrorlosslayer() {
  if (_internal_has_meansquarederrorlosslayer()) {
    if (GetArenaForAllocation() == nullptr) {
      delete LossLayerType_.meansquarederrorlosslayer_;
    }
    clear_has_LossLayerType();
  }
}
inline ::CoreML::Specification::MeanSquaredErrorLossLayer* LossLayer::release_meansquarederrorlosslayer() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LossLayer.meanSquaredErrorLossLayer)
  if (_internal_has_meansquarederrorlosslayer()) {
    clear_has_LossLayerType();
      ::CoreML::Specification::MeanSquaredErrorLossLayer* temp = LossLayerType_.meansquarederrorlosslayer_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    LossLayerType_.meansquarederrorlosslayer_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::MeanSquaredErrorLossLayer& LossLayer::_internal_meansquarederrorlosslayer() const {
  return _internal_has_meansquarederrorlosslayer()
      ? *LossLayerType_.meansquarederrorlosslayer_
      : reinterpret_cast< ::CoreML::Specification::MeanSquaredErrorLossLayer&>(::CoreML::Specification::_MeanSquaredErrorLossLayer_default_instance_);
}
inline const ::CoreML::Specification::MeanSquaredErrorLossLayer& LossLayer::meansquarederrorlosslayer() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LossLayer.meanSquaredErrorLossLayer)
  return _internal_meansquarederrorlosslayer();
}
inline ::CoreML::Specification::MeanSquaredErrorLossLayer* LossLayer::unsafe_arena_release_meansquarederrorlosslayer() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.LossLayer.meanSquaredErrorLossLayer)
  if (_internal_has_meansquarederrorlosslayer()) {
    clear_has_LossLayerType();
    ::CoreML::Specification::MeanSquaredErrorLossLayer* temp = LossLayerType_.meansquarederrorlosslayer_;
    LossLayerType_.meansquarederrorlosslayer_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void LossLayer::unsafe_arena_set_allocated_meansquarederrorlosslayer(::CoreML::Specification::MeanSquaredErrorLossLayer* meansquarederrorlosslayer) {
  clear_LossLayerType();
  if (meansquarederrorlosslayer) {
    set_has_meansquarederrorlosslayer();
    LossLayerType_.meansquarederrorlosslayer_ = meansquarederrorlosslayer;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.LossLayer.meanSquaredErrorLossLayer)
}
inline ::CoreML::Specification::MeanSquaredErrorLossLayer* LossLayer::_internal_mutable_meansquarederrorlosslayer() {
  if (!_internal_has_meansquarederrorlosslayer()) {
    clear_LossLayerType();
    set_has_meansquarederrorlosslayer();
    LossLayerType_.meansquarederrorlosslayer_ = CreateMaybeMessage< ::CoreML::Specification::MeanSquaredErrorLossLayer >(GetArenaForAllocation());
  }
  return LossLayerType_.meansquarederrorlosslayer_;
}
inline ::CoreML::Specification::MeanSquaredErrorLossLayer* LossLayer::mutable_meansquarederrorlosslayer() {
  ::CoreML::Specification::MeanSquaredErrorLossLayer* _msg = _internal_mutable_meansquarederrorlosslayer();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LossLayer.meanSquaredErrorLossLayer)
  return _msg;
}

inline bool LossLayer::has_LossLayerType() const {
  return LossLayerType_case() != LOSSLAYERTYPE_NOT_SET;
}
inline void LossLayer::clear_has_LossLayerType() {
  _oneof_case_[0] = LOSSLAYERTYPE_NOT_SET;
}
inline LossLayer::LossLayerTypeCase LossLayer::LossLayerType_case() const {
  return LossLayer::LossLayerTypeCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// CategoricalCrossEntropyLossLayer

// string input = 1;
inline void CategoricalCrossEntropyLossLayer::clear_input() {
  input_.ClearToEmpty();
}
inline const std::string& CategoricalCrossEntropyLossLayer::input() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CategoricalCrossEntropyLossLayer.input)
  return _internal_input();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void CategoricalCrossEntropyLossLayer::set_input(ArgT0&& arg0, ArgT... args) {
 
 input_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:CoreML.Specification.CategoricalCrossEntropyLossLayer.input)
}
inline std::string* CategoricalCrossEntropyLossLayer::mutable_input() {
  std::string* _s = _internal_mutable_input();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CategoricalCrossEntropyLossLayer.input)
  return _s;
}
inline const std::string& CategoricalCrossEntropyLossLayer::_internal_input() const {
  return input_.Get();
}
inline void CategoricalCrossEntropyLossLayer::_internal_set_input(const std::string& value) {
  
  input_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* CategoricalCrossEntropyLossLayer::_internal_mutable_input() {
  
  return input_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* CategoricalCrossEntropyLossLayer::release_input() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CategoricalCrossEntropyLossLayer.input)
  return input_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void CategoricalCrossEntropyLossLayer::set_allocated_input(std::string* input) {
  if (input != nullptr) {
    
  } else {
    
  }
  input_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), input,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (input_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    input_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CategoricalCrossEntropyLossLayer.input)
}

// string target = 2;
inline void CategoricalCrossEntropyLossLayer::clear_target() {
  target_.ClearToEmpty();
}
inline const std::string& CategoricalCrossEntropyLossLayer::target() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CategoricalCrossEntropyLossLayer.target)
  return _internal_target();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void CategoricalCrossEntropyLossLayer::set_target(ArgT0&& arg0, ArgT... args) {
 
 target_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:CoreML.Specification.CategoricalCrossEntropyLossLayer.target)
}
inline std::string* CategoricalCrossEntropyLossLayer::mutable_target() {
  std::string* _s = _internal_mutable_target();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CategoricalCrossEntropyLossLayer.target)
  return _s;
}
inline const std::string& CategoricalCrossEntropyLossLayer::_internal_target() const {
  return target_.Get();
}
inline void CategoricalCrossEntropyLossLayer::_internal_set_target(const std::string& value) {
  
  target_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* CategoricalCrossEntropyLossLayer::_internal_mutable_target() {
  
  return target_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* CategoricalCrossEntropyLossLayer::release_target() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CategoricalCrossEntropyLossLayer.target)
  return target_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void CategoricalCrossEntropyLossLayer::set_allocated_target(std::string* target) {
  if (target != nullptr) {
    
  } else {
    
  }
  target_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), target,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (target_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    target_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CategoricalCrossEntropyLossLayer.target)
}

// -------------------------------------------------------------------

// MeanSquaredErrorLossLayer

// string input = 1;
inline void MeanSquaredErrorLossLayer::clear_input() {
  input_.ClearToEmpty();
}
inline const std::string& MeanSquaredErrorLossLayer::input() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanSquaredErrorLossLayer.input)
  return _internal_input();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void MeanSquaredErrorLossLayer::set_input(ArgT0&& arg0, ArgT... args) {
 
 input_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanSquaredErrorLossLayer.input)
}
inline std::string* MeanSquaredErrorLossLayer::mutable_input() {
  std::string* _s = _internal_mutable_input();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.MeanSquaredErrorLossLayer.input)
  return _s;
}
inline const std::string& MeanSquaredErrorLossLayer::_internal_input() const {
  return input_.Get();
}
inline void MeanSquaredErrorLossLayer::_internal_set_input(const std::string& value) {
  
  input_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* MeanSquaredErrorLossLayer::_internal_mutable_input() {
  
  return input_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* MeanSquaredErrorLossLayer::release_input() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.MeanSquaredErrorLossLayer.input)
  return input_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void MeanSquaredErrorLossLayer::set_allocated_input(std::string* input) {
  if (input != nullptr) {
    
  } else {
    
  }
  input_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), input,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (input_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    input_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.MeanSquaredErrorLossLayer.input)
}

// string target = 2;
inline void MeanSquaredErrorLossLayer::clear_target() {
  target_.ClearToEmpty();
}
inline const std::string& MeanSquaredErrorLossLayer::target() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanSquaredErrorLossLayer.target)
  return _internal_target();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void MeanSquaredErrorLossLayer::set_target(ArgT0&& arg0, ArgT... args) {
 
 target_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanSquaredErrorLossLayer.target)
}
inline std::string* MeanSquaredErrorLossLayer::mutable_target() {
  std::string* _s = _internal_mutable_target();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.MeanSquaredErrorLossLayer.target)
  return _s;
}
inline const std::string& MeanSquaredErrorLossLayer::_internal_target() const {
  return target_.Get();
}
inline void MeanSquaredErrorLossLayer::_internal_set_target(const std::string& value) {
  
  target_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* MeanSquaredErrorLossLayer::_internal_mutable_target() {
  
  return target_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* MeanSquaredErrorLossLayer::release_target() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.MeanSquaredErrorLossLayer.target)
  return target_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void MeanSquaredErrorLossLayer::set_allocated_target(std::string* target) {
  if (target != nullptr) {
    
  } else {
    
  }
  target_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), target,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (target_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    target_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.MeanSquaredErrorLossLayer.target)
}

// -------------------------------------------------------------------

// Optimizer

// .CoreML.Specification.SGDOptimizer sgdOptimizer = 10;
inline bool Optimizer::_internal_has_sgdoptimizer() const {
  return OptimizerType_case() == kSgdOptimizer;
}
inline bool Optimizer::has_sgdoptimizer() const {
  return _internal_has_sgdoptimizer();
}
inline void Optimizer::set_has_sgdoptimizer() {
  _oneof_case_[0] = kSgdOptimizer;
}
inline void Optimizer::clear_sgdoptimizer() {
  if (_internal_has_sgdoptimizer()) {
    if (GetArenaForAllocation() == nullptr) {
      delete OptimizerType_.sgdoptimizer_;
    }
    clear_has_OptimizerType();
  }
}
inline ::CoreML::Specification::SGDOptimizer* Optimizer::release_sgdoptimizer() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.Optimizer.sgdOptimizer)
  if (_internal_has_sgdoptimizer()) {
    clear_has_OptimizerType();
      ::CoreML::Specification::SGDOptimizer* temp = OptimizerType_.sgdoptimizer_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    OptimizerType_.sgdoptimizer_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::SGDOptimizer& Optimizer::_internal_sgdoptimizer() const {
  return _internal_has_sgdoptimizer()
      ? *OptimizerType_.sgdoptimizer_
      : reinterpret_cast< ::CoreML::Specification::SGDOptimizer&>(::CoreML::Specification::_SGDOptimizer_default_instance_);
}
inline const ::CoreML::Specification::SGDOptimizer& Optimizer::sgdoptimizer() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Optimizer.sgdOptimizer)
  return _internal_sgdoptimizer();
}
inline ::CoreML::Specification::SGDOptimizer* Optimizer::unsafe_arena_release_sgdoptimizer() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.Optimizer.sgdOptimizer)
  if (_internal_has_sgdoptimizer()) {
    clear_has_OptimizerType();
    ::CoreML::Specification::SGDOptimizer* temp = OptimizerType_.sgdoptimizer_;
    OptimizerType_.sgdoptimizer_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void Optimizer::unsafe_arena_set_allocated_sgdoptimizer(::CoreML::Specification::SGDOptimizer* sgdoptimizer) {
  clear_OptimizerType();
  if (sgdoptimizer) {
    set_has_sgdoptimizer();
    OptimizerType_.sgdoptimizer_ = sgdoptimizer;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.Optimizer.sgdOptimizer)
}
inline ::CoreML::Specification::SGDOptimizer* Optimizer::_internal_mutable_sgdoptimizer() {
  if (!_internal_has_sgdoptimizer()) {
    clear_OptimizerType();
    set_has_sgdoptimizer();
    OptimizerType_.sgdoptimizer_ = CreateMaybeMessage< ::CoreML::Specification::SGDOptimizer >(GetArenaForAllocation());
  }
  return OptimizerType_.sgdoptimizer_;
}
inline ::CoreML::Specification::SGDOptimizer* Optimizer::mutable_sgdoptimizer() {
  ::CoreML::Specification::SGDOptimizer* _msg = _internal_mutable_sgdoptimizer();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.Optimizer.sgdOptimizer)
  return _msg;
}

// .CoreML.Specification.AdamOptimizer adamOptimizer = 11;
inline bool Optimizer::_internal_has_adamoptimizer() const {
  return OptimizerType_case() == kAdamOptimizer;
}
inline bool Optimizer::has_adamoptimizer() const {
  return _internal_has_adamoptimizer();
}
inline void Optimizer::set_has_adamoptimizer() {
  _oneof_case_[0] = kAdamOptimizer;
}
inline void Optimizer::clear_adamoptimizer() {
  if (_internal_has_adamoptimizer()) {
    if (GetArenaForAllocation() == nullptr) {
      delete OptimizerType_.adamoptimizer_;
    }
    clear_has_OptimizerType();
  }
}
inline ::CoreML::Specification::AdamOptimizer* Optimizer::release_adamoptimizer() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.Optimizer.adamOptimizer)
  if (_internal_has_adamoptimizer()) {
    clear_has_OptimizerType();
      ::CoreML::Specification::AdamOptimizer* temp = OptimizerType_.adamoptimizer_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    OptimizerType_.adamoptimizer_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::CoreML::Specification::AdamOptimizer& Optimizer::_internal_adamoptimizer() const {
  return _internal_has_adamoptimizer()
      ? *OptimizerType_.adamoptimizer_
      : reinterpret_cast< ::CoreML::Specification::AdamOptimizer&>(::CoreML::Specification::_AdamOptimizer_default_instance_);
}
inline const ::CoreML::Specification::AdamOptimizer& Optimizer::adamoptimizer() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Optimizer.adamOptimizer)
  return _internal_adamoptimizer();
}
inline ::CoreML::Specification::AdamOptimizer* Optimizer::unsafe_arena_release_adamoptimizer() {
  // @@protoc_insertion_point(field_unsafe_arena_release:CoreML.Specification.Optimizer.adamOptimizer)
  if (_internal_has_adamoptimizer()) {
    clear_has_OptimizerType();
    ::CoreML::Specification::AdamOptimizer* temp = OptimizerType_.adamoptimizer_;
    OptimizerType_.adamoptimizer_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void Optimizer::unsafe_arena_set_allocated_adamoptimizer(::CoreML::Specification::AdamOptimizer* adamoptimizer) {
  clear_OptimizerType();
  if (adamoptimizer) {
    set_has_adamoptimizer();
    OptimizerType_.adamoptimizer_ = adamoptimizer;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.Optimizer.adamOptimizer)
}
inline ::CoreML::Specification::AdamOptimizer* Optimizer::_internal_mutable_adamoptimizer() {
  if (!_internal_has_adamoptimizer()) {
    clear_OptimizerType();
    set_has_adamoptimizer();
    OptimizerType_.adamoptimizer_ = CreateMaybeMessage< ::CoreML::Specification::AdamOptimizer >(GetArenaForAllocation());
  }
  return OptimizerType_.adamoptimizer_;
}
inline ::CoreML::Specification::AdamOptimizer* Optimizer::mutable_adamoptimizer() {
  ::CoreML::Specification::AdamOptimizer* _msg = _internal_mutable_adamoptimizer();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.Optimizer.adamOptimizer)
  return _msg;
}

inline bool Optimizer::has_OptimizerType() const {
  return OptimizerType_case() != OPTIMIZERTYPE_NOT_SET;
}
inline void Optimizer::clear_has_OptimizerType() {
  _oneof_case_[0] = OPTIMIZERTYPE_NOT_SET;
}
inline Optimizer::OptimizerTypeCase Optimizer::OptimizerType_case() const {
  return Optimizer::OptimizerTypeCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// SGDOptimizer

// .CoreML.Specification.DoubleParameter learningRate = 1;
inline bool SGDOptimizer::_internal_has_learningrate() const {
  return this != internal_default_instance() && learningrate_ != nullptr;
}
inline bool SGDOptimizer::has_learningrate() const {
  return _internal_has_learningrate();
}
inline const ::CoreML::Specification::DoubleParameter& SGDOptimizer::_internal_learningrate() const {
  const ::CoreML::Specification::DoubleParameter* p = learningrate_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::DoubleParameter&>(
      ::CoreML::Specification::_DoubleParameter_default_instance_);
}
inline const ::CoreML::Specification::DoubleParameter& SGDOptimizer::learningrate() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SGDOptimizer.learningRate)
  return _internal_learningrate();
}
inline void SGDOptimizer::unsafe_arena_set_allocated_learningrate(
    ::CoreML::Specification::DoubleParameter* learningrate) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(learningrate_);
  }
  learningrate_ = learningrate;
  if (learningrate) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.SGDOptimizer.learningRate)
}
inline ::CoreML::Specification::DoubleParameter* SGDOptimizer::release_learningrate() {
  
  ::CoreML::Specification::DoubleParameter* temp = learningrate_;
  learningrate_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::DoubleParameter* SGDOptimizer::unsafe_arena_release_learningrate() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SGDOptimizer.learningRate)
  
  ::CoreML::Specification::DoubleParameter* temp = learningrate_;
  learningrate_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::DoubleParameter* SGDOptimizer::_internal_mutable_learningrate() {
  
  if (learningrate_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::DoubleParameter>(GetArenaForAllocation());
    learningrate_ = p;
  }
  return learningrate_;
}
inline ::CoreML::Specification::DoubleParameter* SGDOptimizer::mutable_learningrate() {
  ::CoreML::Specification::DoubleParameter* _msg = _internal_mutable_learningrate();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SGDOptimizer.learningRate)
  return _msg;
}
inline void SGDOptimizer::set_allocated_learningrate(::CoreML::Specification::DoubleParameter* learningrate) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(learningrate_);
  }
  if (learningrate) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<
            ::PROTOBUF_NAMESPACE_ID::MessageLite>::GetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(learningrate));
    if (message_arena != submessage_arena) {
      learningrate = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, learningrate, submessage_arena);
    }
    
  } else {
    
  }
  learningrate_ = learningrate;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SGDOptimizer.learningRate)
}

// .CoreML.Specification.Int64Parameter miniBatchSize = 2;
inline bool SGDOptimizer::_internal_has_minibatchsize() const {
  return this != internal_default_instance() && minibatchsize_ != nullptr;
}
inline bool SGDOptimizer::has_minibatchsize() const {
  return _internal_has_minibatchsize();
}
inline const ::CoreML::Specification::Int64Parameter& SGDOptimizer::_internal_minibatchsize() const {
  const ::CoreML::Specification::Int64Parameter* p = minibatchsize_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::Int64Parameter&>(
      ::CoreML::Specification::_Int64Parameter_default_instance_);
}
inline const ::CoreML::Specification::Int64Parameter& SGDOptimizer::minibatchsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SGDOptimizer.miniBatchSize)
  return _internal_minibatchsize();
}
inline void SGDOptimizer::unsafe_arena_set_allocated_minibatchsize(
    ::CoreML::Specification::Int64Parameter* minibatchsize) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(minibatchsize_);
  }
  minibatchsize_ = minibatchsize;
  if (minibatchsize) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.SGDOptimizer.miniBatchSize)
}
inline ::CoreML::Specification::Int64Parameter* SGDOptimizer::release_minibatchsize() {
  
  ::CoreML::Specification::Int64Parameter* temp = minibatchsize_;
  minibatchsize_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::Int64Parameter* SGDOptimizer::unsafe_arena_release_minibatchsize() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SGDOptimizer.miniBatchSize)
  
  ::CoreML::Specification::Int64Parameter* temp = minibatchsize_;
  minibatchsize_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::Int64Parameter* SGDOptimizer::_internal_mutable_minibatchsize() {
  
  if (minibatchsize_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::Int64Parameter>(GetArenaForAllocation());
    minibatchsize_ = p;
  }
  return minibatchsize_;
}
inline ::CoreML::Specification::Int64Parameter* SGDOptimizer::mutable_minibatchsize() {
  ::CoreML::Specification::Int64Parameter* _msg = _internal_mutable_minibatchsize();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SGDOptimizer.miniBatchSize)
  return _msg;
}
inline void SGDOptimizer::set_allocated_minibatchsize(::CoreML::Specification::Int64Parameter* minibatchsize) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(minibatchsize_);
  }
  if (minibatchsize) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<
            ::PROTOBUF_NAMESPACE_ID::MessageLite>::GetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(minibatchsize));
    if (message_arena != submessage_arena) {
      minibatchsize = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, minibatchsize, submessage_arena);
    }
    
  } else {
    
  }
  minibatchsize_ = minibatchsize;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SGDOptimizer.miniBatchSize)
}

// .CoreML.Specification.DoubleParameter momentum = 3;
inline bool SGDOptimizer::_internal_has_momentum() const {
  return this != internal_default_instance() && momentum_ != nullptr;
}
inline bool SGDOptimizer::has_momentum() const {
  return _internal_has_momentum();
}
inline const ::CoreML::Specification::DoubleParameter& SGDOptimizer::_internal_momentum() const {
  const ::CoreML::Specification::DoubleParameter* p = momentum_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::DoubleParameter&>(
      ::CoreML::Specification::_DoubleParameter_default_instance_);
}
inline const ::CoreML::Specification::DoubleParameter& SGDOptimizer::momentum() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SGDOptimizer.momentum)
  return _internal_momentum();
}
inline void SGDOptimizer::unsafe_arena_set_allocated_momentum(
    ::CoreML::Specification::DoubleParameter* momentum) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(momentum_);
  }
  momentum_ = momentum;
  if (momentum) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.SGDOptimizer.momentum)
}
inline ::CoreML::Specification::DoubleParameter* SGDOptimizer::release_momentum() {
  
  ::CoreML::Specification::DoubleParameter* temp = momentum_;
  momentum_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::DoubleParameter* SGDOptimizer::unsafe_arena_release_momentum() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SGDOptimizer.momentum)
  
  ::CoreML::Specification::DoubleParameter* temp = momentum_;
  momentum_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::DoubleParameter* SGDOptimizer::_internal_mutable_momentum() {
  
  if (momentum_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::DoubleParameter>(GetArenaForAllocation());
    momentum_ = p;
  }
  return momentum_;
}
inline ::CoreML::Specification::DoubleParameter* SGDOptimizer::mutable_momentum() {
  ::CoreML::Specification::DoubleParameter* _msg = _internal_mutable_momentum();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SGDOptimizer.momentum)
  return _msg;
}
inline void SGDOptimizer::set_allocated_momentum(::CoreML::Specification::DoubleParameter* momentum) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(momentum_);
  }
  if (momentum) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<
            ::PROTOBUF_NAMESPACE_ID::MessageLite>::GetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(momentum));
    if (message_arena != submessage_arena) {
      momentum = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, momentum, submessage_arena);
    }
    
  } else {
    
  }
  momentum_ = momentum;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SGDOptimizer.momentum)
}

// -------------------------------------------------------------------

// AdamOptimizer

// .CoreML.Specification.DoubleParameter learningRate = 1;
inline bool AdamOptimizer::_internal_has_learningrate() const {
  return this != internal_default_instance() && learningrate_ != nullptr;
}
inline bool AdamOptimizer::has_learningrate() const {
  return _internal_has_learningrate();
}
inline const ::CoreML::Specification::DoubleParameter& AdamOptimizer::_internal_learningrate() const {
  const ::CoreML::Specification::DoubleParameter* p = learningrate_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::DoubleParameter&>(
      ::CoreML::Specification::_DoubleParameter_default_instance_);
}
inline const ::CoreML::Specification::DoubleParameter& AdamOptimizer::learningrate() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AdamOptimizer.learningRate)
  return _internal_learningrate();
}
inline void AdamOptimizer::unsafe_arena_set_allocated_learningrate(
    ::CoreML::Specification::DoubleParameter* learningrate) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(learningrate_);
  }
  learningrate_ = learningrate;
  if (learningrate) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.AdamOptimizer.learningRate)
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::release_learningrate() {
  
  ::CoreML::Specification::DoubleParameter* temp = learningrate_;
  learningrate_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::unsafe_arena_release_learningrate() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.AdamOptimizer.learningRate)
  
  ::CoreML::Specification::DoubleParameter* temp = learningrate_;
  learningrate_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::_internal_mutable_learningrate() {
  
  if (learningrate_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::DoubleParameter>(GetArenaForAllocation());
    learningrate_ = p;
  }
  return learningrate_;
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::mutable_learningrate() {
  ::CoreML::Specification::DoubleParameter* _msg = _internal_mutable_learningrate();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.AdamOptimizer.learningRate)
  return _msg;
}
inline void AdamOptimizer::set_allocated_learningrate(::CoreML::Specification::DoubleParameter* learningrate) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(learningrate_);
  }
  if (learningrate) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<
            ::PROTOBUF_NAMESPACE_ID::MessageLite>::GetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(learningrate));
    if (message_arena != submessage_arena) {
      learningrate = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, learningrate, submessage_arena);
    }
    
  } else {
    
  }
  learningrate_ = learningrate;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.AdamOptimizer.learningRate)
}

// .CoreML.Specification.Int64Parameter miniBatchSize = 2;
inline bool AdamOptimizer::_internal_has_minibatchsize() const {
  return this != internal_default_instance() && minibatchsize_ != nullptr;
}
inline bool AdamOptimizer::has_minibatchsize() const {
  return _internal_has_minibatchsize();
}
inline const ::CoreML::Specification::Int64Parameter& AdamOptimizer::_internal_minibatchsize() const {
  const ::CoreML::Specification::Int64Parameter* p = minibatchsize_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::Int64Parameter&>(
      ::CoreML::Specification::_Int64Parameter_default_instance_);
}
inline const ::CoreML::Specification::Int64Parameter& AdamOptimizer::minibatchsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AdamOptimizer.miniBatchSize)
  return _internal_minibatchsize();
}
inline void AdamOptimizer::unsafe_arena_set_allocated_minibatchsize(
    ::CoreML::Specification::Int64Parameter* minibatchsize) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(minibatchsize_);
  }
  minibatchsize_ = minibatchsize;
  if (minibatchsize) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.AdamOptimizer.miniBatchSize)
}
inline ::CoreML::Specification::Int64Parameter* AdamOptimizer::release_minibatchsize() {
  
  ::CoreML::Specification::Int64Parameter* temp = minibatchsize_;
  minibatchsize_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::Int64Parameter* AdamOptimizer::unsafe_arena_release_minibatchsize() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.AdamOptimizer.miniBatchSize)
  
  ::CoreML::Specification::Int64Parameter* temp = minibatchsize_;
  minibatchsize_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::Int64Parameter* AdamOptimizer::_internal_mutable_minibatchsize() {
  
  if (minibatchsize_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::Int64Parameter>(GetArenaForAllocation());
    minibatchsize_ = p;
  }
  return minibatchsize_;
}
inline ::CoreML::Specification::Int64Parameter* AdamOptimizer::mutable_minibatchsize() {
  ::CoreML::Specification::Int64Parameter* _msg = _internal_mutable_minibatchsize();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.AdamOptimizer.miniBatchSize)
  return _msg;
}
inline void AdamOptimizer::set_allocated_minibatchsize(::CoreML::Specification::Int64Parameter* minibatchsize) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(minibatchsize_);
  }
  if (minibatchsize) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<
            ::PROTOBUF_NAMESPACE_ID::MessageLite>::GetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(minibatchsize));
    if (message_arena != submessage_arena) {
      minibatchsize = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, minibatchsize, submessage_arena);
    }
    
  } else {
    
  }
  minibatchsize_ = minibatchsize;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.AdamOptimizer.miniBatchSize)
}

// .CoreML.Specification.DoubleParameter beta1 = 3;
inline bool AdamOptimizer::_internal_has_beta1() const {
  return this != internal_default_instance() && beta1_ != nullptr;
}
inline bool AdamOptimizer::has_beta1() const {
  return _internal_has_beta1();
}
inline const ::CoreML::Specification::DoubleParameter& AdamOptimizer::_internal_beta1() const {
  const ::CoreML::Specification::DoubleParameter* p = beta1_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::DoubleParameter&>(
      ::CoreML::Specification::_DoubleParameter_default_instance_);
}
inline const ::CoreML::Specification::DoubleParameter& AdamOptimizer::beta1() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AdamOptimizer.beta1)
  return _internal_beta1();
}
inline void AdamOptimizer::unsafe_arena_set_allocated_beta1(
    ::CoreML::Specification::DoubleParameter* beta1) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(beta1_);
  }
  beta1_ = beta1;
  if (beta1) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.AdamOptimizer.beta1)
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::release_beta1() {
  
  ::CoreML::Specification::DoubleParameter* temp = beta1_;
  beta1_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::unsafe_arena_release_beta1() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.AdamOptimizer.beta1)
  
  ::CoreML::Specification::DoubleParameter* temp = beta1_;
  beta1_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::_internal_mutable_beta1() {
  
  if (beta1_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::DoubleParameter>(GetArenaForAllocation());
    beta1_ = p;
  }
  return beta1_;
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::mutable_beta1() {
  ::CoreML::Specification::DoubleParameter* _msg = _internal_mutable_beta1();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.AdamOptimizer.beta1)
  return _msg;
}
inline void AdamOptimizer::set_allocated_beta1(::CoreML::Specification::DoubleParameter* beta1) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(beta1_);
  }
  if (beta1) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<
            ::PROTOBUF_NAMESPACE_ID::MessageLite>::GetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(beta1));
    if (message_arena != submessage_arena) {
      beta1 = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, beta1, submessage_arena);
    }
    
  } else {
    
  }
  beta1_ = beta1;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.AdamOptimizer.beta1)
}

// .CoreML.Specification.DoubleParameter beta2 = 4;
inline bool AdamOptimizer::_internal_has_beta2() const {
  return this != internal_default_instance() && beta2_ != nullptr;
}
inline bool AdamOptimizer::has_beta2() const {
  return _internal_has_beta2();
}
inline const ::CoreML::Specification::DoubleParameter& AdamOptimizer::_internal_beta2() const {
  const ::CoreML::Specification::DoubleParameter* p = beta2_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::DoubleParameter&>(
      ::CoreML::Specification::_DoubleParameter_default_instance_);
}
inline const ::CoreML::Specification::DoubleParameter& AdamOptimizer::beta2() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AdamOptimizer.beta2)
  return _internal_beta2();
}
inline void AdamOptimizer::unsafe_arena_set_allocated_beta2(
    ::CoreML::Specification::DoubleParameter* beta2) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(beta2_);
  }
  beta2_ = beta2;
  if (beta2) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.AdamOptimizer.beta2)
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::release_beta2() {
  
  ::CoreML::Specification::DoubleParameter* temp = beta2_;
  beta2_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::unsafe_arena_release_beta2() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.AdamOptimizer.beta2)
  
  ::CoreML::Specification::DoubleParameter* temp = beta2_;
  beta2_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::_internal_mutable_beta2() {
  
  if (beta2_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::DoubleParameter>(GetArenaForAllocation());
    beta2_ = p;
  }
  return beta2_;
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::mutable_beta2() {
  ::CoreML::Specification::DoubleParameter* _msg = _internal_mutable_beta2();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.AdamOptimizer.beta2)
  return _msg;
}
inline void AdamOptimizer::set_allocated_beta2(::CoreML::Specification::DoubleParameter* beta2) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(beta2_);
  }
  if (beta2) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<
            ::PROTOBUF_NAMESPACE_ID::MessageLite>::GetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(beta2));
    if (message_arena != submessage_arena) {
      beta2 = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, beta2, submessage_arena);
    }
    
  } else {
    
  }
  beta2_ = beta2;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.AdamOptimizer.beta2)
}

// .CoreML.Specification.DoubleParameter eps = 5;
inline bool AdamOptimizer::_internal_has_eps() const {
  return this != internal_default_instance() && eps_ != nullptr;
}
inline bool AdamOptimizer::has_eps() const {
  return _internal_has_eps();
}
inline const ::CoreML::Specification::DoubleParameter& AdamOptimizer::_internal_eps() const {
  const ::CoreML::Specification::DoubleParameter* p = eps_;
  return p != nullptr ? *p : reinterpret_cast<const ::CoreML::Specification::DoubleParameter&>(
      ::CoreML::Specification::_DoubleParameter_default_instance_);
}
inline const ::CoreML::Specification::DoubleParameter& AdamOptimizer::eps() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AdamOptimizer.eps)
  return _internal_eps();
}
inline void AdamOptimizer::unsafe_arena_set_allocated_eps(
    ::CoreML::Specification::DoubleParameter* eps) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(eps_);
  }
  eps_ = eps;
  if (eps) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:CoreML.Specification.AdamOptimizer.eps)
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::release_eps() {
  
  ::CoreML::Specification::DoubleParameter* temp = eps_;
  eps_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::unsafe_arena_release_eps() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.AdamOptimizer.eps)
  
  ::CoreML::Specification::DoubleParameter* temp = eps_;
  eps_ = nullptr;
  return temp;
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::_internal_mutable_eps() {
  
  if (eps_ == nullptr) {
    auto* p = CreateMaybeMessage<::CoreML::Specification::DoubleParameter>(GetArenaForAllocation());
    eps_ = p;
  }
  return eps_;
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::mutable_eps() {
  ::CoreML::Specification::DoubleParameter* _msg = _internal_mutable_eps();
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.AdamOptimizer.eps)
  return _msg;
}
inline void AdamOptimizer::set_allocated_eps(::CoreML::Specification::DoubleParameter* eps) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(eps_);
  }
  if (eps) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<
            ::PROTOBUF_NAMESPACE_ID::MessageLite>::GetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(eps));
    if (message_arena != submessage_arena) {
      eps = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, eps, submessage_arena);
    }
    
  } else {
    
  }
  eps_ = eps;
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.AdamOptimizer.eps)
}

#ifdef __GNUC__
  #pragma GCC diagnostic pop
#endif  // __GNUC__
// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)

}  // namespace Specification
}  // namespace CoreML

PROTOBUF_NAMESPACE_OPEN

template <> struct is_proto_enum< ::CoreML::Specification::SamePadding_SamePaddingMode> : ::std::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::SamplingMode_Method> : ::std::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::BoxCoordinatesMode_Coordinates> : ::std::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::Convolution3DLayerParams_PaddingType> : ::std::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::PoolingLayerParams_PoolingType> : ::std::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::Pooling3DLayerParams_PoolingType3D> : ::std::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::Pooling3DLayerParams_Pooling3DPaddingType> : ::std::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::GlobalPooling3DLayerParams_GlobalPoolingType3D> : ::std::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::UnaryFunctionLayerParams_Operation> : ::std::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::UpsampleLayerParams_InterpolationMode> : ::std::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::UpsampleLayerParams_LinearUpsampleMode> : ::std::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::FlattenLayerParams_FlattenOrder> : ::std::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder> : ::std::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType> : ::std::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::SliceLayerParams_SliceAxis> : ::std::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::ReduceLayerParams_ReduceOperation> : ::std::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::ReduceLayerParams_ReduceAxis> : ::std::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::GeluLayerParams_GeluMode> : ::std::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping> : ::std::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::NeuralNetworkImageShapeMapping> : ::std::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::ScatterMode> : ::std::true_type {};

PROTOBUF_NAMESPACE_CLOSE

// @@protoc_insertion_point(global_scope)

#include <google/protobuf/port_undef.inc>
#endif  // GOOGLE_PROTOBUF_INCLUDED_GOOGLE_PROTOBUF_INCLUDED_NeuralNetwork_2eproto
