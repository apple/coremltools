// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: NeuralNetwork.proto

#define INTERNAL_SUPPRESS_PROTOBUF_FIELD_DEPRECATION
#include "NeuralNetwork.pb.h"

#include <algorithm>

#include <google/protobuf/stubs/common.h>
#include <google/protobuf/stubs/port.h>
#include <google/protobuf/stubs/once.h>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/wire_format_lite_inl.h>
#include <google/protobuf/io/zero_copy_stream_impl_lite.h>
// @@protoc_insertion_point(includes)

namespace CoreML {
namespace Specification {
class NeuralNetworkDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetwork> {
} _NeuralNetwork_default_instance_;
class NeuralNetworkImageScalerDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkImageScaler> {
} _NeuralNetworkImageScaler_default_instance_;
class NeuralNetworkMeanImageDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkMeanImage> {
} _NeuralNetworkMeanImage_default_instance_;
class NeuralNetworkPreprocessingDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkPreprocessing> {
  public:
  const ::CoreML::Specification::NeuralNetworkImageScaler* scaler_;
  const ::CoreML::Specification::NeuralNetworkMeanImage* meanimage_;
} _NeuralNetworkPreprocessing_default_instance_;
class ActivationReLUDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationReLU> {
} _ActivationReLU_default_instance_;
class ActivationLeakyReLUDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationLeakyReLU> {
} _ActivationLeakyReLU_default_instance_;
class ActivationTanhDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationTanh> {
} _ActivationTanh_default_instance_;
class ActivationScaledTanhDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationScaledTanh> {
} _ActivationScaledTanh_default_instance_;
class ActivationSigmoidDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationSigmoid> {
} _ActivationSigmoid_default_instance_;
class ActivationLinearDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationLinear> {
} _ActivationLinear_default_instance_;
class ActivationSigmoidHardDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationSigmoidHard> {
} _ActivationSigmoidHard_default_instance_;
class ActivationPReLUDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationPReLU> {
} _ActivationPReLU_default_instance_;
class ActivationELUDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationELU> {
} _ActivationELU_default_instance_;
class ActivationThresholdedReLUDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationThresholdedReLU> {
} _ActivationThresholdedReLU_default_instance_;
class ActivationSoftsignDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationSoftsign> {
} _ActivationSoftsign_default_instance_;
class ActivationSoftplusDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationSoftplus> {
} _ActivationSoftplus_default_instance_;
class ActivationParametricSoftplusDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationParametricSoftplus> {
} _ActivationParametricSoftplus_default_instance_;
class ActivationParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationParams> {
  public:
  const ::CoreML::Specification::ActivationLinear* linear_;
  const ::CoreML::Specification::ActivationReLU* relu_;
  const ::CoreML::Specification::ActivationLeakyReLU* leakyrelu_;
  const ::CoreML::Specification::ActivationThresholdedReLU* thresholdedrelu_;
  const ::CoreML::Specification::ActivationPReLU* prelu_;
  const ::CoreML::Specification::ActivationTanh* tanh_;
  const ::CoreML::Specification::ActivationScaledTanh* scaledtanh_;
  const ::CoreML::Specification::ActivationSigmoid* sigmoid_;
  const ::CoreML::Specification::ActivationSigmoidHard* sigmoidhard_;
  const ::CoreML::Specification::ActivationELU* elu_;
  const ::CoreML::Specification::ActivationSoftsign* softsign_;
  const ::CoreML::Specification::ActivationSoftplus* softplus_;
  const ::CoreML::Specification::ActivationParametricSoftplus* parametricsoftplus_;
} _ActivationParams_default_instance_;
class NeuralNetworkLayerDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkLayer> {
  public:
  const ::CoreML::Specification::ConvolutionLayerParams* convolution_;
  const ::CoreML::Specification::PoolingLayerParams* pooling_;
  const ::CoreML::Specification::ActivationParams* activation_;
  const ::CoreML::Specification::InnerProductLayerParams* innerproduct_;
  const ::CoreML::Specification::EmbeddingLayerParams* embedding_;
  const ::CoreML::Specification::BatchnormLayerParams* batchnorm_;
  const ::CoreML::Specification::MeanVarianceNormalizeLayerParams* mvn_;
  const ::CoreML::Specification::L2NormalizeLayerParams* l2normalize_;
  const ::CoreML::Specification::SoftmaxLayerParams* softmax_;
  const ::CoreML::Specification::LRNLayerParams* lrn_;
  const ::CoreML::Specification::CropLayerParams* crop_;
  const ::CoreML::Specification::PaddingLayerParams* padding_;
  const ::CoreML::Specification::UpsampleLayerParams* upsample_;
  const ::CoreML::Specification::UnaryFunctionLayerParams* unary_;
  const ::CoreML::Specification::AddLayerParams* add_;
  const ::CoreML::Specification::MultiplyLayerParams* multiply_;
  const ::CoreML::Specification::AverageLayerParams* average_;
  const ::CoreML::Specification::ScaleLayerParams* scale_;
  const ::CoreML::Specification::BiasLayerParams* bias_;
  const ::CoreML::Specification::MaxLayerParams* max_;
  const ::CoreML::Specification::MinLayerParams* min_;
  const ::CoreML::Specification::DotProductLayerParams* dot_;
  const ::CoreML::Specification::ReduceLayerParams* reduce_;
  const ::CoreML::Specification::LoadConstantLayerParams* loadconstant_;
  const ::CoreML::Specification::ReshapeLayerParams* reshape_;
  const ::CoreML::Specification::FlattenLayerParams* flatten_;
  const ::CoreML::Specification::PermuteLayerParams* permute_;
  const ::CoreML::Specification::ConcatLayerParams* concat_;
  const ::CoreML::Specification::SplitLayerParams* split_;
  const ::CoreML::Specification::SequenceRepeatLayerParams* sequencerepeat_;
  const ::CoreML::Specification::ReorganizeDataLayerParams* reorganizedata_;
  const ::CoreML::Specification::SliceLayerParams* slice_;
  const ::CoreML::Specification::SimpleRecurrentLayerParams* simplerecurrent_;
  const ::CoreML::Specification::GRULayerParams* gru_;
  const ::CoreML::Specification::UniDirectionalLSTMLayerParams* unidirectionallstm_;
  const ::CoreML::Specification::BiDirectionalLSTMLayerParams* bidirectionallstm_;
  const ::CoreML::Specification::CustomLayerParams* custom_;
} _NeuralNetworkLayer_default_instance_;
class BorderAmounts_EdgeSizesDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BorderAmounts_EdgeSizes> {
} _BorderAmounts_EdgeSizes_default_instance_;
class BorderAmountsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BorderAmounts> {
} _BorderAmounts_default_instance_;
class ValidPaddingDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ValidPadding> {
} _ValidPadding_default_instance_;
class SamePaddingDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SamePadding> {
} _SamePadding_default_instance_;
class WeightParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<WeightParams> {
} _WeightParams_default_instance_;
class QuantizationParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<QuantizationParams> {
  public:
  const ::CoreML::Specification::LinearQuantizationParams* linearquantization_;
  const ::CoreML::Specification::LookUpTableQuantizationParams* lookuptablequantization_;
} _QuantizationParams_default_instance_;
class LinearQuantizationParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LinearQuantizationParams> {
} _LinearQuantizationParams_default_instance_;
class LookUpTableQuantizationParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LookUpTableQuantizationParams> {
} _LookUpTableQuantizationParams_default_instance_;
class ConvolutionLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ConvolutionLayerParams> {
  public:
  const ::CoreML::Specification::ValidPadding* valid_;
  const ::CoreML::Specification::SamePadding* same_;
} _ConvolutionLayerParams_default_instance_;
class InnerProductLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<InnerProductLayerParams> {
} _InnerProductLayerParams_default_instance_;
class EmbeddingLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<EmbeddingLayerParams> {
} _EmbeddingLayerParams_default_instance_;
class BatchnormLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BatchnormLayerParams> {
} _BatchnormLayerParams_default_instance_;
class PoolingLayerParams_ValidCompletePaddingDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PoolingLayerParams_ValidCompletePadding> {
} _PoolingLayerParams_ValidCompletePadding_default_instance_;
class PoolingLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PoolingLayerParams> {
  public:
  const ::CoreML::Specification::ValidPadding* valid_;
  const ::CoreML::Specification::SamePadding* same_;
  const ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* includelastpixel_;
} _PoolingLayerParams_default_instance_;
class PaddingLayerParams_PaddingConstantDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PaddingLayerParams_PaddingConstant> {
} _PaddingLayerParams_PaddingConstant_default_instance_;
class PaddingLayerParams_PaddingReflectionDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PaddingLayerParams_PaddingReflection> {
} _PaddingLayerParams_PaddingReflection_default_instance_;
class PaddingLayerParams_PaddingReplicationDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PaddingLayerParams_PaddingReplication> {
} _PaddingLayerParams_PaddingReplication_default_instance_;
class PaddingLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PaddingLayerParams> {
  public:
  const ::CoreML::Specification::PaddingLayerParams_PaddingConstant* constant_;
  const ::CoreML::Specification::PaddingLayerParams_PaddingReflection* reflection_;
  const ::CoreML::Specification::PaddingLayerParams_PaddingReplication* replication_;
} _PaddingLayerParams_default_instance_;
class ConcatLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ConcatLayerParams> {
} _ConcatLayerParams_default_instance_;
class LRNLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LRNLayerParams> {
} _LRNLayerParams_default_instance_;
class SoftmaxLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SoftmaxLayerParams> {
} _SoftmaxLayerParams_default_instance_;
class SplitLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SplitLayerParams> {
} _SplitLayerParams_default_instance_;
class AddLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<AddLayerParams> {
} _AddLayerParams_default_instance_;
class MultiplyLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<MultiplyLayerParams> {
} _MultiplyLayerParams_default_instance_;
class UnaryFunctionLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<UnaryFunctionLayerParams> {
} _UnaryFunctionLayerParams_default_instance_;
class UpsampleLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<UpsampleLayerParams> {
} _UpsampleLayerParams_default_instance_;
class BiasLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BiasLayerParams> {
} _BiasLayerParams_default_instance_;
class ScaleLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ScaleLayerParams> {
} _ScaleLayerParams_default_instance_;
class LoadConstantLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LoadConstantLayerParams> {
} _LoadConstantLayerParams_default_instance_;
class L2NormalizeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<L2NormalizeLayerParams> {
} _L2NormalizeLayerParams_default_instance_;
class FlattenLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<FlattenLayerParams> {
} _FlattenLayerParams_default_instance_;
class ReshapeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReshapeLayerParams> {
} _ReshapeLayerParams_default_instance_;
class PermuteLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PermuteLayerParams> {
} _PermuteLayerParams_default_instance_;
class ReorganizeDataLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReorganizeDataLayerParams> {
} _ReorganizeDataLayerParams_default_instance_;
class SliceLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SliceLayerParams> {
} _SliceLayerParams_default_instance_;
class ReduceLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReduceLayerParams> {
} _ReduceLayerParams_default_instance_;
class CropLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CropLayerParams> {
} _CropLayerParams_default_instance_;
class AverageLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<AverageLayerParams> {
} _AverageLayerParams_default_instance_;
class MaxLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<MaxLayerParams> {
} _MaxLayerParams_default_instance_;
class MinLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<MinLayerParams> {
} _MinLayerParams_default_instance_;
class DotProductLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<DotProductLayerParams> {
} _DotProductLayerParams_default_instance_;
class MeanVarianceNormalizeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<MeanVarianceNormalizeLayerParams> {
} _MeanVarianceNormalizeLayerParams_default_instance_;
class SequenceRepeatLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SequenceRepeatLayerParams> {
} _SequenceRepeatLayerParams_default_instance_;
class SimpleRecurrentLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SimpleRecurrentLayerParams> {
} _SimpleRecurrentLayerParams_default_instance_;
class GRULayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<GRULayerParams> {
} _GRULayerParams_default_instance_;
class LSTMParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LSTMParams> {
} _LSTMParams_default_instance_;
class LSTMWeightParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LSTMWeightParams> {
} _LSTMWeightParams_default_instance_;
class UniDirectionalLSTMLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<UniDirectionalLSTMLayerParams> {
} _UniDirectionalLSTMLayerParams_default_instance_;
class BiDirectionalLSTMLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BiDirectionalLSTMLayerParams> {
} _BiDirectionalLSTMLayerParams_default_instance_;
class CustomLayerParams_CustomLayerParamValueDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CustomLayerParams_CustomLayerParamValue> {
  public:
  double doublevalue_;
  ::google::protobuf::internal::ArenaStringPtr stringvalue_;
  ::google::protobuf::int32 intvalue_;
  ::google::protobuf::int64 longvalue_;
  bool boolvalue_;
} _CustomLayerParams_CustomLayerParamValue_default_instance_;
class CustomLayerParams_ParametersEntryDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CustomLayerParams::CustomLayerParams_ParametersEntry> {
} _CustomLayerParams_ParametersEntry_default_instance_;
class CustomLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CustomLayerParams> {
} _CustomLayerParams_default_instance_;
class NeuralNetworkClassifierDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkClassifier> {
  public:
  const ::CoreML::Specification::StringVector* stringclasslabels_;
  const ::CoreML::Specification::Int64Vector* int64classlabels_;
} _NeuralNetworkClassifier_default_instance_;
class NeuralNetworkRegressorDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkRegressor> {
} _NeuralNetworkRegressor_default_instance_;

namespace protobuf_NeuralNetwork_2eproto {

PROTOBUF_CONSTEXPR_VAR ::google::protobuf::internal::ParseTableField
    const TableStruct::entries[] = {
  {0, 0, 0, ::google::protobuf::internal::kInvalidMask, 0, 0},
};

PROTOBUF_CONSTEXPR_VAR ::google::protobuf::internal::AuxillaryParseTableField
    const TableStruct::aux[] = {
  ::google::protobuf::internal::AuxillaryParseTableField(),
};
PROTOBUF_CONSTEXPR_VAR ::google::protobuf::internal::ParseTable const
    TableStruct::schema[] = {
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
};


void TableStruct::Shutdown() {
  _NeuralNetwork_default_instance_.Shutdown();
  _NeuralNetworkImageScaler_default_instance_.Shutdown();
  _NeuralNetworkMeanImage_default_instance_.Shutdown();
  _NeuralNetworkPreprocessing_default_instance_.Shutdown();
  _ActivationReLU_default_instance_.Shutdown();
  _ActivationLeakyReLU_default_instance_.Shutdown();
  _ActivationTanh_default_instance_.Shutdown();
  _ActivationScaledTanh_default_instance_.Shutdown();
  _ActivationSigmoid_default_instance_.Shutdown();
  _ActivationLinear_default_instance_.Shutdown();
  _ActivationSigmoidHard_default_instance_.Shutdown();
  _ActivationPReLU_default_instance_.Shutdown();
  _ActivationELU_default_instance_.Shutdown();
  _ActivationThresholdedReLU_default_instance_.Shutdown();
  _ActivationSoftsign_default_instance_.Shutdown();
  _ActivationSoftplus_default_instance_.Shutdown();
  _ActivationParametricSoftplus_default_instance_.Shutdown();
  _ActivationParams_default_instance_.Shutdown();
  _NeuralNetworkLayer_default_instance_.Shutdown();
  _BorderAmounts_EdgeSizes_default_instance_.Shutdown();
  _BorderAmounts_default_instance_.Shutdown();
  _ValidPadding_default_instance_.Shutdown();
  _SamePadding_default_instance_.Shutdown();
  _WeightParams_default_instance_.Shutdown();
  _QuantizationParams_default_instance_.Shutdown();
  _LinearQuantizationParams_default_instance_.Shutdown();
  _LookUpTableQuantizationParams_default_instance_.Shutdown();
  _ConvolutionLayerParams_default_instance_.Shutdown();
  _InnerProductLayerParams_default_instance_.Shutdown();
  _EmbeddingLayerParams_default_instance_.Shutdown();
  _BatchnormLayerParams_default_instance_.Shutdown();
  _PoolingLayerParams_ValidCompletePadding_default_instance_.Shutdown();
  _PoolingLayerParams_default_instance_.Shutdown();
  _PaddingLayerParams_PaddingConstant_default_instance_.Shutdown();
  _PaddingLayerParams_PaddingReflection_default_instance_.Shutdown();
  _PaddingLayerParams_PaddingReplication_default_instance_.Shutdown();
  _PaddingLayerParams_default_instance_.Shutdown();
  _ConcatLayerParams_default_instance_.Shutdown();
  _LRNLayerParams_default_instance_.Shutdown();
  _SoftmaxLayerParams_default_instance_.Shutdown();
  _SplitLayerParams_default_instance_.Shutdown();
  _AddLayerParams_default_instance_.Shutdown();
  _MultiplyLayerParams_default_instance_.Shutdown();
  _UnaryFunctionLayerParams_default_instance_.Shutdown();
  _UpsampleLayerParams_default_instance_.Shutdown();
  _BiasLayerParams_default_instance_.Shutdown();
  _ScaleLayerParams_default_instance_.Shutdown();
  _LoadConstantLayerParams_default_instance_.Shutdown();
  _L2NormalizeLayerParams_default_instance_.Shutdown();
  _FlattenLayerParams_default_instance_.Shutdown();
  _ReshapeLayerParams_default_instance_.Shutdown();
  _PermuteLayerParams_default_instance_.Shutdown();
  _ReorganizeDataLayerParams_default_instance_.Shutdown();
  _SliceLayerParams_default_instance_.Shutdown();
  _ReduceLayerParams_default_instance_.Shutdown();
  _CropLayerParams_default_instance_.Shutdown();
  _AverageLayerParams_default_instance_.Shutdown();
  _MaxLayerParams_default_instance_.Shutdown();
  _MinLayerParams_default_instance_.Shutdown();
  _DotProductLayerParams_default_instance_.Shutdown();
  _MeanVarianceNormalizeLayerParams_default_instance_.Shutdown();
  _SequenceRepeatLayerParams_default_instance_.Shutdown();
  _SimpleRecurrentLayerParams_default_instance_.Shutdown();
  _GRULayerParams_default_instance_.Shutdown();
  _LSTMParams_default_instance_.Shutdown();
  _LSTMWeightParams_default_instance_.Shutdown();
  _UniDirectionalLSTMLayerParams_default_instance_.Shutdown();
  _BiDirectionalLSTMLayerParams_default_instance_.Shutdown();
  _CustomLayerParams_CustomLayerParamValue_default_instance_.Shutdown();
  _CustomLayerParams_default_instance_.Shutdown();
  _NeuralNetworkClassifier_default_instance_.Shutdown();
  _NeuralNetworkRegressor_default_instance_.Shutdown();
}

void TableStruct::InitDefaultsImpl() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  ::google::protobuf::internal::InitProtobufDefaults();
  ::CoreML::Specification::protobuf_DataStructures_2eproto::InitDefaults();
  _NeuralNetwork_default_instance_.DefaultConstruct();
  _NeuralNetworkImageScaler_default_instance_.DefaultConstruct();
  _NeuralNetworkMeanImage_default_instance_.DefaultConstruct();
  _NeuralNetworkPreprocessing_default_instance_.DefaultConstruct();
  _ActivationReLU_default_instance_.DefaultConstruct();
  _ActivationLeakyReLU_default_instance_.DefaultConstruct();
  _ActivationTanh_default_instance_.DefaultConstruct();
  _ActivationScaledTanh_default_instance_.DefaultConstruct();
  _ActivationSigmoid_default_instance_.DefaultConstruct();
  _ActivationLinear_default_instance_.DefaultConstruct();
  _ActivationSigmoidHard_default_instance_.DefaultConstruct();
  _ActivationPReLU_default_instance_.DefaultConstruct();
  _ActivationELU_default_instance_.DefaultConstruct();
  _ActivationThresholdedReLU_default_instance_.DefaultConstruct();
  _ActivationSoftsign_default_instance_.DefaultConstruct();
  _ActivationSoftplus_default_instance_.DefaultConstruct();
  _ActivationParametricSoftplus_default_instance_.DefaultConstruct();
  _ActivationParams_default_instance_.DefaultConstruct();
  _NeuralNetworkLayer_default_instance_.DefaultConstruct();
  _BorderAmounts_EdgeSizes_default_instance_.DefaultConstruct();
  _BorderAmounts_default_instance_.DefaultConstruct();
  _ValidPadding_default_instance_.DefaultConstruct();
  _SamePadding_default_instance_.DefaultConstruct();
  _WeightParams_default_instance_.DefaultConstruct();
  _QuantizationParams_default_instance_.DefaultConstruct();
  _LinearQuantizationParams_default_instance_.DefaultConstruct();
  _LookUpTableQuantizationParams_default_instance_.DefaultConstruct();
  _ConvolutionLayerParams_default_instance_.DefaultConstruct();
  _InnerProductLayerParams_default_instance_.DefaultConstruct();
  _EmbeddingLayerParams_default_instance_.DefaultConstruct();
  _BatchnormLayerParams_default_instance_.DefaultConstruct();
  _PoolingLayerParams_ValidCompletePadding_default_instance_.DefaultConstruct();
  _PoolingLayerParams_default_instance_.DefaultConstruct();
  _PaddingLayerParams_PaddingConstant_default_instance_.DefaultConstruct();
  _PaddingLayerParams_PaddingReflection_default_instance_.DefaultConstruct();
  _PaddingLayerParams_PaddingReplication_default_instance_.DefaultConstruct();
  _PaddingLayerParams_default_instance_.DefaultConstruct();
  _ConcatLayerParams_default_instance_.DefaultConstruct();
  _LRNLayerParams_default_instance_.DefaultConstruct();
  _SoftmaxLayerParams_default_instance_.DefaultConstruct();
  _SplitLayerParams_default_instance_.DefaultConstruct();
  _AddLayerParams_default_instance_.DefaultConstruct();
  _MultiplyLayerParams_default_instance_.DefaultConstruct();
  _UnaryFunctionLayerParams_default_instance_.DefaultConstruct();
  _UpsampleLayerParams_default_instance_.DefaultConstruct();
  _BiasLayerParams_default_instance_.DefaultConstruct();
  _ScaleLayerParams_default_instance_.DefaultConstruct();
  _LoadConstantLayerParams_default_instance_.DefaultConstruct();
  _L2NormalizeLayerParams_default_instance_.DefaultConstruct();
  _FlattenLayerParams_default_instance_.DefaultConstruct();
  _ReshapeLayerParams_default_instance_.DefaultConstruct();
  _PermuteLayerParams_default_instance_.DefaultConstruct();
  _ReorganizeDataLayerParams_default_instance_.DefaultConstruct();
  _SliceLayerParams_default_instance_.DefaultConstruct();
  _ReduceLayerParams_default_instance_.DefaultConstruct();
  _CropLayerParams_default_instance_.DefaultConstruct();
  _AverageLayerParams_default_instance_.DefaultConstruct();
  _MaxLayerParams_default_instance_.DefaultConstruct();
  _MinLayerParams_default_instance_.DefaultConstruct();
  _DotProductLayerParams_default_instance_.DefaultConstruct();
  _MeanVarianceNormalizeLayerParams_default_instance_.DefaultConstruct();
  _SequenceRepeatLayerParams_default_instance_.DefaultConstruct();
  _SimpleRecurrentLayerParams_default_instance_.DefaultConstruct();
  _GRULayerParams_default_instance_.DefaultConstruct();
  _LSTMParams_default_instance_.DefaultConstruct();
  _LSTMWeightParams_default_instance_.DefaultConstruct();
  _UniDirectionalLSTMLayerParams_default_instance_.DefaultConstruct();
  _BiDirectionalLSTMLayerParams_default_instance_.DefaultConstruct();
  _CustomLayerParams_CustomLayerParamValue_default_instance_.DefaultConstruct();
  _CustomLayerParams_ParametersEntry_default_instance_.DefaultConstruct();
  _CustomLayerParams_default_instance_.DefaultConstruct();
  _NeuralNetworkClassifier_default_instance_.DefaultConstruct();
  _NeuralNetworkRegressor_default_instance_.DefaultConstruct();
  _ActivationPReLU_default_instance_.get_mutable()->alpha_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _ActivationParametricSoftplus_default_instance_.get_mutable()->alpha_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _ActivationParametricSoftplus_default_instance_.get_mutable()->beta_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _ValidPadding_default_instance_.get_mutable()->paddingamounts_ = const_cast< ::CoreML::Specification::BorderAmounts*>(
      ::CoreML::Specification::BorderAmounts::internal_default_instance());
  _WeightParams_default_instance_.get_mutable()->quantization_ = const_cast< ::CoreML::Specification::QuantizationParams*>(
      ::CoreML::Specification::QuantizationParams::internal_default_instance());
  _ConvolutionLayerParams_default_instance_.get_mutable()->weights_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _ConvolutionLayerParams_default_instance_.get_mutable()->bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _InnerProductLayerParams_default_instance_.get_mutable()->weights_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _InnerProductLayerParams_default_instance_.get_mutable()->bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _EmbeddingLayerParams_default_instance_.get_mutable()->weights_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _EmbeddingLayerParams_default_instance_.get_mutable()->bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _BatchnormLayerParams_default_instance_.get_mutable()->gamma_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _BatchnormLayerParams_default_instance_.get_mutable()->beta_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _BatchnormLayerParams_default_instance_.get_mutable()->mean_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _BatchnormLayerParams_default_instance_.get_mutable()->variance_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _PaddingLayerParams_default_instance_.get_mutable()->paddingamounts_ = const_cast< ::CoreML::Specification::BorderAmounts*>(
      ::CoreML::Specification::BorderAmounts::internal_default_instance());
  _BiasLayerParams_default_instance_.get_mutable()->bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _ScaleLayerParams_default_instance_.get_mutable()->scale_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _ScaleLayerParams_default_instance_.get_mutable()->bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LoadConstantLayerParams_default_instance_.get_mutable()->data_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _CropLayerParams_default_instance_.get_mutable()->cropamounts_ = const_cast< ::CoreML::Specification::BorderAmounts*>(
      ::CoreML::Specification::BorderAmounts::internal_default_instance());
  _SimpleRecurrentLayerParams_default_instance_.get_mutable()->activation_ = const_cast< ::CoreML::Specification::ActivationParams*>(
      ::CoreML::Specification::ActivationParams::internal_default_instance());
  _SimpleRecurrentLayerParams_default_instance_.get_mutable()->weightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _SimpleRecurrentLayerParams_default_instance_.get_mutable()->recursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _SimpleRecurrentLayerParams_default_instance_.get_mutable()->biasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->updategateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->resetgateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->outputgateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->updategaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->resetgaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->outputgaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->updategatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->resetgatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->outputgatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->inputgateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->forgetgateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->blockinputweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->outputgateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->inputgaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->forgetgaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->blockinputrecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->outputgaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->inputgatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->forgetgatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->blockinputbiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->outputgatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->inputgatepeepholevector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->forgetgatepeepholevector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->outputgatepeepholevector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _UniDirectionalLSTMLayerParams_default_instance_.get_mutable()->params_ = const_cast< ::CoreML::Specification::LSTMParams*>(
      ::CoreML::Specification::LSTMParams::internal_default_instance());
  _UniDirectionalLSTMLayerParams_default_instance_.get_mutable()->weightparams_ = const_cast< ::CoreML::Specification::LSTMWeightParams*>(
      ::CoreML::Specification::LSTMWeightParams::internal_default_instance());
  _BiDirectionalLSTMLayerParams_default_instance_.get_mutable()->params_ = const_cast< ::CoreML::Specification::LSTMParams*>(
      ::CoreML::Specification::LSTMParams::internal_default_instance());
  _CustomLayerParams_ParametersEntry_default_instance_.get_mutable()->set_default_instance(_CustomLayerParams_ParametersEntry_default_instance_.get_mutable());
  _CustomLayerParams_ParametersEntry_default_instance_.get_mutable()->InitAsDefaultInstance();
}

void InitDefaults() {
  static GOOGLE_PROTOBUF_DECLARE_ONCE(once);
  ::google::protobuf::GoogleOnceInit(&once, &TableStruct::InitDefaultsImpl);
}
void AddDescriptorsImpl() {
  InitDefaults();
  ::CoreML::Specification::protobuf_DataStructures_2eproto::AddDescriptors();
  ::google::protobuf::internal::OnShutdown(&TableStruct::Shutdown);
}

void AddDescriptors() {
  static GOOGLE_PROTOBUF_DECLARE_ONCE(once);
  ::google::protobuf::GoogleOnceInit(&once, &AddDescriptorsImpl);
}
#ifdef GOOGLE_PROTOBUF_NO_STATIC_INITIALIZER
// Force AddDescriptors() to be called at static initialization time.
struct StaticDescriptorInitializer {
  StaticDescriptorInitializer() {
    AddDescriptors();
  }
} static_descriptor_initializer;
#endif  // GOOGLE_PROTOBUF_NO_STATIC_INITIALIZER

}  // namespace protobuf_NeuralNetwork_2eproto

bool SamePadding_SamePaddingMode_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const SamePadding_SamePaddingMode SamePadding::BOTTOM_RIGHT_HEAVY;
const SamePadding_SamePaddingMode SamePadding::TOP_LEFT_HEAVY;
const SamePadding_SamePaddingMode SamePadding::SamePaddingMode_MIN;
const SamePadding_SamePaddingMode SamePadding::SamePaddingMode_MAX;
const int SamePadding::SamePaddingMode_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool PoolingLayerParams_PoolingType_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const PoolingLayerParams_PoolingType PoolingLayerParams::MAX;
const PoolingLayerParams_PoolingType PoolingLayerParams::AVERAGE;
const PoolingLayerParams_PoolingType PoolingLayerParams::L2;
const PoolingLayerParams_PoolingType PoolingLayerParams::PoolingType_MIN;
const PoolingLayerParams_PoolingType PoolingLayerParams::PoolingType_MAX;
const int PoolingLayerParams::PoolingType_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool UnaryFunctionLayerParams_Operation_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
    case 4:
    case 5:
    case 6:
    case 7:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::SQRT;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::RSQRT;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::INVERSE;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::POWER;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::EXP;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::LOG;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::ABS;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::THRESHOLD;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::Operation_MIN;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::Operation_MAX;
const int UnaryFunctionLayerParams::Operation_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool UpsampleLayerParams_InterpolationMode_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const UpsampleLayerParams_InterpolationMode UpsampleLayerParams::NN;
const UpsampleLayerParams_InterpolationMode UpsampleLayerParams::BILINEAR;
const UpsampleLayerParams_InterpolationMode UpsampleLayerParams::InterpolationMode_MIN;
const UpsampleLayerParams_InterpolationMode UpsampleLayerParams::InterpolationMode_MAX;
const int UpsampleLayerParams::InterpolationMode_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool FlattenLayerParams_FlattenOrder_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const FlattenLayerParams_FlattenOrder FlattenLayerParams::CHANNEL_FIRST;
const FlattenLayerParams_FlattenOrder FlattenLayerParams::CHANNEL_LAST;
const FlattenLayerParams_FlattenOrder FlattenLayerParams::FlattenOrder_MIN;
const FlattenLayerParams_FlattenOrder FlattenLayerParams::FlattenOrder_MAX;
const int FlattenLayerParams::FlattenOrder_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool ReshapeLayerParams_ReshapeOrder_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::CHANNEL_FIRST;
const ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::CHANNEL_LAST;
const ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::ReshapeOrder_MIN;
const ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::ReshapeOrder_MAX;
const int ReshapeLayerParams::ReshapeOrder_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool ReorganizeDataLayerParams_ReorganizationType_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams::SPACE_TO_DEPTH;
const ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams::DEPTH_TO_SPACE;
const ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams::ReorganizationType_MIN;
const ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams::ReorganizationType_MAX;
const int ReorganizeDataLayerParams::ReorganizationType_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool SliceLayerParams_SliceAxis_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const SliceLayerParams_SliceAxis SliceLayerParams::CHANNEL_AXIS;
const SliceLayerParams_SliceAxis SliceLayerParams::HEIGHT_AXIS;
const SliceLayerParams_SliceAxis SliceLayerParams::WIDTH_AXIS;
const SliceLayerParams_SliceAxis SliceLayerParams::SliceAxis_MIN;
const SliceLayerParams_SliceAxis SliceLayerParams::SliceAxis_MAX;
const int SliceLayerParams::SliceAxis_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool ReduceLayerParams_ReduceOperation_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
    case 4:
    case 5:
    case 6:
    case 7:
    case 8:
    case 9:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const ReduceLayerParams_ReduceOperation ReduceLayerParams::SUM;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::AVG;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::PROD;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::LOGSUM;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::SUMSQUARE;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::L1;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::L2;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::MAX;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::MIN;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::ARGMAX;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::ReduceOperation_MIN;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::ReduceOperation_MAX;
const int ReduceLayerParams::ReduceOperation_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool ReduceLayerParams_ReduceAxis_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
    case 4:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const ReduceLayerParams_ReduceAxis ReduceLayerParams::CHW;
const ReduceLayerParams_ReduceAxis ReduceLayerParams::HW;
const ReduceLayerParams_ReduceAxis ReduceLayerParams::C;
const ReduceLayerParams_ReduceAxis ReduceLayerParams::H;
const ReduceLayerParams_ReduceAxis ReduceLayerParams::W;
const ReduceLayerParams_ReduceAxis ReduceLayerParams::ReduceAxis_MIN;
const ReduceLayerParams_ReduceAxis ReduceLayerParams::ReduceAxis_MAX;
const int ReduceLayerParams::ReduceAxis_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetwork::kLayersFieldNumber;
const int NeuralNetwork::kPreprocessingFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetwork::NeuralNetwork()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetwork)
}
NeuralNetwork::NeuralNetwork(const NeuralNetwork& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      layers_(from.layers_),
      preprocessing_(from.preprocessing_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetwork)
}

void NeuralNetwork::SharedCtor() {
  _cached_size_ = 0;
}

NeuralNetwork::~NeuralNetwork() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetwork)
  SharedDtor();
}

void NeuralNetwork::SharedDtor() {
}

void NeuralNetwork::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetwork& NeuralNetwork::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NeuralNetwork* NeuralNetwork::New(::google::protobuf::Arena* arena) const {
  NeuralNetwork* n = new NeuralNetwork;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetwork::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetwork)
  layers_.Clear();
  preprocessing_.Clear();
}

bool NeuralNetwork::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetwork)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_layers()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_preprocessing()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetwork)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetwork)
  return false;
#undef DO_
}

void NeuralNetwork::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetwork)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  for (unsigned int i = 0, n = this->layers_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, this->layers(i), output);
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  for (unsigned int i = 0, n = this->preprocessing_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, this->preprocessing(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetwork)
}

size_t NeuralNetwork::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetwork)
  size_t total_size = 0;

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  {
    unsigned int count = this->layers_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->layers(i));
    }
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  {
    unsigned int count = this->preprocessing_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->preprocessing(i));
    }
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetwork::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetwork*>(&from));
}

void NeuralNetwork::MergeFrom(const NeuralNetwork& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetwork)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  layers_.MergeFrom(from.layers_);
  preprocessing_.MergeFrom(from.preprocessing_);
}

void NeuralNetwork::CopyFrom(const NeuralNetwork& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetwork)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NeuralNetwork::IsInitialized() const {
  return true;
}

void NeuralNetwork::Swap(NeuralNetwork* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetwork::InternalSwap(NeuralNetwork* other) {
  layers_.InternalSwap(&other->layers_);
  preprocessing_.InternalSwap(&other->preprocessing_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetwork::GetTypeName() const {
  return "CoreML.Specification.NeuralNetwork";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetwork

// repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
int NeuralNetwork::layers_size() const {
  return layers_.size();
}
void NeuralNetwork::clear_layers() {
  layers_.Clear();
}
const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetwork::layers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.layers)
  return layers_.Get(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetwork::mutable_layers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetwork.layers)
  return layers_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetwork::add_layers() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetwork.layers)
  return layers_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
NeuralNetwork::mutable_layers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetwork.layers)
  return &layers_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
NeuralNetwork::layers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetwork.layers)
  return layers_;
}

// repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
int NeuralNetwork::preprocessing_size() const {
  return preprocessing_.size();
}
void NeuralNetwork::clear_preprocessing() {
  preprocessing_.Clear();
}
const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetwork::preprocessing(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_.Get(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetwork::mutable_preprocessing(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetwork::add_preprocessing() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
NeuralNetwork::mutable_preprocessing() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetwork.preprocessing)
  return &preprocessing_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
NeuralNetwork::preprocessing() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkImageScaler::kChannelScaleFieldNumber;
const int NeuralNetworkImageScaler::kBlueBiasFieldNumber;
const int NeuralNetworkImageScaler::kGreenBiasFieldNumber;
const int NeuralNetworkImageScaler::kRedBiasFieldNumber;
const int NeuralNetworkImageScaler::kGrayBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkImageScaler::NeuralNetworkImageScaler()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkImageScaler)
}
NeuralNetworkImageScaler::NeuralNetworkImageScaler(const NeuralNetworkImageScaler& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&graybias_, &from.graybias_,
    reinterpret_cast<char*>(&redbias_) -
    reinterpret_cast<char*>(&graybias_) + sizeof(redbias_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkImageScaler)
}

void NeuralNetworkImageScaler::SharedCtor() {
  ::memset(&graybias_, 0, reinterpret_cast<char*>(&redbias_) -
    reinterpret_cast<char*>(&graybias_) + sizeof(redbias_));
  _cached_size_ = 0;
}

NeuralNetworkImageScaler::~NeuralNetworkImageScaler() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkImageScaler)
  SharedDtor();
}

void NeuralNetworkImageScaler::SharedDtor() {
}

void NeuralNetworkImageScaler::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkImageScaler& NeuralNetworkImageScaler::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NeuralNetworkImageScaler* NeuralNetworkImageScaler::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkImageScaler* n = new NeuralNetworkImageScaler;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkImageScaler::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkImageScaler)
  ::memset(&graybias_, 0, reinterpret_cast<char*>(&redbias_) -
    reinterpret_cast<char*>(&graybias_) + sizeof(redbias_));
}

bool NeuralNetworkImageScaler::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkImageScaler)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float channelScale = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(85u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &channelscale_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float blueBias = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(165u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &bluebias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float greenBias = 21;
      case 21: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(173u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &greenbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float redBias = 22;
      case 22: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(181u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &redbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float grayBias = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(245u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &graybias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkImageScaler)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkImageScaler)
  return false;
#undef DO_
}

void NeuralNetworkImageScaler::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkImageScaler)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float channelScale = 10;
  if (this->channelscale() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(10, this->channelscale(), output);
  }

  // float blueBias = 20;
  if (this->bluebias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(20, this->bluebias(), output);
  }

  // float greenBias = 21;
  if (this->greenbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(21, this->greenbias(), output);
  }

  // float redBias = 22;
  if (this->redbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(22, this->redbias(), output);
  }

  // float grayBias = 30;
  if (this->graybias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(30, this->graybias(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkImageScaler)
}

size_t NeuralNetworkImageScaler::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkImageScaler)
  size_t total_size = 0;

  // float grayBias = 30;
  if (this->graybias() != 0) {
    total_size += 2 + 4;
  }

  // float channelScale = 10;
  if (this->channelscale() != 0) {
    total_size += 1 + 4;
  }

  // float blueBias = 20;
  if (this->bluebias() != 0) {
    total_size += 2 + 4;
  }

  // float greenBias = 21;
  if (this->greenbias() != 0) {
    total_size += 2 + 4;
  }

  // float redBias = 22;
  if (this->redbias() != 0) {
    total_size += 2 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkImageScaler::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkImageScaler*>(&from));
}

void NeuralNetworkImageScaler::MergeFrom(const NeuralNetworkImageScaler& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkImageScaler)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.graybias() != 0) {
    set_graybias(from.graybias());
  }
  if (from.channelscale() != 0) {
    set_channelscale(from.channelscale());
  }
  if (from.bluebias() != 0) {
    set_bluebias(from.bluebias());
  }
  if (from.greenbias() != 0) {
    set_greenbias(from.greenbias());
  }
  if (from.redbias() != 0) {
    set_redbias(from.redbias());
  }
}

void NeuralNetworkImageScaler::CopyFrom(const NeuralNetworkImageScaler& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkImageScaler)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NeuralNetworkImageScaler::IsInitialized() const {
  return true;
}

void NeuralNetworkImageScaler::Swap(NeuralNetworkImageScaler* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkImageScaler::InternalSwap(NeuralNetworkImageScaler* other) {
  std::swap(graybias_, other->graybias_);
  std::swap(channelscale_, other->channelscale_);
  std::swap(bluebias_, other->bluebias_);
  std::swap(greenbias_, other->greenbias_);
  std::swap(redbias_, other->redbias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkImageScaler::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkImageScaler";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkImageScaler

// float channelScale = 10;
void NeuralNetworkImageScaler::clear_channelscale() {
  channelscale_ = 0;
}
float NeuralNetworkImageScaler::channelscale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.channelScale)
  return channelscale_;
}
void NeuralNetworkImageScaler::set_channelscale(float value) {
  
  channelscale_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.channelScale)
}

// float blueBias = 20;
void NeuralNetworkImageScaler::clear_bluebias() {
  bluebias_ = 0;
}
float NeuralNetworkImageScaler::bluebias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.blueBias)
  return bluebias_;
}
void NeuralNetworkImageScaler::set_bluebias(float value) {
  
  bluebias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.blueBias)
}

// float greenBias = 21;
void NeuralNetworkImageScaler::clear_greenbias() {
  greenbias_ = 0;
}
float NeuralNetworkImageScaler::greenbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.greenBias)
  return greenbias_;
}
void NeuralNetworkImageScaler::set_greenbias(float value) {
  
  greenbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.greenBias)
}

// float redBias = 22;
void NeuralNetworkImageScaler::clear_redbias() {
  redbias_ = 0;
}
float NeuralNetworkImageScaler::redbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.redBias)
  return redbias_;
}
void NeuralNetworkImageScaler::set_redbias(float value) {
  
  redbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.redBias)
}

// float grayBias = 30;
void NeuralNetworkImageScaler::clear_graybias() {
  graybias_ = 0;
}
float NeuralNetworkImageScaler::graybias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.grayBias)
  return graybias_;
}
void NeuralNetworkImageScaler::set_graybias(float value) {
  
  graybias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.grayBias)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkMeanImage::kMeanImageFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkMeanImage::NeuralNetworkMeanImage()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkMeanImage)
}
NeuralNetworkMeanImage::NeuralNetworkMeanImage(const NeuralNetworkMeanImage& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      meanimage_(from.meanimage_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkMeanImage)
}

void NeuralNetworkMeanImage::SharedCtor() {
  _cached_size_ = 0;
}

NeuralNetworkMeanImage::~NeuralNetworkMeanImage() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkMeanImage)
  SharedDtor();
}

void NeuralNetworkMeanImage::SharedDtor() {
}

void NeuralNetworkMeanImage::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkMeanImage& NeuralNetworkMeanImage::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NeuralNetworkMeanImage* NeuralNetworkMeanImage::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkMeanImage* n = new NeuralNetworkMeanImage;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkMeanImage::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkMeanImage)
  meanimage_.Clear();
}

bool NeuralNetworkMeanImage::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkMeanImage)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated float meanImage = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, this->mutable_meanimage())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(13u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 1, 10u, input, this->mutable_meanimage())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkMeanImage)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkMeanImage)
  return false;
#undef DO_
}

void NeuralNetworkMeanImage::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkMeanImage)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated float meanImage = 1;
  if (this->meanimage_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_meanimage_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteFloatArray(
      this->meanimage().data(), this->meanimage_size(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkMeanImage)
}

size_t NeuralNetworkMeanImage::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkMeanImage)
  size_t total_size = 0;

  // repeated float meanImage = 1;
  {
    unsigned int count = this->meanimage_size();
    size_t data_size = 4UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _meanimage_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkMeanImage::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkMeanImage*>(&from));
}

void NeuralNetworkMeanImage::MergeFrom(const NeuralNetworkMeanImage& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkMeanImage)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  meanimage_.MergeFrom(from.meanimage_);
}

void NeuralNetworkMeanImage::CopyFrom(const NeuralNetworkMeanImage& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkMeanImage)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NeuralNetworkMeanImage::IsInitialized() const {
  return true;
}

void NeuralNetworkMeanImage::Swap(NeuralNetworkMeanImage* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkMeanImage::InternalSwap(NeuralNetworkMeanImage* other) {
  meanimage_.InternalSwap(&other->meanimage_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkMeanImage::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkMeanImage";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkMeanImage

// repeated float meanImage = 1;
int NeuralNetworkMeanImage::meanimage_size() const {
  return meanimage_.size();
}
void NeuralNetworkMeanImage::clear_meanimage() {
  meanimage_.Clear();
}
float NeuralNetworkMeanImage::meanimage(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
  return meanimage_.Get(index);
}
void NeuralNetworkMeanImage::set_meanimage(int index, float value) {
  meanimage_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
}
void NeuralNetworkMeanImage::add_meanimage(float value) {
  meanimage_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
}
const ::google::protobuf::RepeatedField< float >&
NeuralNetworkMeanImage::meanimage() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
  return meanimage_;
}
::google::protobuf::RepeatedField< float >*
NeuralNetworkMeanImage::mutable_meanimage() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
  return &meanimage_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkPreprocessing::kFeatureNameFieldNumber;
const int NeuralNetworkPreprocessing::kScalerFieldNumber;
const int NeuralNetworkPreprocessing::kMeanImageFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkPreprocessing::NeuralNetworkPreprocessing()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkPreprocessing)
}
NeuralNetworkPreprocessing::NeuralNetworkPreprocessing(const NeuralNetworkPreprocessing& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  featurename_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.featurename().size() > 0) {
    featurename_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.featurename_);
  }
  clear_has_preprocessor();
  switch (from.preprocessor_case()) {
    case kScaler: {
      mutable_scaler()->::CoreML::Specification::NeuralNetworkImageScaler::MergeFrom(from.scaler());
      break;
    }
    case kMeanImage: {
      mutable_meanimage()->::CoreML::Specification::NeuralNetworkMeanImage::MergeFrom(from.meanimage());
      break;
    }
    case PREPROCESSOR_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkPreprocessing)
}

void NeuralNetworkPreprocessing::SharedCtor() {
  featurename_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_preprocessor();
  _cached_size_ = 0;
}

NeuralNetworkPreprocessing::~NeuralNetworkPreprocessing() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkPreprocessing)
  SharedDtor();
}

void NeuralNetworkPreprocessing::SharedDtor() {
  featurename_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (has_preprocessor()) {
    clear_preprocessor();
  }
}

void NeuralNetworkPreprocessing::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkPreprocessing& NeuralNetworkPreprocessing::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NeuralNetworkPreprocessing* NeuralNetworkPreprocessing::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkPreprocessing* n = new NeuralNetworkPreprocessing;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkPreprocessing::clear_preprocessor() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.NeuralNetworkPreprocessing)
  switch (preprocessor_case()) {
    case kScaler: {
      delete preprocessor_.scaler_;
      break;
    }
    case kMeanImage: {
      delete preprocessor_.meanimage_;
      break;
    }
    case PREPROCESSOR_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = PREPROCESSOR_NOT_SET;
}


void NeuralNetworkPreprocessing::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkPreprocessing)
  featurename_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_preprocessor();
}

bool NeuralNetworkPreprocessing::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkPreprocessing)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // string featureName = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_featurename()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->featurename().data(), this->featurename().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.NeuralNetworkPreprocessing.featureName"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_scaler()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
      case 11: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(90u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_meanimage()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkPreprocessing)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkPreprocessing)
  return false;
#undef DO_
}

void NeuralNetworkPreprocessing::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkPreprocessing)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // string featureName = 1;
  if (this->featurename().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->featurename().data(), this->featurename().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.NeuralNetworkPreprocessing.featureName");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      1, this->featurename(), output);
  }

  // .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
  if (has_scaler()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *preprocessor_.scaler_, output);
  }

  // .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
  if (has_meanimage()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      11, *preprocessor_.meanimage_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkPreprocessing)
}

size_t NeuralNetworkPreprocessing::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkPreprocessing)
  size_t total_size = 0;

  // string featureName = 1;
  if (this->featurename().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->featurename());
  }

  switch (preprocessor_case()) {
    // .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
    case kScaler: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *preprocessor_.scaler_);
      break;
    }
    // .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
    case kMeanImage: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *preprocessor_.meanimage_);
      break;
    }
    case PREPROCESSOR_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkPreprocessing::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkPreprocessing*>(&from));
}

void NeuralNetworkPreprocessing::MergeFrom(const NeuralNetworkPreprocessing& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkPreprocessing)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.featurename().size() > 0) {

    featurename_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.featurename_);
  }
  switch (from.preprocessor_case()) {
    case kScaler: {
      mutable_scaler()->::CoreML::Specification::NeuralNetworkImageScaler::MergeFrom(from.scaler());
      break;
    }
    case kMeanImage: {
      mutable_meanimage()->::CoreML::Specification::NeuralNetworkMeanImage::MergeFrom(from.meanimage());
      break;
    }
    case PREPROCESSOR_NOT_SET: {
      break;
    }
  }
}

void NeuralNetworkPreprocessing::CopyFrom(const NeuralNetworkPreprocessing& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkPreprocessing)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NeuralNetworkPreprocessing::IsInitialized() const {
  return true;
}

void NeuralNetworkPreprocessing::Swap(NeuralNetworkPreprocessing* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkPreprocessing::InternalSwap(NeuralNetworkPreprocessing* other) {
  featurename_.Swap(&other->featurename_);
  std::swap(preprocessor_, other->preprocessor_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkPreprocessing::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkPreprocessing";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkPreprocessing

// string featureName = 1;
void NeuralNetworkPreprocessing::clear_featurename() {
  featurename_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& NeuralNetworkPreprocessing::featurename() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
  return featurename_.GetNoArena();
}
void NeuralNetworkPreprocessing::set_featurename(const ::std::string& value) {
  
  featurename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
#if LANG_CXX11
void NeuralNetworkPreprocessing::set_featurename(::std::string&& value) {
  
  featurename_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
#endif
void NeuralNetworkPreprocessing::set_featurename(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  featurename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
void NeuralNetworkPreprocessing::set_featurename(const char* value, size_t size) {
  
  featurename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
::std::string* NeuralNetworkPreprocessing::mutable_featurename() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
  return featurename_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* NeuralNetworkPreprocessing::release_featurename() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
  
  return featurename_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void NeuralNetworkPreprocessing::set_allocated_featurename(::std::string* featurename) {
  if (featurename != NULL) {
    
  } else {
    
  }
  featurename_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), featurename);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}

// .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
bool NeuralNetworkPreprocessing::has_scaler() const {
  return preprocessor_case() == kScaler;
}
void NeuralNetworkPreprocessing::set_has_scaler() {
  _oneof_case_[0] = kScaler;
}
void NeuralNetworkPreprocessing::clear_scaler() {
  if (has_scaler()) {
    delete preprocessor_.scaler_;
    clear_has_preprocessor();
  }
}
 const ::CoreML::Specification::NeuralNetworkImageScaler& NeuralNetworkPreprocessing::scaler() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  return has_scaler()
      ? *preprocessor_.scaler_
      : ::CoreML::Specification::NeuralNetworkImageScaler::default_instance();
}
::CoreML::Specification::NeuralNetworkImageScaler* NeuralNetworkPreprocessing::mutable_scaler() {
  if (!has_scaler()) {
    clear_preprocessor();
    set_has_scaler();
    preprocessor_.scaler_ = new ::CoreML::Specification::NeuralNetworkImageScaler;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  return preprocessor_.scaler_;
}
::CoreML::Specification::NeuralNetworkImageScaler* NeuralNetworkPreprocessing::release_scaler() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  if (has_scaler()) {
    clear_has_preprocessor();
    ::CoreML::Specification::NeuralNetworkImageScaler* temp = preprocessor_.scaler_;
    preprocessor_.scaler_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkPreprocessing::set_allocated_scaler(::CoreML::Specification::NeuralNetworkImageScaler* scaler) {
  clear_preprocessor();
  if (scaler) {
    set_has_scaler();
    preprocessor_.scaler_ = scaler;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
}

// .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
bool NeuralNetworkPreprocessing::has_meanimage() const {
  return preprocessor_case() == kMeanImage;
}
void NeuralNetworkPreprocessing::set_has_meanimage() {
  _oneof_case_[0] = kMeanImage;
}
void NeuralNetworkPreprocessing::clear_meanimage() {
  if (has_meanimage()) {
    delete preprocessor_.meanimage_;
    clear_has_preprocessor();
  }
}
 const ::CoreML::Specification::NeuralNetworkMeanImage& NeuralNetworkPreprocessing::meanimage() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  return has_meanimage()
      ? *preprocessor_.meanimage_
      : ::CoreML::Specification::NeuralNetworkMeanImage::default_instance();
}
::CoreML::Specification::NeuralNetworkMeanImage* NeuralNetworkPreprocessing::mutable_meanimage() {
  if (!has_meanimage()) {
    clear_preprocessor();
    set_has_meanimage();
    preprocessor_.meanimage_ = new ::CoreML::Specification::NeuralNetworkMeanImage;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  return preprocessor_.meanimage_;
}
::CoreML::Specification::NeuralNetworkMeanImage* NeuralNetworkPreprocessing::release_meanimage() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  if (has_meanimage()) {
    clear_has_preprocessor();
    ::CoreML::Specification::NeuralNetworkMeanImage* temp = preprocessor_.meanimage_;
    preprocessor_.meanimage_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkPreprocessing::set_allocated_meanimage(::CoreML::Specification::NeuralNetworkMeanImage* meanimage) {
  clear_preprocessor();
  if (meanimage) {
    set_has_meanimage();
    preprocessor_.meanimage_ = meanimage;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
}

bool NeuralNetworkPreprocessing::has_preprocessor() const {
  return preprocessor_case() != PREPROCESSOR_NOT_SET;
}
void NeuralNetworkPreprocessing::clear_has_preprocessor() {
  _oneof_case_[0] = PREPROCESSOR_NOT_SET;
}
NeuralNetworkPreprocessing::PreprocessorCase NeuralNetworkPreprocessing::preprocessor_case() const {
  return NeuralNetworkPreprocessing::PreprocessorCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationReLU::ActivationReLU()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationReLU)
}
ActivationReLU::ActivationReLU(const ActivationReLU& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationReLU)
}

void ActivationReLU::SharedCtor() {
  _cached_size_ = 0;
}

ActivationReLU::~ActivationReLU() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationReLU)
  SharedDtor();
}

void ActivationReLU::SharedDtor() {
}

void ActivationReLU::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationReLU& ActivationReLU::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationReLU* ActivationReLU::New(::google::protobuf::Arena* arena) const {
  ActivationReLU* n = new ActivationReLU;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationReLU::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationReLU)
}

bool ActivationReLU::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationReLU)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationReLU)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationReLU)
  return false;
#undef DO_
}

void ActivationReLU::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationReLU)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationReLU)
}

size_t ActivationReLU::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationReLU)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationReLU::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationReLU*>(&from));
}

void ActivationReLU::MergeFrom(const ActivationReLU& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationReLU)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ActivationReLU::CopyFrom(const ActivationReLU& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationReLU)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationReLU::IsInitialized() const {
  return true;
}

void ActivationReLU::Swap(ActivationReLU* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationReLU::InternalSwap(ActivationReLU* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationReLU::GetTypeName() const {
  return "CoreML.Specification.ActivationReLU";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationReLU

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationLeakyReLU::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationLeakyReLU::ActivationLeakyReLU()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationLeakyReLU)
}
ActivationLeakyReLU::ActivationLeakyReLU(const ActivationLeakyReLU& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationLeakyReLU)
}

void ActivationLeakyReLU::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

ActivationLeakyReLU::~ActivationLeakyReLU() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationLeakyReLU)
  SharedDtor();
}

void ActivationLeakyReLU::SharedDtor() {
}

void ActivationLeakyReLU::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationLeakyReLU& ActivationLeakyReLU::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationLeakyReLU* ActivationLeakyReLU::New(::google::protobuf::Arena* arena) const {
  ActivationLeakyReLU* n = new ActivationLeakyReLU;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationLeakyReLU::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationLeakyReLU)
  alpha_ = 0;
}

bool ActivationLeakyReLU::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationLeakyReLU)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationLeakyReLU)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationLeakyReLU)
  return false;
#undef DO_
}

void ActivationLeakyReLU::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationLeakyReLU)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationLeakyReLU)
}

size_t ActivationLeakyReLU::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationLeakyReLU)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationLeakyReLU::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationLeakyReLU*>(&from));
}

void ActivationLeakyReLU::MergeFrom(const ActivationLeakyReLU& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationLeakyReLU)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void ActivationLeakyReLU::CopyFrom(const ActivationLeakyReLU& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationLeakyReLU)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationLeakyReLU::IsInitialized() const {
  return true;
}

void ActivationLeakyReLU::Swap(ActivationLeakyReLU* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationLeakyReLU::InternalSwap(ActivationLeakyReLU* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationLeakyReLU::GetTypeName() const {
  return "CoreML.Specification.ActivationLeakyReLU";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationLeakyReLU

// float alpha = 1;
void ActivationLeakyReLU::clear_alpha() {
  alpha_ = 0;
}
float ActivationLeakyReLU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationLeakyReLU.alpha)
  return alpha_;
}
void ActivationLeakyReLU::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationLeakyReLU.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationTanh::ActivationTanh()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationTanh)
}
ActivationTanh::ActivationTanh(const ActivationTanh& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationTanh)
}

void ActivationTanh::SharedCtor() {
  _cached_size_ = 0;
}

ActivationTanh::~ActivationTanh() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationTanh)
  SharedDtor();
}

void ActivationTanh::SharedDtor() {
}

void ActivationTanh::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationTanh& ActivationTanh::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationTanh* ActivationTanh::New(::google::protobuf::Arena* arena) const {
  ActivationTanh* n = new ActivationTanh;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationTanh::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationTanh)
}

bool ActivationTanh::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationTanh)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationTanh)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationTanh)
  return false;
#undef DO_
}

void ActivationTanh::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationTanh)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationTanh)
}

size_t ActivationTanh::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationTanh)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationTanh::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationTanh*>(&from));
}

void ActivationTanh::MergeFrom(const ActivationTanh& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationTanh)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ActivationTanh::CopyFrom(const ActivationTanh& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationTanh)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationTanh::IsInitialized() const {
  return true;
}

void ActivationTanh::Swap(ActivationTanh* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationTanh::InternalSwap(ActivationTanh* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationTanh::GetTypeName() const {
  return "CoreML.Specification.ActivationTanh";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationTanh

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationScaledTanh::kAlphaFieldNumber;
const int ActivationScaledTanh::kBetaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationScaledTanh::ActivationScaledTanh()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationScaledTanh)
}
ActivationScaledTanh::ActivationScaledTanh(const ActivationScaledTanh& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&alpha_, &from.alpha_,
    reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationScaledTanh)
}

void ActivationScaledTanh::SharedCtor() {
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  _cached_size_ = 0;
}

ActivationScaledTanh::~ActivationScaledTanh() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationScaledTanh)
  SharedDtor();
}

void ActivationScaledTanh::SharedDtor() {
}

void ActivationScaledTanh::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationScaledTanh& ActivationScaledTanh::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationScaledTanh* ActivationScaledTanh::New(::google::protobuf::Arena* arena) const {
  ActivationScaledTanh* n = new ActivationScaledTanh;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationScaledTanh::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationScaledTanh)
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
}

bool ActivationScaledTanh::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationScaledTanh)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float beta = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &beta_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationScaledTanh)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationScaledTanh)
  return false;
#undef DO_
}

void ActivationScaledTanh::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationScaledTanh)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // float beta = 2;
  if (this->beta() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->beta(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationScaledTanh)
}

size_t ActivationScaledTanh::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationScaledTanh)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  // float beta = 2;
  if (this->beta() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationScaledTanh::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationScaledTanh*>(&from));
}

void ActivationScaledTanh::MergeFrom(const ActivationScaledTanh& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationScaledTanh)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
  if (from.beta() != 0) {
    set_beta(from.beta());
  }
}

void ActivationScaledTanh::CopyFrom(const ActivationScaledTanh& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationScaledTanh)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationScaledTanh::IsInitialized() const {
  return true;
}

void ActivationScaledTanh::Swap(ActivationScaledTanh* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationScaledTanh::InternalSwap(ActivationScaledTanh* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(beta_, other->beta_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationScaledTanh::GetTypeName() const {
  return "CoreML.Specification.ActivationScaledTanh";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationScaledTanh

// float alpha = 1;
void ActivationScaledTanh::clear_alpha() {
  alpha_ = 0;
}
float ActivationScaledTanh::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationScaledTanh.alpha)
  return alpha_;
}
void ActivationScaledTanh::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationScaledTanh.alpha)
}

// float beta = 2;
void ActivationScaledTanh::clear_beta() {
  beta_ = 0;
}
float ActivationScaledTanh::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationScaledTanh.beta)
  return beta_;
}
void ActivationScaledTanh::set_beta(float value) {
  
  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationScaledTanh.beta)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationSigmoid::ActivationSigmoid()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationSigmoid)
}
ActivationSigmoid::ActivationSigmoid(const ActivationSigmoid& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationSigmoid)
}

void ActivationSigmoid::SharedCtor() {
  _cached_size_ = 0;
}

ActivationSigmoid::~ActivationSigmoid() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationSigmoid)
  SharedDtor();
}

void ActivationSigmoid::SharedDtor() {
}

void ActivationSigmoid::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationSigmoid& ActivationSigmoid::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationSigmoid* ActivationSigmoid::New(::google::protobuf::Arena* arena) const {
  ActivationSigmoid* n = new ActivationSigmoid;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationSigmoid::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationSigmoid)
}

bool ActivationSigmoid::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationSigmoid)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationSigmoid)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationSigmoid)
  return false;
#undef DO_
}

void ActivationSigmoid::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationSigmoid)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationSigmoid)
}

size_t ActivationSigmoid::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationSigmoid)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationSigmoid::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationSigmoid*>(&from));
}

void ActivationSigmoid::MergeFrom(const ActivationSigmoid& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationSigmoid)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ActivationSigmoid::CopyFrom(const ActivationSigmoid& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationSigmoid)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationSigmoid::IsInitialized() const {
  return true;
}

void ActivationSigmoid::Swap(ActivationSigmoid* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationSigmoid::InternalSwap(ActivationSigmoid* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationSigmoid::GetTypeName() const {
  return "CoreML.Specification.ActivationSigmoid";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationSigmoid

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationLinear::kAlphaFieldNumber;
const int ActivationLinear::kBetaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationLinear::ActivationLinear()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationLinear)
}
ActivationLinear::ActivationLinear(const ActivationLinear& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&alpha_, &from.alpha_,
    reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationLinear)
}

void ActivationLinear::SharedCtor() {
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  _cached_size_ = 0;
}

ActivationLinear::~ActivationLinear() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationLinear)
  SharedDtor();
}

void ActivationLinear::SharedDtor() {
}

void ActivationLinear::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationLinear& ActivationLinear::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationLinear* ActivationLinear::New(::google::protobuf::Arena* arena) const {
  ActivationLinear* n = new ActivationLinear;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationLinear::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationLinear)
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
}

bool ActivationLinear::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationLinear)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float beta = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &beta_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationLinear)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationLinear)
  return false;
#undef DO_
}

void ActivationLinear::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationLinear)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // float beta = 2;
  if (this->beta() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->beta(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationLinear)
}

size_t ActivationLinear::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationLinear)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  // float beta = 2;
  if (this->beta() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationLinear::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationLinear*>(&from));
}

void ActivationLinear::MergeFrom(const ActivationLinear& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationLinear)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
  if (from.beta() != 0) {
    set_beta(from.beta());
  }
}

void ActivationLinear::CopyFrom(const ActivationLinear& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationLinear)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationLinear::IsInitialized() const {
  return true;
}

void ActivationLinear::Swap(ActivationLinear* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationLinear::InternalSwap(ActivationLinear* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(beta_, other->beta_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationLinear::GetTypeName() const {
  return "CoreML.Specification.ActivationLinear";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationLinear

// float alpha = 1;
void ActivationLinear::clear_alpha() {
  alpha_ = 0;
}
float ActivationLinear::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationLinear.alpha)
  return alpha_;
}
void ActivationLinear::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationLinear.alpha)
}

// float beta = 2;
void ActivationLinear::clear_beta() {
  beta_ = 0;
}
float ActivationLinear::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationLinear.beta)
  return beta_;
}
void ActivationLinear::set_beta(float value) {
  
  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationLinear.beta)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationSigmoidHard::kAlphaFieldNumber;
const int ActivationSigmoidHard::kBetaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationSigmoidHard::ActivationSigmoidHard()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationSigmoidHard)
}
ActivationSigmoidHard::ActivationSigmoidHard(const ActivationSigmoidHard& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&alpha_, &from.alpha_,
    reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationSigmoidHard)
}

void ActivationSigmoidHard::SharedCtor() {
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  _cached_size_ = 0;
}

ActivationSigmoidHard::~ActivationSigmoidHard() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationSigmoidHard)
  SharedDtor();
}

void ActivationSigmoidHard::SharedDtor() {
}

void ActivationSigmoidHard::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationSigmoidHard& ActivationSigmoidHard::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationSigmoidHard* ActivationSigmoidHard::New(::google::protobuf::Arena* arena) const {
  ActivationSigmoidHard* n = new ActivationSigmoidHard;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationSigmoidHard::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationSigmoidHard)
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
}

bool ActivationSigmoidHard::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationSigmoidHard)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float beta = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &beta_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationSigmoidHard)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationSigmoidHard)
  return false;
#undef DO_
}

void ActivationSigmoidHard::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationSigmoidHard)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // float beta = 2;
  if (this->beta() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->beta(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationSigmoidHard)
}

size_t ActivationSigmoidHard::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationSigmoidHard)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  // float beta = 2;
  if (this->beta() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationSigmoidHard::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationSigmoidHard*>(&from));
}

void ActivationSigmoidHard::MergeFrom(const ActivationSigmoidHard& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationSigmoidHard)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
  if (from.beta() != 0) {
    set_beta(from.beta());
  }
}

void ActivationSigmoidHard::CopyFrom(const ActivationSigmoidHard& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationSigmoidHard)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationSigmoidHard::IsInitialized() const {
  return true;
}

void ActivationSigmoidHard::Swap(ActivationSigmoidHard* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationSigmoidHard::InternalSwap(ActivationSigmoidHard* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(beta_, other->beta_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationSigmoidHard::GetTypeName() const {
  return "CoreML.Specification.ActivationSigmoidHard";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationSigmoidHard

// float alpha = 1;
void ActivationSigmoidHard::clear_alpha() {
  alpha_ = 0;
}
float ActivationSigmoidHard::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationSigmoidHard.alpha)
  return alpha_;
}
void ActivationSigmoidHard::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationSigmoidHard.alpha)
}

// float beta = 2;
void ActivationSigmoidHard::clear_beta() {
  beta_ = 0;
}
float ActivationSigmoidHard::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationSigmoidHard.beta)
  return beta_;
}
void ActivationSigmoidHard::set_beta(float value) {
  
  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationSigmoidHard.beta)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationPReLU::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationPReLU::ActivationPReLU()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationPReLU)
}
ActivationPReLU::ActivationPReLU(const ActivationPReLU& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_alpha()) {
    alpha_ = new ::CoreML::Specification::WeightParams(*from.alpha_);
  } else {
    alpha_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationPReLU)
}

void ActivationPReLU::SharedCtor() {
  alpha_ = NULL;
  _cached_size_ = 0;
}

ActivationPReLU::~ActivationPReLU() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationPReLU)
  SharedDtor();
}

void ActivationPReLU::SharedDtor() {
  if (this != internal_default_instance()) {
    delete alpha_;
  }
}

void ActivationPReLU::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationPReLU& ActivationPReLU::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationPReLU* ActivationPReLU::New(::google::protobuf::Arena* arena) const {
  ActivationPReLU* n = new ActivationPReLU;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationPReLU::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationPReLU)
  if (GetArenaNoVirtual() == NULL && alpha_ != NULL) {
    delete alpha_;
  }
  alpha_ = NULL;
}

bool ActivationPReLU::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationPReLU)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.WeightParams alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_alpha()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationPReLU)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationPReLU)
  return false;
#undef DO_
}

void ActivationPReLU::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationPReLU)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.WeightParams alpha = 1;
  if (this->has_alpha()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->alpha_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationPReLU)
}

size_t ActivationPReLU::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationPReLU)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams alpha = 1;
  if (this->has_alpha()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->alpha_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationPReLU::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationPReLU*>(&from));
}

void ActivationPReLU::MergeFrom(const ActivationPReLU& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationPReLU)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_alpha()) {
    mutable_alpha()->::CoreML::Specification::WeightParams::MergeFrom(from.alpha());
  }
}

void ActivationPReLU::CopyFrom(const ActivationPReLU& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationPReLU)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationPReLU::IsInitialized() const {
  return true;
}

void ActivationPReLU::Swap(ActivationPReLU* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationPReLU::InternalSwap(ActivationPReLU* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationPReLU::GetTypeName() const {
  return "CoreML.Specification.ActivationPReLU";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationPReLU

// .CoreML.Specification.WeightParams alpha = 1;
bool ActivationPReLU::has_alpha() const {
  return this != internal_default_instance() && alpha_ != NULL;
}
void ActivationPReLU::clear_alpha() {
  if (GetArenaNoVirtual() == NULL && alpha_ != NULL) delete alpha_;
  alpha_ = NULL;
}
const ::CoreML::Specification::WeightParams& ActivationPReLU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationPReLU.alpha)
  return alpha_ != NULL ? *alpha_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ActivationPReLU::mutable_alpha() {
  
  if (alpha_ == NULL) {
    alpha_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationPReLU.alpha)
  return alpha_;
}
::CoreML::Specification::WeightParams* ActivationPReLU::release_alpha() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationPReLU.alpha)
  
  ::CoreML::Specification::WeightParams* temp = alpha_;
  alpha_ = NULL;
  return temp;
}
void ActivationPReLU::set_allocated_alpha(::CoreML::Specification::WeightParams* alpha) {
  delete alpha_;
  alpha_ = alpha;
  if (alpha) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationPReLU.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationELU::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationELU::ActivationELU()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationELU)
}
ActivationELU::ActivationELU(const ActivationELU& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationELU)
}

void ActivationELU::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

ActivationELU::~ActivationELU() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationELU)
  SharedDtor();
}

void ActivationELU::SharedDtor() {
}

void ActivationELU::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationELU& ActivationELU::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationELU* ActivationELU::New(::google::protobuf::Arena* arena) const {
  ActivationELU* n = new ActivationELU;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationELU::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationELU)
  alpha_ = 0;
}

bool ActivationELU::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationELU)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationELU)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationELU)
  return false;
#undef DO_
}

void ActivationELU::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationELU)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationELU)
}

size_t ActivationELU::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationELU)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationELU::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationELU*>(&from));
}

void ActivationELU::MergeFrom(const ActivationELU& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationELU)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void ActivationELU::CopyFrom(const ActivationELU& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationELU)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationELU::IsInitialized() const {
  return true;
}

void ActivationELU::Swap(ActivationELU* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationELU::InternalSwap(ActivationELU* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationELU::GetTypeName() const {
  return "CoreML.Specification.ActivationELU";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationELU

// float alpha = 1;
void ActivationELU::clear_alpha() {
  alpha_ = 0;
}
float ActivationELU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationELU.alpha)
  return alpha_;
}
void ActivationELU::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationELU.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationThresholdedReLU::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationThresholdedReLU::ActivationThresholdedReLU()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationThresholdedReLU)
}
ActivationThresholdedReLU::ActivationThresholdedReLU(const ActivationThresholdedReLU& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationThresholdedReLU)
}

void ActivationThresholdedReLU::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

ActivationThresholdedReLU::~ActivationThresholdedReLU() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationThresholdedReLU)
  SharedDtor();
}

void ActivationThresholdedReLU::SharedDtor() {
}

void ActivationThresholdedReLU::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationThresholdedReLU& ActivationThresholdedReLU::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationThresholdedReLU* ActivationThresholdedReLU::New(::google::protobuf::Arena* arena) const {
  ActivationThresholdedReLU* n = new ActivationThresholdedReLU;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationThresholdedReLU::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationThresholdedReLU)
  alpha_ = 0;
}

bool ActivationThresholdedReLU::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationThresholdedReLU)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationThresholdedReLU)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationThresholdedReLU)
  return false;
#undef DO_
}

void ActivationThresholdedReLU::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationThresholdedReLU)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationThresholdedReLU)
}

size_t ActivationThresholdedReLU::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationThresholdedReLU)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationThresholdedReLU::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationThresholdedReLU*>(&from));
}

void ActivationThresholdedReLU::MergeFrom(const ActivationThresholdedReLU& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationThresholdedReLU)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void ActivationThresholdedReLU::CopyFrom(const ActivationThresholdedReLU& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationThresholdedReLU)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationThresholdedReLU::IsInitialized() const {
  return true;
}

void ActivationThresholdedReLU::Swap(ActivationThresholdedReLU* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationThresholdedReLU::InternalSwap(ActivationThresholdedReLU* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationThresholdedReLU::GetTypeName() const {
  return "CoreML.Specification.ActivationThresholdedReLU";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationThresholdedReLU

// float alpha = 1;
void ActivationThresholdedReLU::clear_alpha() {
  alpha_ = 0;
}
float ActivationThresholdedReLU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationThresholdedReLU.alpha)
  return alpha_;
}
void ActivationThresholdedReLU::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationThresholdedReLU.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationSoftsign::ActivationSoftsign()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationSoftsign)
}
ActivationSoftsign::ActivationSoftsign(const ActivationSoftsign& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationSoftsign)
}

void ActivationSoftsign::SharedCtor() {
  _cached_size_ = 0;
}

ActivationSoftsign::~ActivationSoftsign() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationSoftsign)
  SharedDtor();
}

void ActivationSoftsign::SharedDtor() {
}

void ActivationSoftsign::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationSoftsign& ActivationSoftsign::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationSoftsign* ActivationSoftsign::New(::google::protobuf::Arena* arena) const {
  ActivationSoftsign* n = new ActivationSoftsign;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationSoftsign::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationSoftsign)
}

bool ActivationSoftsign::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationSoftsign)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationSoftsign)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationSoftsign)
  return false;
#undef DO_
}

void ActivationSoftsign::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationSoftsign)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationSoftsign)
}

size_t ActivationSoftsign::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationSoftsign)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationSoftsign::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationSoftsign*>(&from));
}

void ActivationSoftsign::MergeFrom(const ActivationSoftsign& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationSoftsign)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ActivationSoftsign::CopyFrom(const ActivationSoftsign& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationSoftsign)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationSoftsign::IsInitialized() const {
  return true;
}

void ActivationSoftsign::Swap(ActivationSoftsign* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationSoftsign::InternalSwap(ActivationSoftsign* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationSoftsign::GetTypeName() const {
  return "CoreML.Specification.ActivationSoftsign";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationSoftsign

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationSoftplus::ActivationSoftplus()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationSoftplus)
}
ActivationSoftplus::ActivationSoftplus(const ActivationSoftplus& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationSoftplus)
}

void ActivationSoftplus::SharedCtor() {
  _cached_size_ = 0;
}

ActivationSoftplus::~ActivationSoftplus() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationSoftplus)
  SharedDtor();
}

void ActivationSoftplus::SharedDtor() {
}

void ActivationSoftplus::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationSoftplus& ActivationSoftplus::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationSoftplus* ActivationSoftplus::New(::google::protobuf::Arena* arena) const {
  ActivationSoftplus* n = new ActivationSoftplus;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationSoftplus::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationSoftplus)
}

bool ActivationSoftplus::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationSoftplus)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationSoftplus)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationSoftplus)
  return false;
#undef DO_
}

void ActivationSoftplus::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationSoftplus)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationSoftplus)
}

size_t ActivationSoftplus::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationSoftplus)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationSoftplus::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationSoftplus*>(&from));
}

void ActivationSoftplus::MergeFrom(const ActivationSoftplus& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationSoftplus)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ActivationSoftplus::CopyFrom(const ActivationSoftplus& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationSoftplus)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationSoftplus::IsInitialized() const {
  return true;
}

void ActivationSoftplus::Swap(ActivationSoftplus* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationSoftplus::InternalSwap(ActivationSoftplus* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationSoftplus::GetTypeName() const {
  return "CoreML.Specification.ActivationSoftplus";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationSoftplus

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationParametricSoftplus::kAlphaFieldNumber;
const int ActivationParametricSoftplus::kBetaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationParametricSoftplus::ActivationParametricSoftplus()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationParametricSoftplus)
}
ActivationParametricSoftplus::ActivationParametricSoftplus(const ActivationParametricSoftplus& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_alpha()) {
    alpha_ = new ::CoreML::Specification::WeightParams(*from.alpha_);
  } else {
    alpha_ = NULL;
  }
  if (from.has_beta()) {
    beta_ = new ::CoreML::Specification::WeightParams(*from.beta_);
  } else {
    beta_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationParametricSoftplus)
}

void ActivationParametricSoftplus::SharedCtor() {
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  _cached_size_ = 0;
}

ActivationParametricSoftplus::~ActivationParametricSoftplus() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationParametricSoftplus)
  SharedDtor();
}

void ActivationParametricSoftplus::SharedDtor() {
  if (this != internal_default_instance()) {
    delete alpha_;
  }
  if (this != internal_default_instance()) {
    delete beta_;
  }
}

void ActivationParametricSoftplus::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationParametricSoftplus& ActivationParametricSoftplus::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationParametricSoftplus* ActivationParametricSoftplus::New(::google::protobuf::Arena* arena) const {
  ActivationParametricSoftplus* n = new ActivationParametricSoftplus;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationParametricSoftplus::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationParametricSoftplus)
  if (GetArenaNoVirtual() == NULL && alpha_ != NULL) {
    delete alpha_;
  }
  alpha_ = NULL;
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) {
    delete beta_;
  }
  beta_ = NULL;
}

bool ActivationParametricSoftplus::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationParametricSoftplus)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.WeightParams alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_alpha()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams beta = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_beta()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationParametricSoftplus)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationParametricSoftplus)
  return false;
#undef DO_
}

void ActivationParametricSoftplus::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationParametricSoftplus)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.WeightParams alpha = 1;
  if (this->has_alpha()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->alpha_, output);
  }

  // .CoreML.Specification.WeightParams beta = 2;
  if (this->has_beta()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->beta_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationParametricSoftplus)
}

size_t ActivationParametricSoftplus::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationParametricSoftplus)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams alpha = 1;
  if (this->has_alpha()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->alpha_);
  }

  // .CoreML.Specification.WeightParams beta = 2;
  if (this->has_beta()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->beta_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationParametricSoftplus::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationParametricSoftplus*>(&from));
}

void ActivationParametricSoftplus::MergeFrom(const ActivationParametricSoftplus& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationParametricSoftplus)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_alpha()) {
    mutable_alpha()->::CoreML::Specification::WeightParams::MergeFrom(from.alpha());
  }
  if (from.has_beta()) {
    mutable_beta()->::CoreML::Specification::WeightParams::MergeFrom(from.beta());
  }
}

void ActivationParametricSoftplus::CopyFrom(const ActivationParametricSoftplus& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationParametricSoftplus)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationParametricSoftplus::IsInitialized() const {
  return true;
}

void ActivationParametricSoftplus::Swap(ActivationParametricSoftplus* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationParametricSoftplus::InternalSwap(ActivationParametricSoftplus* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(beta_, other->beta_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationParametricSoftplus::GetTypeName() const {
  return "CoreML.Specification.ActivationParametricSoftplus";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationParametricSoftplus

// .CoreML.Specification.WeightParams alpha = 1;
bool ActivationParametricSoftplus::has_alpha() const {
  return this != internal_default_instance() && alpha_ != NULL;
}
void ActivationParametricSoftplus::clear_alpha() {
  if (GetArenaNoVirtual() == NULL && alpha_ != NULL) delete alpha_;
  alpha_ = NULL;
}
const ::CoreML::Specification::WeightParams& ActivationParametricSoftplus::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParametricSoftplus.alpha)
  return alpha_ != NULL ? *alpha_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ActivationParametricSoftplus::mutable_alpha() {
  
  if (alpha_ == NULL) {
    alpha_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParametricSoftplus.alpha)
  return alpha_;
}
::CoreML::Specification::WeightParams* ActivationParametricSoftplus::release_alpha() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParametricSoftplus.alpha)
  
  ::CoreML::Specification::WeightParams* temp = alpha_;
  alpha_ = NULL;
  return temp;
}
void ActivationParametricSoftplus::set_allocated_alpha(::CoreML::Specification::WeightParams* alpha) {
  delete alpha_;
  alpha_ = alpha;
  if (alpha) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParametricSoftplus.alpha)
}

// .CoreML.Specification.WeightParams beta = 2;
bool ActivationParametricSoftplus::has_beta() const {
  return this != internal_default_instance() && beta_ != NULL;
}
void ActivationParametricSoftplus::clear_beta() {
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) delete beta_;
  beta_ = NULL;
}
const ::CoreML::Specification::WeightParams& ActivationParametricSoftplus::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParametricSoftplus.beta)
  return beta_ != NULL ? *beta_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ActivationParametricSoftplus::mutable_beta() {
  
  if (beta_ == NULL) {
    beta_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParametricSoftplus.beta)
  return beta_;
}
::CoreML::Specification::WeightParams* ActivationParametricSoftplus::release_beta() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParametricSoftplus.beta)
  
  ::CoreML::Specification::WeightParams* temp = beta_;
  beta_ = NULL;
  return temp;
}
void ActivationParametricSoftplus::set_allocated_beta(::CoreML::Specification::WeightParams* beta) {
  delete beta_;
  beta_ = beta;
  if (beta) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParametricSoftplus.beta)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationParams::kLinearFieldNumber;
const int ActivationParams::kReLUFieldNumber;
const int ActivationParams::kLeakyReLUFieldNumber;
const int ActivationParams::kThresholdedReLUFieldNumber;
const int ActivationParams::kPReLUFieldNumber;
const int ActivationParams::kTanhFieldNumber;
const int ActivationParams::kScaledTanhFieldNumber;
const int ActivationParams::kSigmoidFieldNumber;
const int ActivationParams::kSigmoidHardFieldNumber;
const int ActivationParams::kELUFieldNumber;
const int ActivationParams::kSoftsignFieldNumber;
const int ActivationParams::kSoftplusFieldNumber;
const int ActivationParams::kParametricSoftplusFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationParams::ActivationParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationParams)
}
ActivationParams::ActivationParams(const ActivationParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  clear_has_NonlinearityType();
  switch (from.NonlinearityType_case()) {
    case kLinear: {
      mutable_linear()->::CoreML::Specification::ActivationLinear::MergeFrom(from.linear());
      break;
    }
    case kReLU: {
      mutable_relu()->::CoreML::Specification::ActivationReLU::MergeFrom(from.relu());
      break;
    }
    case kLeakyReLU: {
      mutable_leakyrelu()->::CoreML::Specification::ActivationLeakyReLU::MergeFrom(from.leakyrelu());
      break;
    }
    case kThresholdedReLU: {
      mutable_thresholdedrelu()->::CoreML::Specification::ActivationThresholdedReLU::MergeFrom(from.thresholdedrelu());
      break;
    }
    case kPReLU: {
      mutable_prelu()->::CoreML::Specification::ActivationPReLU::MergeFrom(from.prelu());
      break;
    }
    case kTanh: {
      mutable_tanh()->::CoreML::Specification::ActivationTanh::MergeFrom(from.tanh());
      break;
    }
    case kScaledTanh: {
      mutable_scaledtanh()->::CoreML::Specification::ActivationScaledTanh::MergeFrom(from.scaledtanh());
      break;
    }
    case kSigmoid: {
      mutable_sigmoid()->::CoreML::Specification::ActivationSigmoid::MergeFrom(from.sigmoid());
      break;
    }
    case kSigmoidHard: {
      mutable_sigmoidhard()->::CoreML::Specification::ActivationSigmoidHard::MergeFrom(from.sigmoidhard());
      break;
    }
    case kELU: {
      mutable_elu()->::CoreML::Specification::ActivationELU::MergeFrom(from.elu());
      break;
    }
    case kSoftsign: {
      mutable_softsign()->::CoreML::Specification::ActivationSoftsign::MergeFrom(from.softsign());
      break;
    }
    case kSoftplus: {
      mutable_softplus()->::CoreML::Specification::ActivationSoftplus::MergeFrom(from.softplus());
      break;
    }
    case kParametricSoftplus: {
      mutable_parametricsoftplus()->::CoreML::Specification::ActivationParametricSoftplus::MergeFrom(from.parametricsoftplus());
      break;
    }
    case NONLINEARITYTYPE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationParams)
}

void ActivationParams::SharedCtor() {
  clear_has_NonlinearityType();
  _cached_size_ = 0;
}

ActivationParams::~ActivationParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationParams)
  SharedDtor();
}

void ActivationParams::SharedDtor() {
  if (has_NonlinearityType()) {
    clear_NonlinearityType();
  }
}

void ActivationParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationParams& ActivationParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationParams* ActivationParams::New(::google::protobuf::Arena* arena) const {
  ActivationParams* n = new ActivationParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationParams::clear_NonlinearityType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.ActivationParams)
  switch (NonlinearityType_case()) {
    case kLinear: {
      delete NonlinearityType_.linear_;
      break;
    }
    case kReLU: {
      delete NonlinearityType_.relu_;
      break;
    }
    case kLeakyReLU: {
      delete NonlinearityType_.leakyrelu_;
      break;
    }
    case kThresholdedReLU: {
      delete NonlinearityType_.thresholdedrelu_;
      break;
    }
    case kPReLU: {
      delete NonlinearityType_.prelu_;
      break;
    }
    case kTanh: {
      delete NonlinearityType_.tanh_;
      break;
    }
    case kScaledTanh: {
      delete NonlinearityType_.scaledtanh_;
      break;
    }
    case kSigmoid: {
      delete NonlinearityType_.sigmoid_;
      break;
    }
    case kSigmoidHard: {
      delete NonlinearityType_.sigmoidhard_;
      break;
    }
    case kELU: {
      delete NonlinearityType_.elu_;
      break;
    }
    case kSoftsign: {
      delete NonlinearityType_.softsign_;
      break;
    }
    case kSoftplus: {
      delete NonlinearityType_.softplus_;
      break;
    }
    case kParametricSoftplus: {
      delete NonlinearityType_.parametricsoftplus_;
      break;
    }
    case NONLINEARITYTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = NONLINEARITYTYPE_NOT_SET;
}


void ActivationParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationParams)
  clear_NonlinearityType();
}

bool ActivationParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.ActivationLinear linear = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(42u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_linear()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationReLU ReLU = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_relu()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
      case 15: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(122u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_leakyrelu()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_thresholdedrelu()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationPReLU PReLU = 25;
      case 25: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(202u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_prelu()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationTanh tanh = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_tanh()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
      case 31: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(250u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_scaledtanh()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationSigmoid sigmoid = 40;
      case 40: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_sigmoid()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
      case 41: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(330u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_sigmoidhard()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationELU ELU = 50;
      case 50: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(402u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_elu()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationSoftsign softsign = 60;
      case 60: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(482u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_softsign()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationSoftplus softplus = 70;
      case 70: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(562u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_softplus()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
      case 71: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(570u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_parametricsoftplus()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationParams)
  return false;
#undef DO_
}

void ActivationParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.ActivationLinear linear = 5;
  if (has_linear()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      5, *NonlinearityType_.linear_, output);
  }

  // .CoreML.Specification.ActivationReLU ReLU = 10;
  if (has_relu()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *NonlinearityType_.relu_, output);
  }

  // .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
  if (has_leakyrelu()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      15, *NonlinearityType_.leakyrelu_, output);
  }

  // .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
  if (has_thresholdedrelu()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *NonlinearityType_.thresholdedrelu_, output);
  }

  // .CoreML.Specification.ActivationPReLU PReLU = 25;
  if (has_prelu()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      25, *NonlinearityType_.prelu_, output);
  }

  // .CoreML.Specification.ActivationTanh tanh = 30;
  if (has_tanh()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      30, *NonlinearityType_.tanh_, output);
  }

  // .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
  if (has_scaledtanh()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      31, *NonlinearityType_.scaledtanh_, output);
  }

  // .CoreML.Specification.ActivationSigmoid sigmoid = 40;
  if (has_sigmoid()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      40, *NonlinearityType_.sigmoid_, output);
  }

  // .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
  if (has_sigmoidhard()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      41, *NonlinearityType_.sigmoidhard_, output);
  }

  // .CoreML.Specification.ActivationELU ELU = 50;
  if (has_elu()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      50, *NonlinearityType_.elu_, output);
  }

  // .CoreML.Specification.ActivationSoftsign softsign = 60;
  if (has_softsign()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      60, *NonlinearityType_.softsign_, output);
  }

  // .CoreML.Specification.ActivationSoftplus softplus = 70;
  if (has_softplus()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      70, *NonlinearityType_.softplus_, output);
  }

  // .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
  if (has_parametricsoftplus()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      71, *NonlinearityType_.parametricsoftplus_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationParams)
}

size_t ActivationParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationParams)
  size_t total_size = 0;

  switch (NonlinearityType_case()) {
    // .CoreML.Specification.ActivationLinear linear = 5;
    case kLinear: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.linear_);
      break;
    }
    // .CoreML.Specification.ActivationReLU ReLU = 10;
    case kReLU: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.relu_);
      break;
    }
    // .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
    case kLeakyReLU: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.leakyrelu_);
      break;
    }
    // .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
    case kThresholdedReLU: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.thresholdedrelu_);
      break;
    }
    // .CoreML.Specification.ActivationPReLU PReLU = 25;
    case kPReLU: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.prelu_);
      break;
    }
    // .CoreML.Specification.ActivationTanh tanh = 30;
    case kTanh: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.tanh_);
      break;
    }
    // .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
    case kScaledTanh: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.scaledtanh_);
      break;
    }
    // .CoreML.Specification.ActivationSigmoid sigmoid = 40;
    case kSigmoid: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.sigmoid_);
      break;
    }
    // .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
    case kSigmoidHard: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.sigmoidhard_);
      break;
    }
    // .CoreML.Specification.ActivationELU ELU = 50;
    case kELU: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.elu_);
      break;
    }
    // .CoreML.Specification.ActivationSoftsign softsign = 60;
    case kSoftsign: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.softsign_);
      break;
    }
    // .CoreML.Specification.ActivationSoftplus softplus = 70;
    case kSoftplus: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.softplus_);
      break;
    }
    // .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
    case kParametricSoftplus: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.parametricsoftplus_);
      break;
    }
    case NONLINEARITYTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationParams*>(&from));
}

void ActivationParams::MergeFrom(const ActivationParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  switch (from.NonlinearityType_case()) {
    case kLinear: {
      mutable_linear()->::CoreML::Specification::ActivationLinear::MergeFrom(from.linear());
      break;
    }
    case kReLU: {
      mutable_relu()->::CoreML::Specification::ActivationReLU::MergeFrom(from.relu());
      break;
    }
    case kLeakyReLU: {
      mutable_leakyrelu()->::CoreML::Specification::ActivationLeakyReLU::MergeFrom(from.leakyrelu());
      break;
    }
    case kThresholdedReLU: {
      mutable_thresholdedrelu()->::CoreML::Specification::ActivationThresholdedReLU::MergeFrom(from.thresholdedrelu());
      break;
    }
    case kPReLU: {
      mutable_prelu()->::CoreML::Specification::ActivationPReLU::MergeFrom(from.prelu());
      break;
    }
    case kTanh: {
      mutable_tanh()->::CoreML::Specification::ActivationTanh::MergeFrom(from.tanh());
      break;
    }
    case kScaledTanh: {
      mutable_scaledtanh()->::CoreML::Specification::ActivationScaledTanh::MergeFrom(from.scaledtanh());
      break;
    }
    case kSigmoid: {
      mutable_sigmoid()->::CoreML::Specification::ActivationSigmoid::MergeFrom(from.sigmoid());
      break;
    }
    case kSigmoidHard: {
      mutable_sigmoidhard()->::CoreML::Specification::ActivationSigmoidHard::MergeFrom(from.sigmoidhard());
      break;
    }
    case kELU: {
      mutable_elu()->::CoreML::Specification::ActivationELU::MergeFrom(from.elu());
      break;
    }
    case kSoftsign: {
      mutable_softsign()->::CoreML::Specification::ActivationSoftsign::MergeFrom(from.softsign());
      break;
    }
    case kSoftplus: {
      mutable_softplus()->::CoreML::Specification::ActivationSoftplus::MergeFrom(from.softplus());
      break;
    }
    case kParametricSoftplus: {
      mutable_parametricsoftplus()->::CoreML::Specification::ActivationParametricSoftplus::MergeFrom(from.parametricsoftplus());
      break;
    }
    case NONLINEARITYTYPE_NOT_SET: {
      break;
    }
  }
}

void ActivationParams::CopyFrom(const ActivationParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationParams::IsInitialized() const {
  return true;
}

void ActivationParams::Swap(ActivationParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationParams::InternalSwap(ActivationParams* other) {
  std::swap(NonlinearityType_, other->NonlinearityType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationParams::GetTypeName() const {
  return "CoreML.Specification.ActivationParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationParams

// .CoreML.Specification.ActivationLinear linear = 5;
bool ActivationParams::has_linear() const {
  return NonlinearityType_case() == kLinear;
}
void ActivationParams::set_has_linear() {
  _oneof_case_[0] = kLinear;
}
void ActivationParams::clear_linear() {
  if (has_linear()) {
    delete NonlinearityType_.linear_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationLinear& ActivationParams::linear() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.linear)
  return has_linear()
      ? *NonlinearityType_.linear_
      : ::CoreML::Specification::ActivationLinear::default_instance();
}
::CoreML::Specification::ActivationLinear* ActivationParams::mutable_linear() {
  if (!has_linear()) {
    clear_NonlinearityType();
    set_has_linear();
    NonlinearityType_.linear_ = new ::CoreML::Specification::ActivationLinear;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.linear)
  return NonlinearityType_.linear_;
}
::CoreML::Specification::ActivationLinear* ActivationParams::release_linear() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.linear)
  if (has_linear()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationLinear* temp = NonlinearityType_.linear_;
    NonlinearityType_.linear_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_linear(::CoreML::Specification::ActivationLinear* linear) {
  clear_NonlinearityType();
  if (linear) {
    set_has_linear();
    NonlinearityType_.linear_ = linear;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.linear)
}

// .CoreML.Specification.ActivationReLU ReLU = 10;
bool ActivationParams::has_relu() const {
  return NonlinearityType_case() == kReLU;
}
void ActivationParams::set_has_relu() {
  _oneof_case_[0] = kReLU;
}
void ActivationParams::clear_relu() {
  if (has_relu()) {
    delete NonlinearityType_.relu_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationReLU& ActivationParams::relu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.ReLU)
  return has_relu()
      ? *NonlinearityType_.relu_
      : ::CoreML::Specification::ActivationReLU::default_instance();
}
::CoreML::Specification::ActivationReLU* ActivationParams::mutable_relu() {
  if (!has_relu()) {
    clear_NonlinearityType();
    set_has_relu();
    NonlinearityType_.relu_ = new ::CoreML::Specification::ActivationReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.ReLU)
  return NonlinearityType_.relu_;
}
::CoreML::Specification::ActivationReLU* ActivationParams::release_relu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.ReLU)
  if (has_relu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationReLU* temp = NonlinearityType_.relu_;
    NonlinearityType_.relu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_relu(::CoreML::Specification::ActivationReLU* relu) {
  clear_NonlinearityType();
  if (relu) {
    set_has_relu();
    NonlinearityType_.relu_ = relu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.ReLU)
}

// .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
bool ActivationParams::has_leakyrelu() const {
  return NonlinearityType_case() == kLeakyReLU;
}
void ActivationParams::set_has_leakyrelu() {
  _oneof_case_[0] = kLeakyReLU;
}
void ActivationParams::clear_leakyrelu() {
  if (has_leakyrelu()) {
    delete NonlinearityType_.leakyrelu_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationLeakyReLU& ActivationParams::leakyrelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.leakyReLU)
  return has_leakyrelu()
      ? *NonlinearityType_.leakyrelu_
      : ::CoreML::Specification::ActivationLeakyReLU::default_instance();
}
::CoreML::Specification::ActivationLeakyReLU* ActivationParams::mutable_leakyrelu() {
  if (!has_leakyrelu()) {
    clear_NonlinearityType();
    set_has_leakyrelu();
    NonlinearityType_.leakyrelu_ = new ::CoreML::Specification::ActivationLeakyReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.leakyReLU)
  return NonlinearityType_.leakyrelu_;
}
::CoreML::Specification::ActivationLeakyReLU* ActivationParams::release_leakyrelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.leakyReLU)
  if (has_leakyrelu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationLeakyReLU* temp = NonlinearityType_.leakyrelu_;
    NonlinearityType_.leakyrelu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_leakyrelu(::CoreML::Specification::ActivationLeakyReLU* leakyrelu) {
  clear_NonlinearityType();
  if (leakyrelu) {
    set_has_leakyrelu();
    NonlinearityType_.leakyrelu_ = leakyrelu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.leakyReLU)
}

// .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
bool ActivationParams::has_thresholdedrelu() const {
  return NonlinearityType_case() == kThresholdedReLU;
}
void ActivationParams::set_has_thresholdedrelu() {
  _oneof_case_[0] = kThresholdedReLU;
}
void ActivationParams::clear_thresholdedrelu() {
  if (has_thresholdedrelu()) {
    delete NonlinearityType_.thresholdedrelu_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationThresholdedReLU& ActivationParams::thresholdedrelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.thresholdedReLU)
  return has_thresholdedrelu()
      ? *NonlinearityType_.thresholdedrelu_
      : ::CoreML::Specification::ActivationThresholdedReLU::default_instance();
}
::CoreML::Specification::ActivationThresholdedReLU* ActivationParams::mutable_thresholdedrelu() {
  if (!has_thresholdedrelu()) {
    clear_NonlinearityType();
    set_has_thresholdedrelu();
    NonlinearityType_.thresholdedrelu_ = new ::CoreML::Specification::ActivationThresholdedReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.thresholdedReLU)
  return NonlinearityType_.thresholdedrelu_;
}
::CoreML::Specification::ActivationThresholdedReLU* ActivationParams::release_thresholdedrelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.thresholdedReLU)
  if (has_thresholdedrelu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationThresholdedReLU* temp = NonlinearityType_.thresholdedrelu_;
    NonlinearityType_.thresholdedrelu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_thresholdedrelu(::CoreML::Specification::ActivationThresholdedReLU* thresholdedrelu) {
  clear_NonlinearityType();
  if (thresholdedrelu) {
    set_has_thresholdedrelu();
    NonlinearityType_.thresholdedrelu_ = thresholdedrelu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.thresholdedReLU)
}

// .CoreML.Specification.ActivationPReLU PReLU = 25;
bool ActivationParams::has_prelu() const {
  return NonlinearityType_case() == kPReLU;
}
void ActivationParams::set_has_prelu() {
  _oneof_case_[0] = kPReLU;
}
void ActivationParams::clear_prelu() {
  if (has_prelu()) {
    delete NonlinearityType_.prelu_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationPReLU& ActivationParams::prelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.PReLU)
  return has_prelu()
      ? *NonlinearityType_.prelu_
      : ::CoreML::Specification::ActivationPReLU::default_instance();
}
::CoreML::Specification::ActivationPReLU* ActivationParams::mutable_prelu() {
  if (!has_prelu()) {
    clear_NonlinearityType();
    set_has_prelu();
    NonlinearityType_.prelu_ = new ::CoreML::Specification::ActivationPReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.PReLU)
  return NonlinearityType_.prelu_;
}
::CoreML::Specification::ActivationPReLU* ActivationParams::release_prelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.PReLU)
  if (has_prelu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationPReLU* temp = NonlinearityType_.prelu_;
    NonlinearityType_.prelu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_prelu(::CoreML::Specification::ActivationPReLU* prelu) {
  clear_NonlinearityType();
  if (prelu) {
    set_has_prelu();
    NonlinearityType_.prelu_ = prelu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.PReLU)
}

// .CoreML.Specification.ActivationTanh tanh = 30;
bool ActivationParams::has_tanh() const {
  return NonlinearityType_case() == kTanh;
}
void ActivationParams::set_has_tanh() {
  _oneof_case_[0] = kTanh;
}
void ActivationParams::clear_tanh() {
  if (has_tanh()) {
    delete NonlinearityType_.tanh_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationTanh& ActivationParams::tanh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.tanh)
  return has_tanh()
      ? *NonlinearityType_.tanh_
      : ::CoreML::Specification::ActivationTanh::default_instance();
}
::CoreML::Specification::ActivationTanh* ActivationParams::mutable_tanh() {
  if (!has_tanh()) {
    clear_NonlinearityType();
    set_has_tanh();
    NonlinearityType_.tanh_ = new ::CoreML::Specification::ActivationTanh;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.tanh)
  return NonlinearityType_.tanh_;
}
::CoreML::Specification::ActivationTanh* ActivationParams::release_tanh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.tanh)
  if (has_tanh()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationTanh* temp = NonlinearityType_.tanh_;
    NonlinearityType_.tanh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_tanh(::CoreML::Specification::ActivationTanh* tanh) {
  clear_NonlinearityType();
  if (tanh) {
    set_has_tanh();
    NonlinearityType_.tanh_ = tanh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.tanh)
}

// .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
bool ActivationParams::has_scaledtanh() const {
  return NonlinearityType_case() == kScaledTanh;
}
void ActivationParams::set_has_scaledtanh() {
  _oneof_case_[0] = kScaledTanh;
}
void ActivationParams::clear_scaledtanh() {
  if (has_scaledtanh()) {
    delete NonlinearityType_.scaledtanh_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationScaledTanh& ActivationParams::scaledtanh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.scaledTanh)
  return has_scaledtanh()
      ? *NonlinearityType_.scaledtanh_
      : ::CoreML::Specification::ActivationScaledTanh::default_instance();
}
::CoreML::Specification::ActivationScaledTanh* ActivationParams::mutable_scaledtanh() {
  if (!has_scaledtanh()) {
    clear_NonlinearityType();
    set_has_scaledtanh();
    NonlinearityType_.scaledtanh_ = new ::CoreML::Specification::ActivationScaledTanh;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.scaledTanh)
  return NonlinearityType_.scaledtanh_;
}
::CoreML::Specification::ActivationScaledTanh* ActivationParams::release_scaledtanh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.scaledTanh)
  if (has_scaledtanh()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationScaledTanh* temp = NonlinearityType_.scaledtanh_;
    NonlinearityType_.scaledtanh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_scaledtanh(::CoreML::Specification::ActivationScaledTanh* scaledtanh) {
  clear_NonlinearityType();
  if (scaledtanh) {
    set_has_scaledtanh();
    NonlinearityType_.scaledtanh_ = scaledtanh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.scaledTanh)
}

// .CoreML.Specification.ActivationSigmoid sigmoid = 40;
bool ActivationParams::has_sigmoid() const {
  return NonlinearityType_case() == kSigmoid;
}
void ActivationParams::set_has_sigmoid() {
  _oneof_case_[0] = kSigmoid;
}
void ActivationParams::clear_sigmoid() {
  if (has_sigmoid()) {
    delete NonlinearityType_.sigmoid_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationSigmoid& ActivationParams::sigmoid() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.sigmoid)
  return has_sigmoid()
      ? *NonlinearityType_.sigmoid_
      : ::CoreML::Specification::ActivationSigmoid::default_instance();
}
::CoreML::Specification::ActivationSigmoid* ActivationParams::mutable_sigmoid() {
  if (!has_sigmoid()) {
    clear_NonlinearityType();
    set_has_sigmoid();
    NonlinearityType_.sigmoid_ = new ::CoreML::Specification::ActivationSigmoid;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.sigmoid)
  return NonlinearityType_.sigmoid_;
}
::CoreML::Specification::ActivationSigmoid* ActivationParams::release_sigmoid() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.sigmoid)
  if (has_sigmoid()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSigmoid* temp = NonlinearityType_.sigmoid_;
    NonlinearityType_.sigmoid_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_sigmoid(::CoreML::Specification::ActivationSigmoid* sigmoid) {
  clear_NonlinearityType();
  if (sigmoid) {
    set_has_sigmoid();
    NonlinearityType_.sigmoid_ = sigmoid;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.sigmoid)
}

// .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
bool ActivationParams::has_sigmoidhard() const {
  return NonlinearityType_case() == kSigmoidHard;
}
void ActivationParams::set_has_sigmoidhard() {
  _oneof_case_[0] = kSigmoidHard;
}
void ActivationParams::clear_sigmoidhard() {
  if (has_sigmoidhard()) {
    delete NonlinearityType_.sigmoidhard_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationSigmoidHard& ActivationParams::sigmoidhard() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.sigmoidHard)
  return has_sigmoidhard()
      ? *NonlinearityType_.sigmoidhard_
      : ::CoreML::Specification::ActivationSigmoidHard::default_instance();
}
::CoreML::Specification::ActivationSigmoidHard* ActivationParams::mutable_sigmoidhard() {
  if (!has_sigmoidhard()) {
    clear_NonlinearityType();
    set_has_sigmoidhard();
    NonlinearityType_.sigmoidhard_ = new ::CoreML::Specification::ActivationSigmoidHard;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.sigmoidHard)
  return NonlinearityType_.sigmoidhard_;
}
::CoreML::Specification::ActivationSigmoidHard* ActivationParams::release_sigmoidhard() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.sigmoidHard)
  if (has_sigmoidhard()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSigmoidHard* temp = NonlinearityType_.sigmoidhard_;
    NonlinearityType_.sigmoidhard_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_sigmoidhard(::CoreML::Specification::ActivationSigmoidHard* sigmoidhard) {
  clear_NonlinearityType();
  if (sigmoidhard) {
    set_has_sigmoidhard();
    NonlinearityType_.sigmoidhard_ = sigmoidhard;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.sigmoidHard)
}

// .CoreML.Specification.ActivationELU ELU = 50;
bool ActivationParams::has_elu() const {
  return NonlinearityType_case() == kELU;
}
void ActivationParams::set_has_elu() {
  _oneof_case_[0] = kELU;
}
void ActivationParams::clear_elu() {
  if (has_elu()) {
    delete NonlinearityType_.elu_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationELU& ActivationParams::elu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.ELU)
  return has_elu()
      ? *NonlinearityType_.elu_
      : ::CoreML::Specification::ActivationELU::default_instance();
}
::CoreML::Specification::ActivationELU* ActivationParams::mutable_elu() {
  if (!has_elu()) {
    clear_NonlinearityType();
    set_has_elu();
    NonlinearityType_.elu_ = new ::CoreML::Specification::ActivationELU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.ELU)
  return NonlinearityType_.elu_;
}
::CoreML::Specification::ActivationELU* ActivationParams::release_elu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.ELU)
  if (has_elu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationELU* temp = NonlinearityType_.elu_;
    NonlinearityType_.elu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_elu(::CoreML::Specification::ActivationELU* elu) {
  clear_NonlinearityType();
  if (elu) {
    set_has_elu();
    NonlinearityType_.elu_ = elu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.ELU)
}

// .CoreML.Specification.ActivationSoftsign softsign = 60;
bool ActivationParams::has_softsign() const {
  return NonlinearityType_case() == kSoftsign;
}
void ActivationParams::set_has_softsign() {
  _oneof_case_[0] = kSoftsign;
}
void ActivationParams::clear_softsign() {
  if (has_softsign()) {
    delete NonlinearityType_.softsign_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationSoftsign& ActivationParams::softsign() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.softsign)
  return has_softsign()
      ? *NonlinearityType_.softsign_
      : ::CoreML::Specification::ActivationSoftsign::default_instance();
}
::CoreML::Specification::ActivationSoftsign* ActivationParams::mutable_softsign() {
  if (!has_softsign()) {
    clear_NonlinearityType();
    set_has_softsign();
    NonlinearityType_.softsign_ = new ::CoreML::Specification::ActivationSoftsign;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.softsign)
  return NonlinearityType_.softsign_;
}
::CoreML::Specification::ActivationSoftsign* ActivationParams::release_softsign() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.softsign)
  if (has_softsign()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSoftsign* temp = NonlinearityType_.softsign_;
    NonlinearityType_.softsign_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_softsign(::CoreML::Specification::ActivationSoftsign* softsign) {
  clear_NonlinearityType();
  if (softsign) {
    set_has_softsign();
    NonlinearityType_.softsign_ = softsign;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.softsign)
}

// .CoreML.Specification.ActivationSoftplus softplus = 70;
bool ActivationParams::has_softplus() const {
  return NonlinearityType_case() == kSoftplus;
}
void ActivationParams::set_has_softplus() {
  _oneof_case_[0] = kSoftplus;
}
void ActivationParams::clear_softplus() {
  if (has_softplus()) {
    delete NonlinearityType_.softplus_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationSoftplus& ActivationParams::softplus() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.softplus)
  return has_softplus()
      ? *NonlinearityType_.softplus_
      : ::CoreML::Specification::ActivationSoftplus::default_instance();
}
::CoreML::Specification::ActivationSoftplus* ActivationParams::mutable_softplus() {
  if (!has_softplus()) {
    clear_NonlinearityType();
    set_has_softplus();
    NonlinearityType_.softplus_ = new ::CoreML::Specification::ActivationSoftplus;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.softplus)
  return NonlinearityType_.softplus_;
}
::CoreML::Specification::ActivationSoftplus* ActivationParams::release_softplus() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.softplus)
  if (has_softplus()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSoftplus* temp = NonlinearityType_.softplus_;
    NonlinearityType_.softplus_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_softplus(::CoreML::Specification::ActivationSoftplus* softplus) {
  clear_NonlinearityType();
  if (softplus) {
    set_has_softplus();
    NonlinearityType_.softplus_ = softplus;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.softplus)
}

// .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
bool ActivationParams::has_parametricsoftplus() const {
  return NonlinearityType_case() == kParametricSoftplus;
}
void ActivationParams::set_has_parametricsoftplus() {
  _oneof_case_[0] = kParametricSoftplus;
}
void ActivationParams::clear_parametricsoftplus() {
  if (has_parametricsoftplus()) {
    delete NonlinearityType_.parametricsoftplus_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationParametricSoftplus& ActivationParams::parametricsoftplus() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.parametricSoftplus)
  return has_parametricsoftplus()
      ? *NonlinearityType_.parametricsoftplus_
      : ::CoreML::Specification::ActivationParametricSoftplus::default_instance();
}
::CoreML::Specification::ActivationParametricSoftplus* ActivationParams::mutable_parametricsoftplus() {
  if (!has_parametricsoftplus()) {
    clear_NonlinearityType();
    set_has_parametricsoftplus();
    NonlinearityType_.parametricsoftplus_ = new ::CoreML::Specification::ActivationParametricSoftplus;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.parametricSoftplus)
  return NonlinearityType_.parametricsoftplus_;
}
::CoreML::Specification::ActivationParametricSoftplus* ActivationParams::release_parametricsoftplus() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.parametricSoftplus)
  if (has_parametricsoftplus()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationParametricSoftplus* temp = NonlinearityType_.parametricsoftplus_;
    NonlinearityType_.parametricsoftplus_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_parametricsoftplus(::CoreML::Specification::ActivationParametricSoftplus* parametricsoftplus) {
  clear_NonlinearityType();
  if (parametricsoftplus) {
    set_has_parametricsoftplus();
    NonlinearityType_.parametricsoftplus_ = parametricsoftplus;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.parametricSoftplus)
}

bool ActivationParams::has_NonlinearityType() const {
  return NonlinearityType_case() != NONLINEARITYTYPE_NOT_SET;
}
void ActivationParams::clear_has_NonlinearityType() {
  _oneof_case_[0] = NONLINEARITYTYPE_NOT_SET;
}
ActivationParams::NonlinearityTypeCase ActivationParams::NonlinearityType_case() const {
  return ActivationParams::NonlinearityTypeCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkLayer::kNameFieldNumber;
const int NeuralNetworkLayer::kInputFieldNumber;
const int NeuralNetworkLayer::kOutputFieldNumber;
const int NeuralNetworkLayer::kConvolutionFieldNumber;
const int NeuralNetworkLayer::kPoolingFieldNumber;
const int NeuralNetworkLayer::kActivationFieldNumber;
const int NeuralNetworkLayer::kInnerProductFieldNumber;
const int NeuralNetworkLayer::kEmbeddingFieldNumber;
const int NeuralNetworkLayer::kBatchnormFieldNumber;
const int NeuralNetworkLayer::kMvnFieldNumber;
const int NeuralNetworkLayer::kL2NormalizeFieldNumber;
const int NeuralNetworkLayer::kSoftmaxFieldNumber;
const int NeuralNetworkLayer::kLrnFieldNumber;
const int NeuralNetworkLayer::kCropFieldNumber;
const int NeuralNetworkLayer::kPaddingFieldNumber;
const int NeuralNetworkLayer::kUpsampleFieldNumber;
const int NeuralNetworkLayer::kUnaryFieldNumber;
const int NeuralNetworkLayer::kAddFieldNumber;
const int NeuralNetworkLayer::kMultiplyFieldNumber;
const int NeuralNetworkLayer::kAverageFieldNumber;
const int NeuralNetworkLayer::kScaleFieldNumber;
const int NeuralNetworkLayer::kBiasFieldNumber;
const int NeuralNetworkLayer::kMaxFieldNumber;
const int NeuralNetworkLayer::kMinFieldNumber;
const int NeuralNetworkLayer::kDotFieldNumber;
const int NeuralNetworkLayer::kReduceFieldNumber;
const int NeuralNetworkLayer::kLoadConstantFieldNumber;
const int NeuralNetworkLayer::kReshapeFieldNumber;
const int NeuralNetworkLayer::kFlattenFieldNumber;
const int NeuralNetworkLayer::kPermuteFieldNumber;
const int NeuralNetworkLayer::kConcatFieldNumber;
const int NeuralNetworkLayer::kSplitFieldNumber;
const int NeuralNetworkLayer::kSequenceRepeatFieldNumber;
const int NeuralNetworkLayer::kReorganizeDataFieldNumber;
const int NeuralNetworkLayer::kSliceFieldNumber;
const int NeuralNetworkLayer::kSimpleRecurrentFieldNumber;
const int NeuralNetworkLayer::kGruFieldNumber;
const int NeuralNetworkLayer::kUniDirectionalLSTMFieldNumber;
const int NeuralNetworkLayer::kBiDirectionalLSTMFieldNumber;
const int NeuralNetworkLayer::kCustomFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkLayer::NeuralNetworkLayer()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkLayer)
}
NeuralNetworkLayer::NeuralNetworkLayer(const NeuralNetworkLayer& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      input_(from.input_),
      output_(from.output_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  name_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.name().size() > 0) {
    name_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.name_);
  }
  clear_has_layer();
  switch (from.layer_case()) {
    case kConvolution: {
      mutable_convolution()->::CoreML::Specification::ConvolutionLayerParams::MergeFrom(from.convolution());
      break;
    }
    case kPooling: {
      mutable_pooling()->::CoreML::Specification::PoolingLayerParams::MergeFrom(from.pooling());
      break;
    }
    case kActivation: {
      mutable_activation()->::CoreML::Specification::ActivationParams::MergeFrom(from.activation());
      break;
    }
    case kInnerProduct: {
      mutable_innerproduct()->::CoreML::Specification::InnerProductLayerParams::MergeFrom(from.innerproduct());
      break;
    }
    case kEmbedding: {
      mutable_embedding()->::CoreML::Specification::EmbeddingLayerParams::MergeFrom(from.embedding());
      break;
    }
    case kBatchnorm: {
      mutable_batchnorm()->::CoreML::Specification::BatchnormLayerParams::MergeFrom(from.batchnorm());
      break;
    }
    case kMvn: {
      mutable_mvn()->::CoreML::Specification::MeanVarianceNormalizeLayerParams::MergeFrom(from.mvn());
      break;
    }
    case kL2Normalize: {
      mutable_l2normalize()->::CoreML::Specification::L2NormalizeLayerParams::MergeFrom(from.l2normalize());
      break;
    }
    case kSoftmax: {
      mutable_softmax()->::CoreML::Specification::SoftmaxLayerParams::MergeFrom(from.softmax());
      break;
    }
    case kLrn: {
      mutable_lrn()->::CoreML::Specification::LRNLayerParams::MergeFrom(from.lrn());
      break;
    }
    case kCrop: {
      mutable_crop()->::CoreML::Specification::CropLayerParams::MergeFrom(from.crop());
      break;
    }
    case kPadding: {
      mutable_padding()->::CoreML::Specification::PaddingLayerParams::MergeFrom(from.padding());
      break;
    }
    case kUpsample: {
      mutable_upsample()->::CoreML::Specification::UpsampleLayerParams::MergeFrom(from.upsample());
      break;
    }
    case kUnary: {
      mutable_unary()->::CoreML::Specification::UnaryFunctionLayerParams::MergeFrom(from.unary());
      break;
    }
    case kAdd: {
      mutable_add()->::CoreML::Specification::AddLayerParams::MergeFrom(from.add());
      break;
    }
    case kMultiply: {
      mutable_multiply()->::CoreML::Specification::MultiplyLayerParams::MergeFrom(from.multiply());
      break;
    }
    case kAverage: {
      mutable_average()->::CoreML::Specification::AverageLayerParams::MergeFrom(from.average());
      break;
    }
    case kScale: {
      mutable_scale()->::CoreML::Specification::ScaleLayerParams::MergeFrom(from.scale());
      break;
    }
    case kBias: {
      mutable_bias()->::CoreML::Specification::BiasLayerParams::MergeFrom(from.bias());
      break;
    }
    case kMax: {
      mutable_max()->::CoreML::Specification::MaxLayerParams::MergeFrom(from.max());
      break;
    }
    case kMin: {
      mutable_min()->::CoreML::Specification::MinLayerParams::MergeFrom(from.min());
      break;
    }
    case kDot: {
      mutable_dot()->::CoreML::Specification::DotProductLayerParams::MergeFrom(from.dot());
      break;
    }
    case kReduce: {
      mutable_reduce()->::CoreML::Specification::ReduceLayerParams::MergeFrom(from.reduce());
      break;
    }
    case kLoadConstant: {
      mutable_loadconstant()->::CoreML::Specification::LoadConstantLayerParams::MergeFrom(from.loadconstant());
      break;
    }
    case kReshape: {
      mutable_reshape()->::CoreML::Specification::ReshapeLayerParams::MergeFrom(from.reshape());
      break;
    }
    case kFlatten: {
      mutable_flatten()->::CoreML::Specification::FlattenLayerParams::MergeFrom(from.flatten());
      break;
    }
    case kPermute: {
      mutable_permute()->::CoreML::Specification::PermuteLayerParams::MergeFrom(from.permute());
      break;
    }
    case kConcat: {
      mutable_concat()->::CoreML::Specification::ConcatLayerParams::MergeFrom(from.concat());
      break;
    }
    case kSplit: {
      mutable_split()->::CoreML::Specification::SplitLayerParams::MergeFrom(from.split());
      break;
    }
    case kSequenceRepeat: {
      mutable_sequencerepeat()->::CoreML::Specification::SequenceRepeatLayerParams::MergeFrom(from.sequencerepeat());
      break;
    }
    case kReorganizeData: {
      mutable_reorganizedata()->::CoreML::Specification::ReorganizeDataLayerParams::MergeFrom(from.reorganizedata());
      break;
    }
    case kSlice: {
      mutable_slice()->::CoreML::Specification::SliceLayerParams::MergeFrom(from.slice());
      break;
    }
    case kSimpleRecurrent: {
      mutable_simplerecurrent()->::CoreML::Specification::SimpleRecurrentLayerParams::MergeFrom(from.simplerecurrent());
      break;
    }
    case kGru: {
      mutable_gru()->::CoreML::Specification::GRULayerParams::MergeFrom(from.gru());
      break;
    }
    case kUniDirectionalLSTM: {
      mutable_unidirectionallstm()->::CoreML::Specification::UniDirectionalLSTMLayerParams::MergeFrom(from.unidirectionallstm());
      break;
    }
    case kBiDirectionalLSTM: {
      mutable_bidirectionallstm()->::CoreML::Specification::BiDirectionalLSTMLayerParams::MergeFrom(from.bidirectionallstm());
      break;
    }
    case kCustom: {
      mutable_custom()->::CoreML::Specification::CustomLayerParams::MergeFrom(from.custom());
      break;
    }
    case LAYER_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkLayer)
}

void NeuralNetworkLayer::SharedCtor() {
  name_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_layer();
  _cached_size_ = 0;
}

NeuralNetworkLayer::~NeuralNetworkLayer() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkLayer)
  SharedDtor();
}

void NeuralNetworkLayer::SharedDtor() {
  name_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (has_layer()) {
    clear_layer();
  }
}

void NeuralNetworkLayer::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkLayer& NeuralNetworkLayer::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NeuralNetworkLayer* NeuralNetworkLayer::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkLayer* n = new NeuralNetworkLayer;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkLayer::clear_layer() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.NeuralNetworkLayer)
  switch (layer_case()) {
    case kConvolution: {
      delete layer_.convolution_;
      break;
    }
    case kPooling: {
      delete layer_.pooling_;
      break;
    }
    case kActivation: {
      delete layer_.activation_;
      break;
    }
    case kInnerProduct: {
      delete layer_.innerproduct_;
      break;
    }
    case kEmbedding: {
      delete layer_.embedding_;
      break;
    }
    case kBatchnorm: {
      delete layer_.batchnorm_;
      break;
    }
    case kMvn: {
      delete layer_.mvn_;
      break;
    }
    case kL2Normalize: {
      delete layer_.l2normalize_;
      break;
    }
    case kSoftmax: {
      delete layer_.softmax_;
      break;
    }
    case kLrn: {
      delete layer_.lrn_;
      break;
    }
    case kCrop: {
      delete layer_.crop_;
      break;
    }
    case kPadding: {
      delete layer_.padding_;
      break;
    }
    case kUpsample: {
      delete layer_.upsample_;
      break;
    }
    case kUnary: {
      delete layer_.unary_;
      break;
    }
    case kAdd: {
      delete layer_.add_;
      break;
    }
    case kMultiply: {
      delete layer_.multiply_;
      break;
    }
    case kAverage: {
      delete layer_.average_;
      break;
    }
    case kScale: {
      delete layer_.scale_;
      break;
    }
    case kBias: {
      delete layer_.bias_;
      break;
    }
    case kMax: {
      delete layer_.max_;
      break;
    }
    case kMin: {
      delete layer_.min_;
      break;
    }
    case kDot: {
      delete layer_.dot_;
      break;
    }
    case kReduce: {
      delete layer_.reduce_;
      break;
    }
    case kLoadConstant: {
      delete layer_.loadconstant_;
      break;
    }
    case kReshape: {
      delete layer_.reshape_;
      break;
    }
    case kFlatten: {
      delete layer_.flatten_;
      break;
    }
    case kPermute: {
      delete layer_.permute_;
      break;
    }
    case kConcat: {
      delete layer_.concat_;
      break;
    }
    case kSplit: {
      delete layer_.split_;
      break;
    }
    case kSequenceRepeat: {
      delete layer_.sequencerepeat_;
      break;
    }
    case kReorganizeData: {
      delete layer_.reorganizedata_;
      break;
    }
    case kSlice: {
      delete layer_.slice_;
      break;
    }
    case kSimpleRecurrent: {
      delete layer_.simplerecurrent_;
      break;
    }
    case kGru: {
      delete layer_.gru_;
      break;
    }
    case kUniDirectionalLSTM: {
      delete layer_.unidirectionallstm_;
      break;
    }
    case kBiDirectionalLSTM: {
      delete layer_.bidirectionallstm_;
      break;
    }
    case kCustom: {
      delete layer_.custom_;
      break;
    }
    case LAYER_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = LAYER_NOT_SET;
}


void NeuralNetworkLayer::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkLayer)
  input_.Clear();
  output_.Clear();
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_layer();
}

bool NeuralNetworkLayer::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkLayer)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // string name = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_name()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->name().data(), this->name().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.NeuralNetworkLayer.name"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated string input = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->add_input()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->input(this->input_size() - 1).data(),
            this->input(this->input_size() - 1).length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.NeuralNetworkLayer.input"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated string output = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->add_output()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->output(this->output_size() - 1).data(),
            this->output(this->output_size() - 1).length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.NeuralNetworkLayer.output"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ConvolutionLayerParams convolution = 100;
      case 100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(802u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_convolution()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.PoolingLayerParams pooling = 120;
      case 120: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(962u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_pooling()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationParams activation = 130;
      case 130: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1042u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_activation()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
      case 140: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1122u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_innerproduct()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.EmbeddingLayerParams embedding = 150;
      case 150: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1202u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_embedding()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
      case 160: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1282u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_batchnorm()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
      case 165: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_mvn()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
      case 170: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1362u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_l2normalize()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SoftmaxLayerParams softmax = 175;
      case 175: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1402u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_softmax()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LRNLayerParams lrn = 180;
      case 180: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1442u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_lrn()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.CropLayerParams crop = 190;
      case 190: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1522u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_crop()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.PaddingLayerParams padding = 200;
      case 200: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1602u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_padding()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.UpsampleLayerParams upsample = 210;
      case 210: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1682u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_upsample()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
      case 220: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1762u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_unary()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.AddLayerParams add = 230;
      case 230: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1842u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_add()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.MultiplyLayerParams multiply = 231;
      case 231: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1850u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_multiply()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.AverageLayerParams average = 240;
      case 240: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1922u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_average()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ScaleLayerParams scale = 245;
      case 245: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1962u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_scale()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BiasLayerParams bias = 250;
      case 250: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2002u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.MaxLayerParams max = 260;
      case 260: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2082u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_max()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.MinLayerParams min = 261;
      case 261: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2090u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_min()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.DotProductLayerParams dot = 270;
      case 270: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_dot()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReduceLayerParams reduce = 280;
      case 280: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reduce()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
      case 290: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_loadconstant()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReshapeLayerParams reshape = 300;
      case 300: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2402u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reshape()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.FlattenLayerParams flatten = 301;
      case 301: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2410u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_flatten()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.PermuteLayerParams permute = 310;
      case 310: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2482u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_permute()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ConcatLayerParams concat = 320;
      case 320: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2562u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_concat()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SplitLayerParams split = 330;
      case 330: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2642u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_split()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
      case 340: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2722u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_sequencerepeat()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReorganizeDataLayerParams reorganizeData = 345;
      case 345: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2762u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reorganizedata()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SliceLayerParams slice = 350;
      case 350: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2802u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_slice()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
      case 400: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(3202u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_simplerecurrent()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.GRULayerParams gru = 410;
      case 410: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(3282u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_gru()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
      case 420: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(3362u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_unidirectionallstm()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
      case 430: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(3442u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bidirectionallstm()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.CustomLayerParams custom = 500;
      case 500: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(4002u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_custom()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkLayer)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkLayer)
  return false;
#undef DO_
}

void NeuralNetworkLayer::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkLayer)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // string name = 1;
  if (this->name().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->name().data(), this->name().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.NeuralNetworkLayer.name");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      1, this->name(), output);
  }

  // repeated string input = 2;
  for (int i = 0, n = this->input_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->input(i).data(), this->input(i).length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.NeuralNetworkLayer.input");
    ::google::protobuf::internal::WireFormatLite::WriteString(
      2, this->input(i), output);
  }

  // repeated string output = 3;
  for (int i = 0, n = this->output_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->output(i).data(), this->output(i).length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.NeuralNetworkLayer.output");
    ::google::protobuf::internal::WireFormatLite::WriteString(
      3, this->output(i), output);
  }

  // .CoreML.Specification.ConvolutionLayerParams convolution = 100;
  if (has_convolution()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      100, *layer_.convolution_, output);
  }

  // .CoreML.Specification.PoolingLayerParams pooling = 120;
  if (has_pooling()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      120, *layer_.pooling_, output);
  }

  // .CoreML.Specification.ActivationParams activation = 130;
  if (has_activation()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      130, *layer_.activation_, output);
  }

  // .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
  if (has_innerproduct()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      140, *layer_.innerproduct_, output);
  }

  // .CoreML.Specification.EmbeddingLayerParams embedding = 150;
  if (has_embedding()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      150, *layer_.embedding_, output);
  }

  // .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
  if (has_batchnorm()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      160, *layer_.batchnorm_, output);
  }

  // .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
  if (has_mvn()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      165, *layer_.mvn_, output);
  }

  // .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
  if (has_l2normalize()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      170, *layer_.l2normalize_, output);
  }

  // .CoreML.Specification.SoftmaxLayerParams softmax = 175;
  if (has_softmax()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      175, *layer_.softmax_, output);
  }

  // .CoreML.Specification.LRNLayerParams lrn = 180;
  if (has_lrn()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      180, *layer_.lrn_, output);
  }

  // .CoreML.Specification.CropLayerParams crop = 190;
  if (has_crop()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      190, *layer_.crop_, output);
  }

  // .CoreML.Specification.PaddingLayerParams padding = 200;
  if (has_padding()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      200, *layer_.padding_, output);
  }

  // .CoreML.Specification.UpsampleLayerParams upsample = 210;
  if (has_upsample()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      210, *layer_.upsample_, output);
  }

  // .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
  if (has_unary()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      220, *layer_.unary_, output);
  }

  // .CoreML.Specification.AddLayerParams add = 230;
  if (has_add()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      230, *layer_.add_, output);
  }

  // .CoreML.Specification.MultiplyLayerParams multiply = 231;
  if (has_multiply()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      231, *layer_.multiply_, output);
  }

  // .CoreML.Specification.AverageLayerParams average = 240;
  if (has_average()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      240, *layer_.average_, output);
  }

  // .CoreML.Specification.ScaleLayerParams scale = 245;
  if (has_scale()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      245, *layer_.scale_, output);
  }

  // .CoreML.Specification.BiasLayerParams bias = 250;
  if (has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      250, *layer_.bias_, output);
  }

  // .CoreML.Specification.MaxLayerParams max = 260;
  if (has_max()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      260, *layer_.max_, output);
  }

  // .CoreML.Specification.MinLayerParams min = 261;
  if (has_min()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      261, *layer_.min_, output);
  }

  // .CoreML.Specification.DotProductLayerParams dot = 270;
  if (has_dot()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      270, *layer_.dot_, output);
  }

  // .CoreML.Specification.ReduceLayerParams reduce = 280;
  if (has_reduce()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      280, *layer_.reduce_, output);
  }

  // .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
  if (has_loadconstant()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      290, *layer_.loadconstant_, output);
  }

  // .CoreML.Specification.ReshapeLayerParams reshape = 300;
  if (has_reshape()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      300, *layer_.reshape_, output);
  }

  // .CoreML.Specification.FlattenLayerParams flatten = 301;
  if (has_flatten()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      301, *layer_.flatten_, output);
  }

  // .CoreML.Specification.PermuteLayerParams permute = 310;
  if (has_permute()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      310, *layer_.permute_, output);
  }

  // .CoreML.Specification.ConcatLayerParams concat = 320;
  if (has_concat()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      320, *layer_.concat_, output);
  }

  // .CoreML.Specification.SplitLayerParams split = 330;
  if (has_split()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      330, *layer_.split_, output);
  }

  // .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
  if (has_sequencerepeat()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      340, *layer_.sequencerepeat_, output);
  }

  // .CoreML.Specification.ReorganizeDataLayerParams reorganizeData = 345;
  if (has_reorganizedata()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      345, *layer_.reorganizedata_, output);
  }

  // .CoreML.Specification.SliceLayerParams slice = 350;
  if (has_slice()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      350, *layer_.slice_, output);
  }

  // .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
  if (has_simplerecurrent()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      400, *layer_.simplerecurrent_, output);
  }

  // .CoreML.Specification.GRULayerParams gru = 410;
  if (has_gru()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      410, *layer_.gru_, output);
  }

  // .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
  if (has_unidirectionallstm()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      420, *layer_.unidirectionallstm_, output);
  }

  // .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
  if (has_bidirectionallstm()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      430, *layer_.bidirectionallstm_, output);
  }

  // .CoreML.Specification.CustomLayerParams custom = 500;
  if (has_custom()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      500, *layer_.custom_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkLayer)
}

size_t NeuralNetworkLayer::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkLayer)
  size_t total_size = 0;

  // repeated string input = 2;
  total_size += 1 *
      ::google::protobuf::internal::FromIntSize(this->input_size());
  for (int i = 0, n = this->input_size(); i < n; i++) {
    total_size += ::google::protobuf::internal::WireFormatLite::StringSize(
      this->input(i));
  }

  // repeated string output = 3;
  total_size += 1 *
      ::google::protobuf::internal::FromIntSize(this->output_size());
  for (int i = 0, n = this->output_size(); i < n; i++) {
    total_size += ::google::protobuf::internal::WireFormatLite::StringSize(
      this->output(i));
  }

  // string name = 1;
  if (this->name().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->name());
  }

  switch (layer_case()) {
    // .CoreML.Specification.ConvolutionLayerParams convolution = 100;
    case kConvolution: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.convolution_);
      break;
    }
    // .CoreML.Specification.PoolingLayerParams pooling = 120;
    case kPooling: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.pooling_);
      break;
    }
    // .CoreML.Specification.ActivationParams activation = 130;
    case kActivation: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.activation_);
      break;
    }
    // .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
    case kInnerProduct: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.innerproduct_);
      break;
    }
    // .CoreML.Specification.EmbeddingLayerParams embedding = 150;
    case kEmbedding: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.embedding_);
      break;
    }
    // .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
    case kBatchnorm: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.batchnorm_);
      break;
    }
    // .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
    case kMvn: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.mvn_);
      break;
    }
    // .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
    case kL2Normalize: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.l2normalize_);
      break;
    }
    // .CoreML.Specification.SoftmaxLayerParams softmax = 175;
    case kSoftmax: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.softmax_);
      break;
    }
    // .CoreML.Specification.LRNLayerParams lrn = 180;
    case kLrn: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.lrn_);
      break;
    }
    // .CoreML.Specification.CropLayerParams crop = 190;
    case kCrop: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.crop_);
      break;
    }
    // .CoreML.Specification.PaddingLayerParams padding = 200;
    case kPadding: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.padding_);
      break;
    }
    // .CoreML.Specification.UpsampleLayerParams upsample = 210;
    case kUpsample: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.upsample_);
      break;
    }
    // .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
    case kUnary: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.unary_);
      break;
    }
    // .CoreML.Specification.AddLayerParams add = 230;
    case kAdd: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.add_);
      break;
    }
    // .CoreML.Specification.MultiplyLayerParams multiply = 231;
    case kMultiply: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.multiply_);
      break;
    }
    // .CoreML.Specification.AverageLayerParams average = 240;
    case kAverage: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.average_);
      break;
    }
    // .CoreML.Specification.ScaleLayerParams scale = 245;
    case kScale: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.scale_);
      break;
    }
    // .CoreML.Specification.BiasLayerParams bias = 250;
    case kBias: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.bias_);
      break;
    }
    // .CoreML.Specification.MaxLayerParams max = 260;
    case kMax: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.max_);
      break;
    }
    // .CoreML.Specification.MinLayerParams min = 261;
    case kMin: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.min_);
      break;
    }
    // .CoreML.Specification.DotProductLayerParams dot = 270;
    case kDot: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.dot_);
      break;
    }
    // .CoreML.Specification.ReduceLayerParams reduce = 280;
    case kReduce: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reduce_);
      break;
    }
    // .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
    case kLoadConstant: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.loadconstant_);
      break;
    }
    // .CoreML.Specification.ReshapeLayerParams reshape = 300;
    case kReshape: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reshape_);
      break;
    }
    // .CoreML.Specification.FlattenLayerParams flatten = 301;
    case kFlatten: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.flatten_);
      break;
    }
    // .CoreML.Specification.PermuteLayerParams permute = 310;
    case kPermute: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.permute_);
      break;
    }
    // .CoreML.Specification.ConcatLayerParams concat = 320;
    case kConcat: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.concat_);
      break;
    }
    // .CoreML.Specification.SplitLayerParams split = 330;
    case kSplit: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.split_);
      break;
    }
    // .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
    case kSequenceRepeat: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.sequencerepeat_);
      break;
    }
    // .CoreML.Specification.ReorganizeDataLayerParams reorganizeData = 345;
    case kReorganizeData: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reorganizedata_);
      break;
    }
    // .CoreML.Specification.SliceLayerParams slice = 350;
    case kSlice: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.slice_);
      break;
    }
    // .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
    case kSimpleRecurrent: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.simplerecurrent_);
      break;
    }
    // .CoreML.Specification.GRULayerParams gru = 410;
    case kGru: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.gru_);
      break;
    }
    // .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
    case kUniDirectionalLSTM: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.unidirectionallstm_);
      break;
    }
    // .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
    case kBiDirectionalLSTM: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.bidirectionallstm_);
      break;
    }
    // .CoreML.Specification.CustomLayerParams custom = 500;
    case kCustom: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.custom_);
      break;
    }
    case LAYER_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkLayer::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkLayer*>(&from));
}

void NeuralNetworkLayer::MergeFrom(const NeuralNetworkLayer& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkLayer)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  input_.MergeFrom(from.input_);
  output_.MergeFrom(from.output_);
  if (from.name().size() > 0) {

    name_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.name_);
  }
  switch (from.layer_case()) {
    case kConvolution: {
      mutable_convolution()->::CoreML::Specification::ConvolutionLayerParams::MergeFrom(from.convolution());
      break;
    }
    case kPooling: {
      mutable_pooling()->::CoreML::Specification::PoolingLayerParams::MergeFrom(from.pooling());
      break;
    }
    case kActivation: {
      mutable_activation()->::CoreML::Specification::ActivationParams::MergeFrom(from.activation());
      break;
    }
    case kInnerProduct: {
      mutable_innerproduct()->::CoreML::Specification::InnerProductLayerParams::MergeFrom(from.innerproduct());
      break;
    }
    case kEmbedding: {
      mutable_embedding()->::CoreML::Specification::EmbeddingLayerParams::MergeFrom(from.embedding());
      break;
    }
    case kBatchnorm: {
      mutable_batchnorm()->::CoreML::Specification::BatchnormLayerParams::MergeFrom(from.batchnorm());
      break;
    }
    case kMvn: {
      mutable_mvn()->::CoreML::Specification::MeanVarianceNormalizeLayerParams::MergeFrom(from.mvn());
      break;
    }
    case kL2Normalize: {
      mutable_l2normalize()->::CoreML::Specification::L2NormalizeLayerParams::MergeFrom(from.l2normalize());
      break;
    }
    case kSoftmax: {
      mutable_softmax()->::CoreML::Specification::SoftmaxLayerParams::MergeFrom(from.softmax());
      break;
    }
    case kLrn: {
      mutable_lrn()->::CoreML::Specification::LRNLayerParams::MergeFrom(from.lrn());
      break;
    }
    case kCrop: {
      mutable_crop()->::CoreML::Specification::CropLayerParams::MergeFrom(from.crop());
      break;
    }
    case kPadding: {
      mutable_padding()->::CoreML::Specification::PaddingLayerParams::MergeFrom(from.padding());
      break;
    }
    case kUpsample: {
      mutable_upsample()->::CoreML::Specification::UpsampleLayerParams::MergeFrom(from.upsample());
      break;
    }
    case kUnary: {
      mutable_unary()->::CoreML::Specification::UnaryFunctionLayerParams::MergeFrom(from.unary());
      break;
    }
    case kAdd: {
      mutable_add()->::CoreML::Specification::AddLayerParams::MergeFrom(from.add());
      break;
    }
    case kMultiply: {
      mutable_multiply()->::CoreML::Specification::MultiplyLayerParams::MergeFrom(from.multiply());
      break;
    }
    case kAverage: {
      mutable_average()->::CoreML::Specification::AverageLayerParams::MergeFrom(from.average());
      break;
    }
    case kScale: {
      mutable_scale()->::CoreML::Specification::ScaleLayerParams::MergeFrom(from.scale());
      break;
    }
    case kBias: {
      mutable_bias()->::CoreML::Specification::BiasLayerParams::MergeFrom(from.bias());
      break;
    }
    case kMax: {
      mutable_max()->::CoreML::Specification::MaxLayerParams::MergeFrom(from.max());
      break;
    }
    case kMin: {
      mutable_min()->::CoreML::Specification::MinLayerParams::MergeFrom(from.min());
      break;
    }
    case kDot: {
      mutable_dot()->::CoreML::Specification::DotProductLayerParams::MergeFrom(from.dot());
      break;
    }
    case kReduce: {
      mutable_reduce()->::CoreML::Specification::ReduceLayerParams::MergeFrom(from.reduce());
      break;
    }
    case kLoadConstant: {
      mutable_loadconstant()->::CoreML::Specification::LoadConstantLayerParams::MergeFrom(from.loadconstant());
      break;
    }
    case kReshape: {
      mutable_reshape()->::CoreML::Specification::ReshapeLayerParams::MergeFrom(from.reshape());
      break;
    }
    case kFlatten: {
      mutable_flatten()->::CoreML::Specification::FlattenLayerParams::MergeFrom(from.flatten());
      break;
    }
    case kPermute: {
      mutable_permute()->::CoreML::Specification::PermuteLayerParams::MergeFrom(from.permute());
      break;
    }
    case kConcat: {
      mutable_concat()->::CoreML::Specification::ConcatLayerParams::MergeFrom(from.concat());
      break;
    }
    case kSplit: {
      mutable_split()->::CoreML::Specification::SplitLayerParams::MergeFrom(from.split());
      break;
    }
    case kSequenceRepeat: {
      mutable_sequencerepeat()->::CoreML::Specification::SequenceRepeatLayerParams::MergeFrom(from.sequencerepeat());
      break;
    }
    case kReorganizeData: {
      mutable_reorganizedata()->::CoreML::Specification::ReorganizeDataLayerParams::MergeFrom(from.reorganizedata());
      break;
    }
    case kSlice: {
      mutable_slice()->::CoreML::Specification::SliceLayerParams::MergeFrom(from.slice());
      break;
    }
    case kSimpleRecurrent: {
      mutable_simplerecurrent()->::CoreML::Specification::SimpleRecurrentLayerParams::MergeFrom(from.simplerecurrent());
      break;
    }
    case kGru: {
      mutable_gru()->::CoreML::Specification::GRULayerParams::MergeFrom(from.gru());
      break;
    }
    case kUniDirectionalLSTM: {
      mutable_unidirectionallstm()->::CoreML::Specification::UniDirectionalLSTMLayerParams::MergeFrom(from.unidirectionallstm());
      break;
    }
    case kBiDirectionalLSTM: {
      mutable_bidirectionallstm()->::CoreML::Specification::BiDirectionalLSTMLayerParams::MergeFrom(from.bidirectionallstm());
      break;
    }
    case kCustom: {
      mutable_custom()->::CoreML::Specification::CustomLayerParams::MergeFrom(from.custom());
      break;
    }
    case LAYER_NOT_SET: {
      break;
    }
  }
}

void NeuralNetworkLayer::CopyFrom(const NeuralNetworkLayer& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkLayer)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NeuralNetworkLayer::IsInitialized() const {
  return true;
}

void NeuralNetworkLayer::Swap(NeuralNetworkLayer* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkLayer::InternalSwap(NeuralNetworkLayer* other) {
  input_.InternalSwap(&other->input_);
  output_.InternalSwap(&other->output_);
  name_.Swap(&other->name_);
  std::swap(layer_, other->layer_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkLayer::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkLayer";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkLayer

// string name = 1;
void NeuralNetworkLayer::clear_name() {
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& NeuralNetworkLayer::name() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.name)
  return name_.GetNoArena();
}
void NeuralNetworkLayer::set_name(const ::std::string& value) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.name)
}
#if LANG_CXX11
void NeuralNetworkLayer::set_name(::std::string&& value) {
  
  name_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.NeuralNetworkLayer.name)
}
#endif
void NeuralNetworkLayer::set_name(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkLayer.name)
}
void NeuralNetworkLayer::set_name(const char* value, size_t size) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkLayer.name)
}
::std::string* NeuralNetworkLayer::mutable_name() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.name)
  return name_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* NeuralNetworkLayer::release_name() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.name)
  
  return name_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void NeuralNetworkLayer::set_allocated_name(::std::string* name) {
  if (name != NULL) {
    
  } else {
    
  }
  name_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), name);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.name)
}

// repeated string input = 2;
int NeuralNetworkLayer::input_size() const {
  return input_.size();
}
void NeuralNetworkLayer::clear_input() {
  input_.Clear();
}
const ::std::string& NeuralNetworkLayer::input(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.input)
  return input_.Get(index);
}
::std::string* NeuralNetworkLayer::mutable_input(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.input)
  return input_.Mutable(index);
}
void NeuralNetworkLayer::set_input(int index, const ::std::string& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.input)
  input_.Mutable(index)->assign(value);
}
#if LANG_CXX11
void NeuralNetworkLayer::set_input(int index, ::std::string&& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.input)
  input_.Mutable(index)->assign(std::move(value));
}
#endif
void NeuralNetworkLayer::set_input(int index, const char* value) {
  GOOGLE_DCHECK(value != NULL);
  input_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkLayer.input)
}
void NeuralNetworkLayer::set_input(int index, const char* value, size_t size) {
  input_.Mutable(index)->assign(
    reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkLayer.input)
}
::std::string* NeuralNetworkLayer::add_input() {
  // @@protoc_insertion_point(field_add_mutable:CoreML.Specification.NeuralNetworkLayer.input)
  return input_.Add();
}
void NeuralNetworkLayer::add_input(const ::std::string& value) {
  input_.Add()->assign(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.input)
}
#if LANG_CXX11
void NeuralNetworkLayer::add_input(::std::string&& value) {
  input_.Add(std::move(value));
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.input)
}
#endif
void NeuralNetworkLayer::add_input(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  input_.Add()->assign(value);
  // @@protoc_insertion_point(field_add_char:CoreML.Specification.NeuralNetworkLayer.input)
}
void NeuralNetworkLayer::add_input(const char* value, size_t size) {
  input_.Add()->assign(reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_add_pointer:CoreML.Specification.NeuralNetworkLayer.input)
}
const ::google::protobuf::RepeatedPtrField< ::std::string>&
NeuralNetworkLayer::input() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.input)
  return input_;
}
::google::protobuf::RepeatedPtrField< ::std::string>*
NeuralNetworkLayer::mutable_input() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.input)
  return &input_;
}

// repeated string output = 3;
int NeuralNetworkLayer::output_size() const {
  return output_.size();
}
void NeuralNetworkLayer::clear_output() {
  output_.Clear();
}
const ::std::string& NeuralNetworkLayer::output(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.output)
  return output_.Get(index);
}
::std::string* NeuralNetworkLayer::mutable_output(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.output)
  return output_.Mutable(index);
}
void NeuralNetworkLayer::set_output(int index, const ::std::string& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.output)
  output_.Mutable(index)->assign(value);
}
#if LANG_CXX11
void NeuralNetworkLayer::set_output(int index, ::std::string&& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.output)
  output_.Mutable(index)->assign(std::move(value));
}
#endif
void NeuralNetworkLayer::set_output(int index, const char* value) {
  GOOGLE_DCHECK(value != NULL);
  output_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkLayer.output)
}
void NeuralNetworkLayer::set_output(int index, const char* value, size_t size) {
  output_.Mutable(index)->assign(
    reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkLayer.output)
}
::std::string* NeuralNetworkLayer::add_output() {
  // @@protoc_insertion_point(field_add_mutable:CoreML.Specification.NeuralNetworkLayer.output)
  return output_.Add();
}
void NeuralNetworkLayer::add_output(const ::std::string& value) {
  output_.Add()->assign(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.output)
}
#if LANG_CXX11
void NeuralNetworkLayer::add_output(::std::string&& value) {
  output_.Add(std::move(value));
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.output)
}
#endif
void NeuralNetworkLayer::add_output(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  output_.Add()->assign(value);
  // @@protoc_insertion_point(field_add_char:CoreML.Specification.NeuralNetworkLayer.output)
}
void NeuralNetworkLayer::add_output(const char* value, size_t size) {
  output_.Add()->assign(reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_add_pointer:CoreML.Specification.NeuralNetworkLayer.output)
}
const ::google::protobuf::RepeatedPtrField< ::std::string>&
NeuralNetworkLayer::output() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.output)
  return output_;
}
::google::protobuf::RepeatedPtrField< ::std::string>*
NeuralNetworkLayer::mutable_output() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.output)
  return &output_;
}

// .CoreML.Specification.ConvolutionLayerParams convolution = 100;
bool NeuralNetworkLayer::has_convolution() const {
  return layer_case() == kConvolution;
}
void NeuralNetworkLayer::set_has_convolution() {
  _oneof_case_[0] = kConvolution;
}
void NeuralNetworkLayer::clear_convolution() {
  if (has_convolution()) {
    delete layer_.convolution_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ConvolutionLayerParams& NeuralNetworkLayer::convolution() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.convolution)
  return has_convolution()
      ? *layer_.convolution_
      : ::CoreML::Specification::ConvolutionLayerParams::default_instance();
}
::CoreML::Specification::ConvolutionLayerParams* NeuralNetworkLayer::mutable_convolution() {
  if (!has_convolution()) {
    clear_layer();
    set_has_convolution();
    layer_.convolution_ = new ::CoreML::Specification::ConvolutionLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.convolution)
  return layer_.convolution_;
}
::CoreML::Specification::ConvolutionLayerParams* NeuralNetworkLayer::release_convolution() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.convolution)
  if (has_convolution()) {
    clear_has_layer();
    ::CoreML::Specification::ConvolutionLayerParams* temp = layer_.convolution_;
    layer_.convolution_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_convolution(::CoreML::Specification::ConvolutionLayerParams* convolution) {
  clear_layer();
  if (convolution) {
    set_has_convolution();
    layer_.convolution_ = convolution;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.convolution)
}

// .CoreML.Specification.PoolingLayerParams pooling = 120;
bool NeuralNetworkLayer::has_pooling() const {
  return layer_case() == kPooling;
}
void NeuralNetworkLayer::set_has_pooling() {
  _oneof_case_[0] = kPooling;
}
void NeuralNetworkLayer::clear_pooling() {
  if (has_pooling()) {
    delete layer_.pooling_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::PoolingLayerParams& NeuralNetworkLayer::pooling() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.pooling)
  return has_pooling()
      ? *layer_.pooling_
      : ::CoreML::Specification::PoolingLayerParams::default_instance();
}
::CoreML::Specification::PoolingLayerParams* NeuralNetworkLayer::mutable_pooling() {
  if (!has_pooling()) {
    clear_layer();
    set_has_pooling();
    layer_.pooling_ = new ::CoreML::Specification::PoolingLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.pooling)
  return layer_.pooling_;
}
::CoreML::Specification::PoolingLayerParams* NeuralNetworkLayer::release_pooling() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.pooling)
  if (has_pooling()) {
    clear_has_layer();
    ::CoreML::Specification::PoolingLayerParams* temp = layer_.pooling_;
    layer_.pooling_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_pooling(::CoreML::Specification::PoolingLayerParams* pooling) {
  clear_layer();
  if (pooling) {
    set_has_pooling();
    layer_.pooling_ = pooling;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.pooling)
}

// .CoreML.Specification.ActivationParams activation = 130;
bool NeuralNetworkLayer::has_activation() const {
  return layer_case() == kActivation;
}
void NeuralNetworkLayer::set_has_activation() {
  _oneof_case_[0] = kActivation;
}
void NeuralNetworkLayer::clear_activation() {
  if (has_activation()) {
    delete layer_.activation_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ActivationParams& NeuralNetworkLayer::activation() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.activation)
  return has_activation()
      ? *layer_.activation_
      : ::CoreML::Specification::ActivationParams::default_instance();
}
::CoreML::Specification::ActivationParams* NeuralNetworkLayer::mutable_activation() {
  if (!has_activation()) {
    clear_layer();
    set_has_activation();
    layer_.activation_ = new ::CoreML::Specification::ActivationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.activation)
  return layer_.activation_;
}
::CoreML::Specification::ActivationParams* NeuralNetworkLayer::release_activation() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.activation)
  if (has_activation()) {
    clear_has_layer();
    ::CoreML::Specification::ActivationParams* temp = layer_.activation_;
    layer_.activation_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_activation(::CoreML::Specification::ActivationParams* activation) {
  clear_layer();
  if (activation) {
    set_has_activation();
    layer_.activation_ = activation;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.activation)
}

// .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
bool NeuralNetworkLayer::has_innerproduct() const {
  return layer_case() == kInnerProduct;
}
void NeuralNetworkLayer::set_has_innerproduct() {
  _oneof_case_[0] = kInnerProduct;
}
void NeuralNetworkLayer::clear_innerproduct() {
  if (has_innerproduct()) {
    delete layer_.innerproduct_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::InnerProductLayerParams& NeuralNetworkLayer::innerproduct() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  return has_innerproduct()
      ? *layer_.innerproduct_
      : ::CoreML::Specification::InnerProductLayerParams::default_instance();
}
::CoreML::Specification::InnerProductLayerParams* NeuralNetworkLayer::mutable_innerproduct() {
  if (!has_innerproduct()) {
    clear_layer();
    set_has_innerproduct();
    layer_.innerproduct_ = new ::CoreML::Specification::InnerProductLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  return layer_.innerproduct_;
}
::CoreML::Specification::InnerProductLayerParams* NeuralNetworkLayer::release_innerproduct() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  if (has_innerproduct()) {
    clear_has_layer();
    ::CoreML::Specification::InnerProductLayerParams* temp = layer_.innerproduct_;
    layer_.innerproduct_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_innerproduct(::CoreML::Specification::InnerProductLayerParams* innerproduct) {
  clear_layer();
  if (innerproduct) {
    set_has_innerproduct();
    layer_.innerproduct_ = innerproduct;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.innerProduct)
}

// .CoreML.Specification.EmbeddingLayerParams embedding = 150;
bool NeuralNetworkLayer::has_embedding() const {
  return layer_case() == kEmbedding;
}
void NeuralNetworkLayer::set_has_embedding() {
  _oneof_case_[0] = kEmbedding;
}
void NeuralNetworkLayer::clear_embedding() {
  if (has_embedding()) {
    delete layer_.embedding_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::EmbeddingLayerParams& NeuralNetworkLayer::embedding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.embedding)
  return has_embedding()
      ? *layer_.embedding_
      : ::CoreML::Specification::EmbeddingLayerParams::default_instance();
}
::CoreML::Specification::EmbeddingLayerParams* NeuralNetworkLayer::mutable_embedding() {
  if (!has_embedding()) {
    clear_layer();
    set_has_embedding();
    layer_.embedding_ = new ::CoreML::Specification::EmbeddingLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.embedding)
  return layer_.embedding_;
}
::CoreML::Specification::EmbeddingLayerParams* NeuralNetworkLayer::release_embedding() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.embedding)
  if (has_embedding()) {
    clear_has_layer();
    ::CoreML::Specification::EmbeddingLayerParams* temp = layer_.embedding_;
    layer_.embedding_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_embedding(::CoreML::Specification::EmbeddingLayerParams* embedding) {
  clear_layer();
  if (embedding) {
    set_has_embedding();
    layer_.embedding_ = embedding;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.embedding)
}

// .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
bool NeuralNetworkLayer::has_batchnorm() const {
  return layer_case() == kBatchnorm;
}
void NeuralNetworkLayer::set_has_batchnorm() {
  _oneof_case_[0] = kBatchnorm;
}
void NeuralNetworkLayer::clear_batchnorm() {
  if (has_batchnorm()) {
    delete layer_.batchnorm_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BatchnormLayerParams& NeuralNetworkLayer::batchnorm() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  return has_batchnorm()
      ? *layer_.batchnorm_
      : ::CoreML::Specification::BatchnormLayerParams::default_instance();
}
::CoreML::Specification::BatchnormLayerParams* NeuralNetworkLayer::mutable_batchnorm() {
  if (!has_batchnorm()) {
    clear_layer();
    set_has_batchnorm();
    layer_.batchnorm_ = new ::CoreML::Specification::BatchnormLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  return layer_.batchnorm_;
}
::CoreML::Specification::BatchnormLayerParams* NeuralNetworkLayer::release_batchnorm() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  if (has_batchnorm()) {
    clear_has_layer();
    ::CoreML::Specification::BatchnormLayerParams* temp = layer_.batchnorm_;
    layer_.batchnorm_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_batchnorm(::CoreML::Specification::BatchnormLayerParams* batchnorm) {
  clear_layer();
  if (batchnorm) {
    set_has_batchnorm();
    layer_.batchnorm_ = batchnorm;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.batchnorm)
}

// .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
bool NeuralNetworkLayer::has_mvn() const {
  return layer_case() == kMvn;
}
void NeuralNetworkLayer::set_has_mvn() {
  _oneof_case_[0] = kMvn;
}
void NeuralNetworkLayer::clear_mvn() {
  if (has_mvn()) {
    delete layer_.mvn_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::MeanVarianceNormalizeLayerParams& NeuralNetworkLayer::mvn() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.mvn)
  return has_mvn()
      ? *layer_.mvn_
      : ::CoreML::Specification::MeanVarianceNormalizeLayerParams::default_instance();
}
::CoreML::Specification::MeanVarianceNormalizeLayerParams* NeuralNetworkLayer::mutable_mvn() {
  if (!has_mvn()) {
    clear_layer();
    set_has_mvn();
    layer_.mvn_ = new ::CoreML::Specification::MeanVarianceNormalizeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.mvn)
  return layer_.mvn_;
}
::CoreML::Specification::MeanVarianceNormalizeLayerParams* NeuralNetworkLayer::release_mvn() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.mvn)
  if (has_mvn()) {
    clear_has_layer();
    ::CoreML::Specification::MeanVarianceNormalizeLayerParams* temp = layer_.mvn_;
    layer_.mvn_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_mvn(::CoreML::Specification::MeanVarianceNormalizeLayerParams* mvn) {
  clear_layer();
  if (mvn) {
    set_has_mvn();
    layer_.mvn_ = mvn;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.mvn)
}

// .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
bool NeuralNetworkLayer::has_l2normalize() const {
  return layer_case() == kL2Normalize;
}
void NeuralNetworkLayer::set_has_l2normalize() {
  _oneof_case_[0] = kL2Normalize;
}
void NeuralNetworkLayer::clear_l2normalize() {
  if (has_l2normalize()) {
    delete layer_.l2normalize_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::L2NormalizeLayerParams& NeuralNetworkLayer::l2normalize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  return has_l2normalize()
      ? *layer_.l2normalize_
      : ::CoreML::Specification::L2NormalizeLayerParams::default_instance();
}
::CoreML::Specification::L2NormalizeLayerParams* NeuralNetworkLayer::mutable_l2normalize() {
  if (!has_l2normalize()) {
    clear_layer();
    set_has_l2normalize();
    layer_.l2normalize_ = new ::CoreML::Specification::L2NormalizeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  return layer_.l2normalize_;
}
::CoreML::Specification::L2NormalizeLayerParams* NeuralNetworkLayer::release_l2normalize() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  if (has_l2normalize()) {
    clear_has_layer();
    ::CoreML::Specification::L2NormalizeLayerParams* temp = layer_.l2normalize_;
    layer_.l2normalize_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_l2normalize(::CoreML::Specification::L2NormalizeLayerParams* l2normalize) {
  clear_layer();
  if (l2normalize) {
    set_has_l2normalize();
    layer_.l2normalize_ = l2normalize;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.l2normalize)
}

// .CoreML.Specification.SoftmaxLayerParams softmax = 175;
bool NeuralNetworkLayer::has_softmax() const {
  return layer_case() == kSoftmax;
}
void NeuralNetworkLayer::set_has_softmax() {
  _oneof_case_[0] = kSoftmax;
}
void NeuralNetworkLayer::clear_softmax() {
  if (has_softmax()) {
    delete layer_.softmax_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SoftmaxLayerParams& NeuralNetworkLayer::softmax() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.softmax)
  return has_softmax()
      ? *layer_.softmax_
      : ::CoreML::Specification::SoftmaxLayerParams::default_instance();
}
::CoreML::Specification::SoftmaxLayerParams* NeuralNetworkLayer::mutable_softmax() {
  if (!has_softmax()) {
    clear_layer();
    set_has_softmax();
    layer_.softmax_ = new ::CoreML::Specification::SoftmaxLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.softmax)
  return layer_.softmax_;
}
::CoreML::Specification::SoftmaxLayerParams* NeuralNetworkLayer::release_softmax() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.softmax)
  if (has_softmax()) {
    clear_has_layer();
    ::CoreML::Specification::SoftmaxLayerParams* temp = layer_.softmax_;
    layer_.softmax_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_softmax(::CoreML::Specification::SoftmaxLayerParams* softmax) {
  clear_layer();
  if (softmax) {
    set_has_softmax();
    layer_.softmax_ = softmax;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.softmax)
}

// .CoreML.Specification.LRNLayerParams lrn = 180;
bool NeuralNetworkLayer::has_lrn() const {
  return layer_case() == kLrn;
}
void NeuralNetworkLayer::set_has_lrn() {
  _oneof_case_[0] = kLrn;
}
void NeuralNetworkLayer::clear_lrn() {
  if (has_lrn()) {
    delete layer_.lrn_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LRNLayerParams& NeuralNetworkLayer::lrn() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.lrn)
  return has_lrn()
      ? *layer_.lrn_
      : ::CoreML::Specification::LRNLayerParams::default_instance();
}
::CoreML::Specification::LRNLayerParams* NeuralNetworkLayer::mutable_lrn() {
  if (!has_lrn()) {
    clear_layer();
    set_has_lrn();
    layer_.lrn_ = new ::CoreML::Specification::LRNLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.lrn)
  return layer_.lrn_;
}
::CoreML::Specification::LRNLayerParams* NeuralNetworkLayer::release_lrn() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.lrn)
  if (has_lrn()) {
    clear_has_layer();
    ::CoreML::Specification::LRNLayerParams* temp = layer_.lrn_;
    layer_.lrn_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_lrn(::CoreML::Specification::LRNLayerParams* lrn) {
  clear_layer();
  if (lrn) {
    set_has_lrn();
    layer_.lrn_ = lrn;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.lrn)
}

// .CoreML.Specification.CropLayerParams crop = 190;
bool NeuralNetworkLayer::has_crop() const {
  return layer_case() == kCrop;
}
void NeuralNetworkLayer::set_has_crop() {
  _oneof_case_[0] = kCrop;
}
void NeuralNetworkLayer::clear_crop() {
  if (has_crop()) {
    delete layer_.crop_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::CropLayerParams& NeuralNetworkLayer::crop() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.crop)
  return has_crop()
      ? *layer_.crop_
      : ::CoreML::Specification::CropLayerParams::default_instance();
}
::CoreML::Specification::CropLayerParams* NeuralNetworkLayer::mutable_crop() {
  if (!has_crop()) {
    clear_layer();
    set_has_crop();
    layer_.crop_ = new ::CoreML::Specification::CropLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.crop)
  return layer_.crop_;
}
::CoreML::Specification::CropLayerParams* NeuralNetworkLayer::release_crop() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.crop)
  if (has_crop()) {
    clear_has_layer();
    ::CoreML::Specification::CropLayerParams* temp = layer_.crop_;
    layer_.crop_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_crop(::CoreML::Specification::CropLayerParams* crop) {
  clear_layer();
  if (crop) {
    set_has_crop();
    layer_.crop_ = crop;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.crop)
}

// .CoreML.Specification.PaddingLayerParams padding = 200;
bool NeuralNetworkLayer::has_padding() const {
  return layer_case() == kPadding;
}
void NeuralNetworkLayer::set_has_padding() {
  _oneof_case_[0] = kPadding;
}
void NeuralNetworkLayer::clear_padding() {
  if (has_padding()) {
    delete layer_.padding_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::PaddingLayerParams& NeuralNetworkLayer::padding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.padding)
  return has_padding()
      ? *layer_.padding_
      : ::CoreML::Specification::PaddingLayerParams::default_instance();
}
::CoreML::Specification::PaddingLayerParams* NeuralNetworkLayer::mutable_padding() {
  if (!has_padding()) {
    clear_layer();
    set_has_padding();
    layer_.padding_ = new ::CoreML::Specification::PaddingLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.padding)
  return layer_.padding_;
}
::CoreML::Specification::PaddingLayerParams* NeuralNetworkLayer::release_padding() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.padding)
  if (has_padding()) {
    clear_has_layer();
    ::CoreML::Specification::PaddingLayerParams* temp = layer_.padding_;
    layer_.padding_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_padding(::CoreML::Specification::PaddingLayerParams* padding) {
  clear_layer();
  if (padding) {
    set_has_padding();
    layer_.padding_ = padding;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.padding)
}

// .CoreML.Specification.UpsampleLayerParams upsample = 210;
bool NeuralNetworkLayer::has_upsample() const {
  return layer_case() == kUpsample;
}
void NeuralNetworkLayer::set_has_upsample() {
  _oneof_case_[0] = kUpsample;
}
void NeuralNetworkLayer::clear_upsample() {
  if (has_upsample()) {
    delete layer_.upsample_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::UpsampleLayerParams& NeuralNetworkLayer::upsample() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.upsample)
  return has_upsample()
      ? *layer_.upsample_
      : ::CoreML::Specification::UpsampleLayerParams::default_instance();
}
::CoreML::Specification::UpsampleLayerParams* NeuralNetworkLayer::mutable_upsample() {
  if (!has_upsample()) {
    clear_layer();
    set_has_upsample();
    layer_.upsample_ = new ::CoreML::Specification::UpsampleLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.upsample)
  return layer_.upsample_;
}
::CoreML::Specification::UpsampleLayerParams* NeuralNetworkLayer::release_upsample() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.upsample)
  if (has_upsample()) {
    clear_has_layer();
    ::CoreML::Specification::UpsampleLayerParams* temp = layer_.upsample_;
    layer_.upsample_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_upsample(::CoreML::Specification::UpsampleLayerParams* upsample) {
  clear_layer();
  if (upsample) {
    set_has_upsample();
    layer_.upsample_ = upsample;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.upsample)
}

// .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
bool NeuralNetworkLayer::has_unary() const {
  return layer_case() == kUnary;
}
void NeuralNetworkLayer::set_has_unary() {
  _oneof_case_[0] = kUnary;
}
void NeuralNetworkLayer::clear_unary() {
  if (has_unary()) {
    delete layer_.unary_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::UnaryFunctionLayerParams& NeuralNetworkLayer::unary() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.unary)
  return has_unary()
      ? *layer_.unary_
      : ::CoreML::Specification::UnaryFunctionLayerParams::default_instance();
}
::CoreML::Specification::UnaryFunctionLayerParams* NeuralNetworkLayer::mutable_unary() {
  if (!has_unary()) {
    clear_layer();
    set_has_unary();
    layer_.unary_ = new ::CoreML::Specification::UnaryFunctionLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.unary)
  return layer_.unary_;
}
::CoreML::Specification::UnaryFunctionLayerParams* NeuralNetworkLayer::release_unary() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.unary)
  if (has_unary()) {
    clear_has_layer();
    ::CoreML::Specification::UnaryFunctionLayerParams* temp = layer_.unary_;
    layer_.unary_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_unary(::CoreML::Specification::UnaryFunctionLayerParams* unary) {
  clear_layer();
  if (unary) {
    set_has_unary();
    layer_.unary_ = unary;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.unary)
}

// .CoreML.Specification.AddLayerParams add = 230;
bool NeuralNetworkLayer::has_add() const {
  return layer_case() == kAdd;
}
void NeuralNetworkLayer::set_has_add() {
  _oneof_case_[0] = kAdd;
}
void NeuralNetworkLayer::clear_add() {
  if (has_add()) {
    delete layer_.add_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::AddLayerParams& NeuralNetworkLayer::add() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.add)
  return has_add()
      ? *layer_.add_
      : ::CoreML::Specification::AddLayerParams::default_instance();
}
::CoreML::Specification::AddLayerParams* NeuralNetworkLayer::mutable_add() {
  if (!has_add()) {
    clear_layer();
    set_has_add();
    layer_.add_ = new ::CoreML::Specification::AddLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.add)
  return layer_.add_;
}
::CoreML::Specification::AddLayerParams* NeuralNetworkLayer::release_add() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.add)
  if (has_add()) {
    clear_has_layer();
    ::CoreML::Specification::AddLayerParams* temp = layer_.add_;
    layer_.add_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_add(::CoreML::Specification::AddLayerParams* add) {
  clear_layer();
  if (add) {
    set_has_add();
    layer_.add_ = add;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.add)
}

// .CoreML.Specification.MultiplyLayerParams multiply = 231;
bool NeuralNetworkLayer::has_multiply() const {
  return layer_case() == kMultiply;
}
void NeuralNetworkLayer::set_has_multiply() {
  _oneof_case_[0] = kMultiply;
}
void NeuralNetworkLayer::clear_multiply() {
  if (has_multiply()) {
    delete layer_.multiply_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::MultiplyLayerParams& NeuralNetworkLayer::multiply() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.multiply)
  return has_multiply()
      ? *layer_.multiply_
      : ::CoreML::Specification::MultiplyLayerParams::default_instance();
}
::CoreML::Specification::MultiplyLayerParams* NeuralNetworkLayer::mutable_multiply() {
  if (!has_multiply()) {
    clear_layer();
    set_has_multiply();
    layer_.multiply_ = new ::CoreML::Specification::MultiplyLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.multiply)
  return layer_.multiply_;
}
::CoreML::Specification::MultiplyLayerParams* NeuralNetworkLayer::release_multiply() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.multiply)
  if (has_multiply()) {
    clear_has_layer();
    ::CoreML::Specification::MultiplyLayerParams* temp = layer_.multiply_;
    layer_.multiply_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_multiply(::CoreML::Specification::MultiplyLayerParams* multiply) {
  clear_layer();
  if (multiply) {
    set_has_multiply();
    layer_.multiply_ = multiply;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.multiply)
}

// .CoreML.Specification.AverageLayerParams average = 240;
bool NeuralNetworkLayer::has_average() const {
  return layer_case() == kAverage;
}
void NeuralNetworkLayer::set_has_average() {
  _oneof_case_[0] = kAverage;
}
void NeuralNetworkLayer::clear_average() {
  if (has_average()) {
    delete layer_.average_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::AverageLayerParams& NeuralNetworkLayer::average() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.average)
  return has_average()
      ? *layer_.average_
      : ::CoreML::Specification::AverageLayerParams::default_instance();
}
::CoreML::Specification::AverageLayerParams* NeuralNetworkLayer::mutable_average() {
  if (!has_average()) {
    clear_layer();
    set_has_average();
    layer_.average_ = new ::CoreML::Specification::AverageLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.average)
  return layer_.average_;
}
::CoreML::Specification::AverageLayerParams* NeuralNetworkLayer::release_average() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.average)
  if (has_average()) {
    clear_has_layer();
    ::CoreML::Specification::AverageLayerParams* temp = layer_.average_;
    layer_.average_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_average(::CoreML::Specification::AverageLayerParams* average) {
  clear_layer();
  if (average) {
    set_has_average();
    layer_.average_ = average;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.average)
}

// .CoreML.Specification.ScaleLayerParams scale = 245;
bool NeuralNetworkLayer::has_scale() const {
  return layer_case() == kScale;
}
void NeuralNetworkLayer::set_has_scale() {
  _oneof_case_[0] = kScale;
}
void NeuralNetworkLayer::clear_scale() {
  if (has_scale()) {
    delete layer_.scale_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ScaleLayerParams& NeuralNetworkLayer::scale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.scale)
  return has_scale()
      ? *layer_.scale_
      : ::CoreML::Specification::ScaleLayerParams::default_instance();
}
::CoreML::Specification::ScaleLayerParams* NeuralNetworkLayer::mutable_scale() {
  if (!has_scale()) {
    clear_layer();
    set_has_scale();
    layer_.scale_ = new ::CoreML::Specification::ScaleLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.scale)
  return layer_.scale_;
}
::CoreML::Specification::ScaleLayerParams* NeuralNetworkLayer::release_scale() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.scale)
  if (has_scale()) {
    clear_has_layer();
    ::CoreML::Specification::ScaleLayerParams* temp = layer_.scale_;
    layer_.scale_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_scale(::CoreML::Specification::ScaleLayerParams* scale) {
  clear_layer();
  if (scale) {
    set_has_scale();
    layer_.scale_ = scale;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.scale)
}

// .CoreML.Specification.BiasLayerParams bias = 250;
bool NeuralNetworkLayer::has_bias() const {
  return layer_case() == kBias;
}
void NeuralNetworkLayer::set_has_bias() {
  _oneof_case_[0] = kBias;
}
void NeuralNetworkLayer::clear_bias() {
  if (has_bias()) {
    delete layer_.bias_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BiasLayerParams& NeuralNetworkLayer::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.bias)
  return has_bias()
      ? *layer_.bias_
      : ::CoreML::Specification::BiasLayerParams::default_instance();
}
::CoreML::Specification::BiasLayerParams* NeuralNetworkLayer::mutable_bias() {
  if (!has_bias()) {
    clear_layer();
    set_has_bias();
    layer_.bias_ = new ::CoreML::Specification::BiasLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.bias)
  return layer_.bias_;
}
::CoreML::Specification::BiasLayerParams* NeuralNetworkLayer::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.bias)
  if (has_bias()) {
    clear_has_layer();
    ::CoreML::Specification::BiasLayerParams* temp = layer_.bias_;
    layer_.bias_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_bias(::CoreML::Specification::BiasLayerParams* bias) {
  clear_layer();
  if (bias) {
    set_has_bias();
    layer_.bias_ = bias;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.bias)
}

// .CoreML.Specification.MaxLayerParams max = 260;
bool NeuralNetworkLayer::has_max() const {
  return layer_case() == kMax;
}
void NeuralNetworkLayer::set_has_max() {
  _oneof_case_[0] = kMax;
}
void NeuralNetworkLayer::clear_max() {
  if (has_max()) {
    delete layer_.max_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::MaxLayerParams& NeuralNetworkLayer::max() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.max)
  return has_max()
      ? *layer_.max_
      : ::CoreML::Specification::MaxLayerParams::default_instance();
}
::CoreML::Specification::MaxLayerParams* NeuralNetworkLayer::mutable_max() {
  if (!has_max()) {
    clear_layer();
    set_has_max();
    layer_.max_ = new ::CoreML::Specification::MaxLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.max)
  return layer_.max_;
}
::CoreML::Specification::MaxLayerParams* NeuralNetworkLayer::release_max() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.max)
  if (has_max()) {
    clear_has_layer();
    ::CoreML::Specification::MaxLayerParams* temp = layer_.max_;
    layer_.max_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_max(::CoreML::Specification::MaxLayerParams* max) {
  clear_layer();
  if (max) {
    set_has_max();
    layer_.max_ = max;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.max)
}

// .CoreML.Specification.MinLayerParams min = 261;
bool NeuralNetworkLayer::has_min() const {
  return layer_case() == kMin;
}
void NeuralNetworkLayer::set_has_min() {
  _oneof_case_[0] = kMin;
}
void NeuralNetworkLayer::clear_min() {
  if (has_min()) {
    delete layer_.min_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::MinLayerParams& NeuralNetworkLayer::min() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.min)
  return has_min()
      ? *layer_.min_
      : ::CoreML::Specification::MinLayerParams::default_instance();
}
::CoreML::Specification::MinLayerParams* NeuralNetworkLayer::mutable_min() {
  if (!has_min()) {
    clear_layer();
    set_has_min();
    layer_.min_ = new ::CoreML::Specification::MinLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.min)
  return layer_.min_;
}
::CoreML::Specification::MinLayerParams* NeuralNetworkLayer::release_min() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.min)
  if (has_min()) {
    clear_has_layer();
    ::CoreML::Specification::MinLayerParams* temp = layer_.min_;
    layer_.min_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_min(::CoreML::Specification::MinLayerParams* min) {
  clear_layer();
  if (min) {
    set_has_min();
    layer_.min_ = min;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.min)
}

// .CoreML.Specification.DotProductLayerParams dot = 270;
bool NeuralNetworkLayer::has_dot() const {
  return layer_case() == kDot;
}
void NeuralNetworkLayer::set_has_dot() {
  _oneof_case_[0] = kDot;
}
void NeuralNetworkLayer::clear_dot() {
  if (has_dot()) {
    delete layer_.dot_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::DotProductLayerParams& NeuralNetworkLayer::dot() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.dot)
  return has_dot()
      ? *layer_.dot_
      : ::CoreML::Specification::DotProductLayerParams::default_instance();
}
::CoreML::Specification::DotProductLayerParams* NeuralNetworkLayer::mutable_dot() {
  if (!has_dot()) {
    clear_layer();
    set_has_dot();
    layer_.dot_ = new ::CoreML::Specification::DotProductLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.dot)
  return layer_.dot_;
}
::CoreML::Specification::DotProductLayerParams* NeuralNetworkLayer::release_dot() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.dot)
  if (has_dot()) {
    clear_has_layer();
    ::CoreML::Specification::DotProductLayerParams* temp = layer_.dot_;
    layer_.dot_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_dot(::CoreML::Specification::DotProductLayerParams* dot) {
  clear_layer();
  if (dot) {
    set_has_dot();
    layer_.dot_ = dot;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.dot)
}

// .CoreML.Specification.ReduceLayerParams reduce = 280;
bool NeuralNetworkLayer::has_reduce() const {
  return layer_case() == kReduce;
}
void NeuralNetworkLayer::set_has_reduce() {
  _oneof_case_[0] = kReduce;
}
void NeuralNetworkLayer::clear_reduce() {
  if (has_reduce()) {
    delete layer_.reduce_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReduceLayerParams& NeuralNetworkLayer::reduce() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduce)
  return has_reduce()
      ? *layer_.reduce_
      : ::CoreML::Specification::ReduceLayerParams::default_instance();
}
::CoreML::Specification::ReduceLayerParams* NeuralNetworkLayer::mutable_reduce() {
  if (!has_reduce()) {
    clear_layer();
    set_has_reduce();
    layer_.reduce_ = new ::CoreML::Specification::ReduceLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduce)
  return layer_.reduce_;
}
::CoreML::Specification::ReduceLayerParams* NeuralNetworkLayer::release_reduce() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduce)
  if (has_reduce()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceLayerParams* temp = layer_.reduce_;
    layer_.reduce_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reduce(::CoreML::Specification::ReduceLayerParams* reduce) {
  clear_layer();
  if (reduce) {
    set_has_reduce();
    layer_.reduce_ = reduce;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduce)
}

// .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
bool NeuralNetworkLayer::has_loadconstant() const {
  return layer_case() == kLoadConstant;
}
void NeuralNetworkLayer::set_has_loadconstant() {
  _oneof_case_[0] = kLoadConstant;
}
void NeuralNetworkLayer::clear_loadconstant() {
  if (has_loadconstant()) {
    delete layer_.loadconstant_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LoadConstantLayerParams& NeuralNetworkLayer::loadconstant() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  return has_loadconstant()
      ? *layer_.loadconstant_
      : ::CoreML::Specification::LoadConstantLayerParams::default_instance();
}
::CoreML::Specification::LoadConstantLayerParams* NeuralNetworkLayer::mutable_loadconstant() {
  if (!has_loadconstant()) {
    clear_layer();
    set_has_loadconstant();
    layer_.loadconstant_ = new ::CoreML::Specification::LoadConstantLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  return layer_.loadconstant_;
}
::CoreML::Specification::LoadConstantLayerParams* NeuralNetworkLayer::release_loadconstant() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  if (has_loadconstant()) {
    clear_has_layer();
    ::CoreML::Specification::LoadConstantLayerParams* temp = layer_.loadconstant_;
    layer_.loadconstant_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_loadconstant(::CoreML::Specification::LoadConstantLayerParams* loadconstant) {
  clear_layer();
  if (loadconstant) {
    set_has_loadconstant();
    layer_.loadconstant_ = loadconstant;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.loadConstant)
}

// .CoreML.Specification.ReshapeLayerParams reshape = 300;
bool NeuralNetworkLayer::has_reshape() const {
  return layer_case() == kReshape;
}
void NeuralNetworkLayer::set_has_reshape() {
  _oneof_case_[0] = kReshape;
}
void NeuralNetworkLayer::clear_reshape() {
  if (has_reshape()) {
    delete layer_.reshape_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReshapeLayerParams& NeuralNetworkLayer::reshape() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reshape)
  return has_reshape()
      ? *layer_.reshape_
      : ::CoreML::Specification::ReshapeLayerParams::default_instance();
}
::CoreML::Specification::ReshapeLayerParams* NeuralNetworkLayer::mutable_reshape() {
  if (!has_reshape()) {
    clear_layer();
    set_has_reshape();
    layer_.reshape_ = new ::CoreML::Specification::ReshapeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reshape)
  return layer_.reshape_;
}
::CoreML::Specification::ReshapeLayerParams* NeuralNetworkLayer::release_reshape() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reshape)
  if (has_reshape()) {
    clear_has_layer();
    ::CoreML::Specification::ReshapeLayerParams* temp = layer_.reshape_;
    layer_.reshape_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reshape(::CoreML::Specification::ReshapeLayerParams* reshape) {
  clear_layer();
  if (reshape) {
    set_has_reshape();
    layer_.reshape_ = reshape;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reshape)
}

// .CoreML.Specification.FlattenLayerParams flatten = 301;
bool NeuralNetworkLayer::has_flatten() const {
  return layer_case() == kFlatten;
}
void NeuralNetworkLayer::set_has_flatten() {
  _oneof_case_[0] = kFlatten;
}
void NeuralNetworkLayer::clear_flatten() {
  if (has_flatten()) {
    delete layer_.flatten_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::FlattenLayerParams& NeuralNetworkLayer::flatten() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.flatten)
  return has_flatten()
      ? *layer_.flatten_
      : ::CoreML::Specification::FlattenLayerParams::default_instance();
}
::CoreML::Specification::FlattenLayerParams* NeuralNetworkLayer::mutable_flatten() {
  if (!has_flatten()) {
    clear_layer();
    set_has_flatten();
    layer_.flatten_ = new ::CoreML::Specification::FlattenLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.flatten)
  return layer_.flatten_;
}
::CoreML::Specification::FlattenLayerParams* NeuralNetworkLayer::release_flatten() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.flatten)
  if (has_flatten()) {
    clear_has_layer();
    ::CoreML::Specification::FlattenLayerParams* temp = layer_.flatten_;
    layer_.flatten_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_flatten(::CoreML::Specification::FlattenLayerParams* flatten) {
  clear_layer();
  if (flatten) {
    set_has_flatten();
    layer_.flatten_ = flatten;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.flatten)
}

// .CoreML.Specification.PermuteLayerParams permute = 310;
bool NeuralNetworkLayer::has_permute() const {
  return layer_case() == kPermute;
}
void NeuralNetworkLayer::set_has_permute() {
  _oneof_case_[0] = kPermute;
}
void NeuralNetworkLayer::clear_permute() {
  if (has_permute()) {
    delete layer_.permute_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::PermuteLayerParams& NeuralNetworkLayer::permute() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.permute)
  return has_permute()
      ? *layer_.permute_
      : ::CoreML::Specification::PermuteLayerParams::default_instance();
}
::CoreML::Specification::PermuteLayerParams* NeuralNetworkLayer::mutable_permute() {
  if (!has_permute()) {
    clear_layer();
    set_has_permute();
    layer_.permute_ = new ::CoreML::Specification::PermuteLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.permute)
  return layer_.permute_;
}
::CoreML::Specification::PermuteLayerParams* NeuralNetworkLayer::release_permute() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.permute)
  if (has_permute()) {
    clear_has_layer();
    ::CoreML::Specification::PermuteLayerParams* temp = layer_.permute_;
    layer_.permute_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_permute(::CoreML::Specification::PermuteLayerParams* permute) {
  clear_layer();
  if (permute) {
    set_has_permute();
    layer_.permute_ = permute;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.permute)
}

// .CoreML.Specification.ConcatLayerParams concat = 320;
bool NeuralNetworkLayer::has_concat() const {
  return layer_case() == kConcat;
}
void NeuralNetworkLayer::set_has_concat() {
  _oneof_case_[0] = kConcat;
}
void NeuralNetworkLayer::clear_concat() {
  if (has_concat()) {
    delete layer_.concat_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ConcatLayerParams& NeuralNetworkLayer::concat() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.concat)
  return has_concat()
      ? *layer_.concat_
      : ::CoreML::Specification::ConcatLayerParams::default_instance();
}
::CoreML::Specification::ConcatLayerParams* NeuralNetworkLayer::mutable_concat() {
  if (!has_concat()) {
    clear_layer();
    set_has_concat();
    layer_.concat_ = new ::CoreML::Specification::ConcatLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.concat)
  return layer_.concat_;
}
::CoreML::Specification::ConcatLayerParams* NeuralNetworkLayer::release_concat() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.concat)
  if (has_concat()) {
    clear_has_layer();
    ::CoreML::Specification::ConcatLayerParams* temp = layer_.concat_;
    layer_.concat_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_concat(::CoreML::Specification::ConcatLayerParams* concat) {
  clear_layer();
  if (concat) {
    set_has_concat();
    layer_.concat_ = concat;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.concat)
}

// .CoreML.Specification.SplitLayerParams split = 330;
bool NeuralNetworkLayer::has_split() const {
  return layer_case() == kSplit;
}
void NeuralNetworkLayer::set_has_split() {
  _oneof_case_[0] = kSplit;
}
void NeuralNetworkLayer::clear_split() {
  if (has_split()) {
    delete layer_.split_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SplitLayerParams& NeuralNetworkLayer::split() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.split)
  return has_split()
      ? *layer_.split_
      : ::CoreML::Specification::SplitLayerParams::default_instance();
}
::CoreML::Specification::SplitLayerParams* NeuralNetworkLayer::mutable_split() {
  if (!has_split()) {
    clear_layer();
    set_has_split();
    layer_.split_ = new ::CoreML::Specification::SplitLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.split)
  return layer_.split_;
}
::CoreML::Specification::SplitLayerParams* NeuralNetworkLayer::release_split() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.split)
  if (has_split()) {
    clear_has_layer();
    ::CoreML::Specification::SplitLayerParams* temp = layer_.split_;
    layer_.split_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_split(::CoreML::Specification::SplitLayerParams* split) {
  clear_layer();
  if (split) {
    set_has_split();
    layer_.split_ = split;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.split)
}

// .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
bool NeuralNetworkLayer::has_sequencerepeat() const {
  return layer_case() == kSequenceRepeat;
}
void NeuralNetworkLayer::set_has_sequencerepeat() {
  _oneof_case_[0] = kSequenceRepeat;
}
void NeuralNetworkLayer::clear_sequencerepeat() {
  if (has_sequencerepeat()) {
    delete layer_.sequencerepeat_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SequenceRepeatLayerParams& NeuralNetworkLayer::sequencerepeat() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  return has_sequencerepeat()
      ? *layer_.sequencerepeat_
      : ::CoreML::Specification::SequenceRepeatLayerParams::default_instance();
}
::CoreML::Specification::SequenceRepeatLayerParams* NeuralNetworkLayer::mutable_sequencerepeat() {
  if (!has_sequencerepeat()) {
    clear_layer();
    set_has_sequencerepeat();
    layer_.sequencerepeat_ = new ::CoreML::Specification::SequenceRepeatLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  return layer_.sequencerepeat_;
}
::CoreML::Specification::SequenceRepeatLayerParams* NeuralNetworkLayer::release_sequencerepeat() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  if (has_sequencerepeat()) {
    clear_has_layer();
    ::CoreML::Specification::SequenceRepeatLayerParams* temp = layer_.sequencerepeat_;
    layer_.sequencerepeat_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_sequencerepeat(::CoreML::Specification::SequenceRepeatLayerParams* sequencerepeat) {
  clear_layer();
  if (sequencerepeat) {
    set_has_sequencerepeat();
    layer_.sequencerepeat_ = sequencerepeat;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
}

// .CoreML.Specification.ReorganizeDataLayerParams reorganizeData = 345;
bool NeuralNetworkLayer::has_reorganizedata() const {
  return layer_case() == kReorganizeData;
}
void NeuralNetworkLayer::set_has_reorganizedata() {
  _oneof_case_[0] = kReorganizeData;
}
void NeuralNetworkLayer::clear_reorganizedata() {
  if (has_reorganizedata()) {
    delete layer_.reorganizedata_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReorganizeDataLayerParams& NeuralNetworkLayer::reorganizedata() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reorganizeData)
  return has_reorganizedata()
      ? *layer_.reorganizedata_
      : ::CoreML::Specification::ReorganizeDataLayerParams::default_instance();
}
::CoreML::Specification::ReorganizeDataLayerParams* NeuralNetworkLayer::mutable_reorganizedata() {
  if (!has_reorganizedata()) {
    clear_layer();
    set_has_reorganizedata();
    layer_.reorganizedata_ = new ::CoreML::Specification::ReorganizeDataLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reorganizeData)
  return layer_.reorganizedata_;
}
::CoreML::Specification::ReorganizeDataLayerParams* NeuralNetworkLayer::release_reorganizedata() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reorganizeData)
  if (has_reorganizedata()) {
    clear_has_layer();
    ::CoreML::Specification::ReorganizeDataLayerParams* temp = layer_.reorganizedata_;
    layer_.reorganizedata_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reorganizedata(::CoreML::Specification::ReorganizeDataLayerParams* reorganizedata) {
  clear_layer();
  if (reorganizedata) {
    set_has_reorganizedata();
    layer_.reorganizedata_ = reorganizedata;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reorganizeData)
}

// .CoreML.Specification.SliceLayerParams slice = 350;
bool NeuralNetworkLayer::has_slice() const {
  return layer_case() == kSlice;
}
void NeuralNetworkLayer::set_has_slice() {
  _oneof_case_[0] = kSlice;
}
void NeuralNetworkLayer::clear_slice() {
  if (has_slice()) {
    delete layer_.slice_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SliceLayerParams& NeuralNetworkLayer::slice() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.slice)
  return has_slice()
      ? *layer_.slice_
      : ::CoreML::Specification::SliceLayerParams::default_instance();
}
::CoreML::Specification::SliceLayerParams* NeuralNetworkLayer::mutable_slice() {
  if (!has_slice()) {
    clear_layer();
    set_has_slice();
    layer_.slice_ = new ::CoreML::Specification::SliceLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.slice)
  return layer_.slice_;
}
::CoreML::Specification::SliceLayerParams* NeuralNetworkLayer::release_slice() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.slice)
  if (has_slice()) {
    clear_has_layer();
    ::CoreML::Specification::SliceLayerParams* temp = layer_.slice_;
    layer_.slice_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_slice(::CoreML::Specification::SliceLayerParams* slice) {
  clear_layer();
  if (slice) {
    set_has_slice();
    layer_.slice_ = slice;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.slice)
}

// .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
bool NeuralNetworkLayer::has_simplerecurrent() const {
  return layer_case() == kSimpleRecurrent;
}
void NeuralNetworkLayer::set_has_simplerecurrent() {
  _oneof_case_[0] = kSimpleRecurrent;
}
void NeuralNetworkLayer::clear_simplerecurrent() {
  if (has_simplerecurrent()) {
    delete layer_.simplerecurrent_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SimpleRecurrentLayerParams& NeuralNetworkLayer::simplerecurrent() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  return has_simplerecurrent()
      ? *layer_.simplerecurrent_
      : ::CoreML::Specification::SimpleRecurrentLayerParams::default_instance();
}
::CoreML::Specification::SimpleRecurrentLayerParams* NeuralNetworkLayer::mutable_simplerecurrent() {
  if (!has_simplerecurrent()) {
    clear_layer();
    set_has_simplerecurrent();
    layer_.simplerecurrent_ = new ::CoreML::Specification::SimpleRecurrentLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  return layer_.simplerecurrent_;
}
::CoreML::Specification::SimpleRecurrentLayerParams* NeuralNetworkLayer::release_simplerecurrent() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  if (has_simplerecurrent()) {
    clear_has_layer();
    ::CoreML::Specification::SimpleRecurrentLayerParams* temp = layer_.simplerecurrent_;
    layer_.simplerecurrent_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_simplerecurrent(::CoreML::Specification::SimpleRecurrentLayerParams* simplerecurrent) {
  clear_layer();
  if (simplerecurrent) {
    set_has_simplerecurrent();
    layer_.simplerecurrent_ = simplerecurrent;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
}

// .CoreML.Specification.GRULayerParams gru = 410;
bool NeuralNetworkLayer::has_gru() const {
  return layer_case() == kGru;
}
void NeuralNetworkLayer::set_has_gru() {
  _oneof_case_[0] = kGru;
}
void NeuralNetworkLayer::clear_gru() {
  if (has_gru()) {
    delete layer_.gru_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::GRULayerParams& NeuralNetworkLayer::gru() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.gru)
  return has_gru()
      ? *layer_.gru_
      : ::CoreML::Specification::GRULayerParams::default_instance();
}
::CoreML::Specification::GRULayerParams* NeuralNetworkLayer::mutable_gru() {
  if (!has_gru()) {
    clear_layer();
    set_has_gru();
    layer_.gru_ = new ::CoreML::Specification::GRULayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.gru)
  return layer_.gru_;
}
::CoreML::Specification::GRULayerParams* NeuralNetworkLayer::release_gru() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.gru)
  if (has_gru()) {
    clear_has_layer();
    ::CoreML::Specification::GRULayerParams* temp = layer_.gru_;
    layer_.gru_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_gru(::CoreML::Specification::GRULayerParams* gru) {
  clear_layer();
  if (gru) {
    set_has_gru();
    layer_.gru_ = gru;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.gru)
}

// .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
bool NeuralNetworkLayer::has_unidirectionallstm() const {
  return layer_case() == kUniDirectionalLSTM;
}
void NeuralNetworkLayer::set_has_unidirectionallstm() {
  _oneof_case_[0] = kUniDirectionalLSTM;
}
void NeuralNetworkLayer::clear_unidirectionallstm() {
  if (has_unidirectionallstm()) {
    delete layer_.unidirectionallstm_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::UniDirectionalLSTMLayerParams& NeuralNetworkLayer::unidirectionallstm() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  return has_unidirectionallstm()
      ? *layer_.unidirectionallstm_
      : ::CoreML::Specification::UniDirectionalLSTMLayerParams::default_instance();
}
::CoreML::Specification::UniDirectionalLSTMLayerParams* NeuralNetworkLayer::mutable_unidirectionallstm() {
  if (!has_unidirectionallstm()) {
    clear_layer();
    set_has_unidirectionallstm();
    layer_.unidirectionallstm_ = new ::CoreML::Specification::UniDirectionalLSTMLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  return layer_.unidirectionallstm_;
}
::CoreML::Specification::UniDirectionalLSTMLayerParams* NeuralNetworkLayer::release_unidirectionallstm() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  if (has_unidirectionallstm()) {
    clear_has_layer();
    ::CoreML::Specification::UniDirectionalLSTMLayerParams* temp = layer_.unidirectionallstm_;
    layer_.unidirectionallstm_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_unidirectionallstm(::CoreML::Specification::UniDirectionalLSTMLayerParams* unidirectionallstm) {
  clear_layer();
  if (unidirectionallstm) {
    set_has_unidirectionallstm();
    layer_.unidirectionallstm_ = unidirectionallstm;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
}

// .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
bool NeuralNetworkLayer::has_bidirectionallstm() const {
  return layer_case() == kBiDirectionalLSTM;
}
void NeuralNetworkLayer::set_has_bidirectionallstm() {
  _oneof_case_[0] = kBiDirectionalLSTM;
}
void NeuralNetworkLayer::clear_bidirectionallstm() {
  if (has_bidirectionallstm()) {
    delete layer_.bidirectionallstm_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BiDirectionalLSTMLayerParams& NeuralNetworkLayer::bidirectionallstm() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  return has_bidirectionallstm()
      ? *layer_.bidirectionallstm_
      : ::CoreML::Specification::BiDirectionalLSTMLayerParams::default_instance();
}
::CoreML::Specification::BiDirectionalLSTMLayerParams* NeuralNetworkLayer::mutable_bidirectionallstm() {
  if (!has_bidirectionallstm()) {
    clear_layer();
    set_has_bidirectionallstm();
    layer_.bidirectionallstm_ = new ::CoreML::Specification::BiDirectionalLSTMLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  return layer_.bidirectionallstm_;
}
::CoreML::Specification::BiDirectionalLSTMLayerParams* NeuralNetworkLayer::release_bidirectionallstm() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  if (has_bidirectionallstm()) {
    clear_has_layer();
    ::CoreML::Specification::BiDirectionalLSTMLayerParams* temp = layer_.bidirectionallstm_;
    layer_.bidirectionallstm_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_bidirectionallstm(::CoreML::Specification::BiDirectionalLSTMLayerParams* bidirectionallstm) {
  clear_layer();
  if (bidirectionallstm) {
    set_has_bidirectionallstm();
    layer_.bidirectionallstm_ = bidirectionallstm;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
}

// .CoreML.Specification.CustomLayerParams custom = 500;
bool NeuralNetworkLayer::has_custom() const {
  return layer_case() == kCustom;
}
void NeuralNetworkLayer::set_has_custom() {
  _oneof_case_[0] = kCustom;
}
void NeuralNetworkLayer::clear_custom() {
  if (has_custom()) {
    delete layer_.custom_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::CustomLayerParams& NeuralNetworkLayer::custom() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.custom)
  return has_custom()
      ? *layer_.custom_
      : ::CoreML::Specification::CustomLayerParams::default_instance();
}
::CoreML::Specification::CustomLayerParams* NeuralNetworkLayer::mutable_custom() {
  if (!has_custom()) {
    clear_layer();
    set_has_custom();
    layer_.custom_ = new ::CoreML::Specification::CustomLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.custom)
  return layer_.custom_;
}
::CoreML::Specification::CustomLayerParams* NeuralNetworkLayer::release_custom() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.custom)
  if (has_custom()) {
    clear_has_layer();
    ::CoreML::Specification::CustomLayerParams* temp = layer_.custom_;
    layer_.custom_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_custom(::CoreML::Specification::CustomLayerParams* custom) {
  clear_layer();
  if (custom) {
    set_has_custom();
    layer_.custom_ = custom;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.custom)
}

bool NeuralNetworkLayer::has_layer() const {
  return layer_case() != LAYER_NOT_SET;
}
void NeuralNetworkLayer::clear_has_layer() {
  _oneof_case_[0] = LAYER_NOT_SET;
}
NeuralNetworkLayer::LayerCase NeuralNetworkLayer::layer_case() const {
  return NeuralNetworkLayer::LayerCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BorderAmounts_EdgeSizes::kStartEdgeSizeFieldNumber;
const int BorderAmounts_EdgeSizes::kEndEdgeSizeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BorderAmounts_EdgeSizes::BorderAmounts_EdgeSizes()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BorderAmounts.EdgeSizes)
}
BorderAmounts_EdgeSizes::BorderAmounts_EdgeSizes(const BorderAmounts_EdgeSizes& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&startedgesize_, &from.startedgesize_,
    reinterpret_cast<char*>(&endedgesize_) -
    reinterpret_cast<char*>(&startedgesize_) + sizeof(endedgesize_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BorderAmounts.EdgeSizes)
}

void BorderAmounts_EdgeSizes::SharedCtor() {
  ::memset(&startedgesize_, 0, reinterpret_cast<char*>(&endedgesize_) -
    reinterpret_cast<char*>(&startedgesize_) + sizeof(endedgesize_));
  _cached_size_ = 0;
}

BorderAmounts_EdgeSizes::~BorderAmounts_EdgeSizes() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BorderAmounts.EdgeSizes)
  SharedDtor();
}

void BorderAmounts_EdgeSizes::SharedDtor() {
}

void BorderAmounts_EdgeSizes::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BorderAmounts_EdgeSizes& BorderAmounts_EdgeSizes::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BorderAmounts_EdgeSizes* BorderAmounts_EdgeSizes::New(::google::protobuf::Arena* arena) const {
  BorderAmounts_EdgeSizes* n = new BorderAmounts_EdgeSizes;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BorderAmounts_EdgeSizes::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  ::memset(&startedgesize_, 0, reinterpret_cast<char*>(&endedgesize_) -
    reinterpret_cast<char*>(&startedgesize_) + sizeof(endedgesize_));
}

bool BorderAmounts_EdgeSizes::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 startEdgeSize = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &startedgesize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 endEdgeSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &endedgesize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BorderAmounts.EdgeSizes)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BorderAmounts.EdgeSizes)
  return false;
#undef DO_
}

void BorderAmounts_EdgeSizes::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 startEdgeSize = 1;
  if (this->startedgesize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->startedgesize(), output);
  }

  // uint64 endEdgeSize = 2;
  if (this->endedgesize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->endedgesize(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BorderAmounts.EdgeSizes)
}

size_t BorderAmounts_EdgeSizes::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  size_t total_size = 0;

  // uint64 startEdgeSize = 1;
  if (this->startedgesize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->startedgesize());
  }

  // uint64 endEdgeSize = 2;
  if (this->endedgesize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->endedgesize());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BorderAmounts_EdgeSizes::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BorderAmounts_EdgeSizes*>(&from));
}

void BorderAmounts_EdgeSizes::MergeFrom(const BorderAmounts_EdgeSizes& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.startedgesize() != 0) {
    set_startedgesize(from.startedgesize());
  }
  if (from.endedgesize() != 0) {
    set_endedgesize(from.endedgesize());
  }
}

void BorderAmounts_EdgeSizes::CopyFrom(const BorderAmounts_EdgeSizes& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BorderAmounts_EdgeSizes::IsInitialized() const {
  return true;
}

void BorderAmounts_EdgeSizes::Swap(BorderAmounts_EdgeSizes* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BorderAmounts_EdgeSizes::InternalSwap(BorderAmounts_EdgeSizes* other) {
  std::swap(startedgesize_, other->startedgesize_);
  std::swap(endedgesize_, other->endedgesize_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BorderAmounts_EdgeSizes::GetTypeName() const {
  return "CoreML.Specification.BorderAmounts.EdgeSizes";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BorderAmounts_EdgeSizes

// uint64 startEdgeSize = 1;
void BorderAmounts_EdgeSizes::clear_startedgesize() {
  startedgesize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BorderAmounts_EdgeSizes::startedgesize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BorderAmounts.EdgeSizes.startEdgeSize)
  return startedgesize_;
}
void BorderAmounts_EdgeSizes::set_startedgesize(::google::protobuf::uint64 value) {
  
  startedgesize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BorderAmounts.EdgeSizes.startEdgeSize)
}

// uint64 endEdgeSize = 2;
void BorderAmounts_EdgeSizes::clear_endedgesize() {
  endedgesize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BorderAmounts_EdgeSizes::endedgesize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BorderAmounts.EdgeSizes.endEdgeSize)
  return endedgesize_;
}
void BorderAmounts_EdgeSizes::set_endedgesize(::google::protobuf::uint64 value) {
  
  endedgesize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BorderAmounts.EdgeSizes.endEdgeSize)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BorderAmounts::kBorderAmountsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BorderAmounts::BorderAmounts()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BorderAmounts)
}
BorderAmounts::BorderAmounts(const BorderAmounts& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      borderamounts_(from.borderamounts_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BorderAmounts)
}

void BorderAmounts::SharedCtor() {
  _cached_size_ = 0;
}

BorderAmounts::~BorderAmounts() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BorderAmounts)
  SharedDtor();
}

void BorderAmounts::SharedDtor() {
}

void BorderAmounts::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BorderAmounts& BorderAmounts::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BorderAmounts* BorderAmounts::New(::google::protobuf::Arena* arena) const {
  BorderAmounts* n = new BorderAmounts;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BorderAmounts::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BorderAmounts)
  borderamounts_.Clear();
}

bool BorderAmounts::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BorderAmounts)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_borderamounts()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BorderAmounts)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BorderAmounts)
  return false;
#undef DO_
}

void BorderAmounts::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BorderAmounts)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
  for (unsigned int i = 0, n = this->borderamounts_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, this->borderamounts(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BorderAmounts)
}

size_t BorderAmounts::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BorderAmounts)
  size_t total_size = 0;

  // repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
  {
    unsigned int count = this->borderamounts_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->borderamounts(i));
    }
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BorderAmounts::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BorderAmounts*>(&from));
}

void BorderAmounts::MergeFrom(const BorderAmounts& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BorderAmounts)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  borderamounts_.MergeFrom(from.borderamounts_);
}

void BorderAmounts::CopyFrom(const BorderAmounts& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BorderAmounts)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BorderAmounts::IsInitialized() const {
  return true;
}

void BorderAmounts::Swap(BorderAmounts* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BorderAmounts::InternalSwap(BorderAmounts* other) {
  borderamounts_.InternalSwap(&other->borderamounts_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BorderAmounts::GetTypeName() const {
  return "CoreML.Specification.BorderAmounts";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BorderAmounts

// repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
int BorderAmounts::borderamounts_size() const {
  return borderamounts_.size();
}
void BorderAmounts::clear_borderamounts() {
  borderamounts_.Clear();
}
const ::CoreML::Specification::BorderAmounts_EdgeSizes& BorderAmounts::borderamounts(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_.Get(index);
}
::CoreML::Specification::BorderAmounts_EdgeSizes* BorderAmounts::mutable_borderamounts(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_.Mutable(index);
}
::CoreML::Specification::BorderAmounts_EdgeSizes* BorderAmounts::add_borderamounts() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes >*
BorderAmounts::mutable_borderamounts() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BorderAmounts.borderAmounts)
  return &borderamounts_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes >&
BorderAmounts::borderamounts() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ValidPadding::kPaddingAmountsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ValidPadding::ValidPadding()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ValidPadding)
}
ValidPadding::ValidPadding(const ValidPadding& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_paddingamounts()) {
    paddingamounts_ = new ::CoreML::Specification::BorderAmounts(*from.paddingamounts_);
  } else {
    paddingamounts_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ValidPadding)
}

void ValidPadding::SharedCtor() {
  paddingamounts_ = NULL;
  _cached_size_ = 0;
}

ValidPadding::~ValidPadding() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ValidPadding)
  SharedDtor();
}

void ValidPadding::SharedDtor() {
  if (this != internal_default_instance()) {
    delete paddingamounts_;
  }
}

void ValidPadding::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ValidPadding& ValidPadding::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ValidPadding* ValidPadding::New(::google::protobuf::Arena* arena) const {
  ValidPadding* n = new ValidPadding;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ValidPadding::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ValidPadding)
  if (GetArenaNoVirtual() == NULL && paddingamounts_ != NULL) {
    delete paddingamounts_;
  }
  paddingamounts_ = NULL;
}

bool ValidPadding::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ValidPadding)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.BorderAmounts paddingAmounts = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_paddingamounts()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ValidPadding)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ValidPadding)
  return false;
#undef DO_
}

void ValidPadding::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ValidPadding)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.BorderAmounts paddingAmounts = 1;
  if (this->has_paddingamounts()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->paddingamounts_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ValidPadding)
}

size_t ValidPadding::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ValidPadding)
  size_t total_size = 0;

  // .CoreML.Specification.BorderAmounts paddingAmounts = 1;
  if (this->has_paddingamounts()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->paddingamounts_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ValidPadding::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ValidPadding*>(&from));
}

void ValidPadding::MergeFrom(const ValidPadding& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ValidPadding)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_paddingamounts()) {
    mutable_paddingamounts()->::CoreML::Specification::BorderAmounts::MergeFrom(from.paddingamounts());
  }
}

void ValidPadding::CopyFrom(const ValidPadding& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ValidPadding)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ValidPadding::IsInitialized() const {
  return true;
}

void ValidPadding::Swap(ValidPadding* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ValidPadding::InternalSwap(ValidPadding* other) {
  std::swap(paddingamounts_, other->paddingamounts_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ValidPadding::GetTypeName() const {
  return "CoreML.Specification.ValidPadding";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ValidPadding

// .CoreML.Specification.BorderAmounts paddingAmounts = 1;
bool ValidPadding::has_paddingamounts() const {
  return this != internal_default_instance() && paddingamounts_ != NULL;
}
void ValidPadding::clear_paddingamounts() {
  if (GetArenaNoVirtual() == NULL && paddingamounts_ != NULL) delete paddingamounts_;
  paddingamounts_ = NULL;
}
const ::CoreML::Specification::BorderAmounts& ValidPadding::paddingamounts() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ValidPadding.paddingAmounts)
  return paddingamounts_ != NULL ? *paddingamounts_
                         : *::CoreML::Specification::BorderAmounts::internal_default_instance();
}
::CoreML::Specification::BorderAmounts* ValidPadding::mutable_paddingamounts() {
  
  if (paddingamounts_ == NULL) {
    paddingamounts_ = new ::CoreML::Specification::BorderAmounts;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ValidPadding.paddingAmounts)
  return paddingamounts_;
}
::CoreML::Specification::BorderAmounts* ValidPadding::release_paddingamounts() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ValidPadding.paddingAmounts)
  
  ::CoreML::Specification::BorderAmounts* temp = paddingamounts_;
  paddingamounts_ = NULL;
  return temp;
}
void ValidPadding::set_allocated_paddingamounts(::CoreML::Specification::BorderAmounts* paddingamounts) {
  delete paddingamounts_;
  paddingamounts_ = paddingamounts;
  if (paddingamounts) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ValidPadding.paddingAmounts)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SamePadding::kAsymmetryModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SamePadding::SamePadding()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SamePadding)
}
SamePadding::SamePadding(const SamePadding& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  asymmetrymode_ = from.asymmetrymode_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SamePadding)
}

void SamePadding::SharedCtor() {
  asymmetrymode_ = 0;
  _cached_size_ = 0;
}

SamePadding::~SamePadding() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SamePadding)
  SharedDtor();
}

void SamePadding::SharedDtor() {
}

void SamePadding::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SamePadding& SamePadding::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SamePadding* SamePadding::New(::google::protobuf::Arena* arena) const {
  SamePadding* n = new SamePadding;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SamePadding::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SamePadding)
  asymmetrymode_ = 0;
}

bool SamePadding::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SamePadding)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_asymmetrymode(static_cast< ::CoreML::Specification::SamePadding_SamePaddingMode >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SamePadding)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SamePadding)
  return false;
#undef DO_
}

void SamePadding::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SamePadding)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
  if (this->asymmetrymode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->asymmetrymode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SamePadding)
}

size_t SamePadding::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SamePadding)
  size_t total_size = 0;

  // .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
  if (this->asymmetrymode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->asymmetrymode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SamePadding::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SamePadding*>(&from));
}

void SamePadding::MergeFrom(const SamePadding& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SamePadding)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.asymmetrymode() != 0) {
    set_asymmetrymode(from.asymmetrymode());
  }
}

void SamePadding::CopyFrom(const SamePadding& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SamePadding)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SamePadding::IsInitialized() const {
  return true;
}

void SamePadding::Swap(SamePadding* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SamePadding::InternalSwap(SamePadding* other) {
  std::swap(asymmetrymode_, other->asymmetrymode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SamePadding::GetTypeName() const {
  return "CoreML.Specification.SamePadding";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SamePadding

// .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
void SamePadding::clear_asymmetrymode() {
  asymmetrymode_ = 0;
}
::CoreML::Specification::SamePadding_SamePaddingMode SamePadding::asymmetrymode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SamePadding.asymmetryMode)
  return static_cast< ::CoreML::Specification::SamePadding_SamePaddingMode >(asymmetrymode_);
}
void SamePadding::set_asymmetrymode(::CoreML::Specification::SamePadding_SamePaddingMode value) {
  
  asymmetrymode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SamePadding.asymmetryMode)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int WeightParams::kFloatValueFieldNumber;
const int WeightParams::kFloat16ValueFieldNumber;
const int WeightParams::kRawValueFieldNumber;
const int WeightParams::kQuantizationFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

WeightParams::WeightParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.WeightParams)
}
WeightParams::WeightParams(const WeightParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      floatvalue_(from.floatvalue_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  float16value_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.float16value().size() > 0) {
    float16value_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.float16value_);
  }
  rawvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.rawvalue().size() > 0) {
    rawvalue_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.rawvalue_);
  }
  if (from.has_quantization()) {
    quantization_ = new ::CoreML::Specification::QuantizationParams(*from.quantization_);
  } else {
    quantization_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.WeightParams)
}

void WeightParams::SharedCtor() {
  float16value_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  rawvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  quantization_ = NULL;
  _cached_size_ = 0;
}

WeightParams::~WeightParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.WeightParams)
  SharedDtor();
}

void WeightParams::SharedDtor() {
  float16value_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  rawvalue_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (this != internal_default_instance()) {
    delete quantization_;
  }
}

void WeightParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const WeightParams& WeightParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

WeightParams* WeightParams::New(::google::protobuf::Arena* arena) const {
  WeightParams* n = new WeightParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void WeightParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.WeightParams)
  floatvalue_.Clear();
  float16value_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  rawvalue_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (GetArenaNoVirtual() == NULL && quantization_ != NULL) {
    delete quantization_;
  }
  quantization_ = NULL;
}

bool WeightParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.WeightParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated float floatValue = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, this->mutable_floatvalue())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(13u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 1, 10u, input, this->mutable_floatvalue())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bytes float16Value = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadBytes(
                input, this->mutable_float16value()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bytes rawValue = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadBytes(
                input, this->mutable_rawvalue()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.QuantizationParams quantization = 40;
      case 40: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_quantization()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.WeightParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.WeightParams)
  return false;
#undef DO_
}

void WeightParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.WeightParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated float floatValue = 1;
  if (this->floatvalue_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_floatvalue_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteFloatArray(
      this->floatvalue().data(), this->floatvalue_size(), output);
  }

  // bytes float16Value = 2;
  if (this->float16value().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBytesMaybeAliased(
      2, this->float16value(), output);
  }

  // bytes rawValue = 30;
  if (this->rawvalue().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBytesMaybeAliased(
      30, this->rawvalue(), output);
  }

  // .CoreML.Specification.QuantizationParams quantization = 40;
  if (this->has_quantization()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      40, *this->quantization_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.WeightParams)
}

size_t WeightParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.WeightParams)
  size_t total_size = 0;

  // repeated float floatValue = 1;
  {
    unsigned int count = this->floatvalue_size();
    size_t data_size = 4UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _floatvalue_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // bytes float16Value = 2;
  if (this->float16value().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::BytesSize(
        this->float16value());
  }

  // bytes rawValue = 30;
  if (this->rawvalue().size() > 0) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::BytesSize(
        this->rawvalue());
  }

  // .CoreML.Specification.QuantizationParams quantization = 40;
  if (this->has_quantization()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->quantization_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void WeightParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const WeightParams*>(&from));
}

void WeightParams::MergeFrom(const WeightParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.WeightParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  floatvalue_.MergeFrom(from.floatvalue_);
  if (from.float16value().size() > 0) {

    float16value_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.float16value_);
  }
  if (from.rawvalue().size() > 0) {

    rawvalue_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.rawvalue_);
  }
  if (from.has_quantization()) {
    mutable_quantization()->::CoreML::Specification::QuantizationParams::MergeFrom(from.quantization());
  }
}

void WeightParams::CopyFrom(const WeightParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.WeightParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool WeightParams::IsInitialized() const {
  return true;
}

void WeightParams::Swap(WeightParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void WeightParams::InternalSwap(WeightParams* other) {
  floatvalue_.InternalSwap(&other->floatvalue_);
  float16value_.Swap(&other->float16value_);
  rawvalue_.Swap(&other->rawvalue_);
  std::swap(quantization_, other->quantization_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string WeightParams::GetTypeName() const {
  return "CoreML.Specification.WeightParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// WeightParams

// repeated float floatValue = 1;
int WeightParams::floatvalue_size() const {
  return floatvalue_.size();
}
void WeightParams::clear_floatvalue() {
  floatvalue_.Clear();
}
float WeightParams::floatvalue(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.floatValue)
  return floatvalue_.Get(index);
}
void WeightParams::set_floatvalue(int index, float value) {
  floatvalue_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.floatValue)
}
void WeightParams::add_floatvalue(float value) {
  floatvalue_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.WeightParams.floatValue)
}
const ::google::protobuf::RepeatedField< float >&
WeightParams::floatvalue() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.WeightParams.floatValue)
  return floatvalue_;
}
::google::protobuf::RepeatedField< float >*
WeightParams::mutable_floatvalue() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.WeightParams.floatValue)
  return &floatvalue_;
}

// bytes float16Value = 2;
void WeightParams::clear_float16value() {
  float16value_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& WeightParams::float16value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.float16Value)
  return float16value_.GetNoArena();
}
void WeightParams::set_float16value(const ::std::string& value) {
  
  float16value_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.float16Value)
}
#if LANG_CXX11
void WeightParams::set_float16value(::std::string&& value) {
  
  float16value_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.WeightParams.float16Value)
}
#endif
void WeightParams::set_float16value(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  float16value_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.WeightParams.float16Value)
}
void WeightParams::set_float16value(const void* value, size_t size) {
  
  float16value_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.WeightParams.float16Value)
}
::std::string* WeightParams::mutable_float16value() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.WeightParams.float16Value)
  return float16value_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* WeightParams::release_float16value() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.WeightParams.float16Value)
  
  return float16value_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void WeightParams::set_allocated_float16value(::std::string* float16value) {
  if (float16value != NULL) {
    
  } else {
    
  }
  float16value_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), float16value);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.WeightParams.float16Value)
}

// bytes rawValue = 30;
void WeightParams::clear_rawvalue() {
  rawvalue_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& WeightParams::rawvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.rawValue)
  return rawvalue_.GetNoArena();
}
void WeightParams::set_rawvalue(const ::std::string& value) {
  
  rawvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.rawValue)
}
#if LANG_CXX11
void WeightParams::set_rawvalue(::std::string&& value) {
  
  rawvalue_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.WeightParams.rawValue)
}
#endif
void WeightParams::set_rawvalue(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  rawvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.WeightParams.rawValue)
}
void WeightParams::set_rawvalue(const void* value, size_t size) {
  
  rawvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.WeightParams.rawValue)
}
::std::string* WeightParams::mutable_rawvalue() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.WeightParams.rawValue)
  return rawvalue_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* WeightParams::release_rawvalue() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.WeightParams.rawValue)
  
  return rawvalue_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void WeightParams::set_allocated_rawvalue(::std::string* rawvalue) {
  if (rawvalue != NULL) {
    
  } else {
    
  }
  rawvalue_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), rawvalue);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.WeightParams.rawValue)
}

// .CoreML.Specification.QuantizationParams quantization = 40;
bool WeightParams::has_quantization() const {
  return this != internal_default_instance() && quantization_ != NULL;
}
void WeightParams::clear_quantization() {
  if (GetArenaNoVirtual() == NULL && quantization_ != NULL) delete quantization_;
  quantization_ = NULL;
}
const ::CoreML::Specification::QuantizationParams& WeightParams::quantization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.quantization)
  return quantization_ != NULL ? *quantization_
                         : *::CoreML::Specification::QuantizationParams::internal_default_instance();
}
::CoreML::Specification::QuantizationParams* WeightParams::mutable_quantization() {
  
  if (quantization_ == NULL) {
    quantization_ = new ::CoreML::Specification::QuantizationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.WeightParams.quantization)
  return quantization_;
}
::CoreML::Specification::QuantizationParams* WeightParams::release_quantization() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.WeightParams.quantization)
  
  ::CoreML::Specification::QuantizationParams* temp = quantization_;
  quantization_ = NULL;
  return temp;
}
void WeightParams::set_allocated_quantization(::CoreML::Specification::QuantizationParams* quantization) {
  delete quantization_;
  quantization_ = quantization;
  if (quantization) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.WeightParams.quantization)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int QuantizationParams::kNumberOfBitsFieldNumber;
const int QuantizationParams::kLinearQuantizationFieldNumber;
const int QuantizationParams::kLookupTableQuantizationFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

QuantizationParams::QuantizationParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.QuantizationParams)
}
QuantizationParams::QuantizationParams(const QuantizationParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  numberofbits_ = from.numberofbits_;
  clear_has_QuantizationType();
  switch (from.QuantizationType_case()) {
    case kLinearQuantization: {
      mutable_linearquantization()->::CoreML::Specification::LinearQuantizationParams::MergeFrom(from.linearquantization());
      break;
    }
    case kLookupTableQuantization: {
      mutable_lookuptablequantization()->::CoreML::Specification::LookUpTableQuantizationParams::MergeFrom(from.lookuptablequantization());
      break;
    }
    case QUANTIZATIONTYPE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.QuantizationParams)
}

void QuantizationParams::SharedCtor() {
  numberofbits_ = GOOGLE_ULONGLONG(0);
  clear_has_QuantizationType();
  _cached_size_ = 0;
}

QuantizationParams::~QuantizationParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.QuantizationParams)
  SharedDtor();
}

void QuantizationParams::SharedDtor() {
  if (has_QuantizationType()) {
    clear_QuantizationType();
  }
}

void QuantizationParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const QuantizationParams& QuantizationParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

QuantizationParams* QuantizationParams::New(::google::protobuf::Arena* arena) const {
  QuantizationParams* n = new QuantizationParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void QuantizationParams::clear_QuantizationType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.QuantizationParams)
  switch (QuantizationType_case()) {
    case kLinearQuantization: {
      delete QuantizationType_.linearquantization_;
      break;
    }
    case kLookupTableQuantization: {
      delete QuantizationType_.lookuptablequantization_;
      break;
    }
    case QUANTIZATIONTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = QUANTIZATIONTYPE_NOT_SET;
}


void QuantizationParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.QuantizationParams)
  numberofbits_ = GOOGLE_ULONGLONG(0);
  clear_QuantizationType();
}

bool QuantizationParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.QuantizationParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 numberOfBits = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &numberofbits_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LinearQuantizationParams linearQuantization = 101;
      case 101: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(810u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_linearquantization()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LookUpTableQuantizationParams lookupTableQuantization = 102;
      case 102: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(818u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_lookuptablequantization()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.QuantizationParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.QuantizationParams)
  return false;
#undef DO_
}

void QuantizationParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.QuantizationParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 numberOfBits = 1;
  if (this->numberofbits() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->numberofbits(), output);
  }

  // .CoreML.Specification.LinearQuantizationParams linearQuantization = 101;
  if (has_linearquantization()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      101, *QuantizationType_.linearquantization_, output);
  }

  // .CoreML.Specification.LookUpTableQuantizationParams lookupTableQuantization = 102;
  if (has_lookuptablequantization()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      102, *QuantizationType_.lookuptablequantization_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.QuantizationParams)
}

size_t QuantizationParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.QuantizationParams)
  size_t total_size = 0;

  // uint64 numberOfBits = 1;
  if (this->numberofbits() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->numberofbits());
  }

  switch (QuantizationType_case()) {
    // .CoreML.Specification.LinearQuantizationParams linearQuantization = 101;
    case kLinearQuantization: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *QuantizationType_.linearquantization_);
      break;
    }
    // .CoreML.Specification.LookUpTableQuantizationParams lookupTableQuantization = 102;
    case kLookupTableQuantization: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *QuantizationType_.lookuptablequantization_);
      break;
    }
    case QUANTIZATIONTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void QuantizationParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const QuantizationParams*>(&from));
}

void QuantizationParams::MergeFrom(const QuantizationParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.QuantizationParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.numberofbits() != 0) {
    set_numberofbits(from.numberofbits());
  }
  switch (from.QuantizationType_case()) {
    case kLinearQuantization: {
      mutable_linearquantization()->::CoreML::Specification::LinearQuantizationParams::MergeFrom(from.linearquantization());
      break;
    }
    case kLookupTableQuantization: {
      mutable_lookuptablequantization()->::CoreML::Specification::LookUpTableQuantizationParams::MergeFrom(from.lookuptablequantization());
      break;
    }
    case QUANTIZATIONTYPE_NOT_SET: {
      break;
    }
  }
}

void QuantizationParams::CopyFrom(const QuantizationParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.QuantizationParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool QuantizationParams::IsInitialized() const {
  return true;
}

void QuantizationParams::Swap(QuantizationParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void QuantizationParams::InternalSwap(QuantizationParams* other) {
  std::swap(numberofbits_, other->numberofbits_);
  std::swap(QuantizationType_, other->QuantizationType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string QuantizationParams::GetTypeName() const {
  return "CoreML.Specification.QuantizationParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// QuantizationParams

// uint64 numberOfBits = 1;
void QuantizationParams::clear_numberofbits() {
  numberofbits_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 QuantizationParams::numberofbits() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.QuantizationParams.numberOfBits)
  return numberofbits_;
}
void QuantizationParams::set_numberofbits(::google::protobuf::uint64 value) {
  
  numberofbits_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.QuantizationParams.numberOfBits)
}

// .CoreML.Specification.LinearQuantizationParams linearQuantization = 101;
bool QuantizationParams::has_linearquantization() const {
  return QuantizationType_case() == kLinearQuantization;
}
void QuantizationParams::set_has_linearquantization() {
  _oneof_case_[0] = kLinearQuantization;
}
void QuantizationParams::clear_linearquantization() {
  if (has_linearquantization()) {
    delete QuantizationType_.linearquantization_;
    clear_has_QuantizationType();
  }
}
 const ::CoreML::Specification::LinearQuantizationParams& QuantizationParams::linearquantization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.QuantizationParams.linearQuantization)
  return has_linearquantization()
      ? *QuantizationType_.linearquantization_
      : ::CoreML::Specification::LinearQuantizationParams::default_instance();
}
::CoreML::Specification::LinearQuantizationParams* QuantizationParams::mutable_linearquantization() {
  if (!has_linearquantization()) {
    clear_QuantizationType();
    set_has_linearquantization();
    QuantizationType_.linearquantization_ = new ::CoreML::Specification::LinearQuantizationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.QuantizationParams.linearQuantization)
  return QuantizationType_.linearquantization_;
}
::CoreML::Specification::LinearQuantizationParams* QuantizationParams::release_linearquantization() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.QuantizationParams.linearQuantization)
  if (has_linearquantization()) {
    clear_has_QuantizationType();
    ::CoreML::Specification::LinearQuantizationParams* temp = QuantizationType_.linearquantization_;
    QuantizationType_.linearquantization_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void QuantizationParams::set_allocated_linearquantization(::CoreML::Specification::LinearQuantizationParams* linearquantization) {
  clear_QuantizationType();
  if (linearquantization) {
    set_has_linearquantization();
    QuantizationType_.linearquantization_ = linearquantization;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.QuantizationParams.linearQuantization)
}

// .CoreML.Specification.LookUpTableQuantizationParams lookupTableQuantization = 102;
bool QuantizationParams::has_lookuptablequantization() const {
  return QuantizationType_case() == kLookupTableQuantization;
}
void QuantizationParams::set_has_lookuptablequantization() {
  _oneof_case_[0] = kLookupTableQuantization;
}
void QuantizationParams::clear_lookuptablequantization() {
  if (has_lookuptablequantization()) {
    delete QuantizationType_.lookuptablequantization_;
    clear_has_QuantizationType();
  }
}
 const ::CoreML::Specification::LookUpTableQuantizationParams& QuantizationParams::lookuptablequantization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.QuantizationParams.lookupTableQuantization)
  return has_lookuptablequantization()
      ? *QuantizationType_.lookuptablequantization_
      : ::CoreML::Specification::LookUpTableQuantizationParams::default_instance();
}
::CoreML::Specification::LookUpTableQuantizationParams* QuantizationParams::mutable_lookuptablequantization() {
  if (!has_lookuptablequantization()) {
    clear_QuantizationType();
    set_has_lookuptablequantization();
    QuantizationType_.lookuptablequantization_ = new ::CoreML::Specification::LookUpTableQuantizationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.QuantizationParams.lookupTableQuantization)
  return QuantizationType_.lookuptablequantization_;
}
::CoreML::Specification::LookUpTableQuantizationParams* QuantizationParams::release_lookuptablequantization() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.QuantizationParams.lookupTableQuantization)
  if (has_lookuptablequantization()) {
    clear_has_QuantizationType();
    ::CoreML::Specification::LookUpTableQuantizationParams* temp = QuantizationType_.lookuptablequantization_;
    QuantizationType_.lookuptablequantization_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void QuantizationParams::set_allocated_lookuptablequantization(::CoreML::Specification::LookUpTableQuantizationParams* lookuptablequantization) {
  clear_QuantizationType();
  if (lookuptablequantization) {
    set_has_lookuptablequantization();
    QuantizationType_.lookuptablequantization_ = lookuptablequantization;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.QuantizationParams.lookupTableQuantization)
}

bool QuantizationParams::has_QuantizationType() const {
  return QuantizationType_case() != QUANTIZATIONTYPE_NOT_SET;
}
void QuantizationParams::clear_has_QuantizationType() {
  _oneof_case_[0] = QUANTIZATIONTYPE_NOT_SET;
}
QuantizationParams::QuantizationTypeCase QuantizationParams::QuantizationType_case() const {
  return QuantizationParams::QuantizationTypeCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LinearQuantizationParams::kScaleFieldNumber;
const int LinearQuantizationParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LinearQuantizationParams::LinearQuantizationParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LinearQuantizationParams)
}
LinearQuantizationParams::LinearQuantizationParams(const LinearQuantizationParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      scale_(from.scale_),
      bias_(from.bias_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LinearQuantizationParams)
}

void LinearQuantizationParams::SharedCtor() {
  _cached_size_ = 0;
}

LinearQuantizationParams::~LinearQuantizationParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LinearQuantizationParams)
  SharedDtor();
}

void LinearQuantizationParams::SharedDtor() {
}

void LinearQuantizationParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LinearQuantizationParams& LinearQuantizationParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LinearQuantizationParams* LinearQuantizationParams::New(::google::protobuf::Arena* arena) const {
  LinearQuantizationParams* n = new LinearQuantizationParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LinearQuantizationParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LinearQuantizationParams)
  scale_.Clear();
  bias_.Clear();
}

bool LinearQuantizationParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LinearQuantizationParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated float scale = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, this->mutable_scale())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(13u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 1, 10u, input, this->mutable_scale())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated float bias = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, this->mutable_bias())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(21u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 1, 18u, input, this->mutable_bias())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LinearQuantizationParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LinearQuantizationParams)
  return false;
#undef DO_
}

void LinearQuantizationParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LinearQuantizationParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated float scale = 1;
  if (this->scale_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_scale_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteFloatArray(
      this->scale().data(), this->scale_size(), output);
  }

  // repeated float bias = 2;
  if (this->bias_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(2, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_bias_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteFloatArray(
      this->bias().data(), this->bias_size(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LinearQuantizationParams)
}

size_t LinearQuantizationParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LinearQuantizationParams)
  size_t total_size = 0;

  // repeated float scale = 1;
  {
    unsigned int count = this->scale_size();
    size_t data_size = 4UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _scale_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated float bias = 2;
  {
    unsigned int count = this->bias_size();
    size_t data_size = 4UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _bias_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LinearQuantizationParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LinearQuantizationParams*>(&from));
}

void LinearQuantizationParams::MergeFrom(const LinearQuantizationParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LinearQuantizationParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  scale_.MergeFrom(from.scale_);
  bias_.MergeFrom(from.bias_);
}

void LinearQuantizationParams::CopyFrom(const LinearQuantizationParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LinearQuantizationParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LinearQuantizationParams::IsInitialized() const {
  return true;
}

void LinearQuantizationParams::Swap(LinearQuantizationParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LinearQuantizationParams::InternalSwap(LinearQuantizationParams* other) {
  scale_.InternalSwap(&other->scale_);
  bias_.InternalSwap(&other->bias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LinearQuantizationParams::GetTypeName() const {
  return "CoreML.Specification.LinearQuantizationParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LinearQuantizationParams

// repeated float scale = 1;
int LinearQuantizationParams::scale_size() const {
  return scale_.size();
}
void LinearQuantizationParams::clear_scale() {
  scale_.Clear();
}
float LinearQuantizationParams::scale(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LinearQuantizationParams.scale)
  return scale_.Get(index);
}
void LinearQuantizationParams::set_scale(int index, float value) {
  scale_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LinearQuantizationParams.scale)
}
void LinearQuantizationParams::add_scale(float value) {
  scale_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LinearQuantizationParams.scale)
}
const ::google::protobuf::RepeatedField< float >&
LinearQuantizationParams::scale() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LinearQuantizationParams.scale)
  return scale_;
}
::google::protobuf::RepeatedField< float >*
LinearQuantizationParams::mutable_scale() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LinearQuantizationParams.scale)
  return &scale_;
}

// repeated float bias = 2;
int LinearQuantizationParams::bias_size() const {
  return bias_.size();
}
void LinearQuantizationParams::clear_bias() {
  bias_.Clear();
}
float LinearQuantizationParams::bias(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LinearQuantizationParams.bias)
  return bias_.Get(index);
}
void LinearQuantizationParams::set_bias(int index, float value) {
  bias_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LinearQuantizationParams.bias)
}
void LinearQuantizationParams::add_bias(float value) {
  bias_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LinearQuantizationParams.bias)
}
const ::google::protobuf::RepeatedField< float >&
LinearQuantizationParams::bias() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LinearQuantizationParams.bias)
  return bias_;
}
::google::protobuf::RepeatedField< float >*
LinearQuantizationParams::mutable_bias() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LinearQuantizationParams.bias)
  return &bias_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LookUpTableQuantizationParams::kFloatValueFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LookUpTableQuantizationParams::LookUpTableQuantizationParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LookUpTableQuantizationParams)
}
LookUpTableQuantizationParams::LookUpTableQuantizationParams(const LookUpTableQuantizationParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      floatvalue_(from.floatvalue_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LookUpTableQuantizationParams)
}

void LookUpTableQuantizationParams::SharedCtor() {
  _cached_size_ = 0;
}

LookUpTableQuantizationParams::~LookUpTableQuantizationParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LookUpTableQuantizationParams)
  SharedDtor();
}

void LookUpTableQuantizationParams::SharedDtor() {
}

void LookUpTableQuantizationParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LookUpTableQuantizationParams& LookUpTableQuantizationParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LookUpTableQuantizationParams* LookUpTableQuantizationParams::New(::google::protobuf::Arena* arena) const {
  LookUpTableQuantizationParams* n = new LookUpTableQuantizationParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LookUpTableQuantizationParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LookUpTableQuantizationParams)
  floatvalue_.Clear();
}

bool LookUpTableQuantizationParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LookUpTableQuantizationParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated float floatValue = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, this->mutable_floatvalue())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(13u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 1, 10u, input, this->mutable_floatvalue())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LookUpTableQuantizationParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LookUpTableQuantizationParams)
  return false;
#undef DO_
}

void LookUpTableQuantizationParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LookUpTableQuantizationParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated float floatValue = 1;
  if (this->floatvalue_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_floatvalue_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteFloatArray(
      this->floatvalue().data(), this->floatvalue_size(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LookUpTableQuantizationParams)
}

size_t LookUpTableQuantizationParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LookUpTableQuantizationParams)
  size_t total_size = 0;

  // repeated float floatValue = 1;
  {
    unsigned int count = this->floatvalue_size();
    size_t data_size = 4UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _floatvalue_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LookUpTableQuantizationParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LookUpTableQuantizationParams*>(&from));
}

void LookUpTableQuantizationParams::MergeFrom(const LookUpTableQuantizationParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LookUpTableQuantizationParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  floatvalue_.MergeFrom(from.floatvalue_);
}

void LookUpTableQuantizationParams::CopyFrom(const LookUpTableQuantizationParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LookUpTableQuantizationParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LookUpTableQuantizationParams::IsInitialized() const {
  return true;
}

void LookUpTableQuantizationParams::Swap(LookUpTableQuantizationParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LookUpTableQuantizationParams::InternalSwap(LookUpTableQuantizationParams* other) {
  floatvalue_.InternalSwap(&other->floatvalue_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LookUpTableQuantizationParams::GetTypeName() const {
  return "CoreML.Specification.LookUpTableQuantizationParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LookUpTableQuantizationParams

// repeated float floatValue = 1;
int LookUpTableQuantizationParams::floatvalue_size() const {
  return floatvalue_.size();
}
void LookUpTableQuantizationParams::clear_floatvalue() {
  floatvalue_.Clear();
}
float LookUpTableQuantizationParams::floatvalue(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
  return floatvalue_.Get(index);
}
void LookUpTableQuantizationParams::set_floatvalue(int index, float value) {
  floatvalue_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
}
void LookUpTableQuantizationParams::add_floatvalue(float value) {
  floatvalue_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
}
const ::google::protobuf::RepeatedField< float >&
LookUpTableQuantizationParams::floatvalue() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
  return floatvalue_;
}
::google::protobuf::RepeatedField< float >*
LookUpTableQuantizationParams::mutable_floatvalue() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
  return &floatvalue_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ConvolutionLayerParams::kOutputChannelsFieldNumber;
const int ConvolutionLayerParams::kKernelChannelsFieldNumber;
const int ConvolutionLayerParams::kNGroupsFieldNumber;
const int ConvolutionLayerParams::kKernelSizeFieldNumber;
const int ConvolutionLayerParams::kStrideFieldNumber;
const int ConvolutionLayerParams::kDilationFactorFieldNumber;
const int ConvolutionLayerParams::kValidFieldNumber;
const int ConvolutionLayerParams::kSameFieldNumber;
const int ConvolutionLayerParams::kIsDeconvolutionFieldNumber;
const int ConvolutionLayerParams::kHasBiasFieldNumber;
const int ConvolutionLayerParams::kWeightsFieldNumber;
const int ConvolutionLayerParams::kBiasFieldNumber;
const int ConvolutionLayerParams::kOutputShapeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ConvolutionLayerParams::ConvolutionLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ConvolutionLayerParams)
}
ConvolutionLayerParams::ConvolutionLayerParams(const ConvolutionLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      kernelsize_(from.kernelsize_),
      stride_(from.stride_),
      dilationfactor_(from.dilationfactor_),
      outputshape_(from.outputshape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_weights()) {
    weights_ = new ::CoreML::Specification::WeightParams(*from.weights_);
  } else {
    weights_ = NULL;
  }
  if (from.has_bias()) {
    bias_ = new ::CoreML::Specification::WeightParams(*from.bias_);
  } else {
    bias_ = NULL;
  }
  ::memcpy(&outputchannels_, &from.outputchannels_,
    reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&outputchannels_) + sizeof(hasbias_));
  clear_has_ConvolutionPaddingType();
  switch (from.ConvolutionPaddingType_case()) {
    case kValid: {
      mutable_valid()->::CoreML::Specification::ValidPadding::MergeFrom(from.valid());
      break;
    }
    case kSame: {
      mutable_same()->::CoreML::Specification::SamePadding::MergeFrom(from.same());
      break;
    }
    case CONVOLUTIONPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ConvolutionLayerParams)
}

void ConvolutionLayerParams::SharedCtor() {
  ::memset(&weights_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&weights_) + sizeof(hasbias_));
  clear_has_ConvolutionPaddingType();
  _cached_size_ = 0;
}

ConvolutionLayerParams::~ConvolutionLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ConvolutionLayerParams)
  SharedDtor();
}

void ConvolutionLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete weights_;
  }
  if (this != internal_default_instance()) {
    delete bias_;
  }
  if (has_ConvolutionPaddingType()) {
    clear_ConvolutionPaddingType();
  }
}

void ConvolutionLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ConvolutionLayerParams& ConvolutionLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ConvolutionLayerParams* ConvolutionLayerParams::New(::google::protobuf::Arena* arena) const {
  ConvolutionLayerParams* n = new ConvolutionLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ConvolutionLayerParams::clear_ConvolutionPaddingType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.ConvolutionLayerParams)
  switch (ConvolutionPaddingType_case()) {
    case kValid: {
      delete ConvolutionPaddingType_.valid_;
      break;
    }
    case kSame: {
      delete ConvolutionPaddingType_.same_;
      break;
    }
    case CONVOLUTIONPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = CONVOLUTIONPADDINGTYPE_NOT_SET;
}


void ConvolutionLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ConvolutionLayerParams)
  kernelsize_.Clear();
  stride_.Clear();
  dilationfactor_.Clear();
  outputshape_.Clear();
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) {
    delete weights_;
  }
  weights_ = NULL;
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) {
    delete bias_;
  }
  bias_ = NULL;
  ::memset(&outputchannels_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&outputchannels_) + sizeof(hasbias_));
  clear_ConvolutionPaddingType();
}

bool ConvolutionLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ConvolutionLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 outputChannels = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputchannels_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 kernelChannels = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &kernelchannels_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 nGroups = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(80u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &ngroups_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 kernelSize = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_kernelsize())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(160u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 2, 162u, input, this->mutable_kernelsize())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 stride = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(242u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_stride())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(240u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 2, 242u, input, this->mutable_stride())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 dilationFactor = 40;
      case 40: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(322u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_dilationfactor())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(320u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 2, 322u, input, this->mutable_dilationfactor())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ValidPadding valid = 50;
      case 50: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(402u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_valid()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SamePadding same = 51;
      case 51: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(410u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_same()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool isDeconvolution = 60;
      case 60: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(480u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &isdeconvolution_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBias = 70;
      case 70: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(560u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams weights = 90;
      case 90: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(722u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weights()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams bias = 91;
      case 91: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(730u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 outputShape = 100;
      case 100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(802u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_outputshape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(800u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 2, 802u, input, this->mutable_outputshape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ConvolutionLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ConvolutionLayerParams)
  return false;
#undef DO_
}

void ConvolutionLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ConvolutionLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 outputChannels = 1;
  if (this->outputchannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->outputchannels(), output);
  }

  // uint64 kernelChannels = 2;
  if (this->kernelchannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->kernelchannels(), output);
  }

  // uint64 nGroups = 10;
  if (this->ngroups() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(10, this->ngroups(), output);
  }

  // repeated uint64 kernelSize = 20;
  if (this->kernelsize_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(20, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_kernelsize_cached_byte_size_);
  }
  for (int i = 0, n = this->kernelsize_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->kernelsize(i), output);
  }

  // repeated uint64 stride = 30;
  if (this->stride_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(30, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_stride_cached_byte_size_);
  }
  for (int i = 0, n = this->stride_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->stride(i), output);
  }

  // repeated uint64 dilationFactor = 40;
  if (this->dilationfactor_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(40, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_dilationfactor_cached_byte_size_);
  }
  for (int i = 0, n = this->dilationfactor_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->dilationfactor(i), output);
  }

  // .CoreML.Specification.ValidPadding valid = 50;
  if (has_valid()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      50, *ConvolutionPaddingType_.valid_, output);
  }

  // .CoreML.Specification.SamePadding same = 51;
  if (has_same()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      51, *ConvolutionPaddingType_.same_, output);
  }

  // bool isDeconvolution = 60;
  if (this->isdeconvolution() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(60, this->isdeconvolution(), output);
  }

  // bool hasBias = 70;
  if (this->hasbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(70, this->hasbias(), output);
  }

  // .CoreML.Specification.WeightParams weights = 90;
  if (this->has_weights()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      90, *this->weights_, output);
  }

  // .CoreML.Specification.WeightParams bias = 91;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      91, *this->bias_, output);
  }

  // repeated uint64 outputShape = 100;
  if (this->outputshape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(100, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_outputshape_cached_byte_size_);
  }
  for (int i = 0, n = this->outputshape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->outputshape(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ConvolutionLayerParams)
}

size_t ConvolutionLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ConvolutionLayerParams)
  size_t total_size = 0;

  // repeated uint64 kernelSize = 20;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->kernelsize_);
    if (data_size > 0) {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _kernelsize_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated uint64 stride = 30;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->stride_);
    if (data_size > 0) {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _stride_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated uint64 dilationFactor = 40;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->dilationfactor_);
    if (data_size > 0) {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _dilationfactor_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated uint64 outputShape = 100;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->outputshape_);
    if (data_size > 0) {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _outputshape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.WeightParams weights = 90;
  if (this->has_weights()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weights_);
  }

  // .CoreML.Specification.WeightParams bias = 91;
  if (this->has_bias()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  // uint64 outputChannels = 1;
  if (this->outputchannels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputchannels());
  }

  // uint64 kernelChannels = 2;
  if (this->kernelchannels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->kernelchannels());
  }

  // uint64 nGroups = 10;
  if (this->ngroups() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->ngroups());
  }

  // bool isDeconvolution = 60;
  if (this->isdeconvolution() != 0) {
    total_size += 2 + 1;
  }

  // bool hasBias = 70;
  if (this->hasbias() != 0) {
    total_size += 2 + 1;
  }

  switch (ConvolutionPaddingType_case()) {
    // .CoreML.Specification.ValidPadding valid = 50;
    case kValid: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *ConvolutionPaddingType_.valid_);
      break;
    }
    // .CoreML.Specification.SamePadding same = 51;
    case kSame: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *ConvolutionPaddingType_.same_);
      break;
    }
    case CONVOLUTIONPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ConvolutionLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ConvolutionLayerParams*>(&from));
}

void ConvolutionLayerParams::MergeFrom(const ConvolutionLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ConvolutionLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  kernelsize_.MergeFrom(from.kernelsize_);
  stride_.MergeFrom(from.stride_);
  dilationfactor_.MergeFrom(from.dilationfactor_);
  outputshape_.MergeFrom(from.outputshape_);
  if (from.has_weights()) {
    mutable_weights()->::CoreML::Specification::WeightParams::MergeFrom(from.weights());
  }
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
  if (from.outputchannels() != 0) {
    set_outputchannels(from.outputchannels());
  }
  if (from.kernelchannels() != 0) {
    set_kernelchannels(from.kernelchannels());
  }
  if (from.ngroups() != 0) {
    set_ngroups(from.ngroups());
  }
  if (from.isdeconvolution() != 0) {
    set_isdeconvolution(from.isdeconvolution());
  }
  if (from.hasbias() != 0) {
    set_hasbias(from.hasbias());
  }
  switch (from.ConvolutionPaddingType_case()) {
    case kValid: {
      mutable_valid()->::CoreML::Specification::ValidPadding::MergeFrom(from.valid());
      break;
    }
    case kSame: {
      mutable_same()->::CoreML::Specification::SamePadding::MergeFrom(from.same());
      break;
    }
    case CONVOLUTIONPADDINGTYPE_NOT_SET: {
      break;
    }
  }
}

void ConvolutionLayerParams::CopyFrom(const ConvolutionLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ConvolutionLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ConvolutionLayerParams::IsInitialized() const {
  return true;
}

void ConvolutionLayerParams::Swap(ConvolutionLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ConvolutionLayerParams::InternalSwap(ConvolutionLayerParams* other) {
  kernelsize_.InternalSwap(&other->kernelsize_);
  stride_.InternalSwap(&other->stride_);
  dilationfactor_.InternalSwap(&other->dilationfactor_);
  outputshape_.InternalSwap(&other->outputshape_);
  std::swap(weights_, other->weights_);
  std::swap(bias_, other->bias_);
  std::swap(outputchannels_, other->outputchannels_);
  std::swap(kernelchannels_, other->kernelchannels_);
  std::swap(ngroups_, other->ngroups_);
  std::swap(isdeconvolution_, other->isdeconvolution_);
  std::swap(hasbias_, other->hasbias_);
  std::swap(ConvolutionPaddingType_, other->ConvolutionPaddingType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ConvolutionLayerParams::GetTypeName() const {
  return "CoreML.Specification.ConvolutionLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ConvolutionLayerParams

// uint64 outputChannels = 1;
void ConvolutionLayerParams::clear_outputchannels() {
  outputchannels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 ConvolutionLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.outputChannels)
  return outputchannels_;
}
void ConvolutionLayerParams::set_outputchannels(::google::protobuf::uint64 value) {
  
  outputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.outputChannels)
}

// uint64 kernelChannels = 2;
void ConvolutionLayerParams::clear_kernelchannels() {
  kernelchannels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 ConvolutionLayerParams::kernelchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.kernelChannels)
  return kernelchannels_;
}
void ConvolutionLayerParams::set_kernelchannels(::google::protobuf::uint64 value) {
  
  kernelchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.kernelChannels)
}

// uint64 nGroups = 10;
void ConvolutionLayerParams::clear_ngroups() {
  ngroups_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 ConvolutionLayerParams::ngroups() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.nGroups)
  return ngroups_;
}
void ConvolutionLayerParams::set_ngroups(::google::protobuf::uint64 value) {
  
  ngroups_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.nGroups)
}

// repeated uint64 kernelSize = 20;
int ConvolutionLayerParams::kernelsize_size() const {
  return kernelsize_.size();
}
void ConvolutionLayerParams::clear_kernelsize() {
  kernelsize_.Clear();
}
::google::protobuf::uint64 ConvolutionLayerParams::kernelsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.kernelSize)
  return kernelsize_.Get(index);
}
void ConvolutionLayerParams::set_kernelsize(int index, ::google::protobuf::uint64 value) {
  kernelsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.kernelSize)
}
void ConvolutionLayerParams::add_kernelsize(::google::protobuf::uint64 value) {
  kernelsize_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.kernelSize)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::kernelsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.kernelSize)
  return kernelsize_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_kernelsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.kernelSize)
  return &kernelsize_;
}

// repeated uint64 stride = 30;
int ConvolutionLayerParams::stride_size() const {
  return stride_.size();
}
void ConvolutionLayerParams::clear_stride() {
  stride_.Clear();
}
::google::protobuf::uint64 ConvolutionLayerParams::stride(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.stride)
  return stride_.Get(index);
}
void ConvolutionLayerParams::set_stride(int index, ::google::protobuf::uint64 value) {
  stride_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.stride)
}
void ConvolutionLayerParams::add_stride(::google::protobuf::uint64 value) {
  stride_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.stride)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::stride() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.stride)
  return stride_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_stride() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.stride)
  return &stride_;
}

// repeated uint64 dilationFactor = 40;
int ConvolutionLayerParams::dilationfactor_size() const {
  return dilationfactor_.size();
}
void ConvolutionLayerParams::clear_dilationfactor() {
  dilationfactor_.Clear();
}
::google::protobuf::uint64 ConvolutionLayerParams::dilationfactor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
  return dilationfactor_.Get(index);
}
void ConvolutionLayerParams::set_dilationfactor(int index, ::google::protobuf::uint64 value) {
  dilationfactor_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
}
void ConvolutionLayerParams::add_dilationfactor(::google::protobuf::uint64 value) {
  dilationfactor_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::dilationfactor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
  return dilationfactor_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_dilationfactor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
  return &dilationfactor_;
}

// .CoreML.Specification.ValidPadding valid = 50;
bool ConvolutionLayerParams::has_valid() const {
  return ConvolutionPaddingType_case() == kValid;
}
void ConvolutionLayerParams::set_has_valid() {
  _oneof_case_[0] = kValid;
}
void ConvolutionLayerParams::clear_valid() {
  if (has_valid()) {
    delete ConvolutionPaddingType_.valid_;
    clear_has_ConvolutionPaddingType();
  }
}
 const ::CoreML::Specification::ValidPadding& ConvolutionLayerParams::valid() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.valid)
  return has_valid()
      ? *ConvolutionPaddingType_.valid_
      : ::CoreML::Specification::ValidPadding::default_instance();
}
::CoreML::Specification::ValidPadding* ConvolutionLayerParams::mutable_valid() {
  if (!has_valid()) {
    clear_ConvolutionPaddingType();
    set_has_valid();
    ConvolutionPaddingType_.valid_ = new ::CoreML::Specification::ValidPadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.valid)
  return ConvolutionPaddingType_.valid_;
}
::CoreML::Specification::ValidPadding* ConvolutionLayerParams::release_valid() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.valid)
  if (has_valid()) {
    clear_has_ConvolutionPaddingType();
    ::CoreML::Specification::ValidPadding* temp = ConvolutionPaddingType_.valid_;
    ConvolutionPaddingType_.valid_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ConvolutionLayerParams::set_allocated_valid(::CoreML::Specification::ValidPadding* valid) {
  clear_ConvolutionPaddingType();
  if (valid) {
    set_has_valid();
    ConvolutionPaddingType_.valid_ = valid;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.valid)
}

// .CoreML.Specification.SamePadding same = 51;
bool ConvolutionLayerParams::has_same() const {
  return ConvolutionPaddingType_case() == kSame;
}
void ConvolutionLayerParams::set_has_same() {
  _oneof_case_[0] = kSame;
}
void ConvolutionLayerParams::clear_same() {
  if (has_same()) {
    delete ConvolutionPaddingType_.same_;
    clear_has_ConvolutionPaddingType();
  }
}
 const ::CoreML::Specification::SamePadding& ConvolutionLayerParams::same() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.same)
  return has_same()
      ? *ConvolutionPaddingType_.same_
      : ::CoreML::Specification::SamePadding::default_instance();
}
::CoreML::Specification::SamePadding* ConvolutionLayerParams::mutable_same() {
  if (!has_same()) {
    clear_ConvolutionPaddingType();
    set_has_same();
    ConvolutionPaddingType_.same_ = new ::CoreML::Specification::SamePadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.same)
  return ConvolutionPaddingType_.same_;
}
::CoreML::Specification::SamePadding* ConvolutionLayerParams::release_same() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.same)
  if (has_same()) {
    clear_has_ConvolutionPaddingType();
    ::CoreML::Specification::SamePadding* temp = ConvolutionPaddingType_.same_;
    ConvolutionPaddingType_.same_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ConvolutionLayerParams::set_allocated_same(::CoreML::Specification::SamePadding* same) {
  clear_ConvolutionPaddingType();
  if (same) {
    set_has_same();
    ConvolutionPaddingType_.same_ = same;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.same)
}

// bool isDeconvolution = 60;
void ConvolutionLayerParams::clear_isdeconvolution() {
  isdeconvolution_ = false;
}
bool ConvolutionLayerParams::isdeconvolution() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.isDeconvolution)
  return isdeconvolution_;
}
void ConvolutionLayerParams::set_isdeconvolution(bool value) {
  
  isdeconvolution_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.isDeconvolution)
}

// bool hasBias = 70;
void ConvolutionLayerParams::clear_hasbias() {
  hasbias_ = false;
}
bool ConvolutionLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.hasBias)
  return hasbias_;
}
void ConvolutionLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 90;
bool ConvolutionLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
void ConvolutionLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
const ::CoreML::Specification::WeightParams& ConvolutionLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ConvolutionLayerParams::mutable_weights() {
  
  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.weights)
  return weights_;
}
::CoreML::Specification::WeightParams* ConvolutionLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
void ConvolutionLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 91;
bool ConvolutionLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void ConvolutionLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& ConvolutionLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ConvolutionLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* ConvolutionLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void ConvolutionLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.bias)
}

// repeated uint64 outputShape = 100;
int ConvolutionLayerParams::outputshape_size() const {
  return outputshape_.size();
}
void ConvolutionLayerParams::clear_outputshape() {
  outputshape_.Clear();
}
::google::protobuf::uint64 ConvolutionLayerParams::outputshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.outputShape)
  return outputshape_.Get(index);
}
void ConvolutionLayerParams::set_outputshape(int index, ::google::protobuf::uint64 value) {
  outputshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.outputShape)
}
void ConvolutionLayerParams::add_outputshape(::google::protobuf::uint64 value) {
  outputshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.outputShape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::outputshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.outputShape)
  return outputshape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_outputshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.outputShape)
  return &outputshape_;
}

bool ConvolutionLayerParams::has_ConvolutionPaddingType() const {
  return ConvolutionPaddingType_case() != CONVOLUTIONPADDINGTYPE_NOT_SET;
}
void ConvolutionLayerParams::clear_has_ConvolutionPaddingType() {
  _oneof_case_[0] = CONVOLUTIONPADDINGTYPE_NOT_SET;
}
ConvolutionLayerParams::ConvolutionPaddingTypeCase ConvolutionLayerParams::ConvolutionPaddingType_case() const {
  return ConvolutionLayerParams::ConvolutionPaddingTypeCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int InnerProductLayerParams::kInputChannelsFieldNumber;
const int InnerProductLayerParams::kOutputChannelsFieldNumber;
const int InnerProductLayerParams::kHasBiasFieldNumber;
const int InnerProductLayerParams::kWeightsFieldNumber;
const int InnerProductLayerParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

InnerProductLayerParams::InnerProductLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.InnerProductLayerParams)
}
InnerProductLayerParams::InnerProductLayerParams(const InnerProductLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_weights()) {
    weights_ = new ::CoreML::Specification::WeightParams(*from.weights_);
  } else {
    weights_ = NULL;
  }
  if (from.has_bias()) {
    bias_ = new ::CoreML::Specification::WeightParams(*from.bias_);
  } else {
    bias_ = NULL;
  }
  ::memcpy(&inputchannels_, &from.inputchannels_,
    reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&inputchannels_) + sizeof(hasbias_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.InnerProductLayerParams)
}

void InnerProductLayerParams::SharedCtor() {
  ::memset(&weights_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&weights_) + sizeof(hasbias_));
  _cached_size_ = 0;
}

InnerProductLayerParams::~InnerProductLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.InnerProductLayerParams)
  SharedDtor();
}

void InnerProductLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete weights_;
  }
  if (this != internal_default_instance()) {
    delete bias_;
  }
}

void InnerProductLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const InnerProductLayerParams& InnerProductLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

InnerProductLayerParams* InnerProductLayerParams::New(::google::protobuf::Arena* arena) const {
  InnerProductLayerParams* n = new InnerProductLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void InnerProductLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.InnerProductLayerParams)
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) {
    delete weights_;
  }
  weights_ = NULL;
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) {
    delete bias_;
  }
  bias_ = NULL;
  ::memset(&inputchannels_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&inputchannels_) + sizeof(hasbias_));
}

bool InnerProductLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.InnerProductLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 inputChannels = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputchannels_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 outputChannels = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputchannels_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBias = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(80u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams weights = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weights()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams bias = 21;
      case 21: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(170u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.InnerProductLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.InnerProductLayerParams)
  return false;
#undef DO_
}

void InnerProductLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.InnerProductLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 inputChannels = 1;
  if (this->inputchannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputchannels(), output);
  }

  // uint64 outputChannels = 2;
  if (this->outputchannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputchannels(), output);
  }

  // bool hasBias = 10;
  if (this->hasbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(10, this->hasbias(), output);
  }

  // .CoreML.Specification.WeightParams weights = 20;
  if (this->has_weights()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *this->weights_, output);
  }

  // .CoreML.Specification.WeightParams bias = 21;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      21, *this->bias_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.InnerProductLayerParams)
}

size_t InnerProductLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.InnerProductLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams weights = 20;
  if (this->has_weights()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weights_);
  }

  // .CoreML.Specification.WeightParams bias = 21;
  if (this->has_bias()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  // uint64 inputChannels = 1;
  if (this->inputchannels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputchannels());
  }

  // uint64 outputChannels = 2;
  if (this->outputchannels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputchannels());
  }

  // bool hasBias = 10;
  if (this->hasbias() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void InnerProductLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const InnerProductLayerParams*>(&from));
}

void InnerProductLayerParams::MergeFrom(const InnerProductLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.InnerProductLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_weights()) {
    mutable_weights()->::CoreML::Specification::WeightParams::MergeFrom(from.weights());
  }
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
  if (from.inputchannels() != 0) {
    set_inputchannels(from.inputchannels());
  }
  if (from.outputchannels() != 0) {
    set_outputchannels(from.outputchannels());
  }
  if (from.hasbias() != 0) {
    set_hasbias(from.hasbias());
  }
}

void InnerProductLayerParams::CopyFrom(const InnerProductLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.InnerProductLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool InnerProductLayerParams::IsInitialized() const {
  return true;
}

void InnerProductLayerParams::Swap(InnerProductLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void InnerProductLayerParams::InternalSwap(InnerProductLayerParams* other) {
  std::swap(weights_, other->weights_);
  std::swap(bias_, other->bias_);
  std::swap(inputchannels_, other->inputchannels_);
  std::swap(outputchannels_, other->outputchannels_);
  std::swap(hasbias_, other->hasbias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string InnerProductLayerParams::GetTypeName() const {
  return "CoreML.Specification.InnerProductLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// InnerProductLayerParams

// uint64 inputChannels = 1;
void InnerProductLayerParams::clear_inputchannels() {
  inputchannels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 InnerProductLayerParams::inputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.inputChannels)
  return inputchannels_;
}
void InnerProductLayerParams::set_inputchannels(::google::protobuf::uint64 value) {
  
  inputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.inputChannels)
}

// uint64 outputChannels = 2;
void InnerProductLayerParams::clear_outputchannels() {
  outputchannels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 InnerProductLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.outputChannels)
  return outputchannels_;
}
void InnerProductLayerParams::set_outputchannels(::google::protobuf::uint64 value) {
  
  outputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.outputChannels)
}

// bool hasBias = 10;
void InnerProductLayerParams::clear_hasbias() {
  hasbias_ = false;
}
bool InnerProductLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.hasBias)
  return hasbias_;
}
void InnerProductLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 20;
bool InnerProductLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
void InnerProductLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
const ::CoreML::Specification::WeightParams& InnerProductLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* InnerProductLayerParams::mutable_weights() {
  
  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.InnerProductLayerParams.weights)
  return weights_;
}
::CoreML::Specification::WeightParams* InnerProductLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.InnerProductLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
void InnerProductLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.InnerProductLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 21;
bool InnerProductLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void InnerProductLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& InnerProductLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* InnerProductLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.InnerProductLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* InnerProductLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.InnerProductLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void InnerProductLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.InnerProductLayerParams.bias)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int EmbeddingLayerParams::kInputDimFieldNumber;
const int EmbeddingLayerParams::kOutputChannelsFieldNumber;
const int EmbeddingLayerParams::kHasBiasFieldNumber;
const int EmbeddingLayerParams::kWeightsFieldNumber;
const int EmbeddingLayerParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

EmbeddingLayerParams::EmbeddingLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.EmbeddingLayerParams)
}
EmbeddingLayerParams::EmbeddingLayerParams(const EmbeddingLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_weights()) {
    weights_ = new ::CoreML::Specification::WeightParams(*from.weights_);
  } else {
    weights_ = NULL;
  }
  if (from.has_bias()) {
    bias_ = new ::CoreML::Specification::WeightParams(*from.bias_);
  } else {
    bias_ = NULL;
  }
  ::memcpy(&inputdim_, &from.inputdim_,
    reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&inputdim_) + sizeof(hasbias_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.EmbeddingLayerParams)
}

void EmbeddingLayerParams::SharedCtor() {
  ::memset(&weights_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&weights_) + sizeof(hasbias_));
  _cached_size_ = 0;
}

EmbeddingLayerParams::~EmbeddingLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.EmbeddingLayerParams)
  SharedDtor();
}

void EmbeddingLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete weights_;
  }
  if (this != internal_default_instance()) {
    delete bias_;
  }
}

void EmbeddingLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const EmbeddingLayerParams& EmbeddingLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

EmbeddingLayerParams* EmbeddingLayerParams::New(::google::protobuf::Arena* arena) const {
  EmbeddingLayerParams* n = new EmbeddingLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void EmbeddingLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.EmbeddingLayerParams)
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) {
    delete weights_;
  }
  weights_ = NULL;
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) {
    delete bias_;
  }
  bias_ = NULL;
  ::memset(&inputdim_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&inputdim_) + sizeof(hasbias_));
}

bool EmbeddingLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.EmbeddingLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 inputDim = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputdim_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 outputChannels = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputchannels_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBias = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(80u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams weights = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weights()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams bias = 21;
      case 21: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(170u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.EmbeddingLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.EmbeddingLayerParams)
  return false;
#undef DO_
}

void EmbeddingLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.EmbeddingLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 inputDim = 1;
  if (this->inputdim() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputdim(), output);
  }

  // uint64 outputChannels = 2;
  if (this->outputchannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputchannels(), output);
  }

  // bool hasBias = 10;
  if (this->hasbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(10, this->hasbias(), output);
  }

  // .CoreML.Specification.WeightParams weights = 20;
  if (this->has_weights()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *this->weights_, output);
  }

  // .CoreML.Specification.WeightParams bias = 21;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      21, *this->bias_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.EmbeddingLayerParams)
}

size_t EmbeddingLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.EmbeddingLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams weights = 20;
  if (this->has_weights()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weights_);
  }

  // .CoreML.Specification.WeightParams bias = 21;
  if (this->has_bias()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  // uint64 inputDim = 1;
  if (this->inputdim() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputdim());
  }

  // uint64 outputChannels = 2;
  if (this->outputchannels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputchannels());
  }

  // bool hasBias = 10;
  if (this->hasbias() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void EmbeddingLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const EmbeddingLayerParams*>(&from));
}

void EmbeddingLayerParams::MergeFrom(const EmbeddingLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.EmbeddingLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_weights()) {
    mutable_weights()->::CoreML::Specification::WeightParams::MergeFrom(from.weights());
  }
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
  if (from.inputdim() != 0) {
    set_inputdim(from.inputdim());
  }
  if (from.outputchannels() != 0) {
    set_outputchannels(from.outputchannels());
  }
  if (from.hasbias() != 0) {
    set_hasbias(from.hasbias());
  }
}

void EmbeddingLayerParams::CopyFrom(const EmbeddingLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.EmbeddingLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool EmbeddingLayerParams::IsInitialized() const {
  return true;
}

void EmbeddingLayerParams::Swap(EmbeddingLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void EmbeddingLayerParams::InternalSwap(EmbeddingLayerParams* other) {
  std::swap(weights_, other->weights_);
  std::swap(bias_, other->bias_);
  std::swap(inputdim_, other->inputdim_);
  std::swap(outputchannels_, other->outputchannels_);
  std::swap(hasbias_, other->hasbias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string EmbeddingLayerParams::GetTypeName() const {
  return "CoreML.Specification.EmbeddingLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// EmbeddingLayerParams

// uint64 inputDim = 1;
void EmbeddingLayerParams::clear_inputdim() {
  inputdim_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 EmbeddingLayerParams::inputdim() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.inputDim)
  return inputdim_;
}
void EmbeddingLayerParams::set_inputdim(::google::protobuf::uint64 value) {
  
  inputdim_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingLayerParams.inputDim)
}

// uint64 outputChannels = 2;
void EmbeddingLayerParams::clear_outputchannels() {
  outputchannels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 EmbeddingLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.outputChannels)
  return outputchannels_;
}
void EmbeddingLayerParams::set_outputchannels(::google::protobuf::uint64 value) {
  
  outputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingLayerParams.outputChannels)
}

// bool hasBias = 10;
void EmbeddingLayerParams::clear_hasbias() {
  hasbias_ = false;
}
bool EmbeddingLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.hasBias)
  return hasbias_;
}
void EmbeddingLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 20;
bool EmbeddingLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
void EmbeddingLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
const ::CoreML::Specification::WeightParams& EmbeddingLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* EmbeddingLayerParams::mutable_weights() {
  
  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingLayerParams.weights)
  return weights_;
}
::CoreML::Specification::WeightParams* EmbeddingLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
void EmbeddingLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 21;
bool EmbeddingLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void EmbeddingLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& EmbeddingLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* EmbeddingLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* EmbeddingLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void EmbeddingLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingLayerParams.bias)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BatchnormLayerParams::kChannelsFieldNumber;
const int BatchnormLayerParams::kComputeMeanVarFieldNumber;
const int BatchnormLayerParams::kInstanceNormalizationFieldNumber;
const int BatchnormLayerParams::kEpsilonFieldNumber;
const int BatchnormLayerParams::kGammaFieldNumber;
const int BatchnormLayerParams::kBetaFieldNumber;
const int BatchnormLayerParams::kMeanFieldNumber;
const int BatchnormLayerParams::kVarianceFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BatchnormLayerParams::BatchnormLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BatchnormLayerParams)
}
BatchnormLayerParams::BatchnormLayerParams(const BatchnormLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_gamma()) {
    gamma_ = new ::CoreML::Specification::WeightParams(*from.gamma_);
  } else {
    gamma_ = NULL;
  }
  if (from.has_beta()) {
    beta_ = new ::CoreML::Specification::WeightParams(*from.beta_);
  } else {
    beta_ = NULL;
  }
  if (from.has_mean()) {
    mean_ = new ::CoreML::Specification::WeightParams(*from.mean_);
  } else {
    mean_ = NULL;
  }
  if (from.has_variance()) {
    variance_ = new ::CoreML::Specification::WeightParams(*from.variance_);
  } else {
    variance_ = NULL;
  }
  ::memcpy(&channels_, &from.channels_,
    reinterpret_cast<char*>(&epsilon_) -
    reinterpret_cast<char*>(&channels_) + sizeof(epsilon_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BatchnormLayerParams)
}

void BatchnormLayerParams::SharedCtor() {
  ::memset(&gamma_, 0, reinterpret_cast<char*>(&epsilon_) -
    reinterpret_cast<char*>(&gamma_) + sizeof(epsilon_));
  _cached_size_ = 0;
}

BatchnormLayerParams::~BatchnormLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BatchnormLayerParams)
  SharedDtor();
}

void BatchnormLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete gamma_;
  }
  if (this != internal_default_instance()) {
    delete beta_;
  }
  if (this != internal_default_instance()) {
    delete mean_;
  }
  if (this != internal_default_instance()) {
    delete variance_;
  }
}

void BatchnormLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BatchnormLayerParams& BatchnormLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BatchnormLayerParams* BatchnormLayerParams::New(::google::protobuf::Arena* arena) const {
  BatchnormLayerParams* n = new BatchnormLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BatchnormLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BatchnormLayerParams)
  if (GetArenaNoVirtual() == NULL && gamma_ != NULL) {
    delete gamma_;
  }
  gamma_ = NULL;
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) {
    delete beta_;
  }
  beta_ = NULL;
  if (GetArenaNoVirtual() == NULL && mean_ != NULL) {
    delete mean_;
  }
  mean_ = NULL;
  if (GetArenaNoVirtual() == NULL && variance_ != NULL) {
    delete variance_;
  }
  variance_ = NULL;
  ::memset(&channels_, 0, reinterpret_cast<char*>(&epsilon_) -
    reinterpret_cast<char*>(&channels_) + sizeof(epsilon_));
}

bool BatchnormLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BatchnormLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 channels = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &channels_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool computeMeanVar = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(40u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &computemeanvar_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool instanceNormalization = 6;
      case 6: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(48u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &instancenormalization_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float epsilon = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(85u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &epsilon_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams gamma = 15;
      case 15: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(122u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_gamma()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams beta = 16;
      case 16: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(130u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_beta()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams mean = 17;
      case 17: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(138u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_mean()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams variance = 18;
      case 18: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(146u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_variance()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BatchnormLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BatchnormLayerParams)
  return false;
#undef DO_
}

void BatchnormLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BatchnormLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 channels = 1;
  if (this->channels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->channels(), output);
  }

  // bool computeMeanVar = 5;
  if (this->computemeanvar() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(5, this->computemeanvar(), output);
  }

  // bool instanceNormalization = 6;
  if (this->instancenormalization() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(6, this->instancenormalization(), output);
  }

  // float epsilon = 10;
  if (this->epsilon() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(10, this->epsilon(), output);
  }

  // .CoreML.Specification.WeightParams gamma = 15;
  if (this->has_gamma()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      15, *this->gamma_, output);
  }

  // .CoreML.Specification.WeightParams beta = 16;
  if (this->has_beta()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      16, *this->beta_, output);
  }

  // .CoreML.Specification.WeightParams mean = 17;
  if (this->has_mean()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      17, *this->mean_, output);
  }

  // .CoreML.Specification.WeightParams variance = 18;
  if (this->has_variance()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      18, *this->variance_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BatchnormLayerParams)
}

size_t BatchnormLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BatchnormLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams gamma = 15;
  if (this->has_gamma()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->gamma_);
  }

  // .CoreML.Specification.WeightParams beta = 16;
  if (this->has_beta()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->beta_);
  }

  // .CoreML.Specification.WeightParams mean = 17;
  if (this->has_mean()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->mean_);
  }

  // .CoreML.Specification.WeightParams variance = 18;
  if (this->has_variance()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->variance_);
  }

  // uint64 channels = 1;
  if (this->channels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->channels());
  }

  // bool computeMeanVar = 5;
  if (this->computemeanvar() != 0) {
    total_size += 1 + 1;
  }

  // bool instanceNormalization = 6;
  if (this->instancenormalization() != 0) {
    total_size += 1 + 1;
  }

  // float epsilon = 10;
  if (this->epsilon() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BatchnormLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BatchnormLayerParams*>(&from));
}

void BatchnormLayerParams::MergeFrom(const BatchnormLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BatchnormLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_gamma()) {
    mutable_gamma()->::CoreML::Specification::WeightParams::MergeFrom(from.gamma());
  }
  if (from.has_beta()) {
    mutable_beta()->::CoreML::Specification::WeightParams::MergeFrom(from.beta());
  }
  if (from.has_mean()) {
    mutable_mean()->::CoreML::Specification::WeightParams::MergeFrom(from.mean());
  }
  if (from.has_variance()) {
    mutable_variance()->::CoreML::Specification::WeightParams::MergeFrom(from.variance());
  }
  if (from.channels() != 0) {
    set_channels(from.channels());
  }
  if (from.computemeanvar() != 0) {
    set_computemeanvar(from.computemeanvar());
  }
  if (from.instancenormalization() != 0) {
    set_instancenormalization(from.instancenormalization());
  }
  if (from.epsilon() != 0) {
    set_epsilon(from.epsilon());
  }
}

void BatchnormLayerParams::CopyFrom(const BatchnormLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BatchnormLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BatchnormLayerParams::IsInitialized() const {
  return true;
}

void BatchnormLayerParams::Swap(BatchnormLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BatchnormLayerParams::InternalSwap(BatchnormLayerParams* other) {
  std::swap(gamma_, other->gamma_);
  std::swap(beta_, other->beta_);
  std::swap(mean_, other->mean_);
  std::swap(variance_, other->variance_);
  std::swap(channels_, other->channels_);
  std::swap(computemeanvar_, other->computemeanvar_);
  std::swap(instancenormalization_, other->instancenormalization_);
  std::swap(epsilon_, other->epsilon_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BatchnormLayerParams::GetTypeName() const {
  return "CoreML.Specification.BatchnormLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BatchnormLayerParams

// uint64 channels = 1;
void BatchnormLayerParams::clear_channels() {
  channels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BatchnormLayerParams::channels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.channels)
  return channels_;
}
void BatchnormLayerParams::set_channels(::google::protobuf::uint64 value) {
  
  channels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.channels)
}

// bool computeMeanVar = 5;
void BatchnormLayerParams::clear_computemeanvar() {
  computemeanvar_ = false;
}
bool BatchnormLayerParams::computemeanvar() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.computeMeanVar)
  return computemeanvar_;
}
void BatchnormLayerParams::set_computemeanvar(bool value) {
  
  computemeanvar_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.computeMeanVar)
}

// bool instanceNormalization = 6;
void BatchnormLayerParams::clear_instancenormalization() {
  instancenormalization_ = false;
}
bool BatchnormLayerParams::instancenormalization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.instanceNormalization)
  return instancenormalization_;
}
void BatchnormLayerParams::set_instancenormalization(bool value) {
  
  instancenormalization_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.instanceNormalization)
}

// float epsilon = 10;
void BatchnormLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
float BatchnormLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.epsilon)
  return epsilon_;
}
void BatchnormLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.epsilon)
}

// .CoreML.Specification.WeightParams gamma = 15;
bool BatchnormLayerParams::has_gamma() const {
  return this != internal_default_instance() && gamma_ != NULL;
}
void BatchnormLayerParams::clear_gamma() {
  if (GetArenaNoVirtual() == NULL && gamma_ != NULL) delete gamma_;
  gamma_ = NULL;
}
const ::CoreML::Specification::WeightParams& BatchnormLayerParams::gamma() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.gamma)
  return gamma_ != NULL ? *gamma_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_gamma() {
  
  if (gamma_ == NULL) {
    gamma_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.gamma)
  return gamma_;
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::release_gamma() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.gamma)
  
  ::CoreML::Specification::WeightParams* temp = gamma_;
  gamma_ = NULL;
  return temp;
}
void BatchnormLayerParams::set_allocated_gamma(::CoreML::Specification::WeightParams* gamma) {
  delete gamma_;
  gamma_ = gamma;
  if (gamma) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.gamma)
}

// .CoreML.Specification.WeightParams beta = 16;
bool BatchnormLayerParams::has_beta() const {
  return this != internal_default_instance() && beta_ != NULL;
}
void BatchnormLayerParams::clear_beta() {
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) delete beta_;
  beta_ = NULL;
}
const ::CoreML::Specification::WeightParams& BatchnormLayerParams::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.beta)
  return beta_ != NULL ? *beta_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_beta() {
  
  if (beta_ == NULL) {
    beta_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.beta)
  return beta_;
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::release_beta() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.beta)
  
  ::CoreML::Specification::WeightParams* temp = beta_;
  beta_ = NULL;
  return temp;
}
void BatchnormLayerParams::set_allocated_beta(::CoreML::Specification::WeightParams* beta) {
  delete beta_;
  beta_ = beta;
  if (beta) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.beta)
}

// .CoreML.Specification.WeightParams mean = 17;
bool BatchnormLayerParams::has_mean() const {
  return this != internal_default_instance() && mean_ != NULL;
}
void BatchnormLayerParams::clear_mean() {
  if (GetArenaNoVirtual() == NULL && mean_ != NULL) delete mean_;
  mean_ = NULL;
}
const ::CoreML::Specification::WeightParams& BatchnormLayerParams::mean() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.mean)
  return mean_ != NULL ? *mean_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_mean() {
  
  if (mean_ == NULL) {
    mean_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.mean)
  return mean_;
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::release_mean() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.mean)
  
  ::CoreML::Specification::WeightParams* temp = mean_;
  mean_ = NULL;
  return temp;
}
void BatchnormLayerParams::set_allocated_mean(::CoreML::Specification::WeightParams* mean) {
  delete mean_;
  mean_ = mean;
  if (mean) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.mean)
}

// .CoreML.Specification.WeightParams variance = 18;
bool BatchnormLayerParams::has_variance() const {
  return this != internal_default_instance() && variance_ != NULL;
}
void BatchnormLayerParams::clear_variance() {
  if (GetArenaNoVirtual() == NULL && variance_ != NULL) delete variance_;
  variance_ = NULL;
}
const ::CoreML::Specification::WeightParams& BatchnormLayerParams::variance() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.variance)
  return variance_ != NULL ? *variance_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_variance() {
  
  if (variance_ == NULL) {
    variance_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.variance)
  return variance_;
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::release_variance() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.variance)
  
  ::CoreML::Specification::WeightParams* temp = variance_;
  variance_ = NULL;
  return temp;
}
void BatchnormLayerParams::set_allocated_variance(::CoreML::Specification::WeightParams* variance) {
  delete variance_;
  variance_ = variance;
  if (variance) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.variance)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PoolingLayerParams_ValidCompletePadding::kPaddingAmountsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PoolingLayerParams_ValidCompletePadding::PoolingLayerParams_ValidCompletePadding()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
}
PoolingLayerParams_ValidCompletePadding::PoolingLayerParams_ValidCompletePadding(const PoolingLayerParams_ValidCompletePadding& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      paddingamounts_(from.paddingamounts_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
}

void PoolingLayerParams_ValidCompletePadding::SharedCtor() {
  _cached_size_ = 0;
}

PoolingLayerParams_ValidCompletePadding::~PoolingLayerParams_ValidCompletePadding() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  SharedDtor();
}

void PoolingLayerParams_ValidCompletePadding::SharedDtor() {
}

void PoolingLayerParams_ValidCompletePadding::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PoolingLayerParams_ValidCompletePadding& PoolingLayerParams_ValidCompletePadding::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PoolingLayerParams_ValidCompletePadding* PoolingLayerParams_ValidCompletePadding::New(::google::protobuf::Arena* arena) const {
  PoolingLayerParams_ValidCompletePadding* n = new PoolingLayerParams_ValidCompletePadding;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PoolingLayerParams_ValidCompletePadding::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  paddingamounts_.Clear();
}

bool PoolingLayerParams_ValidCompletePadding::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 paddingAmounts = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_paddingamounts())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(80u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 82u, input, this->mutable_paddingamounts())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  return false;
#undef DO_
}

void PoolingLayerParams_ValidCompletePadding::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 paddingAmounts = 10;
  if (this->paddingamounts_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(10, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_paddingamounts_cached_byte_size_);
  }
  for (int i = 0, n = this->paddingamounts_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->paddingamounts(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
}

size_t PoolingLayerParams_ValidCompletePadding::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  size_t total_size = 0;

  // repeated uint64 paddingAmounts = 10;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->paddingamounts_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _paddingamounts_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PoolingLayerParams_ValidCompletePadding::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PoolingLayerParams_ValidCompletePadding*>(&from));
}

void PoolingLayerParams_ValidCompletePadding::MergeFrom(const PoolingLayerParams_ValidCompletePadding& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  paddingamounts_.MergeFrom(from.paddingamounts_);
}

void PoolingLayerParams_ValidCompletePadding::CopyFrom(const PoolingLayerParams_ValidCompletePadding& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PoolingLayerParams_ValidCompletePadding::IsInitialized() const {
  return true;
}

void PoolingLayerParams_ValidCompletePadding::Swap(PoolingLayerParams_ValidCompletePadding* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PoolingLayerParams_ValidCompletePadding::InternalSwap(PoolingLayerParams_ValidCompletePadding* other) {
  paddingamounts_.InternalSwap(&other->paddingamounts_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PoolingLayerParams_ValidCompletePadding::GetTypeName() const {
  return "CoreML.Specification.PoolingLayerParams.ValidCompletePadding";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PoolingLayerParams_ValidCompletePadding

// repeated uint64 paddingAmounts = 10;
int PoolingLayerParams_ValidCompletePadding::paddingamounts_size() const {
  return paddingamounts_.size();
}
void PoolingLayerParams_ValidCompletePadding::clear_paddingamounts() {
  paddingamounts_.Clear();
}
::google::protobuf::uint64 PoolingLayerParams_ValidCompletePadding::paddingamounts(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
  return paddingamounts_.Get(index);
}
void PoolingLayerParams_ValidCompletePadding::set_paddingamounts(int index, ::google::protobuf::uint64 value) {
  paddingamounts_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
}
void PoolingLayerParams_ValidCompletePadding::add_paddingamounts(::google::protobuf::uint64 value) {
  paddingamounts_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PoolingLayerParams_ValidCompletePadding::paddingamounts() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
  return paddingamounts_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PoolingLayerParams_ValidCompletePadding::mutable_paddingamounts() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
  return &paddingamounts_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PoolingLayerParams::kTypeFieldNumber;
const int PoolingLayerParams::kKernelSizeFieldNumber;
const int PoolingLayerParams::kStrideFieldNumber;
const int PoolingLayerParams::kValidFieldNumber;
const int PoolingLayerParams::kSameFieldNumber;
const int PoolingLayerParams::kIncludeLastPixelFieldNumber;
const int PoolingLayerParams::kAvgPoolExcludePaddingFieldNumber;
const int PoolingLayerParams::kGlobalPoolingFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PoolingLayerParams::PoolingLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PoolingLayerParams)
}
PoolingLayerParams::PoolingLayerParams(const PoolingLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      kernelsize_(from.kernelsize_),
      stride_(from.stride_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&type_, &from.type_,
    reinterpret_cast<char*>(&globalpooling_) -
    reinterpret_cast<char*>(&type_) + sizeof(globalpooling_));
  clear_has_PoolingPaddingType();
  switch (from.PoolingPaddingType_case()) {
    case kValid: {
      mutable_valid()->::CoreML::Specification::ValidPadding::MergeFrom(from.valid());
      break;
    }
    case kSame: {
      mutable_same()->::CoreML::Specification::SamePadding::MergeFrom(from.same());
      break;
    }
    case kIncludeLastPixel: {
      mutable_includelastpixel()->::CoreML::Specification::PoolingLayerParams_ValidCompletePadding::MergeFrom(from.includelastpixel());
      break;
    }
    case POOLINGPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PoolingLayerParams)
}

void PoolingLayerParams::SharedCtor() {
  ::memset(&type_, 0, reinterpret_cast<char*>(&globalpooling_) -
    reinterpret_cast<char*>(&type_) + sizeof(globalpooling_));
  clear_has_PoolingPaddingType();
  _cached_size_ = 0;
}

PoolingLayerParams::~PoolingLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PoolingLayerParams)
  SharedDtor();
}

void PoolingLayerParams::SharedDtor() {
  if (has_PoolingPaddingType()) {
    clear_PoolingPaddingType();
  }
}

void PoolingLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PoolingLayerParams& PoolingLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PoolingLayerParams* PoolingLayerParams::New(::google::protobuf::Arena* arena) const {
  PoolingLayerParams* n = new PoolingLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PoolingLayerParams::clear_PoolingPaddingType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.PoolingLayerParams)
  switch (PoolingPaddingType_case()) {
    case kValid: {
      delete PoolingPaddingType_.valid_;
      break;
    }
    case kSame: {
      delete PoolingPaddingType_.same_;
      break;
    }
    case kIncludeLastPixel: {
      delete PoolingPaddingType_.includelastpixel_;
      break;
    }
    case POOLINGPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = POOLINGPADDINGTYPE_NOT_SET;
}


void PoolingLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PoolingLayerParams)
  kernelsize_.Clear();
  stride_.Clear();
  ::memset(&type_, 0, reinterpret_cast<char*>(&globalpooling_) -
    reinterpret_cast<char*>(&type_) + sizeof(globalpooling_));
  clear_PoolingPaddingType();
}

bool PoolingLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PoolingLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_type(static_cast< ::CoreML::Specification::PoolingLayerParams_PoolingType >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 kernelSize = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_kernelsize())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(80u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 82u, input, this->mutable_kernelsize())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 stride = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_stride())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(160u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 2, 162u, input, this->mutable_stride())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ValidPadding valid = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_valid()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SamePadding same = 31;
      case 31: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(250u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_same()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
      case 32: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(258u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_includelastpixel()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool avgPoolExcludePadding = 50;
      case 50: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(400u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &avgpoolexcludepadding_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool globalPooling = 60;
      case 60: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(480u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &globalpooling_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PoolingLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PoolingLayerParams)
  return false;
#undef DO_
}

void PoolingLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PoolingLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
  if (this->type() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->type(), output);
  }

  // repeated uint64 kernelSize = 10;
  if (this->kernelsize_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(10, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_kernelsize_cached_byte_size_);
  }
  for (int i = 0, n = this->kernelsize_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->kernelsize(i), output);
  }

  // repeated uint64 stride = 20;
  if (this->stride_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(20, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_stride_cached_byte_size_);
  }
  for (int i = 0, n = this->stride_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->stride(i), output);
  }

  // .CoreML.Specification.ValidPadding valid = 30;
  if (has_valid()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      30, *PoolingPaddingType_.valid_, output);
  }

  // .CoreML.Specification.SamePadding same = 31;
  if (has_same()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      31, *PoolingPaddingType_.same_, output);
  }

  // .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
  if (has_includelastpixel()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      32, *PoolingPaddingType_.includelastpixel_, output);
  }

  // bool avgPoolExcludePadding = 50;
  if (this->avgpoolexcludepadding() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(50, this->avgpoolexcludepadding(), output);
  }

  // bool globalPooling = 60;
  if (this->globalpooling() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(60, this->globalpooling(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PoolingLayerParams)
}

size_t PoolingLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PoolingLayerParams)
  size_t total_size = 0;

  // repeated uint64 kernelSize = 10;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->kernelsize_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _kernelsize_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated uint64 stride = 20;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->stride_);
    if (data_size > 0) {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _stride_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
  if (this->type() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->type());
  }

  // bool avgPoolExcludePadding = 50;
  if (this->avgpoolexcludepadding() != 0) {
    total_size += 2 + 1;
  }

  // bool globalPooling = 60;
  if (this->globalpooling() != 0) {
    total_size += 2 + 1;
  }

  switch (PoolingPaddingType_case()) {
    // .CoreML.Specification.ValidPadding valid = 30;
    case kValid: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PoolingPaddingType_.valid_);
      break;
    }
    // .CoreML.Specification.SamePadding same = 31;
    case kSame: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PoolingPaddingType_.same_);
      break;
    }
    // .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
    case kIncludeLastPixel: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PoolingPaddingType_.includelastpixel_);
      break;
    }
    case POOLINGPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PoolingLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PoolingLayerParams*>(&from));
}

void PoolingLayerParams::MergeFrom(const PoolingLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PoolingLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  kernelsize_.MergeFrom(from.kernelsize_);
  stride_.MergeFrom(from.stride_);
  if (from.type() != 0) {
    set_type(from.type());
  }
  if (from.avgpoolexcludepadding() != 0) {
    set_avgpoolexcludepadding(from.avgpoolexcludepadding());
  }
  if (from.globalpooling() != 0) {
    set_globalpooling(from.globalpooling());
  }
  switch (from.PoolingPaddingType_case()) {
    case kValid: {
      mutable_valid()->::CoreML::Specification::ValidPadding::MergeFrom(from.valid());
      break;
    }
    case kSame: {
      mutable_same()->::CoreML::Specification::SamePadding::MergeFrom(from.same());
      break;
    }
    case kIncludeLastPixel: {
      mutable_includelastpixel()->::CoreML::Specification::PoolingLayerParams_ValidCompletePadding::MergeFrom(from.includelastpixel());
      break;
    }
    case POOLINGPADDINGTYPE_NOT_SET: {
      break;
    }
  }
}

void PoolingLayerParams::CopyFrom(const PoolingLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PoolingLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PoolingLayerParams::IsInitialized() const {
  return true;
}

void PoolingLayerParams::Swap(PoolingLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PoolingLayerParams::InternalSwap(PoolingLayerParams* other) {
  kernelsize_.InternalSwap(&other->kernelsize_);
  stride_.InternalSwap(&other->stride_);
  std::swap(type_, other->type_);
  std::swap(avgpoolexcludepadding_, other->avgpoolexcludepadding_);
  std::swap(globalpooling_, other->globalpooling_);
  std::swap(PoolingPaddingType_, other->PoolingPaddingType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PoolingLayerParams::GetTypeName() const {
  return "CoreML.Specification.PoolingLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PoolingLayerParams

// .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
void PoolingLayerParams::clear_type() {
  type_ = 0;
}
::CoreML::Specification::PoolingLayerParams_PoolingType PoolingLayerParams::type() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.type)
  return static_cast< ::CoreML::Specification::PoolingLayerParams_PoolingType >(type_);
}
void PoolingLayerParams::set_type(::CoreML::Specification::PoolingLayerParams_PoolingType value) {
  
  type_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.type)
}

// repeated uint64 kernelSize = 10;
int PoolingLayerParams::kernelsize_size() const {
  return kernelsize_.size();
}
void PoolingLayerParams::clear_kernelsize() {
  kernelsize_.Clear();
}
::google::protobuf::uint64 PoolingLayerParams::kernelsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.kernelSize)
  return kernelsize_.Get(index);
}
void PoolingLayerParams::set_kernelsize(int index, ::google::protobuf::uint64 value) {
  kernelsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.kernelSize)
}
void PoolingLayerParams::add_kernelsize(::google::protobuf::uint64 value) {
  kernelsize_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PoolingLayerParams.kernelSize)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PoolingLayerParams::kernelsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PoolingLayerParams.kernelSize)
  return kernelsize_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PoolingLayerParams::mutable_kernelsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PoolingLayerParams.kernelSize)
  return &kernelsize_;
}

// repeated uint64 stride = 20;
int PoolingLayerParams::stride_size() const {
  return stride_.size();
}
void PoolingLayerParams::clear_stride() {
  stride_.Clear();
}
::google::protobuf::uint64 PoolingLayerParams::stride(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.stride)
  return stride_.Get(index);
}
void PoolingLayerParams::set_stride(int index, ::google::protobuf::uint64 value) {
  stride_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.stride)
}
void PoolingLayerParams::add_stride(::google::protobuf::uint64 value) {
  stride_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PoolingLayerParams.stride)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PoolingLayerParams::stride() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PoolingLayerParams.stride)
  return stride_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PoolingLayerParams::mutable_stride() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PoolingLayerParams.stride)
  return &stride_;
}

// .CoreML.Specification.ValidPadding valid = 30;
bool PoolingLayerParams::has_valid() const {
  return PoolingPaddingType_case() == kValid;
}
void PoolingLayerParams::set_has_valid() {
  _oneof_case_[0] = kValid;
}
void PoolingLayerParams::clear_valid() {
  if (has_valid()) {
    delete PoolingPaddingType_.valid_;
    clear_has_PoolingPaddingType();
  }
}
 const ::CoreML::Specification::ValidPadding& PoolingLayerParams::valid() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.valid)
  return has_valid()
      ? *PoolingPaddingType_.valid_
      : ::CoreML::Specification::ValidPadding::default_instance();
}
::CoreML::Specification::ValidPadding* PoolingLayerParams::mutable_valid() {
  if (!has_valid()) {
    clear_PoolingPaddingType();
    set_has_valid();
    PoolingPaddingType_.valid_ = new ::CoreML::Specification::ValidPadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PoolingLayerParams.valid)
  return PoolingPaddingType_.valid_;
}
::CoreML::Specification::ValidPadding* PoolingLayerParams::release_valid() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PoolingLayerParams.valid)
  if (has_valid()) {
    clear_has_PoolingPaddingType();
    ::CoreML::Specification::ValidPadding* temp = PoolingPaddingType_.valid_;
    PoolingPaddingType_.valid_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PoolingLayerParams::set_allocated_valid(::CoreML::Specification::ValidPadding* valid) {
  clear_PoolingPaddingType();
  if (valid) {
    set_has_valid();
    PoolingPaddingType_.valid_ = valid;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PoolingLayerParams.valid)
}

// .CoreML.Specification.SamePadding same = 31;
bool PoolingLayerParams::has_same() const {
  return PoolingPaddingType_case() == kSame;
}
void PoolingLayerParams::set_has_same() {
  _oneof_case_[0] = kSame;
}
void PoolingLayerParams::clear_same() {
  if (has_same()) {
    delete PoolingPaddingType_.same_;
    clear_has_PoolingPaddingType();
  }
}
 const ::CoreML::Specification::SamePadding& PoolingLayerParams::same() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.same)
  return has_same()
      ? *PoolingPaddingType_.same_
      : ::CoreML::Specification::SamePadding::default_instance();
}
::CoreML::Specification::SamePadding* PoolingLayerParams::mutable_same() {
  if (!has_same()) {
    clear_PoolingPaddingType();
    set_has_same();
    PoolingPaddingType_.same_ = new ::CoreML::Specification::SamePadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PoolingLayerParams.same)
  return PoolingPaddingType_.same_;
}
::CoreML::Specification::SamePadding* PoolingLayerParams::release_same() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PoolingLayerParams.same)
  if (has_same()) {
    clear_has_PoolingPaddingType();
    ::CoreML::Specification::SamePadding* temp = PoolingPaddingType_.same_;
    PoolingPaddingType_.same_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PoolingLayerParams::set_allocated_same(::CoreML::Specification::SamePadding* same) {
  clear_PoolingPaddingType();
  if (same) {
    set_has_same();
    PoolingPaddingType_.same_ = same;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PoolingLayerParams.same)
}

// .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
bool PoolingLayerParams::has_includelastpixel() const {
  return PoolingPaddingType_case() == kIncludeLastPixel;
}
void PoolingLayerParams::set_has_includelastpixel() {
  _oneof_case_[0] = kIncludeLastPixel;
}
void PoolingLayerParams::clear_includelastpixel() {
  if (has_includelastpixel()) {
    delete PoolingPaddingType_.includelastpixel_;
    clear_has_PoolingPaddingType();
  }
}
 const ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding& PoolingLayerParams::includelastpixel() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  return has_includelastpixel()
      ? *PoolingPaddingType_.includelastpixel_
      : ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding::default_instance();
}
::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* PoolingLayerParams::mutable_includelastpixel() {
  if (!has_includelastpixel()) {
    clear_PoolingPaddingType();
    set_has_includelastpixel();
    PoolingPaddingType_.includelastpixel_ = new ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  return PoolingPaddingType_.includelastpixel_;
}
::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* PoolingLayerParams::release_includelastpixel() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  if (has_includelastpixel()) {
    clear_has_PoolingPaddingType();
    ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* temp = PoolingPaddingType_.includelastpixel_;
    PoolingPaddingType_.includelastpixel_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PoolingLayerParams::set_allocated_includelastpixel(::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* includelastpixel) {
  clear_PoolingPaddingType();
  if (includelastpixel) {
    set_has_includelastpixel();
    PoolingPaddingType_.includelastpixel_ = includelastpixel;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PoolingLayerParams.includeLastPixel)
}

// bool avgPoolExcludePadding = 50;
void PoolingLayerParams::clear_avgpoolexcludepadding() {
  avgpoolexcludepadding_ = false;
}
bool PoolingLayerParams::avgpoolexcludepadding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.avgPoolExcludePadding)
  return avgpoolexcludepadding_;
}
void PoolingLayerParams::set_avgpoolexcludepadding(bool value) {
  
  avgpoolexcludepadding_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.avgPoolExcludePadding)
}

// bool globalPooling = 60;
void PoolingLayerParams::clear_globalpooling() {
  globalpooling_ = false;
}
bool PoolingLayerParams::globalpooling() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.globalPooling)
  return globalpooling_;
}
void PoolingLayerParams::set_globalpooling(bool value) {
  
  globalpooling_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.globalPooling)
}

bool PoolingLayerParams::has_PoolingPaddingType() const {
  return PoolingPaddingType_case() != POOLINGPADDINGTYPE_NOT_SET;
}
void PoolingLayerParams::clear_has_PoolingPaddingType() {
  _oneof_case_[0] = POOLINGPADDINGTYPE_NOT_SET;
}
PoolingLayerParams::PoolingPaddingTypeCase PoolingLayerParams::PoolingPaddingType_case() const {
  return PoolingLayerParams::PoolingPaddingTypeCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PaddingLayerParams_PaddingConstant::kValueFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PaddingLayerParams_PaddingConstant::PaddingLayerParams_PaddingConstant()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PaddingLayerParams.PaddingConstant)
}
PaddingLayerParams_PaddingConstant::PaddingLayerParams_PaddingConstant(const PaddingLayerParams_PaddingConstant& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  value_ = from.value_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PaddingLayerParams.PaddingConstant)
}

void PaddingLayerParams_PaddingConstant::SharedCtor() {
  value_ = 0;
  _cached_size_ = 0;
}

PaddingLayerParams_PaddingConstant::~PaddingLayerParams_PaddingConstant() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  SharedDtor();
}

void PaddingLayerParams_PaddingConstant::SharedDtor() {
}

void PaddingLayerParams_PaddingConstant::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PaddingLayerParams_PaddingConstant& PaddingLayerParams_PaddingConstant::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PaddingLayerParams_PaddingConstant* PaddingLayerParams_PaddingConstant::New(::google::protobuf::Arena* arena) const {
  PaddingLayerParams_PaddingConstant* n = new PaddingLayerParams_PaddingConstant;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PaddingLayerParams_PaddingConstant::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  value_ = 0;
}

bool PaddingLayerParams_PaddingConstant::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float value = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &value_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  return false;
#undef DO_
}

void PaddingLayerParams_PaddingConstant::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float value = 1;
  if (this->value() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->value(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PaddingLayerParams.PaddingConstant)
}

size_t PaddingLayerParams_PaddingConstant::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  size_t total_size = 0;

  // float value = 1;
  if (this->value() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PaddingLayerParams_PaddingConstant::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PaddingLayerParams_PaddingConstant*>(&from));
}

void PaddingLayerParams_PaddingConstant::MergeFrom(const PaddingLayerParams_PaddingConstant& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.value() != 0) {
    set_value(from.value());
  }
}

void PaddingLayerParams_PaddingConstant::CopyFrom(const PaddingLayerParams_PaddingConstant& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PaddingLayerParams_PaddingConstant::IsInitialized() const {
  return true;
}

void PaddingLayerParams_PaddingConstant::Swap(PaddingLayerParams_PaddingConstant* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PaddingLayerParams_PaddingConstant::InternalSwap(PaddingLayerParams_PaddingConstant* other) {
  std::swap(value_, other->value_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PaddingLayerParams_PaddingConstant::GetTypeName() const {
  return "CoreML.Specification.PaddingLayerParams.PaddingConstant";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PaddingLayerParams_PaddingConstant

// float value = 1;
void PaddingLayerParams_PaddingConstant::clear_value() {
  value_ = 0;
}
float PaddingLayerParams_PaddingConstant::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.PaddingConstant.value)
  return value_;
}
void PaddingLayerParams_PaddingConstant::set_value(float value) {
  
  value_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PaddingLayerParams.PaddingConstant.value)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PaddingLayerParams_PaddingReflection::PaddingLayerParams_PaddingReflection()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PaddingLayerParams.PaddingReflection)
}
PaddingLayerParams_PaddingReflection::PaddingLayerParams_PaddingReflection(const PaddingLayerParams_PaddingReflection& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PaddingLayerParams.PaddingReflection)
}

void PaddingLayerParams_PaddingReflection::SharedCtor() {
  _cached_size_ = 0;
}

PaddingLayerParams_PaddingReflection::~PaddingLayerParams_PaddingReflection() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  SharedDtor();
}

void PaddingLayerParams_PaddingReflection::SharedDtor() {
}

void PaddingLayerParams_PaddingReflection::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PaddingLayerParams_PaddingReflection& PaddingLayerParams_PaddingReflection::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PaddingLayerParams_PaddingReflection* PaddingLayerParams_PaddingReflection::New(::google::protobuf::Arena* arena) const {
  PaddingLayerParams_PaddingReflection* n = new PaddingLayerParams_PaddingReflection;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PaddingLayerParams_PaddingReflection::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
}

bool PaddingLayerParams_PaddingReflection::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  return false;
#undef DO_
}

void PaddingLayerParams_PaddingReflection::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PaddingLayerParams.PaddingReflection)
}

size_t PaddingLayerParams_PaddingReflection::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PaddingLayerParams_PaddingReflection::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PaddingLayerParams_PaddingReflection*>(&from));
}

void PaddingLayerParams_PaddingReflection::MergeFrom(const PaddingLayerParams_PaddingReflection& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void PaddingLayerParams_PaddingReflection::CopyFrom(const PaddingLayerParams_PaddingReflection& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PaddingLayerParams_PaddingReflection::IsInitialized() const {
  return true;
}

void PaddingLayerParams_PaddingReflection::Swap(PaddingLayerParams_PaddingReflection* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PaddingLayerParams_PaddingReflection::InternalSwap(PaddingLayerParams_PaddingReflection* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PaddingLayerParams_PaddingReflection::GetTypeName() const {
  return "CoreML.Specification.PaddingLayerParams.PaddingReflection";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PaddingLayerParams_PaddingReflection

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PaddingLayerParams_PaddingReplication::PaddingLayerParams_PaddingReplication()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PaddingLayerParams.PaddingReplication)
}
PaddingLayerParams_PaddingReplication::PaddingLayerParams_PaddingReplication(const PaddingLayerParams_PaddingReplication& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PaddingLayerParams.PaddingReplication)
}

void PaddingLayerParams_PaddingReplication::SharedCtor() {
  _cached_size_ = 0;
}

PaddingLayerParams_PaddingReplication::~PaddingLayerParams_PaddingReplication() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  SharedDtor();
}

void PaddingLayerParams_PaddingReplication::SharedDtor() {
}

void PaddingLayerParams_PaddingReplication::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PaddingLayerParams_PaddingReplication& PaddingLayerParams_PaddingReplication::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PaddingLayerParams_PaddingReplication* PaddingLayerParams_PaddingReplication::New(::google::protobuf::Arena* arena) const {
  PaddingLayerParams_PaddingReplication* n = new PaddingLayerParams_PaddingReplication;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PaddingLayerParams_PaddingReplication::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
}

bool PaddingLayerParams_PaddingReplication::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  return false;
#undef DO_
}

void PaddingLayerParams_PaddingReplication::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PaddingLayerParams.PaddingReplication)
}

size_t PaddingLayerParams_PaddingReplication::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PaddingLayerParams_PaddingReplication::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PaddingLayerParams_PaddingReplication*>(&from));
}

void PaddingLayerParams_PaddingReplication::MergeFrom(const PaddingLayerParams_PaddingReplication& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void PaddingLayerParams_PaddingReplication::CopyFrom(const PaddingLayerParams_PaddingReplication& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PaddingLayerParams_PaddingReplication::IsInitialized() const {
  return true;
}

void PaddingLayerParams_PaddingReplication::Swap(PaddingLayerParams_PaddingReplication* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PaddingLayerParams_PaddingReplication::InternalSwap(PaddingLayerParams_PaddingReplication* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PaddingLayerParams_PaddingReplication::GetTypeName() const {
  return "CoreML.Specification.PaddingLayerParams.PaddingReplication";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PaddingLayerParams_PaddingReplication

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PaddingLayerParams::kConstantFieldNumber;
const int PaddingLayerParams::kReflectionFieldNumber;
const int PaddingLayerParams::kReplicationFieldNumber;
const int PaddingLayerParams::kPaddingAmountsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PaddingLayerParams::PaddingLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PaddingLayerParams)
}
PaddingLayerParams::PaddingLayerParams(const PaddingLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_paddingamounts()) {
    paddingamounts_ = new ::CoreML::Specification::BorderAmounts(*from.paddingamounts_);
  } else {
    paddingamounts_ = NULL;
  }
  clear_has_PaddingType();
  switch (from.PaddingType_case()) {
    case kConstant: {
      mutable_constant()->::CoreML::Specification::PaddingLayerParams_PaddingConstant::MergeFrom(from.constant());
      break;
    }
    case kReflection: {
      mutable_reflection()->::CoreML::Specification::PaddingLayerParams_PaddingReflection::MergeFrom(from.reflection());
      break;
    }
    case kReplication: {
      mutable_replication()->::CoreML::Specification::PaddingLayerParams_PaddingReplication::MergeFrom(from.replication());
      break;
    }
    case PADDINGTYPE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PaddingLayerParams)
}

void PaddingLayerParams::SharedCtor() {
  paddingamounts_ = NULL;
  clear_has_PaddingType();
  _cached_size_ = 0;
}

PaddingLayerParams::~PaddingLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PaddingLayerParams)
  SharedDtor();
}

void PaddingLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete paddingamounts_;
  }
  if (has_PaddingType()) {
    clear_PaddingType();
  }
}

void PaddingLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PaddingLayerParams& PaddingLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PaddingLayerParams* PaddingLayerParams::New(::google::protobuf::Arena* arena) const {
  PaddingLayerParams* n = new PaddingLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PaddingLayerParams::clear_PaddingType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.PaddingLayerParams)
  switch (PaddingType_case()) {
    case kConstant: {
      delete PaddingType_.constant_;
      break;
    }
    case kReflection: {
      delete PaddingType_.reflection_;
      break;
    }
    case kReplication: {
      delete PaddingType_.replication_;
      break;
    }
    case PADDINGTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = PADDINGTYPE_NOT_SET;
}


void PaddingLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PaddingLayerParams)
  if (GetArenaNoVirtual() == NULL && paddingamounts_ != NULL) {
    delete paddingamounts_;
  }
  paddingamounts_ = NULL;
  clear_PaddingType();
}

bool PaddingLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PaddingLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_constant()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reflection()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_replication()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BorderAmounts paddingAmounts = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_paddingamounts()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PaddingLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PaddingLayerParams)
  return false;
#undef DO_
}

void PaddingLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PaddingLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
  if (has_constant()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *PaddingType_.constant_, output);
  }

  // .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
  if (has_reflection()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *PaddingType_.reflection_, output);
  }

  // .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
  if (has_replication()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      3, *PaddingType_.replication_, output);
  }

  // .CoreML.Specification.BorderAmounts paddingAmounts = 10;
  if (this->has_paddingamounts()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *this->paddingamounts_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PaddingLayerParams)
}

size_t PaddingLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PaddingLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.BorderAmounts paddingAmounts = 10;
  if (this->has_paddingamounts()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->paddingamounts_);
  }

  switch (PaddingType_case()) {
    // .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
    case kConstant: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PaddingType_.constant_);
      break;
    }
    // .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
    case kReflection: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PaddingType_.reflection_);
      break;
    }
    // .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
    case kReplication: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PaddingType_.replication_);
      break;
    }
    case PADDINGTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PaddingLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PaddingLayerParams*>(&from));
}

void PaddingLayerParams::MergeFrom(const PaddingLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PaddingLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_paddingamounts()) {
    mutable_paddingamounts()->::CoreML::Specification::BorderAmounts::MergeFrom(from.paddingamounts());
  }
  switch (from.PaddingType_case()) {
    case kConstant: {
      mutable_constant()->::CoreML::Specification::PaddingLayerParams_PaddingConstant::MergeFrom(from.constant());
      break;
    }
    case kReflection: {
      mutable_reflection()->::CoreML::Specification::PaddingLayerParams_PaddingReflection::MergeFrom(from.reflection());
      break;
    }
    case kReplication: {
      mutable_replication()->::CoreML::Specification::PaddingLayerParams_PaddingReplication::MergeFrom(from.replication());
      break;
    }
    case PADDINGTYPE_NOT_SET: {
      break;
    }
  }
}

void PaddingLayerParams::CopyFrom(const PaddingLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PaddingLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PaddingLayerParams::IsInitialized() const {
  return true;
}

void PaddingLayerParams::Swap(PaddingLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PaddingLayerParams::InternalSwap(PaddingLayerParams* other) {
  std::swap(paddingamounts_, other->paddingamounts_);
  std::swap(PaddingType_, other->PaddingType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PaddingLayerParams::GetTypeName() const {
  return "CoreML.Specification.PaddingLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PaddingLayerParams

// .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
bool PaddingLayerParams::has_constant() const {
  return PaddingType_case() == kConstant;
}
void PaddingLayerParams::set_has_constant() {
  _oneof_case_[0] = kConstant;
}
void PaddingLayerParams::clear_constant() {
  if (has_constant()) {
    delete PaddingType_.constant_;
    clear_has_PaddingType();
  }
}
 const ::CoreML::Specification::PaddingLayerParams_PaddingConstant& PaddingLayerParams::constant() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.constant)
  return has_constant()
      ? *PaddingType_.constant_
      : ::CoreML::Specification::PaddingLayerParams_PaddingConstant::default_instance();
}
::CoreML::Specification::PaddingLayerParams_PaddingConstant* PaddingLayerParams::mutable_constant() {
  if (!has_constant()) {
    clear_PaddingType();
    set_has_constant();
    PaddingType_.constant_ = new ::CoreML::Specification::PaddingLayerParams_PaddingConstant;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.constant)
  return PaddingType_.constant_;
}
::CoreML::Specification::PaddingLayerParams_PaddingConstant* PaddingLayerParams::release_constant() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.constant)
  if (has_constant()) {
    clear_has_PaddingType();
    ::CoreML::Specification::PaddingLayerParams_PaddingConstant* temp = PaddingType_.constant_;
    PaddingType_.constant_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PaddingLayerParams::set_allocated_constant(::CoreML::Specification::PaddingLayerParams_PaddingConstant* constant) {
  clear_PaddingType();
  if (constant) {
    set_has_constant();
    PaddingType_.constant_ = constant;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.constant)
}

// .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
bool PaddingLayerParams::has_reflection() const {
  return PaddingType_case() == kReflection;
}
void PaddingLayerParams::set_has_reflection() {
  _oneof_case_[0] = kReflection;
}
void PaddingLayerParams::clear_reflection() {
  if (has_reflection()) {
    delete PaddingType_.reflection_;
    clear_has_PaddingType();
  }
}
 const ::CoreML::Specification::PaddingLayerParams_PaddingReflection& PaddingLayerParams::reflection() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.reflection)
  return has_reflection()
      ? *PaddingType_.reflection_
      : ::CoreML::Specification::PaddingLayerParams_PaddingReflection::default_instance();
}
::CoreML::Specification::PaddingLayerParams_PaddingReflection* PaddingLayerParams::mutable_reflection() {
  if (!has_reflection()) {
    clear_PaddingType();
    set_has_reflection();
    PaddingType_.reflection_ = new ::CoreML::Specification::PaddingLayerParams_PaddingReflection;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.reflection)
  return PaddingType_.reflection_;
}
::CoreML::Specification::PaddingLayerParams_PaddingReflection* PaddingLayerParams::release_reflection() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.reflection)
  if (has_reflection()) {
    clear_has_PaddingType();
    ::CoreML::Specification::PaddingLayerParams_PaddingReflection* temp = PaddingType_.reflection_;
    PaddingType_.reflection_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PaddingLayerParams::set_allocated_reflection(::CoreML::Specification::PaddingLayerParams_PaddingReflection* reflection) {
  clear_PaddingType();
  if (reflection) {
    set_has_reflection();
    PaddingType_.reflection_ = reflection;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.reflection)
}

// .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
bool PaddingLayerParams::has_replication() const {
  return PaddingType_case() == kReplication;
}
void PaddingLayerParams::set_has_replication() {
  _oneof_case_[0] = kReplication;
}
void PaddingLayerParams::clear_replication() {
  if (has_replication()) {
    delete PaddingType_.replication_;
    clear_has_PaddingType();
  }
}
 const ::CoreML::Specification::PaddingLayerParams_PaddingReplication& PaddingLayerParams::replication() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.replication)
  return has_replication()
      ? *PaddingType_.replication_
      : ::CoreML::Specification::PaddingLayerParams_PaddingReplication::default_instance();
}
::CoreML::Specification::PaddingLayerParams_PaddingReplication* PaddingLayerParams::mutable_replication() {
  if (!has_replication()) {
    clear_PaddingType();
    set_has_replication();
    PaddingType_.replication_ = new ::CoreML::Specification::PaddingLayerParams_PaddingReplication;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.replication)
  return PaddingType_.replication_;
}
::CoreML::Specification::PaddingLayerParams_PaddingReplication* PaddingLayerParams::release_replication() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.replication)
  if (has_replication()) {
    clear_has_PaddingType();
    ::CoreML::Specification::PaddingLayerParams_PaddingReplication* temp = PaddingType_.replication_;
    PaddingType_.replication_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PaddingLayerParams::set_allocated_replication(::CoreML::Specification::PaddingLayerParams_PaddingReplication* replication) {
  clear_PaddingType();
  if (replication) {
    set_has_replication();
    PaddingType_.replication_ = replication;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.replication)
}

// .CoreML.Specification.BorderAmounts paddingAmounts = 10;
bool PaddingLayerParams::has_paddingamounts() const {
  return this != internal_default_instance() && paddingamounts_ != NULL;
}
void PaddingLayerParams::clear_paddingamounts() {
  if (GetArenaNoVirtual() == NULL && paddingamounts_ != NULL) delete paddingamounts_;
  paddingamounts_ = NULL;
}
const ::CoreML::Specification::BorderAmounts& PaddingLayerParams::paddingamounts() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.paddingAmounts)
  return paddingamounts_ != NULL ? *paddingamounts_
                         : *::CoreML::Specification::BorderAmounts::internal_default_instance();
}
::CoreML::Specification::BorderAmounts* PaddingLayerParams::mutable_paddingamounts() {
  
  if (paddingamounts_ == NULL) {
    paddingamounts_ = new ::CoreML::Specification::BorderAmounts;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.paddingAmounts)
  return paddingamounts_;
}
::CoreML::Specification::BorderAmounts* PaddingLayerParams::release_paddingamounts() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.paddingAmounts)
  
  ::CoreML::Specification::BorderAmounts* temp = paddingamounts_;
  paddingamounts_ = NULL;
  return temp;
}
void PaddingLayerParams::set_allocated_paddingamounts(::CoreML::Specification::BorderAmounts* paddingamounts) {
  delete paddingamounts_;
  paddingamounts_ = paddingamounts;
  if (paddingamounts) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.paddingAmounts)
}

bool PaddingLayerParams::has_PaddingType() const {
  return PaddingType_case() != PADDINGTYPE_NOT_SET;
}
void PaddingLayerParams::clear_has_PaddingType() {
  _oneof_case_[0] = PADDINGTYPE_NOT_SET;
}
PaddingLayerParams::PaddingTypeCase PaddingLayerParams::PaddingType_case() const {
  return PaddingLayerParams::PaddingTypeCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ConcatLayerParams::kSequenceConcatFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ConcatLayerParams::ConcatLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ConcatLayerParams)
}
ConcatLayerParams::ConcatLayerParams(const ConcatLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  sequenceconcat_ = from.sequenceconcat_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ConcatLayerParams)
}

void ConcatLayerParams::SharedCtor() {
  sequenceconcat_ = false;
  _cached_size_ = 0;
}

ConcatLayerParams::~ConcatLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ConcatLayerParams)
  SharedDtor();
}

void ConcatLayerParams::SharedDtor() {
}

void ConcatLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ConcatLayerParams& ConcatLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ConcatLayerParams* ConcatLayerParams::New(::google::protobuf::Arena* arena) const {
  ConcatLayerParams* n = new ConcatLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ConcatLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ConcatLayerParams)
  sequenceconcat_ = false;
}

bool ConcatLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ConcatLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // bool sequenceConcat = 100;
      case 100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(800u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &sequenceconcat_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ConcatLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ConcatLayerParams)
  return false;
#undef DO_
}

void ConcatLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ConcatLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // bool sequenceConcat = 100;
  if (this->sequenceconcat() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(100, this->sequenceconcat(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ConcatLayerParams)
}

size_t ConcatLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ConcatLayerParams)
  size_t total_size = 0;

  // bool sequenceConcat = 100;
  if (this->sequenceconcat() != 0) {
    total_size += 2 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ConcatLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ConcatLayerParams*>(&from));
}

void ConcatLayerParams::MergeFrom(const ConcatLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ConcatLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.sequenceconcat() != 0) {
    set_sequenceconcat(from.sequenceconcat());
  }
}

void ConcatLayerParams::CopyFrom(const ConcatLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ConcatLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ConcatLayerParams::IsInitialized() const {
  return true;
}

void ConcatLayerParams::Swap(ConcatLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ConcatLayerParams::InternalSwap(ConcatLayerParams* other) {
  std::swap(sequenceconcat_, other->sequenceconcat_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ConcatLayerParams::GetTypeName() const {
  return "CoreML.Specification.ConcatLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ConcatLayerParams

// bool sequenceConcat = 100;
void ConcatLayerParams::clear_sequenceconcat() {
  sequenceconcat_ = false;
}
bool ConcatLayerParams::sequenceconcat() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConcatLayerParams.sequenceConcat)
  return sequenceconcat_;
}
void ConcatLayerParams::set_sequenceconcat(bool value) {
  
  sequenceconcat_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConcatLayerParams.sequenceConcat)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LRNLayerParams::kAlphaFieldNumber;
const int LRNLayerParams::kBetaFieldNumber;
const int LRNLayerParams::kLocalSizeFieldNumber;
const int LRNLayerParams::kKFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LRNLayerParams::LRNLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LRNLayerParams)
}
LRNLayerParams::LRNLayerParams(const LRNLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&alpha_, &from.alpha_,
    reinterpret_cast<char*>(&k_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(k_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LRNLayerParams)
}

void LRNLayerParams::SharedCtor() {
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&k_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(k_));
  _cached_size_ = 0;
}

LRNLayerParams::~LRNLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LRNLayerParams)
  SharedDtor();
}

void LRNLayerParams::SharedDtor() {
}

void LRNLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LRNLayerParams& LRNLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LRNLayerParams* LRNLayerParams::New(::google::protobuf::Arena* arena) const {
  LRNLayerParams* n = new LRNLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LRNLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LRNLayerParams)
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&k_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(k_));
}

bool LRNLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LRNLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float beta = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &beta_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 localSize = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &localsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float k = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(37u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &k_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LRNLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LRNLayerParams)
  return false;
#undef DO_
}

void LRNLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LRNLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // float beta = 2;
  if (this->beta() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->beta(), output);
  }

  // uint64 localSize = 3;
  if (this->localsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(3, this->localsize(), output);
  }

  // float k = 4;
  if (this->k() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(4, this->k(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LRNLayerParams)
}

size_t LRNLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LRNLayerParams)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  // float beta = 2;
  if (this->beta() != 0) {
    total_size += 1 + 4;
  }

  // uint64 localSize = 3;
  if (this->localsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->localsize());
  }

  // float k = 4;
  if (this->k() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LRNLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LRNLayerParams*>(&from));
}

void LRNLayerParams::MergeFrom(const LRNLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LRNLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
  if (from.beta() != 0) {
    set_beta(from.beta());
  }
  if (from.localsize() != 0) {
    set_localsize(from.localsize());
  }
  if (from.k() != 0) {
    set_k(from.k());
  }
}

void LRNLayerParams::CopyFrom(const LRNLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LRNLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LRNLayerParams::IsInitialized() const {
  return true;
}

void LRNLayerParams::Swap(LRNLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LRNLayerParams::InternalSwap(LRNLayerParams* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(beta_, other->beta_);
  std::swap(localsize_, other->localsize_);
  std::swap(k_, other->k_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LRNLayerParams::GetTypeName() const {
  return "CoreML.Specification.LRNLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LRNLayerParams

// float alpha = 1;
void LRNLayerParams::clear_alpha() {
  alpha_ = 0;
}
float LRNLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.alpha)
  return alpha_;
}
void LRNLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.alpha)
}

// float beta = 2;
void LRNLayerParams::clear_beta() {
  beta_ = 0;
}
float LRNLayerParams::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.beta)
  return beta_;
}
void LRNLayerParams::set_beta(float value) {
  
  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.beta)
}

// uint64 localSize = 3;
void LRNLayerParams::clear_localsize() {
  localsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 LRNLayerParams::localsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.localSize)
  return localsize_;
}
void LRNLayerParams::set_localsize(::google::protobuf::uint64 value) {
  
  localsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.localSize)
}

// float k = 4;
void LRNLayerParams::clear_k() {
  k_ = 0;
}
float LRNLayerParams::k() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.k)
  return k_;
}
void LRNLayerParams::set_k(float value) {
  
  k_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.k)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SoftmaxLayerParams::SoftmaxLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SoftmaxLayerParams)
}
SoftmaxLayerParams::SoftmaxLayerParams(const SoftmaxLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SoftmaxLayerParams)
}

void SoftmaxLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

SoftmaxLayerParams::~SoftmaxLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SoftmaxLayerParams)
  SharedDtor();
}

void SoftmaxLayerParams::SharedDtor() {
}

void SoftmaxLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SoftmaxLayerParams& SoftmaxLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SoftmaxLayerParams* SoftmaxLayerParams::New(::google::protobuf::Arena* arena) const {
  SoftmaxLayerParams* n = new SoftmaxLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SoftmaxLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SoftmaxLayerParams)
}

bool SoftmaxLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SoftmaxLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SoftmaxLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SoftmaxLayerParams)
  return false;
#undef DO_
}

void SoftmaxLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SoftmaxLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SoftmaxLayerParams)
}

size_t SoftmaxLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SoftmaxLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SoftmaxLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SoftmaxLayerParams*>(&from));
}

void SoftmaxLayerParams::MergeFrom(const SoftmaxLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SoftmaxLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void SoftmaxLayerParams::CopyFrom(const SoftmaxLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SoftmaxLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SoftmaxLayerParams::IsInitialized() const {
  return true;
}

void SoftmaxLayerParams::Swap(SoftmaxLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SoftmaxLayerParams::InternalSwap(SoftmaxLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SoftmaxLayerParams::GetTypeName() const {
  return "CoreML.Specification.SoftmaxLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SoftmaxLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SplitLayerParams::kNOutputsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SplitLayerParams::SplitLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SplitLayerParams)
}
SplitLayerParams::SplitLayerParams(const SplitLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  noutputs_ = from.noutputs_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SplitLayerParams)
}

void SplitLayerParams::SharedCtor() {
  noutputs_ = GOOGLE_ULONGLONG(0);
  _cached_size_ = 0;
}

SplitLayerParams::~SplitLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SplitLayerParams)
  SharedDtor();
}

void SplitLayerParams::SharedDtor() {
}

void SplitLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SplitLayerParams& SplitLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SplitLayerParams* SplitLayerParams::New(::google::protobuf::Arena* arena) const {
  SplitLayerParams* n = new SplitLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SplitLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SplitLayerParams)
  noutputs_ = GOOGLE_ULONGLONG(0);
}

bool SplitLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SplitLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 nOutputs = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &noutputs_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SplitLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SplitLayerParams)
  return false;
#undef DO_
}

void SplitLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SplitLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 nOutputs = 1;
  if (this->noutputs() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->noutputs(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SplitLayerParams)
}

size_t SplitLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SplitLayerParams)
  size_t total_size = 0;

  // uint64 nOutputs = 1;
  if (this->noutputs() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->noutputs());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SplitLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SplitLayerParams*>(&from));
}

void SplitLayerParams::MergeFrom(const SplitLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SplitLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.noutputs() != 0) {
    set_noutputs(from.noutputs());
  }
}

void SplitLayerParams::CopyFrom(const SplitLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SplitLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SplitLayerParams::IsInitialized() const {
  return true;
}

void SplitLayerParams::Swap(SplitLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SplitLayerParams::InternalSwap(SplitLayerParams* other) {
  std::swap(noutputs_, other->noutputs_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SplitLayerParams::GetTypeName() const {
  return "CoreML.Specification.SplitLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SplitLayerParams

// uint64 nOutputs = 1;
void SplitLayerParams::clear_noutputs() {
  noutputs_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SplitLayerParams::noutputs() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SplitLayerParams.nOutputs)
  return noutputs_;
}
void SplitLayerParams::set_noutputs(::google::protobuf::uint64 value) {
  
  noutputs_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SplitLayerParams.nOutputs)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int AddLayerParams::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

AddLayerParams::AddLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.AddLayerParams)
}
AddLayerParams::AddLayerParams(const AddLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.AddLayerParams)
}

void AddLayerParams::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

AddLayerParams::~AddLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.AddLayerParams)
  SharedDtor();
}

void AddLayerParams::SharedDtor() {
}

void AddLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const AddLayerParams& AddLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

AddLayerParams* AddLayerParams::New(::google::protobuf::Arena* arena) const {
  AddLayerParams* n = new AddLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void AddLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.AddLayerParams)
  alpha_ = 0;
}

bool AddLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.AddLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.AddLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.AddLayerParams)
  return false;
#undef DO_
}

void AddLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.AddLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.AddLayerParams)
}

size_t AddLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.AddLayerParams)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void AddLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const AddLayerParams*>(&from));
}

void AddLayerParams::MergeFrom(const AddLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.AddLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void AddLayerParams::CopyFrom(const AddLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.AddLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool AddLayerParams::IsInitialized() const {
  return true;
}

void AddLayerParams::Swap(AddLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void AddLayerParams::InternalSwap(AddLayerParams* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string AddLayerParams::GetTypeName() const {
  return "CoreML.Specification.AddLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// AddLayerParams

// float alpha = 1;
void AddLayerParams::clear_alpha() {
  alpha_ = 0;
}
float AddLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AddLayerParams.alpha)
  return alpha_;
}
void AddLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.AddLayerParams.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int MultiplyLayerParams::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MultiplyLayerParams::MultiplyLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MultiplyLayerParams)
}
MultiplyLayerParams::MultiplyLayerParams(const MultiplyLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MultiplyLayerParams)
}

void MultiplyLayerParams::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

MultiplyLayerParams::~MultiplyLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MultiplyLayerParams)
  SharedDtor();
}

void MultiplyLayerParams::SharedDtor() {
}

void MultiplyLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MultiplyLayerParams& MultiplyLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

MultiplyLayerParams* MultiplyLayerParams::New(::google::protobuf::Arena* arena) const {
  MultiplyLayerParams* n = new MultiplyLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MultiplyLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MultiplyLayerParams)
  alpha_ = 0;
}

bool MultiplyLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MultiplyLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MultiplyLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MultiplyLayerParams)
  return false;
#undef DO_
}

void MultiplyLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MultiplyLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MultiplyLayerParams)
}

size_t MultiplyLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MultiplyLayerParams)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MultiplyLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MultiplyLayerParams*>(&from));
}

void MultiplyLayerParams::MergeFrom(const MultiplyLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MultiplyLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void MultiplyLayerParams::CopyFrom(const MultiplyLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MultiplyLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MultiplyLayerParams::IsInitialized() const {
  return true;
}

void MultiplyLayerParams::Swap(MultiplyLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MultiplyLayerParams::InternalSwap(MultiplyLayerParams* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MultiplyLayerParams::GetTypeName() const {
  return "CoreML.Specification.MultiplyLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MultiplyLayerParams

// float alpha = 1;
void MultiplyLayerParams::clear_alpha() {
  alpha_ = 0;
}
float MultiplyLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MultiplyLayerParams.alpha)
  return alpha_;
}
void MultiplyLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MultiplyLayerParams.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int UnaryFunctionLayerParams::kTypeFieldNumber;
const int UnaryFunctionLayerParams::kAlphaFieldNumber;
const int UnaryFunctionLayerParams::kEpsilonFieldNumber;
const int UnaryFunctionLayerParams::kShiftFieldNumber;
const int UnaryFunctionLayerParams::kScaleFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

UnaryFunctionLayerParams::UnaryFunctionLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.UnaryFunctionLayerParams)
}
UnaryFunctionLayerParams::UnaryFunctionLayerParams(const UnaryFunctionLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&type_, &from.type_,
    reinterpret_cast<char*>(&scale_) -
    reinterpret_cast<char*>(&type_) + sizeof(scale_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.UnaryFunctionLayerParams)
}

void UnaryFunctionLayerParams::SharedCtor() {
  ::memset(&type_, 0, reinterpret_cast<char*>(&scale_) -
    reinterpret_cast<char*>(&type_) + sizeof(scale_));
  _cached_size_ = 0;
}

UnaryFunctionLayerParams::~UnaryFunctionLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.UnaryFunctionLayerParams)
  SharedDtor();
}

void UnaryFunctionLayerParams::SharedDtor() {
}

void UnaryFunctionLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const UnaryFunctionLayerParams& UnaryFunctionLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

UnaryFunctionLayerParams* UnaryFunctionLayerParams::New(::google::protobuf::Arena* arena) const {
  UnaryFunctionLayerParams* n = new UnaryFunctionLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void UnaryFunctionLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.UnaryFunctionLayerParams)
  ::memset(&type_, 0, reinterpret_cast<char*>(&scale_) -
    reinterpret_cast<char*>(&type_) + sizeof(scale_));
}

bool UnaryFunctionLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.UnaryFunctionLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_type(static_cast< ::CoreML::Specification::UnaryFunctionLayerParams_Operation >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float alpha = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float epsilon = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(29u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &epsilon_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float shift = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(37u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &shift_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float scale = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(45u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &scale_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.UnaryFunctionLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.UnaryFunctionLayerParams)
  return false;
#undef DO_
}

void UnaryFunctionLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.UnaryFunctionLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
  if (this->type() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->type(), output);
  }

  // float alpha = 2;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->alpha(), output);
  }

  // float epsilon = 3;
  if (this->epsilon() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(3, this->epsilon(), output);
  }

  // float shift = 4;
  if (this->shift() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(4, this->shift(), output);
  }

  // float scale = 5;
  if (this->scale() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(5, this->scale(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.UnaryFunctionLayerParams)
}

size_t UnaryFunctionLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.UnaryFunctionLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
  if (this->type() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->type());
  }

  // float alpha = 2;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  // float epsilon = 3;
  if (this->epsilon() != 0) {
    total_size += 1 + 4;
  }

  // float shift = 4;
  if (this->shift() != 0) {
    total_size += 1 + 4;
  }

  // float scale = 5;
  if (this->scale() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void UnaryFunctionLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const UnaryFunctionLayerParams*>(&from));
}

void UnaryFunctionLayerParams::MergeFrom(const UnaryFunctionLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.UnaryFunctionLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.type() != 0) {
    set_type(from.type());
  }
  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
  if (from.epsilon() != 0) {
    set_epsilon(from.epsilon());
  }
  if (from.shift() != 0) {
    set_shift(from.shift());
  }
  if (from.scale() != 0) {
    set_scale(from.scale());
  }
}

void UnaryFunctionLayerParams::CopyFrom(const UnaryFunctionLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.UnaryFunctionLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool UnaryFunctionLayerParams::IsInitialized() const {
  return true;
}

void UnaryFunctionLayerParams::Swap(UnaryFunctionLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void UnaryFunctionLayerParams::InternalSwap(UnaryFunctionLayerParams* other) {
  std::swap(type_, other->type_);
  std::swap(alpha_, other->alpha_);
  std::swap(epsilon_, other->epsilon_);
  std::swap(shift_, other->shift_);
  std::swap(scale_, other->scale_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string UnaryFunctionLayerParams::GetTypeName() const {
  return "CoreML.Specification.UnaryFunctionLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// UnaryFunctionLayerParams

// .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
void UnaryFunctionLayerParams::clear_type() {
  type_ = 0;
}
::CoreML::Specification::UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::type() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.type)
  return static_cast< ::CoreML::Specification::UnaryFunctionLayerParams_Operation >(type_);
}
void UnaryFunctionLayerParams::set_type(::CoreML::Specification::UnaryFunctionLayerParams_Operation value) {
  
  type_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.type)
}

// float alpha = 2;
void UnaryFunctionLayerParams::clear_alpha() {
  alpha_ = 0;
}
float UnaryFunctionLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.alpha)
  return alpha_;
}
void UnaryFunctionLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.alpha)
}

// float epsilon = 3;
void UnaryFunctionLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
float UnaryFunctionLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.epsilon)
  return epsilon_;
}
void UnaryFunctionLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.epsilon)
}

// float shift = 4;
void UnaryFunctionLayerParams::clear_shift() {
  shift_ = 0;
}
float UnaryFunctionLayerParams::shift() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.shift)
  return shift_;
}
void UnaryFunctionLayerParams::set_shift(float value) {
  
  shift_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.shift)
}

// float scale = 5;
void UnaryFunctionLayerParams::clear_scale() {
  scale_ = 0;
}
float UnaryFunctionLayerParams::scale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.scale)
  return scale_;
}
void UnaryFunctionLayerParams::set_scale(float value) {
  
  scale_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.scale)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int UpsampleLayerParams::kScalingFactorFieldNumber;
const int UpsampleLayerParams::kModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

UpsampleLayerParams::UpsampleLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.UpsampleLayerParams)
}
UpsampleLayerParams::UpsampleLayerParams(const UpsampleLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      scalingfactor_(from.scalingfactor_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  mode_ = from.mode_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.UpsampleLayerParams)
}

void UpsampleLayerParams::SharedCtor() {
  mode_ = 0;
  _cached_size_ = 0;
}

UpsampleLayerParams::~UpsampleLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.UpsampleLayerParams)
  SharedDtor();
}

void UpsampleLayerParams::SharedDtor() {
}

void UpsampleLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const UpsampleLayerParams& UpsampleLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

UpsampleLayerParams* UpsampleLayerParams::New(::google::protobuf::Arena* arena) const {
  UpsampleLayerParams* n = new UpsampleLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void UpsampleLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.UpsampleLayerParams)
  scalingfactor_.Clear();
  mode_ = 0;
}

bool UpsampleLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.UpsampleLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 scalingFactor = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_scalingfactor())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_scalingfactor())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.UpsampleLayerParams.InterpolationMode mode = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(40u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::UpsampleLayerParams_InterpolationMode >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.UpsampleLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.UpsampleLayerParams)
  return false;
#undef DO_
}

void UpsampleLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.UpsampleLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 scalingFactor = 1;
  if (this->scalingfactor_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_scalingfactor_cached_byte_size_);
  }
  for (int i = 0, n = this->scalingfactor_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->scalingfactor(i), output);
  }

  // .CoreML.Specification.UpsampleLayerParams.InterpolationMode mode = 5;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      5, this->mode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.UpsampleLayerParams)
}

size_t UpsampleLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.UpsampleLayerParams)
  size_t total_size = 0;

  // repeated uint64 scalingFactor = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->scalingfactor_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _scalingfactor_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.UpsampleLayerParams.InterpolationMode mode = 5;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void UpsampleLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const UpsampleLayerParams*>(&from));
}

void UpsampleLayerParams::MergeFrom(const UpsampleLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.UpsampleLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  scalingfactor_.MergeFrom(from.scalingfactor_);
  if (from.mode() != 0) {
    set_mode(from.mode());
  }
}

void UpsampleLayerParams::CopyFrom(const UpsampleLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.UpsampleLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool UpsampleLayerParams::IsInitialized() const {
  return true;
}

void UpsampleLayerParams::Swap(UpsampleLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void UpsampleLayerParams::InternalSwap(UpsampleLayerParams* other) {
  scalingfactor_.InternalSwap(&other->scalingfactor_);
  std::swap(mode_, other->mode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string UpsampleLayerParams::GetTypeName() const {
  return "CoreML.Specification.UpsampleLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// UpsampleLayerParams

// repeated uint64 scalingFactor = 1;
int UpsampleLayerParams::scalingfactor_size() const {
  return scalingfactor_.size();
}
void UpsampleLayerParams::clear_scalingfactor() {
  scalingfactor_.Clear();
}
::google::protobuf::uint64 UpsampleLayerParams::scalingfactor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UpsampleLayerParams.scalingFactor)
  return scalingfactor_.Get(index);
}
void UpsampleLayerParams::set_scalingfactor(int index, ::google::protobuf::uint64 value) {
  scalingfactor_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.UpsampleLayerParams.scalingFactor)
}
void UpsampleLayerParams::add_scalingfactor(::google::protobuf::uint64 value) {
  scalingfactor_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.UpsampleLayerParams.scalingFactor)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
UpsampleLayerParams::scalingfactor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.UpsampleLayerParams.scalingFactor)
  return scalingfactor_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
UpsampleLayerParams::mutable_scalingfactor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.UpsampleLayerParams.scalingFactor)
  return &scalingfactor_;
}

// .CoreML.Specification.UpsampleLayerParams.InterpolationMode mode = 5;
void UpsampleLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::UpsampleLayerParams_InterpolationMode UpsampleLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UpsampleLayerParams.mode)
  return static_cast< ::CoreML::Specification::UpsampleLayerParams_InterpolationMode >(mode_);
}
void UpsampleLayerParams::set_mode(::CoreML::Specification::UpsampleLayerParams_InterpolationMode value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UpsampleLayerParams.mode)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BiasLayerParams::kShapeFieldNumber;
const int BiasLayerParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BiasLayerParams::BiasLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BiasLayerParams)
}
BiasLayerParams::BiasLayerParams(const BiasLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      shape_(from.shape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_bias()) {
    bias_ = new ::CoreML::Specification::WeightParams(*from.bias_);
  } else {
    bias_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BiasLayerParams)
}

void BiasLayerParams::SharedCtor() {
  bias_ = NULL;
  _cached_size_ = 0;
}

BiasLayerParams::~BiasLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BiasLayerParams)
  SharedDtor();
}

void BiasLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete bias_;
  }
}

void BiasLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BiasLayerParams& BiasLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BiasLayerParams* BiasLayerParams::New(::google::protobuf::Arena* arena) const {
  BiasLayerParams* n = new BiasLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BiasLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BiasLayerParams)
  shape_.Clear();
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) {
    delete bias_;
  }
  bias_ = NULL;
}

bool BiasLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BiasLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 shape = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_shape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_shape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams bias = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BiasLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BiasLayerParams)
  return false;
#undef DO_
}

void BiasLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BiasLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 shape = 1;
  if (this->shape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_shape_cached_byte_size_);
  }
  for (int i = 0, n = this->shape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->shape(i), output);
  }

  // .CoreML.Specification.WeightParams bias = 2;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->bias_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BiasLayerParams)
}

size_t BiasLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BiasLayerParams)
  size_t total_size = 0;

  // repeated uint64 shape = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->shape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _shape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.WeightParams bias = 2;
  if (this->has_bias()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BiasLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BiasLayerParams*>(&from));
}

void BiasLayerParams::MergeFrom(const BiasLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BiasLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  shape_.MergeFrom(from.shape_);
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
}

void BiasLayerParams::CopyFrom(const BiasLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BiasLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BiasLayerParams::IsInitialized() const {
  return true;
}

void BiasLayerParams::Swap(BiasLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BiasLayerParams::InternalSwap(BiasLayerParams* other) {
  shape_.InternalSwap(&other->shape_);
  std::swap(bias_, other->bias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BiasLayerParams::GetTypeName() const {
  return "CoreML.Specification.BiasLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BiasLayerParams

// repeated uint64 shape = 1;
int BiasLayerParams::shape_size() const {
  return shape_.size();
}
void BiasLayerParams::clear_shape() {
  shape_.Clear();
}
::google::protobuf::uint64 BiasLayerParams::shape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiasLayerParams.shape)
  return shape_.Get(index);
}
void BiasLayerParams::set_shape(int index, ::google::protobuf::uint64 value) {
  shape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BiasLayerParams.shape)
}
void BiasLayerParams::add_shape(::google::protobuf::uint64 value) {
  shape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiasLayerParams.shape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
BiasLayerParams::shape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiasLayerParams.shape)
  return shape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
BiasLayerParams::mutable_shape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiasLayerParams.shape)
  return &shape_;
}

// .CoreML.Specification.WeightParams bias = 2;
bool BiasLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void BiasLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& BiasLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiasLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BiasLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiasLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* BiasLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BiasLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void BiasLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BiasLayerParams.bias)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ScaleLayerParams::kShapeScaleFieldNumber;
const int ScaleLayerParams::kScaleFieldNumber;
const int ScaleLayerParams::kHasBiasFieldNumber;
const int ScaleLayerParams::kShapeBiasFieldNumber;
const int ScaleLayerParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ScaleLayerParams::ScaleLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ScaleLayerParams)
}
ScaleLayerParams::ScaleLayerParams(const ScaleLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      shapescale_(from.shapescale_),
      shapebias_(from.shapebias_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_scale()) {
    scale_ = new ::CoreML::Specification::WeightParams(*from.scale_);
  } else {
    scale_ = NULL;
  }
  if (from.has_bias()) {
    bias_ = new ::CoreML::Specification::WeightParams(*from.bias_);
  } else {
    bias_ = NULL;
  }
  hasbias_ = from.hasbias_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ScaleLayerParams)
}

void ScaleLayerParams::SharedCtor() {
  ::memset(&scale_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&scale_) + sizeof(hasbias_));
  _cached_size_ = 0;
}

ScaleLayerParams::~ScaleLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ScaleLayerParams)
  SharedDtor();
}

void ScaleLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete scale_;
  }
  if (this != internal_default_instance()) {
    delete bias_;
  }
}

void ScaleLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ScaleLayerParams& ScaleLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ScaleLayerParams* ScaleLayerParams::New(::google::protobuf::Arena* arena) const {
  ScaleLayerParams* n = new ScaleLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ScaleLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ScaleLayerParams)
  shapescale_.Clear();
  shapebias_.Clear();
  if (GetArenaNoVirtual() == NULL && scale_ != NULL) {
    delete scale_;
  }
  scale_ = NULL;
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) {
    delete bias_;
  }
  bias_ = NULL;
  hasbias_ = false;
}

bool ScaleLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ScaleLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 shapeScale = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_shapescale())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_shapescale())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams scale = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_scale()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBias = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 shapeBias = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_shapebias())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(32u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 34u, input, this->mutable_shapebias())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams bias = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(42u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ScaleLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ScaleLayerParams)
  return false;
#undef DO_
}

void ScaleLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ScaleLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 shapeScale = 1;
  if (this->shapescale_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_shapescale_cached_byte_size_);
  }
  for (int i = 0, n = this->shapescale_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->shapescale(i), output);
  }

  // .CoreML.Specification.WeightParams scale = 2;
  if (this->has_scale()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->scale_, output);
  }

  // bool hasBias = 3;
  if (this->hasbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(3, this->hasbias(), output);
  }

  // repeated uint64 shapeBias = 4;
  if (this->shapebias_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(4, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_shapebias_cached_byte_size_);
  }
  for (int i = 0, n = this->shapebias_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->shapebias(i), output);
  }

  // .CoreML.Specification.WeightParams bias = 5;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      5, *this->bias_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ScaleLayerParams)
}

size_t ScaleLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ScaleLayerParams)
  size_t total_size = 0;

  // repeated uint64 shapeScale = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->shapescale_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _shapescale_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated uint64 shapeBias = 4;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->shapebias_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _shapebias_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.WeightParams scale = 2;
  if (this->has_scale()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->scale_);
  }

  // .CoreML.Specification.WeightParams bias = 5;
  if (this->has_bias()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  // bool hasBias = 3;
  if (this->hasbias() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ScaleLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ScaleLayerParams*>(&from));
}

void ScaleLayerParams::MergeFrom(const ScaleLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ScaleLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  shapescale_.MergeFrom(from.shapescale_);
  shapebias_.MergeFrom(from.shapebias_);
  if (from.has_scale()) {
    mutable_scale()->::CoreML::Specification::WeightParams::MergeFrom(from.scale());
  }
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
  if (from.hasbias() != 0) {
    set_hasbias(from.hasbias());
  }
}

void ScaleLayerParams::CopyFrom(const ScaleLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ScaleLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ScaleLayerParams::IsInitialized() const {
  return true;
}

void ScaleLayerParams::Swap(ScaleLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ScaleLayerParams::InternalSwap(ScaleLayerParams* other) {
  shapescale_.InternalSwap(&other->shapescale_);
  shapebias_.InternalSwap(&other->shapebias_);
  std::swap(scale_, other->scale_);
  std::swap(bias_, other->bias_);
  std::swap(hasbias_, other->hasbias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ScaleLayerParams::GetTypeName() const {
  return "CoreML.Specification.ScaleLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ScaleLayerParams

// repeated uint64 shapeScale = 1;
int ScaleLayerParams::shapescale_size() const {
  return shapescale_.size();
}
void ScaleLayerParams::clear_shapescale() {
  shapescale_.Clear();
}
::google::protobuf::uint64 ScaleLayerParams::shapescale(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.shapeScale)
  return shapescale_.Get(index);
}
void ScaleLayerParams::set_shapescale(int index, ::google::protobuf::uint64 value) {
  shapescale_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScaleLayerParams.shapeScale)
}
void ScaleLayerParams::add_shapescale(::google::protobuf::uint64 value) {
  shapescale_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ScaleLayerParams.shapeScale)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ScaleLayerParams::shapescale() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ScaleLayerParams.shapeScale)
  return shapescale_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ScaleLayerParams::mutable_shapescale() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ScaleLayerParams.shapeScale)
  return &shapescale_;
}

// .CoreML.Specification.WeightParams scale = 2;
bool ScaleLayerParams::has_scale() const {
  return this != internal_default_instance() && scale_ != NULL;
}
void ScaleLayerParams::clear_scale() {
  if (GetArenaNoVirtual() == NULL && scale_ != NULL) delete scale_;
  scale_ = NULL;
}
const ::CoreML::Specification::WeightParams& ScaleLayerParams::scale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.scale)
  return scale_ != NULL ? *scale_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ScaleLayerParams::mutable_scale() {
  
  if (scale_ == NULL) {
    scale_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ScaleLayerParams.scale)
  return scale_;
}
::CoreML::Specification::WeightParams* ScaleLayerParams::release_scale() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ScaleLayerParams.scale)
  
  ::CoreML::Specification::WeightParams* temp = scale_;
  scale_ = NULL;
  return temp;
}
void ScaleLayerParams::set_allocated_scale(::CoreML::Specification::WeightParams* scale) {
  delete scale_;
  scale_ = scale;
  if (scale) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ScaleLayerParams.scale)
}

// bool hasBias = 3;
void ScaleLayerParams::clear_hasbias() {
  hasbias_ = false;
}
bool ScaleLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.hasBias)
  return hasbias_;
}
void ScaleLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScaleLayerParams.hasBias)
}

// repeated uint64 shapeBias = 4;
int ScaleLayerParams::shapebias_size() const {
  return shapebias_.size();
}
void ScaleLayerParams::clear_shapebias() {
  shapebias_.Clear();
}
::google::protobuf::uint64 ScaleLayerParams::shapebias(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.shapeBias)
  return shapebias_.Get(index);
}
void ScaleLayerParams::set_shapebias(int index, ::google::protobuf::uint64 value) {
  shapebias_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScaleLayerParams.shapeBias)
}
void ScaleLayerParams::add_shapebias(::google::protobuf::uint64 value) {
  shapebias_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ScaleLayerParams.shapeBias)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ScaleLayerParams::shapebias() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ScaleLayerParams.shapeBias)
  return shapebias_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ScaleLayerParams::mutable_shapebias() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ScaleLayerParams.shapeBias)
  return &shapebias_;
}

// .CoreML.Specification.WeightParams bias = 5;
bool ScaleLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void ScaleLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& ScaleLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ScaleLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ScaleLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* ScaleLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ScaleLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void ScaleLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ScaleLayerParams.bias)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LoadConstantLayerParams::kShapeFieldNumber;
const int LoadConstantLayerParams::kDataFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LoadConstantLayerParams::LoadConstantLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LoadConstantLayerParams)
}
LoadConstantLayerParams::LoadConstantLayerParams(const LoadConstantLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      shape_(from.shape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_data()) {
    data_ = new ::CoreML::Specification::WeightParams(*from.data_);
  } else {
    data_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LoadConstantLayerParams)
}

void LoadConstantLayerParams::SharedCtor() {
  data_ = NULL;
  _cached_size_ = 0;
}

LoadConstantLayerParams::~LoadConstantLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LoadConstantLayerParams)
  SharedDtor();
}

void LoadConstantLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete data_;
  }
}

void LoadConstantLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LoadConstantLayerParams& LoadConstantLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LoadConstantLayerParams* LoadConstantLayerParams::New(::google::protobuf::Arena* arena) const {
  LoadConstantLayerParams* n = new LoadConstantLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LoadConstantLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LoadConstantLayerParams)
  shape_.Clear();
  if (GetArenaNoVirtual() == NULL && data_ != NULL) {
    delete data_;
  }
  data_ = NULL;
}

bool LoadConstantLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LoadConstantLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 shape = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_shape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_shape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams data = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_data()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LoadConstantLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LoadConstantLayerParams)
  return false;
#undef DO_
}

void LoadConstantLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LoadConstantLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 shape = 1;
  if (this->shape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_shape_cached_byte_size_);
  }
  for (int i = 0, n = this->shape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->shape(i), output);
  }

  // .CoreML.Specification.WeightParams data = 2;
  if (this->has_data()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->data_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LoadConstantLayerParams)
}

size_t LoadConstantLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LoadConstantLayerParams)
  size_t total_size = 0;

  // repeated uint64 shape = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->shape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _shape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.WeightParams data = 2;
  if (this->has_data()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->data_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LoadConstantLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LoadConstantLayerParams*>(&from));
}

void LoadConstantLayerParams::MergeFrom(const LoadConstantLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LoadConstantLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  shape_.MergeFrom(from.shape_);
  if (from.has_data()) {
    mutable_data()->::CoreML::Specification::WeightParams::MergeFrom(from.data());
  }
}

void LoadConstantLayerParams::CopyFrom(const LoadConstantLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LoadConstantLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LoadConstantLayerParams::IsInitialized() const {
  return true;
}

void LoadConstantLayerParams::Swap(LoadConstantLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LoadConstantLayerParams::InternalSwap(LoadConstantLayerParams* other) {
  shape_.InternalSwap(&other->shape_);
  std::swap(data_, other->data_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LoadConstantLayerParams::GetTypeName() const {
  return "CoreML.Specification.LoadConstantLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LoadConstantLayerParams

// repeated uint64 shape = 1;
int LoadConstantLayerParams::shape_size() const {
  return shape_.size();
}
void LoadConstantLayerParams::clear_shape() {
  shape_.Clear();
}
::google::protobuf::uint64 LoadConstantLayerParams::shape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantLayerParams.shape)
  return shape_.Get(index);
}
void LoadConstantLayerParams::set_shape(int index, ::google::protobuf::uint64 value) {
  shape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LoadConstantLayerParams.shape)
}
void LoadConstantLayerParams::add_shape(::google::protobuf::uint64 value) {
  shape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LoadConstantLayerParams.shape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
LoadConstantLayerParams::shape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LoadConstantLayerParams.shape)
  return shape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
LoadConstantLayerParams::mutable_shape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LoadConstantLayerParams.shape)
  return &shape_;
}

// .CoreML.Specification.WeightParams data = 2;
bool LoadConstantLayerParams::has_data() const {
  return this != internal_default_instance() && data_ != NULL;
}
void LoadConstantLayerParams::clear_data() {
  if (GetArenaNoVirtual() == NULL && data_ != NULL) delete data_;
  data_ = NULL;
}
const ::CoreML::Specification::WeightParams& LoadConstantLayerParams::data() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantLayerParams.data)
  return data_ != NULL ? *data_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LoadConstantLayerParams::mutable_data() {
  
  if (data_ == NULL) {
    data_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoadConstantLayerParams.data)
  return data_;
}
::CoreML::Specification::WeightParams* LoadConstantLayerParams::release_data() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoadConstantLayerParams.data)
  
  ::CoreML::Specification::WeightParams* temp = data_;
  data_ = NULL;
  return temp;
}
void LoadConstantLayerParams::set_allocated_data(::CoreML::Specification::WeightParams* data) {
  delete data_;
  data_ = data;
  if (data) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoadConstantLayerParams.data)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int L2NormalizeLayerParams::kEpsilonFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

L2NormalizeLayerParams::L2NormalizeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.L2NormalizeLayerParams)
}
L2NormalizeLayerParams::L2NormalizeLayerParams(const L2NormalizeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  epsilon_ = from.epsilon_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.L2NormalizeLayerParams)
}

void L2NormalizeLayerParams::SharedCtor() {
  epsilon_ = 0;
  _cached_size_ = 0;
}

L2NormalizeLayerParams::~L2NormalizeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.L2NormalizeLayerParams)
  SharedDtor();
}

void L2NormalizeLayerParams::SharedDtor() {
}

void L2NormalizeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const L2NormalizeLayerParams& L2NormalizeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

L2NormalizeLayerParams* L2NormalizeLayerParams::New(::google::protobuf::Arena* arena) const {
  L2NormalizeLayerParams* n = new L2NormalizeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void L2NormalizeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.L2NormalizeLayerParams)
  epsilon_ = 0;
}

bool L2NormalizeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.L2NormalizeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float epsilon = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &epsilon_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.L2NormalizeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.L2NormalizeLayerParams)
  return false;
#undef DO_
}

void L2NormalizeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.L2NormalizeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float epsilon = 1;
  if (this->epsilon() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->epsilon(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.L2NormalizeLayerParams)
}

size_t L2NormalizeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.L2NormalizeLayerParams)
  size_t total_size = 0;

  // float epsilon = 1;
  if (this->epsilon() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void L2NormalizeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const L2NormalizeLayerParams*>(&from));
}

void L2NormalizeLayerParams::MergeFrom(const L2NormalizeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.L2NormalizeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.epsilon() != 0) {
    set_epsilon(from.epsilon());
  }
}

void L2NormalizeLayerParams::CopyFrom(const L2NormalizeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.L2NormalizeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool L2NormalizeLayerParams::IsInitialized() const {
  return true;
}

void L2NormalizeLayerParams::Swap(L2NormalizeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void L2NormalizeLayerParams::InternalSwap(L2NormalizeLayerParams* other) {
  std::swap(epsilon_, other->epsilon_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string L2NormalizeLayerParams::GetTypeName() const {
  return "CoreML.Specification.L2NormalizeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// L2NormalizeLayerParams

// float epsilon = 1;
void L2NormalizeLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
float L2NormalizeLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.L2NormalizeLayerParams.epsilon)
  return epsilon_;
}
void L2NormalizeLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.L2NormalizeLayerParams.epsilon)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int FlattenLayerParams::kModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

FlattenLayerParams::FlattenLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.FlattenLayerParams)
}
FlattenLayerParams::FlattenLayerParams(const FlattenLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  mode_ = from.mode_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.FlattenLayerParams)
}

void FlattenLayerParams::SharedCtor() {
  mode_ = 0;
  _cached_size_ = 0;
}

FlattenLayerParams::~FlattenLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.FlattenLayerParams)
  SharedDtor();
}

void FlattenLayerParams::SharedDtor() {
}

void FlattenLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const FlattenLayerParams& FlattenLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

FlattenLayerParams* FlattenLayerParams::New(::google::protobuf::Arena* arena) const {
  FlattenLayerParams* n = new FlattenLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void FlattenLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.FlattenLayerParams)
  mode_ = 0;
}

bool FlattenLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.FlattenLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::FlattenLayerParams_FlattenOrder >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.FlattenLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.FlattenLayerParams)
  return false;
#undef DO_
}

void FlattenLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.FlattenLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->mode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.FlattenLayerParams)
}

size_t FlattenLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.FlattenLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void FlattenLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const FlattenLayerParams*>(&from));
}

void FlattenLayerParams::MergeFrom(const FlattenLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.FlattenLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.mode() != 0) {
    set_mode(from.mode());
  }
}

void FlattenLayerParams::CopyFrom(const FlattenLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.FlattenLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool FlattenLayerParams::IsInitialized() const {
  return true;
}

void FlattenLayerParams::Swap(FlattenLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void FlattenLayerParams::InternalSwap(FlattenLayerParams* other) {
  std::swap(mode_, other->mode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string FlattenLayerParams::GetTypeName() const {
  return "CoreML.Specification.FlattenLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// FlattenLayerParams

// .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
void FlattenLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::FlattenLayerParams_FlattenOrder FlattenLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FlattenLayerParams.mode)
  return static_cast< ::CoreML::Specification::FlattenLayerParams_FlattenOrder >(mode_);
}
void FlattenLayerParams::set_mode(::CoreML::Specification::FlattenLayerParams_FlattenOrder value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.FlattenLayerParams.mode)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReshapeLayerParams::kTargetShapeFieldNumber;
const int ReshapeLayerParams::kModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReshapeLayerParams::ReshapeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReshapeLayerParams)
}
ReshapeLayerParams::ReshapeLayerParams(const ReshapeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      targetshape_(from.targetshape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  mode_ = from.mode_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReshapeLayerParams)
}

void ReshapeLayerParams::SharedCtor() {
  mode_ = 0;
  _cached_size_ = 0;
}

ReshapeLayerParams::~ReshapeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReshapeLayerParams)
  SharedDtor();
}

void ReshapeLayerParams::SharedDtor() {
}

void ReshapeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReshapeLayerParams& ReshapeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReshapeLayerParams* ReshapeLayerParams::New(::google::protobuf::Arena* arena) const {
  ReshapeLayerParams* n = new ReshapeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReshapeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReshapeLayerParams)
  targetshape_.Clear();
  mode_ = 0;
}

bool ReshapeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReshapeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 targetShape = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_targetshape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_targetshape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReshapeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReshapeLayerParams)
  return false;
#undef DO_
}

void ReshapeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReshapeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 targetShape = 1;
  if (this->targetshape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_targetshape_cached_byte_size_);
  }
  for (int i = 0, n = this->targetshape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->targetshape(i), output);
  }

  // .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      2, this->mode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReshapeLayerParams)
}

size_t ReshapeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReshapeLayerParams)
  size_t total_size = 0;

  // repeated int64 targetShape = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->targetshape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _targetshape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReshapeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReshapeLayerParams*>(&from));
}

void ReshapeLayerParams::MergeFrom(const ReshapeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReshapeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  targetshape_.MergeFrom(from.targetshape_);
  if (from.mode() != 0) {
    set_mode(from.mode());
  }
}

void ReshapeLayerParams::CopyFrom(const ReshapeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReshapeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReshapeLayerParams::IsInitialized() const {
  return true;
}

void ReshapeLayerParams::Swap(ReshapeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReshapeLayerParams::InternalSwap(ReshapeLayerParams* other) {
  targetshape_.InternalSwap(&other->targetshape_);
  std::swap(mode_, other->mode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReshapeLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReshapeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReshapeLayerParams

// repeated int64 targetShape = 1;
int ReshapeLayerParams::targetshape_size() const {
  return targetshape_.size();
}
void ReshapeLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
::google::protobuf::int64 ReshapeLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReshapeLayerParams.targetShape)
  return targetshape_.Get(index);
}
void ReshapeLayerParams::set_targetshape(int index, ::google::protobuf::int64 value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReshapeLayerParams.targetShape)
}
void ReshapeLayerParams::add_targetshape(::google::protobuf::int64 value) {
  targetshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReshapeLayerParams.targetShape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReshapeLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReshapeLayerParams.targetShape)
  return targetshape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReshapeLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReshapeLayerParams.targetShape)
  return &targetshape_;
}

// .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
void ReshapeLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReshapeLayerParams.mode)
  return static_cast< ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder >(mode_);
}
void ReshapeLayerParams::set_mode(::CoreML::Specification::ReshapeLayerParams_ReshapeOrder value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReshapeLayerParams.mode)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PermuteLayerParams::kAxisFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PermuteLayerParams::PermuteLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PermuteLayerParams)
}
PermuteLayerParams::PermuteLayerParams(const PermuteLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      axis_(from.axis_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PermuteLayerParams)
}

void PermuteLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

PermuteLayerParams::~PermuteLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PermuteLayerParams)
  SharedDtor();
}

void PermuteLayerParams::SharedDtor() {
}

void PermuteLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PermuteLayerParams& PermuteLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PermuteLayerParams* PermuteLayerParams::New(::google::protobuf::Arena* arena) const {
  PermuteLayerParams* n = new PermuteLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PermuteLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PermuteLayerParams)
  axis_.Clear();
}

bool PermuteLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PermuteLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_axis())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_axis())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PermuteLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PermuteLayerParams)
  return false;
#undef DO_
}

void PermuteLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PermuteLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 axis = 1;
  if (this->axis_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_axis_cached_byte_size_);
  }
  for (int i = 0, n = this->axis_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->axis(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PermuteLayerParams)
}

size_t PermuteLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PermuteLayerParams)
  size_t total_size = 0;

  // repeated uint64 axis = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->axis_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _axis_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PermuteLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PermuteLayerParams*>(&from));
}

void PermuteLayerParams::MergeFrom(const PermuteLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PermuteLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  axis_.MergeFrom(from.axis_);
}

void PermuteLayerParams::CopyFrom(const PermuteLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PermuteLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PermuteLayerParams::IsInitialized() const {
  return true;
}

void PermuteLayerParams::Swap(PermuteLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PermuteLayerParams::InternalSwap(PermuteLayerParams* other) {
  axis_.InternalSwap(&other->axis_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PermuteLayerParams::GetTypeName() const {
  return "CoreML.Specification.PermuteLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PermuteLayerParams

// repeated uint64 axis = 1;
int PermuteLayerParams::axis_size() const {
  return axis_.size();
}
void PermuteLayerParams::clear_axis() {
  axis_.Clear();
}
::google::protobuf::uint64 PermuteLayerParams::axis(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PermuteLayerParams.axis)
  return axis_.Get(index);
}
void PermuteLayerParams::set_axis(int index, ::google::protobuf::uint64 value) {
  axis_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PermuteLayerParams.axis)
}
void PermuteLayerParams::add_axis(::google::protobuf::uint64 value) {
  axis_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PermuteLayerParams.axis)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PermuteLayerParams::axis() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PermuteLayerParams.axis)
  return axis_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PermuteLayerParams::mutable_axis() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PermuteLayerParams.axis)
  return &axis_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReorganizeDataLayerParams::kModeFieldNumber;
const int ReorganizeDataLayerParams::kBlockSizeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReorganizeDataLayerParams::ReorganizeDataLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReorganizeDataLayerParams)
}
ReorganizeDataLayerParams::ReorganizeDataLayerParams(const ReorganizeDataLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&blocksize_, &from.blocksize_,
    reinterpret_cast<char*>(&mode_) -
    reinterpret_cast<char*>(&blocksize_) + sizeof(mode_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReorganizeDataLayerParams)
}

void ReorganizeDataLayerParams::SharedCtor() {
  ::memset(&blocksize_, 0, reinterpret_cast<char*>(&mode_) -
    reinterpret_cast<char*>(&blocksize_) + sizeof(mode_));
  _cached_size_ = 0;
}

ReorganizeDataLayerParams::~ReorganizeDataLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReorganizeDataLayerParams)
  SharedDtor();
}

void ReorganizeDataLayerParams::SharedDtor() {
}

void ReorganizeDataLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReorganizeDataLayerParams& ReorganizeDataLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReorganizeDataLayerParams* ReorganizeDataLayerParams::New(::google::protobuf::Arena* arena) const {
  ReorganizeDataLayerParams* n = new ReorganizeDataLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReorganizeDataLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReorganizeDataLayerParams)
  ::memset(&blocksize_, 0, reinterpret_cast<char*>(&mode_) -
    reinterpret_cast<char*>(&blocksize_) + sizeof(mode_));
}

bool ReorganizeDataLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReorganizeDataLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.ReorganizeDataLayerParams.ReorganizationType mode = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 blockSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &blocksize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReorganizeDataLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReorganizeDataLayerParams)
  return false;
#undef DO_
}

void ReorganizeDataLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReorganizeDataLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.ReorganizeDataLayerParams.ReorganizationType mode = 1;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->mode(), output);
  }

  // uint64 blockSize = 2;
  if (this->blocksize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->blocksize(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReorganizeDataLayerParams)
}

size_t ReorganizeDataLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReorganizeDataLayerParams)
  size_t total_size = 0;

  // uint64 blockSize = 2;
  if (this->blocksize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->blocksize());
  }

  // .CoreML.Specification.ReorganizeDataLayerParams.ReorganizationType mode = 1;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReorganizeDataLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReorganizeDataLayerParams*>(&from));
}

void ReorganizeDataLayerParams::MergeFrom(const ReorganizeDataLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReorganizeDataLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.blocksize() != 0) {
    set_blocksize(from.blocksize());
  }
  if (from.mode() != 0) {
    set_mode(from.mode());
  }
}

void ReorganizeDataLayerParams::CopyFrom(const ReorganizeDataLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReorganizeDataLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReorganizeDataLayerParams::IsInitialized() const {
  return true;
}

void ReorganizeDataLayerParams::Swap(ReorganizeDataLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReorganizeDataLayerParams::InternalSwap(ReorganizeDataLayerParams* other) {
  std::swap(blocksize_, other->blocksize_);
  std::swap(mode_, other->mode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReorganizeDataLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReorganizeDataLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReorganizeDataLayerParams

// .CoreML.Specification.ReorganizeDataLayerParams.ReorganizationType mode = 1;
void ReorganizeDataLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReorganizeDataLayerParams.mode)
  return static_cast< ::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType >(mode_);
}
void ReorganizeDataLayerParams::set_mode(::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReorganizeDataLayerParams.mode)
}

// uint64 blockSize = 2;
void ReorganizeDataLayerParams::clear_blocksize() {
  blocksize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 ReorganizeDataLayerParams::blocksize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReorganizeDataLayerParams.blockSize)
  return blocksize_;
}
void ReorganizeDataLayerParams::set_blocksize(::google::protobuf::uint64 value) {
  
  blocksize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReorganizeDataLayerParams.blockSize)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SliceLayerParams::kStartIndexFieldNumber;
const int SliceLayerParams::kEndIndexFieldNumber;
const int SliceLayerParams::kStrideFieldNumber;
const int SliceLayerParams::kAxisFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SliceLayerParams::SliceLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SliceLayerParams)
}
SliceLayerParams::SliceLayerParams(const SliceLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&startindex_, &from.startindex_,
    reinterpret_cast<char*>(&axis_) -
    reinterpret_cast<char*>(&startindex_) + sizeof(axis_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SliceLayerParams)
}

void SliceLayerParams::SharedCtor() {
  ::memset(&startindex_, 0, reinterpret_cast<char*>(&axis_) -
    reinterpret_cast<char*>(&startindex_) + sizeof(axis_));
  _cached_size_ = 0;
}

SliceLayerParams::~SliceLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SliceLayerParams)
  SharedDtor();
}

void SliceLayerParams::SharedDtor() {
}

void SliceLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SliceLayerParams& SliceLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SliceLayerParams* SliceLayerParams::New(::google::protobuf::Arena* arena) const {
  SliceLayerParams* n = new SliceLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SliceLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SliceLayerParams)
  ::memset(&startindex_, 0, reinterpret_cast<char*>(&axis_) -
    reinterpret_cast<char*>(&startindex_) + sizeof(axis_));
}

bool SliceLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SliceLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 startIndex = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &startindex_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // int64 endIndex = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &endindex_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 stride = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &stride_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SliceLayerParams.SliceAxis axis = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(32u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_axis(static_cast< ::CoreML::Specification::SliceLayerParams_SliceAxis >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SliceLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SliceLayerParams)
  return false;
#undef DO_
}

void SliceLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SliceLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 startIndex = 1;
  if (this->startindex() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->startindex(), output);
  }

  // int64 endIndex = 2;
  if (this->endindex() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(2, this->endindex(), output);
  }

  // uint64 stride = 3;
  if (this->stride() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(3, this->stride(), output);
  }

  // .CoreML.Specification.SliceLayerParams.SliceAxis axis = 4;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      4, this->axis(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SliceLayerParams)
}

size_t SliceLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SliceLayerParams)
  size_t total_size = 0;

  // int64 startIndex = 1;
  if (this->startindex() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->startindex());
  }

  // int64 endIndex = 2;
  if (this->endindex() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->endindex());
  }

  // uint64 stride = 3;
  if (this->stride() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->stride());
  }

  // .CoreML.Specification.SliceLayerParams.SliceAxis axis = 4;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->axis());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SliceLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SliceLayerParams*>(&from));
}

void SliceLayerParams::MergeFrom(const SliceLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SliceLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.startindex() != 0) {
    set_startindex(from.startindex());
  }
  if (from.endindex() != 0) {
    set_endindex(from.endindex());
  }
  if (from.stride() != 0) {
    set_stride(from.stride());
  }
  if (from.axis() != 0) {
    set_axis(from.axis());
  }
}

void SliceLayerParams::CopyFrom(const SliceLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SliceLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SliceLayerParams::IsInitialized() const {
  return true;
}

void SliceLayerParams::Swap(SliceLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SliceLayerParams::InternalSwap(SliceLayerParams* other) {
  std::swap(startindex_, other->startindex_);
  std::swap(endindex_, other->endindex_);
  std::swap(stride_, other->stride_);
  std::swap(axis_, other->axis_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SliceLayerParams::GetTypeName() const {
  return "CoreML.Specification.SliceLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SliceLayerParams

// int64 startIndex = 1;
void SliceLayerParams::clear_startindex() {
  startindex_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 SliceLayerParams::startindex() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceLayerParams.startIndex)
  return startindex_;
}
void SliceLayerParams::set_startindex(::google::protobuf::int64 value) {
  
  startindex_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceLayerParams.startIndex)
}

// int64 endIndex = 2;
void SliceLayerParams::clear_endindex() {
  endindex_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 SliceLayerParams::endindex() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceLayerParams.endIndex)
  return endindex_;
}
void SliceLayerParams::set_endindex(::google::protobuf::int64 value) {
  
  endindex_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceLayerParams.endIndex)
}

// uint64 stride = 3;
void SliceLayerParams::clear_stride() {
  stride_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SliceLayerParams::stride() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceLayerParams.stride)
  return stride_;
}
void SliceLayerParams::set_stride(::google::protobuf::uint64 value) {
  
  stride_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceLayerParams.stride)
}

// .CoreML.Specification.SliceLayerParams.SliceAxis axis = 4;
void SliceLayerParams::clear_axis() {
  axis_ = 0;
}
::CoreML::Specification::SliceLayerParams_SliceAxis SliceLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceLayerParams.axis)
  return static_cast< ::CoreML::Specification::SliceLayerParams_SliceAxis >(axis_);
}
void SliceLayerParams::set_axis(::CoreML::Specification::SliceLayerParams_SliceAxis value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceLayerParams.axis)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReduceLayerParams::kModeFieldNumber;
const int ReduceLayerParams::kEpsilonFieldNumber;
const int ReduceLayerParams::kAxisFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReduceLayerParams::ReduceLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReduceLayerParams)
}
ReduceLayerParams::ReduceLayerParams(const ReduceLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&mode_, &from.mode_,
    reinterpret_cast<char*>(&axis_) -
    reinterpret_cast<char*>(&mode_) + sizeof(axis_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReduceLayerParams)
}

void ReduceLayerParams::SharedCtor() {
  ::memset(&mode_, 0, reinterpret_cast<char*>(&axis_) -
    reinterpret_cast<char*>(&mode_) + sizeof(axis_));
  _cached_size_ = 0;
}

ReduceLayerParams::~ReduceLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReduceLayerParams)
  SharedDtor();
}

void ReduceLayerParams::SharedDtor() {
}

void ReduceLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReduceLayerParams& ReduceLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReduceLayerParams* ReduceLayerParams::New(::google::protobuf::Arena* arena) const {
  ReduceLayerParams* n = new ReduceLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReduceLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReduceLayerParams)
  ::memset(&mode_, 0, reinterpret_cast<char*>(&axis_) -
    reinterpret_cast<char*>(&mode_) + sizeof(axis_));
}

bool ReduceLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReduceLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::ReduceLayerParams_ReduceOperation >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float epsilon = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &epsilon_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReduceLayerParams.ReduceAxis axis = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_axis(static_cast< ::CoreML::Specification::ReduceLayerParams_ReduceAxis >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReduceLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReduceLayerParams)
  return false;
#undef DO_
}

void ReduceLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReduceLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->mode(), output);
  }

  // float epsilon = 2;
  if (this->epsilon() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->epsilon(), output);
  }

  // .CoreML.Specification.ReduceLayerParams.ReduceAxis axis = 3;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      3, this->axis(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReduceLayerParams)
}

size_t ReduceLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReduceLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  // float epsilon = 2;
  if (this->epsilon() != 0) {
    total_size += 1 + 4;
  }

  // .CoreML.Specification.ReduceLayerParams.ReduceAxis axis = 3;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->axis());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReduceLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReduceLayerParams*>(&from));
}

void ReduceLayerParams::MergeFrom(const ReduceLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReduceLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.mode() != 0) {
    set_mode(from.mode());
  }
  if (from.epsilon() != 0) {
    set_epsilon(from.epsilon());
  }
  if (from.axis() != 0) {
    set_axis(from.axis());
  }
}

void ReduceLayerParams::CopyFrom(const ReduceLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReduceLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReduceLayerParams::IsInitialized() const {
  return true;
}

void ReduceLayerParams::Swap(ReduceLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReduceLayerParams::InternalSwap(ReduceLayerParams* other) {
  std::swap(mode_, other->mode_);
  std::swap(epsilon_, other->epsilon_);
  std::swap(axis_, other->axis_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReduceLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReduceLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReduceLayerParams

// .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
void ReduceLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::ReduceLayerParams_ReduceOperation ReduceLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLayerParams.mode)
  return static_cast< ::CoreML::Specification::ReduceLayerParams_ReduceOperation >(mode_);
}
void ReduceLayerParams::set_mode(::CoreML::Specification::ReduceLayerParams_ReduceOperation value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLayerParams.mode)
}

// float epsilon = 2;
void ReduceLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
float ReduceLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLayerParams.epsilon)
  return epsilon_;
}
void ReduceLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLayerParams.epsilon)
}

// .CoreML.Specification.ReduceLayerParams.ReduceAxis axis = 3;
void ReduceLayerParams::clear_axis() {
  axis_ = 0;
}
::CoreML::Specification::ReduceLayerParams_ReduceAxis ReduceLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLayerParams.axis)
  return static_cast< ::CoreML::Specification::ReduceLayerParams_ReduceAxis >(axis_);
}
void ReduceLayerParams::set_axis(::CoreML::Specification::ReduceLayerParams_ReduceAxis value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLayerParams.axis)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int CropLayerParams::kCropAmountsFieldNumber;
const int CropLayerParams::kOffsetFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CropLayerParams::CropLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CropLayerParams)
}
CropLayerParams::CropLayerParams(const CropLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      offset_(from.offset_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_cropamounts()) {
    cropamounts_ = new ::CoreML::Specification::BorderAmounts(*from.cropamounts_);
  } else {
    cropamounts_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CropLayerParams)
}

void CropLayerParams::SharedCtor() {
  cropamounts_ = NULL;
  _cached_size_ = 0;
}

CropLayerParams::~CropLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CropLayerParams)
  SharedDtor();
}

void CropLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete cropamounts_;
  }
}

void CropLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CropLayerParams& CropLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

CropLayerParams* CropLayerParams::New(::google::protobuf::Arena* arena) const {
  CropLayerParams* n = new CropLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CropLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CropLayerParams)
  offset_.Clear();
  if (GetArenaNoVirtual() == NULL && cropamounts_ != NULL) {
    delete cropamounts_;
  }
  cropamounts_ = NULL;
}

bool CropLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CropLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.BorderAmounts cropAmounts = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_cropamounts()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 offset = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(42u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_offset())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(40u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 42u, input, this->mutable_offset())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CropLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CropLayerParams)
  return false;
#undef DO_
}

void CropLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CropLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.BorderAmounts cropAmounts = 1;
  if (this->has_cropamounts()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->cropamounts_, output);
  }

  // repeated uint64 offset = 5;
  if (this->offset_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(5, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_offset_cached_byte_size_);
  }
  for (int i = 0, n = this->offset_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->offset(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CropLayerParams)
}

size_t CropLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CropLayerParams)
  size_t total_size = 0;

  // repeated uint64 offset = 5;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->offset_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _offset_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.BorderAmounts cropAmounts = 1;
  if (this->has_cropamounts()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->cropamounts_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CropLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CropLayerParams*>(&from));
}

void CropLayerParams::MergeFrom(const CropLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CropLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  offset_.MergeFrom(from.offset_);
  if (from.has_cropamounts()) {
    mutable_cropamounts()->::CoreML::Specification::BorderAmounts::MergeFrom(from.cropamounts());
  }
}

void CropLayerParams::CopyFrom(const CropLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CropLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CropLayerParams::IsInitialized() const {
  return true;
}

void CropLayerParams::Swap(CropLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CropLayerParams::InternalSwap(CropLayerParams* other) {
  offset_.InternalSwap(&other->offset_);
  std::swap(cropamounts_, other->cropamounts_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CropLayerParams::GetTypeName() const {
  return "CoreML.Specification.CropLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CropLayerParams

// .CoreML.Specification.BorderAmounts cropAmounts = 1;
bool CropLayerParams::has_cropamounts() const {
  return this != internal_default_instance() && cropamounts_ != NULL;
}
void CropLayerParams::clear_cropamounts() {
  if (GetArenaNoVirtual() == NULL && cropamounts_ != NULL) delete cropamounts_;
  cropamounts_ = NULL;
}
const ::CoreML::Specification::BorderAmounts& CropLayerParams::cropamounts() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropLayerParams.cropAmounts)
  return cropamounts_ != NULL ? *cropamounts_
                         : *::CoreML::Specification::BorderAmounts::internal_default_instance();
}
::CoreML::Specification::BorderAmounts* CropLayerParams::mutable_cropamounts() {
  
  if (cropamounts_ == NULL) {
    cropamounts_ = new ::CoreML::Specification::BorderAmounts;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CropLayerParams.cropAmounts)
  return cropamounts_;
}
::CoreML::Specification::BorderAmounts* CropLayerParams::release_cropamounts() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CropLayerParams.cropAmounts)
  
  ::CoreML::Specification::BorderAmounts* temp = cropamounts_;
  cropamounts_ = NULL;
  return temp;
}
void CropLayerParams::set_allocated_cropamounts(::CoreML::Specification::BorderAmounts* cropamounts) {
  delete cropamounts_;
  cropamounts_ = cropamounts;
  if (cropamounts) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CropLayerParams.cropAmounts)
}

// repeated uint64 offset = 5;
int CropLayerParams::offset_size() const {
  return offset_.size();
}
void CropLayerParams::clear_offset() {
  offset_.Clear();
}
::google::protobuf::uint64 CropLayerParams::offset(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropLayerParams.offset)
  return offset_.Get(index);
}
void CropLayerParams::set_offset(int index, ::google::protobuf::uint64 value) {
  offset_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CropLayerParams.offset)
}
void CropLayerParams::add_offset(::google::protobuf::uint64 value) {
  offset_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.CropLayerParams.offset)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
CropLayerParams::offset() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.CropLayerParams.offset)
  return offset_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
CropLayerParams::mutable_offset() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.CropLayerParams.offset)
  return &offset_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

AverageLayerParams::AverageLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.AverageLayerParams)
}
AverageLayerParams::AverageLayerParams(const AverageLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.AverageLayerParams)
}

void AverageLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

AverageLayerParams::~AverageLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.AverageLayerParams)
  SharedDtor();
}

void AverageLayerParams::SharedDtor() {
}

void AverageLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const AverageLayerParams& AverageLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

AverageLayerParams* AverageLayerParams::New(::google::protobuf::Arena* arena) const {
  AverageLayerParams* n = new AverageLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void AverageLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.AverageLayerParams)
}

bool AverageLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.AverageLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.AverageLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.AverageLayerParams)
  return false;
#undef DO_
}

void AverageLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.AverageLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.AverageLayerParams)
}

size_t AverageLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.AverageLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void AverageLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const AverageLayerParams*>(&from));
}

void AverageLayerParams::MergeFrom(const AverageLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.AverageLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void AverageLayerParams::CopyFrom(const AverageLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.AverageLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool AverageLayerParams::IsInitialized() const {
  return true;
}

void AverageLayerParams::Swap(AverageLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void AverageLayerParams::InternalSwap(AverageLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string AverageLayerParams::GetTypeName() const {
  return "CoreML.Specification.AverageLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// AverageLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MaxLayerParams::MaxLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MaxLayerParams)
}
MaxLayerParams::MaxLayerParams(const MaxLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MaxLayerParams)
}

void MaxLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

MaxLayerParams::~MaxLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MaxLayerParams)
  SharedDtor();
}

void MaxLayerParams::SharedDtor() {
}

void MaxLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MaxLayerParams& MaxLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

MaxLayerParams* MaxLayerParams::New(::google::protobuf::Arena* arena) const {
  MaxLayerParams* n = new MaxLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MaxLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MaxLayerParams)
}

bool MaxLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MaxLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MaxLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MaxLayerParams)
  return false;
#undef DO_
}

void MaxLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MaxLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MaxLayerParams)
}

size_t MaxLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MaxLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MaxLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MaxLayerParams*>(&from));
}

void MaxLayerParams::MergeFrom(const MaxLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MaxLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void MaxLayerParams::CopyFrom(const MaxLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MaxLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MaxLayerParams::IsInitialized() const {
  return true;
}

void MaxLayerParams::Swap(MaxLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MaxLayerParams::InternalSwap(MaxLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MaxLayerParams::GetTypeName() const {
  return "CoreML.Specification.MaxLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MaxLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MinLayerParams::MinLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MinLayerParams)
}
MinLayerParams::MinLayerParams(const MinLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MinLayerParams)
}

void MinLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

MinLayerParams::~MinLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MinLayerParams)
  SharedDtor();
}

void MinLayerParams::SharedDtor() {
}

void MinLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MinLayerParams& MinLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

MinLayerParams* MinLayerParams::New(::google::protobuf::Arena* arena) const {
  MinLayerParams* n = new MinLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MinLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MinLayerParams)
}

bool MinLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MinLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MinLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MinLayerParams)
  return false;
#undef DO_
}

void MinLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MinLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MinLayerParams)
}

size_t MinLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MinLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MinLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MinLayerParams*>(&from));
}

void MinLayerParams::MergeFrom(const MinLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MinLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void MinLayerParams::CopyFrom(const MinLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MinLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MinLayerParams::IsInitialized() const {
  return true;
}

void MinLayerParams::Swap(MinLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MinLayerParams::InternalSwap(MinLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MinLayerParams::GetTypeName() const {
  return "CoreML.Specification.MinLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MinLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int DotProductLayerParams::kCosineSimilarityFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

DotProductLayerParams::DotProductLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.DotProductLayerParams)
}
DotProductLayerParams::DotProductLayerParams(const DotProductLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  cosinesimilarity_ = from.cosinesimilarity_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.DotProductLayerParams)
}

void DotProductLayerParams::SharedCtor() {
  cosinesimilarity_ = false;
  _cached_size_ = 0;
}

DotProductLayerParams::~DotProductLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.DotProductLayerParams)
  SharedDtor();
}

void DotProductLayerParams::SharedDtor() {
}

void DotProductLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const DotProductLayerParams& DotProductLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

DotProductLayerParams* DotProductLayerParams::New(::google::protobuf::Arena* arena) const {
  DotProductLayerParams* n = new DotProductLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void DotProductLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.DotProductLayerParams)
  cosinesimilarity_ = false;
}

bool DotProductLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.DotProductLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // bool cosineSimilarity = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &cosinesimilarity_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.DotProductLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.DotProductLayerParams)
  return false;
#undef DO_
}

void DotProductLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.DotProductLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // bool cosineSimilarity = 1;
  if (this->cosinesimilarity() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(1, this->cosinesimilarity(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.DotProductLayerParams)
}

size_t DotProductLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.DotProductLayerParams)
  size_t total_size = 0;

  // bool cosineSimilarity = 1;
  if (this->cosinesimilarity() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void DotProductLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const DotProductLayerParams*>(&from));
}

void DotProductLayerParams::MergeFrom(const DotProductLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.DotProductLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.cosinesimilarity() != 0) {
    set_cosinesimilarity(from.cosinesimilarity());
  }
}

void DotProductLayerParams::CopyFrom(const DotProductLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.DotProductLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool DotProductLayerParams::IsInitialized() const {
  return true;
}

void DotProductLayerParams::Swap(DotProductLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void DotProductLayerParams::InternalSwap(DotProductLayerParams* other) {
  std::swap(cosinesimilarity_, other->cosinesimilarity_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string DotProductLayerParams::GetTypeName() const {
  return "CoreML.Specification.DotProductLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// DotProductLayerParams

// bool cosineSimilarity = 1;
void DotProductLayerParams::clear_cosinesimilarity() {
  cosinesimilarity_ = false;
}
bool DotProductLayerParams::cosinesimilarity() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.DotProductLayerParams.cosineSimilarity)
  return cosinesimilarity_;
}
void DotProductLayerParams::set_cosinesimilarity(bool value) {
  
  cosinesimilarity_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.DotProductLayerParams.cosineSimilarity)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int MeanVarianceNormalizeLayerParams::kAcrossChannelsFieldNumber;
const int MeanVarianceNormalizeLayerParams::kNormalizeVarianceFieldNumber;
const int MeanVarianceNormalizeLayerParams::kEpsilonFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MeanVarianceNormalizeLayerParams::MeanVarianceNormalizeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MeanVarianceNormalizeLayerParams)
}
MeanVarianceNormalizeLayerParams::MeanVarianceNormalizeLayerParams(const MeanVarianceNormalizeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&acrosschannels_, &from.acrosschannels_,
    reinterpret_cast<char*>(&epsilon_) -
    reinterpret_cast<char*>(&acrosschannels_) + sizeof(epsilon_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MeanVarianceNormalizeLayerParams)
}

void MeanVarianceNormalizeLayerParams::SharedCtor() {
  ::memset(&acrosschannels_, 0, reinterpret_cast<char*>(&epsilon_) -
    reinterpret_cast<char*>(&acrosschannels_) + sizeof(epsilon_));
  _cached_size_ = 0;
}

MeanVarianceNormalizeLayerParams::~MeanVarianceNormalizeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  SharedDtor();
}

void MeanVarianceNormalizeLayerParams::SharedDtor() {
}

void MeanVarianceNormalizeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MeanVarianceNormalizeLayerParams& MeanVarianceNormalizeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

MeanVarianceNormalizeLayerParams* MeanVarianceNormalizeLayerParams::New(::google::protobuf::Arena* arena) const {
  MeanVarianceNormalizeLayerParams* n = new MeanVarianceNormalizeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MeanVarianceNormalizeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  ::memset(&acrosschannels_, 0, reinterpret_cast<char*>(&epsilon_) -
    reinterpret_cast<char*>(&acrosschannels_) + sizeof(epsilon_));
}

bool MeanVarianceNormalizeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // bool acrossChannels = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &acrosschannels_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool normalizeVariance = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &normalizevariance_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float epsilon = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(29u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &epsilon_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  return false;
#undef DO_
}

void MeanVarianceNormalizeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // bool acrossChannels = 1;
  if (this->acrosschannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(1, this->acrosschannels(), output);
  }

  // bool normalizeVariance = 2;
  if (this->normalizevariance() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(2, this->normalizevariance(), output);
  }

  // float epsilon = 3;
  if (this->epsilon() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(3, this->epsilon(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MeanVarianceNormalizeLayerParams)
}

size_t MeanVarianceNormalizeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  size_t total_size = 0;

  // bool acrossChannels = 1;
  if (this->acrosschannels() != 0) {
    total_size += 1 + 1;
  }

  // bool normalizeVariance = 2;
  if (this->normalizevariance() != 0) {
    total_size += 1 + 1;
  }

  // float epsilon = 3;
  if (this->epsilon() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MeanVarianceNormalizeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MeanVarianceNormalizeLayerParams*>(&from));
}

void MeanVarianceNormalizeLayerParams::MergeFrom(const MeanVarianceNormalizeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.acrosschannels() != 0) {
    set_acrosschannels(from.acrosschannels());
  }
  if (from.normalizevariance() != 0) {
    set_normalizevariance(from.normalizevariance());
  }
  if (from.epsilon() != 0) {
    set_epsilon(from.epsilon());
  }
}

void MeanVarianceNormalizeLayerParams::CopyFrom(const MeanVarianceNormalizeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MeanVarianceNormalizeLayerParams::IsInitialized() const {
  return true;
}

void MeanVarianceNormalizeLayerParams::Swap(MeanVarianceNormalizeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MeanVarianceNormalizeLayerParams::InternalSwap(MeanVarianceNormalizeLayerParams* other) {
  std::swap(acrosschannels_, other->acrosschannels_);
  std::swap(normalizevariance_, other->normalizevariance_);
  std::swap(epsilon_, other->epsilon_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MeanVarianceNormalizeLayerParams::GetTypeName() const {
  return "CoreML.Specification.MeanVarianceNormalizeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MeanVarianceNormalizeLayerParams

// bool acrossChannels = 1;
void MeanVarianceNormalizeLayerParams::clear_acrosschannels() {
  acrosschannels_ = false;
}
bool MeanVarianceNormalizeLayerParams::acrosschannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanVarianceNormalizeLayerParams.acrossChannels)
  return acrosschannels_;
}
void MeanVarianceNormalizeLayerParams::set_acrosschannels(bool value) {
  
  acrosschannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanVarianceNormalizeLayerParams.acrossChannels)
}

// bool normalizeVariance = 2;
void MeanVarianceNormalizeLayerParams::clear_normalizevariance() {
  normalizevariance_ = false;
}
bool MeanVarianceNormalizeLayerParams::normalizevariance() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanVarianceNormalizeLayerParams.normalizeVariance)
  return normalizevariance_;
}
void MeanVarianceNormalizeLayerParams::set_normalizevariance(bool value) {
  
  normalizevariance_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanVarianceNormalizeLayerParams.normalizeVariance)
}

// float epsilon = 3;
void MeanVarianceNormalizeLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
float MeanVarianceNormalizeLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanVarianceNormalizeLayerParams.epsilon)
  return epsilon_;
}
void MeanVarianceNormalizeLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanVarianceNormalizeLayerParams.epsilon)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SequenceRepeatLayerParams::kNRepetitionsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SequenceRepeatLayerParams::SequenceRepeatLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SequenceRepeatLayerParams)
}
SequenceRepeatLayerParams::SequenceRepeatLayerParams(const SequenceRepeatLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  nrepetitions_ = from.nrepetitions_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SequenceRepeatLayerParams)
}

void SequenceRepeatLayerParams::SharedCtor() {
  nrepetitions_ = GOOGLE_ULONGLONG(0);
  _cached_size_ = 0;
}

SequenceRepeatLayerParams::~SequenceRepeatLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SequenceRepeatLayerParams)
  SharedDtor();
}

void SequenceRepeatLayerParams::SharedDtor() {
}

void SequenceRepeatLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SequenceRepeatLayerParams& SequenceRepeatLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SequenceRepeatLayerParams* SequenceRepeatLayerParams::New(::google::protobuf::Arena* arena) const {
  SequenceRepeatLayerParams* n = new SequenceRepeatLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SequenceRepeatLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SequenceRepeatLayerParams)
  nrepetitions_ = GOOGLE_ULONGLONG(0);
}

bool SequenceRepeatLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SequenceRepeatLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 nRepetitions = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &nrepetitions_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SequenceRepeatLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SequenceRepeatLayerParams)
  return false;
#undef DO_
}

void SequenceRepeatLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SequenceRepeatLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 nRepetitions = 1;
  if (this->nrepetitions() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->nrepetitions(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SequenceRepeatLayerParams)
}

size_t SequenceRepeatLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SequenceRepeatLayerParams)
  size_t total_size = 0;

  // uint64 nRepetitions = 1;
  if (this->nrepetitions() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->nrepetitions());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SequenceRepeatLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SequenceRepeatLayerParams*>(&from));
}

void SequenceRepeatLayerParams::MergeFrom(const SequenceRepeatLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SequenceRepeatLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.nrepetitions() != 0) {
    set_nrepetitions(from.nrepetitions());
  }
}

void SequenceRepeatLayerParams::CopyFrom(const SequenceRepeatLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SequenceRepeatLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SequenceRepeatLayerParams::IsInitialized() const {
  return true;
}

void SequenceRepeatLayerParams::Swap(SequenceRepeatLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SequenceRepeatLayerParams::InternalSwap(SequenceRepeatLayerParams* other) {
  std::swap(nrepetitions_, other->nrepetitions_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SequenceRepeatLayerParams::GetTypeName() const {
  return "CoreML.Specification.SequenceRepeatLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SequenceRepeatLayerParams

// uint64 nRepetitions = 1;
void SequenceRepeatLayerParams::clear_nrepetitions() {
  nrepetitions_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SequenceRepeatLayerParams::nrepetitions() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SequenceRepeatLayerParams.nRepetitions)
  return nrepetitions_;
}
void SequenceRepeatLayerParams::set_nrepetitions(::google::protobuf::uint64 value) {
  
  nrepetitions_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SequenceRepeatLayerParams.nRepetitions)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SimpleRecurrentLayerParams::kInputVectorSizeFieldNumber;
const int SimpleRecurrentLayerParams::kOutputVectorSizeFieldNumber;
const int SimpleRecurrentLayerParams::kActivationFieldNumber;
const int SimpleRecurrentLayerParams::kSequenceOutputFieldNumber;
const int SimpleRecurrentLayerParams::kHasBiasVectorFieldNumber;
const int SimpleRecurrentLayerParams::kWeightMatrixFieldNumber;
const int SimpleRecurrentLayerParams::kRecursionMatrixFieldNumber;
const int SimpleRecurrentLayerParams::kBiasVectorFieldNumber;
const int SimpleRecurrentLayerParams::kReverseInputFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SimpleRecurrentLayerParams::SimpleRecurrentLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SimpleRecurrentLayerParams)
}
SimpleRecurrentLayerParams::SimpleRecurrentLayerParams(const SimpleRecurrentLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_activation()) {
    activation_ = new ::CoreML::Specification::ActivationParams(*from.activation_);
  } else {
    activation_ = NULL;
  }
  if (from.has_weightmatrix()) {
    weightmatrix_ = new ::CoreML::Specification::WeightParams(*from.weightmatrix_);
  } else {
    weightmatrix_ = NULL;
  }
  if (from.has_recursionmatrix()) {
    recursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.recursionmatrix_);
  } else {
    recursionmatrix_ = NULL;
  }
  if (from.has_biasvector()) {
    biasvector_ = new ::CoreML::Specification::WeightParams(*from.biasvector_);
  } else {
    biasvector_ = NULL;
  }
  ::memcpy(&inputvectorsize_, &from.inputvectorsize_,
    reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(reverseinput_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SimpleRecurrentLayerParams)
}

void SimpleRecurrentLayerParams::SharedCtor() {
  ::memset(&activation_, 0, reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&activation_) + sizeof(reverseinput_));
  _cached_size_ = 0;
}

SimpleRecurrentLayerParams::~SimpleRecurrentLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SimpleRecurrentLayerParams)
  SharedDtor();
}

void SimpleRecurrentLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete activation_;
  }
  if (this != internal_default_instance()) {
    delete weightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete recursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete biasvector_;
  }
}

void SimpleRecurrentLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SimpleRecurrentLayerParams& SimpleRecurrentLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SimpleRecurrentLayerParams* SimpleRecurrentLayerParams::New(::google::protobuf::Arena* arena) const {
  SimpleRecurrentLayerParams* n = new SimpleRecurrentLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SimpleRecurrentLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SimpleRecurrentLayerParams)
  if (GetArenaNoVirtual() == NULL && activation_ != NULL) {
    delete activation_;
  }
  activation_ = NULL;
  if (GetArenaNoVirtual() == NULL && weightmatrix_ != NULL) {
    delete weightmatrix_;
  }
  weightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && recursionmatrix_ != NULL) {
    delete recursionmatrix_;
  }
  recursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && biasvector_ != NULL) {
    delete biasvector_;
  }
  biasvector_ = NULL;
  ::memset(&inputvectorsize_, 0, reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(reverseinput_));
}

bool SimpleRecurrentLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SimpleRecurrentLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 inputVectorSize = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 outputVectorSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationParams activation = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_activation()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool sequenceOutput = 15;
      case 15: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(120u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &sequenceoutput_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBiasVector = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(160u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbiasvector_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams weightMatrix = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams recursionMatrix = 31;
      case 31: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(250u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_recursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams biasVector = 32;
      case 32: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(258u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_biasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool reverseInput = 100;
      case 100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(800u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reverseinput_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SimpleRecurrentLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SimpleRecurrentLayerParams)
  return false;
#undef DO_
}

void SimpleRecurrentLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SimpleRecurrentLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputvectorsize(), output);
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputvectorsize(), output);
  }

  // .CoreML.Specification.ActivationParams activation = 10;
  if (this->has_activation()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *this->activation_, output);
  }

  // bool sequenceOutput = 15;
  if (this->sequenceoutput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(15, this->sequenceoutput(), output);
  }

  // bool hasBiasVector = 20;
  if (this->hasbiasvector() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(20, this->hasbiasvector(), output);
  }

  // .CoreML.Specification.WeightParams weightMatrix = 30;
  if (this->has_weightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      30, *this->weightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams recursionMatrix = 31;
  if (this->has_recursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      31, *this->recursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams biasVector = 32;
  if (this->has_biasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      32, *this->biasvector_, output);
  }

  // bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(100, this->reverseinput(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SimpleRecurrentLayerParams)
}

size_t SimpleRecurrentLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SimpleRecurrentLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.ActivationParams activation = 10;
  if (this->has_activation()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->activation_);
  }

  // .CoreML.Specification.WeightParams weightMatrix = 30;
  if (this->has_weightmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weightmatrix_);
  }

  // .CoreML.Specification.WeightParams recursionMatrix = 31;
  if (this->has_recursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->recursionmatrix_);
  }

  // .CoreML.Specification.WeightParams biasVector = 32;
  if (this->has_biasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->biasvector_);
  }

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputvectorsize());
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputvectorsize());
  }

  // bool sequenceOutput = 15;
  if (this->sequenceoutput() != 0) {
    total_size += 1 + 1;
  }

  // bool hasBiasVector = 20;
  if (this->hasbiasvector() != 0) {
    total_size += 2 + 1;
  }

  // bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    total_size += 2 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SimpleRecurrentLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SimpleRecurrentLayerParams*>(&from));
}

void SimpleRecurrentLayerParams::MergeFrom(const SimpleRecurrentLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SimpleRecurrentLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_activation()) {
    mutable_activation()->::CoreML::Specification::ActivationParams::MergeFrom(from.activation());
  }
  if (from.has_weightmatrix()) {
    mutable_weightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.weightmatrix());
  }
  if (from.has_recursionmatrix()) {
    mutable_recursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.recursionmatrix());
  }
  if (from.has_biasvector()) {
    mutable_biasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.biasvector());
  }
  if (from.inputvectorsize() != 0) {
    set_inputvectorsize(from.inputvectorsize());
  }
  if (from.outputvectorsize() != 0) {
    set_outputvectorsize(from.outputvectorsize());
  }
  if (from.sequenceoutput() != 0) {
    set_sequenceoutput(from.sequenceoutput());
  }
  if (from.hasbiasvector() != 0) {
    set_hasbiasvector(from.hasbiasvector());
  }
  if (from.reverseinput() != 0) {
    set_reverseinput(from.reverseinput());
  }
}

void SimpleRecurrentLayerParams::CopyFrom(const SimpleRecurrentLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SimpleRecurrentLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SimpleRecurrentLayerParams::IsInitialized() const {
  return true;
}

void SimpleRecurrentLayerParams::Swap(SimpleRecurrentLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SimpleRecurrentLayerParams::InternalSwap(SimpleRecurrentLayerParams* other) {
  std::swap(activation_, other->activation_);
  std::swap(weightmatrix_, other->weightmatrix_);
  std::swap(recursionmatrix_, other->recursionmatrix_);
  std::swap(biasvector_, other->biasvector_);
  std::swap(inputvectorsize_, other->inputvectorsize_);
  std::swap(outputvectorsize_, other->outputvectorsize_);
  std::swap(sequenceoutput_, other->sequenceoutput_);
  std::swap(hasbiasvector_, other->hasbiasvector_);
  std::swap(reverseinput_, other->reverseinput_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SimpleRecurrentLayerParams::GetTypeName() const {
  return "CoreML.Specification.SimpleRecurrentLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SimpleRecurrentLayerParams

// uint64 inputVectorSize = 1;
void SimpleRecurrentLayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SimpleRecurrentLayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.inputVectorSize)
  return inputvectorsize_;
}
void SimpleRecurrentLayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {
  
  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.inputVectorSize)
}

// uint64 outputVectorSize = 2;
void SimpleRecurrentLayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SimpleRecurrentLayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.outputVectorSize)
  return outputvectorsize_;
}
void SimpleRecurrentLayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {
  
  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.outputVectorSize)
}

// .CoreML.Specification.ActivationParams activation = 10;
bool SimpleRecurrentLayerParams::has_activation() const {
  return this != internal_default_instance() && activation_ != NULL;
}
void SimpleRecurrentLayerParams::clear_activation() {
  if (GetArenaNoVirtual() == NULL && activation_ != NULL) delete activation_;
  activation_ = NULL;
}
const ::CoreML::Specification::ActivationParams& SimpleRecurrentLayerParams::activation() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.activation)
  return activation_ != NULL ? *activation_
                         : *::CoreML::Specification::ActivationParams::internal_default_instance();
}
::CoreML::Specification::ActivationParams* SimpleRecurrentLayerParams::mutable_activation() {
  
  if (activation_ == NULL) {
    activation_ = new ::CoreML::Specification::ActivationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.activation)
  return activation_;
}
::CoreML::Specification::ActivationParams* SimpleRecurrentLayerParams::release_activation() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.activation)
  
  ::CoreML::Specification::ActivationParams* temp = activation_;
  activation_ = NULL;
  return temp;
}
void SimpleRecurrentLayerParams::set_allocated_activation(::CoreML::Specification::ActivationParams* activation) {
  delete activation_;
  activation_ = activation;
  if (activation) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.activation)
}

// bool sequenceOutput = 15;
void SimpleRecurrentLayerParams::clear_sequenceoutput() {
  sequenceoutput_ = false;
}
bool SimpleRecurrentLayerParams::sequenceoutput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.sequenceOutput)
  return sequenceoutput_;
}
void SimpleRecurrentLayerParams::set_sequenceoutput(bool value) {
  
  sequenceoutput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.sequenceOutput)
}

// bool hasBiasVector = 20;
void SimpleRecurrentLayerParams::clear_hasbiasvector() {
  hasbiasvector_ = false;
}
bool SimpleRecurrentLayerParams::hasbiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.hasBiasVector)
  return hasbiasvector_;
}
void SimpleRecurrentLayerParams::set_hasbiasvector(bool value) {
  
  hasbiasvector_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.hasBiasVector)
}

// .CoreML.Specification.WeightParams weightMatrix = 30;
bool SimpleRecurrentLayerParams::has_weightmatrix() const {
  return this != internal_default_instance() && weightmatrix_ != NULL;
}
void SimpleRecurrentLayerParams::clear_weightmatrix() {
  if (GetArenaNoVirtual() == NULL && weightmatrix_ != NULL) delete weightmatrix_;
  weightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::weightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
  return weightmatrix_ != NULL ? *weightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::mutable_weightmatrix() {
  
  if (weightmatrix_ == NULL) {
    weightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
  return weightmatrix_;
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::release_weightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = weightmatrix_;
  weightmatrix_ = NULL;
  return temp;
}
void SimpleRecurrentLayerParams::set_allocated_weightmatrix(::CoreML::Specification::WeightParams* weightmatrix) {
  delete weightmatrix_;
  weightmatrix_ = weightmatrix;
  if (weightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
}

// .CoreML.Specification.WeightParams recursionMatrix = 31;
bool SimpleRecurrentLayerParams::has_recursionmatrix() const {
  return this != internal_default_instance() && recursionmatrix_ != NULL;
}
void SimpleRecurrentLayerParams::clear_recursionmatrix() {
  if (GetArenaNoVirtual() == NULL && recursionmatrix_ != NULL) delete recursionmatrix_;
  recursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::recursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
  return recursionmatrix_ != NULL ? *recursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::mutable_recursionmatrix() {
  
  if (recursionmatrix_ == NULL) {
    recursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
  return recursionmatrix_;
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::release_recursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = recursionmatrix_;
  recursionmatrix_ = NULL;
  return temp;
}
void SimpleRecurrentLayerParams::set_allocated_recursionmatrix(::CoreML::Specification::WeightParams* recursionmatrix) {
  delete recursionmatrix_;
  recursionmatrix_ = recursionmatrix;
  if (recursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
}

// .CoreML.Specification.WeightParams biasVector = 32;
bool SimpleRecurrentLayerParams::has_biasvector() const {
  return this != internal_default_instance() && biasvector_ != NULL;
}
void SimpleRecurrentLayerParams::clear_biasvector() {
  if (GetArenaNoVirtual() == NULL && biasvector_ != NULL) delete biasvector_;
  biasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::biasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
  return biasvector_ != NULL ? *biasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::mutable_biasvector() {
  
  if (biasvector_ == NULL) {
    biasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
  return biasvector_;
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::release_biasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
  
  ::CoreML::Specification::WeightParams* temp = biasvector_;
  biasvector_ = NULL;
  return temp;
}
void SimpleRecurrentLayerParams::set_allocated_biasvector(::CoreML::Specification::WeightParams* biasvector) {
  delete biasvector_;
  biasvector_ = biasvector;
  if (biasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
}

// bool reverseInput = 100;
void SimpleRecurrentLayerParams::clear_reverseinput() {
  reverseinput_ = false;
}
bool SimpleRecurrentLayerParams::reverseinput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.reverseInput)
  return reverseinput_;
}
void SimpleRecurrentLayerParams::set_reverseinput(bool value) {
  
  reverseinput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.reverseInput)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int GRULayerParams::kInputVectorSizeFieldNumber;
const int GRULayerParams::kOutputVectorSizeFieldNumber;
const int GRULayerParams::kActivationsFieldNumber;
const int GRULayerParams::kSequenceOutputFieldNumber;
const int GRULayerParams::kHasBiasVectorsFieldNumber;
const int GRULayerParams::kUpdateGateWeightMatrixFieldNumber;
const int GRULayerParams::kResetGateWeightMatrixFieldNumber;
const int GRULayerParams::kOutputGateWeightMatrixFieldNumber;
const int GRULayerParams::kUpdateGateRecursionMatrixFieldNumber;
const int GRULayerParams::kResetGateRecursionMatrixFieldNumber;
const int GRULayerParams::kOutputGateRecursionMatrixFieldNumber;
const int GRULayerParams::kUpdateGateBiasVectorFieldNumber;
const int GRULayerParams::kResetGateBiasVectorFieldNumber;
const int GRULayerParams::kOutputGateBiasVectorFieldNumber;
const int GRULayerParams::kReverseInputFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

GRULayerParams::GRULayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.GRULayerParams)
}
GRULayerParams::GRULayerParams(const GRULayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      activations_(from.activations_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_updategateweightmatrix()) {
    updategateweightmatrix_ = new ::CoreML::Specification::WeightParams(*from.updategateweightmatrix_);
  } else {
    updategateweightmatrix_ = NULL;
  }
  if (from.has_resetgateweightmatrix()) {
    resetgateweightmatrix_ = new ::CoreML::Specification::WeightParams(*from.resetgateweightmatrix_);
  } else {
    resetgateweightmatrix_ = NULL;
  }
  if (from.has_outputgateweightmatrix()) {
    outputgateweightmatrix_ = new ::CoreML::Specification::WeightParams(*from.outputgateweightmatrix_);
  } else {
    outputgateweightmatrix_ = NULL;
  }
  if (from.has_updategaterecursionmatrix()) {
    updategaterecursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.updategaterecursionmatrix_);
  } else {
    updategaterecursionmatrix_ = NULL;
  }
  if (from.has_resetgaterecursionmatrix()) {
    resetgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.resetgaterecursionmatrix_);
  } else {
    resetgaterecursionmatrix_ = NULL;
  }
  if (from.has_outputgaterecursionmatrix()) {
    outputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.outputgaterecursionmatrix_);
  } else {
    outputgaterecursionmatrix_ = NULL;
  }
  if (from.has_updategatebiasvector()) {
    updategatebiasvector_ = new ::CoreML::Specification::WeightParams(*from.updategatebiasvector_);
  } else {
    updategatebiasvector_ = NULL;
  }
  if (from.has_resetgatebiasvector()) {
    resetgatebiasvector_ = new ::CoreML::Specification::WeightParams(*from.resetgatebiasvector_);
  } else {
    resetgatebiasvector_ = NULL;
  }
  if (from.has_outputgatebiasvector()) {
    outputgatebiasvector_ = new ::CoreML::Specification::WeightParams(*from.outputgatebiasvector_);
  } else {
    outputgatebiasvector_ = NULL;
  }
  ::memcpy(&inputvectorsize_, &from.inputvectorsize_,
    reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(reverseinput_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.GRULayerParams)
}

void GRULayerParams::SharedCtor() {
  ::memset(&updategateweightmatrix_, 0, reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&updategateweightmatrix_) + sizeof(reverseinput_));
  _cached_size_ = 0;
}

GRULayerParams::~GRULayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.GRULayerParams)
  SharedDtor();
}

void GRULayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete updategateweightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete resetgateweightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete outputgateweightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete updategaterecursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete resetgaterecursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete outputgaterecursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete updategatebiasvector_;
  }
  if (this != internal_default_instance()) {
    delete resetgatebiasvector_;
  }
  if (this != internal_default_instance()) {
    delete outputgatebiasvector_;
  }
}

void GRULayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const GRULayerParams& GRULayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

GRULayerParams* GRULayerParams::New(::google::protobuf::Arena* arena) const {
  GRULayerParams* n = new GRULayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void GRULayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.GRULayerParams)
  activations_.Clear();
  if (GetArenaNoVirtual() == NULL && updategateweightmatrix_ != NULL) {
    delete updategateweightmatrix_;
  }
  updategateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && resetgateweightmatrix_ != NULL) {
    delete resetgateweightmatrix_;
  }
  resetgateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgateweightmatrix_ != NULL) {
    delete outputgateweightmatrix_;
  }
  outputgateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && updategaterecursionmatrix_ != NULL) {
    delete updategaterecursionmatrix_;
  }
  updategaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && resetgaterecursionmatrix_ != NULL) {
    delete resetgaterecursionmatrix_;
  }
  resetgaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgaterecursionmatrix_ != NULL) {
    delete outputgaterecursionmatrix_;
  }
  outputgaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && updategatebiasvector_ != NULL) {
    delete updategatebiasvector_;
  }
  updategatebiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && resetgatebiasvector_ != NULL) {
    delete resetgatebiasvector_;
  }
  resetgatebiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgatebiasvector_ != NULL) {
    delete outputgatebiasvector_;
  }
  outputgatebiasvector_ = NULL;
  ::memset(&inputvectorsize_, 0, reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(reverseinput_));
}

bool GRULayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.GRULayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 inputVectorSize = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 outputVectorSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.ActivationParams activations = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_activations()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool sequenceOutput = 15;
      case 15: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(120u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &sequenceoutput_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBiasVectors = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(160u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbiasvectors_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_updategateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
      case 31: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(250u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_resetgateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
      case 32: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(258u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
      case 50: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(402u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_updategaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
      case 51: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(410u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_resetgaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
      case 52: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(418u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams updateGateBiasVector = 70;
      case 70: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(562u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_updategatebiasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams resetGateBiasVector = 71;
      case 71: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(570u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_resetgatebiasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams outputGateBiasVector = 72;
      case 72: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(578u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgatebiasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool reverseInput = 100;
      case 100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(800u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reverseinput_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.GRULayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.GRULayerParams)
  return false;
#undef DO_
}

void GRULayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.GRULayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputvectorsize(), output);
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputvectorsize(), output);
  }

  // repeated .CoreML.Specification.ActivationParams activations = 10;
  for (unsigned int i = 0, n = this->activations_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, this->activations(i), output);
  }

  // bool sequenceOutput = 15;
  if (this->sequenceoutput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(15, this->sequenceoutput(), output);
  }

  // bool hasBiasVectors = 20;
  if (this->hasbiasvectors() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(20, this->hasbiasvectors(), output);
  }

  // .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
  if (this->has_updategateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      30, *this->updategateweightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
  if (this->has_resetgateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      31, *this->resetgateweightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
  if (this->has_outputgateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      32, *this->outputgateweightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
  if (this->has_updategaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      50, *this->updategaterecursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
  if (this->has_resetgaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      51, *this->resetgaterecursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
  if (this->has_outputgaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      52, *this->outputgaterecursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams updateGateBiasVector = 70;
  if (this->has_updategatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      70, *this->updategatebiasvector_, output);
  }

  // .CoreML.Specification.WeightParams resetGateBiasVector = 71;
  if (this->has_resetgatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      71, *this->resetgatebiasvector_, output);
  }

  // .CoreML.Specification.WeightParams outputGateBiasVector = 72;
  if (this->has_outputgatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      72, *this->outputgatebiasvector_, output);
  }

  // bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(100, this->reverseinput(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.GRULayerParams)
}

size_t GRULayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.GRULayerParams)
  size_t total_size = 0;

  // repeated .CoreML.Specification.ActivationParams activations = 10;
  {
    unsigned int count = this->activations_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->activations(i));
    }
  }

  // .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
  if (this->has_updategateweightmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->updategateweightmatrix_);
  }

  // .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
  if (this->has_resetgateweightmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->resetgateweightmatrix_);
  }

  // .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
  if (this->has_outputgateweightmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgateweightmatrix_);
  }

  // .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
  if (this->has_updategaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->updategaterecursionmatrix_);
  }

  // .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
  if (this->has_resetgaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->resetgaterecursionmatrix_);
  }

  // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
  if (this->has_outputgaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgaterecursionmatrix_);
  }

  // .CoreML.Specification.WeightParams updateGateBiasVector = 70;
  if (this->has_updategatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->updategatebiasvector_);
  }

  // .CoreML.Specification.WeightParams resetGateBiasVector = 71;
  if (this->has_resetgatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->resetgatebiasvector_);
  }

  // .CoreML.Specification.WeightParams outputGateBiasVector = 72;
  if (this->has_outputgatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgatebiasvector_);
  }

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputvectorsize());
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputvectorsize());
  }

  // bool sequenceOutput = 15;
  if (this->sequenceoutput() != 0) {
    total_size += 1 + 1;
  }

  // bool hasBiasVectors = 20;
  if (this->hasbiasvectors() != 0) {
    total_size += 2 + 1;
  }

  // bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    total_size += 2 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void GRULayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const GRULayerParams*>(&from));
}

void GRULayerParams::MergeFrom(const GRULayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.GRULayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  activations_.MergeFrom(from.activations_);
  if (from.has_updategateweightmatrix()) {
    mutable_updategateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.updategateweightmatrix());
  }
  if (from.has_resetgateweightmatrix()) {
    mutable_resetgateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.resetgateweightmatrix());
  }
  if (from.has_outputgateweightmatrix()) {
    mutable_outputgateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgateweightmatrix());
  }
  if (from.has_updategaterecursionmatrix()) {
    mutable_updategaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.updategaterecursionmatrix());
  }
  if (from.has_resetgaterecursionmatrix()) {
    mutable_resetgaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.resetgaterecursionmatrix());
  }
  if (from.has_outputgaterecursionmatrix()) {
    mutable_outputgaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgaterecursionmatrix());
  }
  if (from.has_updategatebiasvector()) {
    mutable_updategatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.updategatebiasvector());
  }
  if (from.has_resetgatebiasvector()) {
    mutable_resetgatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.resetgatebiasvector());
  }
  if (from.has_outputgatebiasvector()) {
    mutable_outputgatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgatebiasvector());
  }
  if (from.inputvectorsize() != 0) {
    set_inputvectorsize(from.inputvectorsize());
  }
  if (from.outputvectorsize() != 0) {
    set_outputvectorsize(from.outputvectorsize());
  }
  if (from.sequenceoutput() != 0) {
    set_sequenceoutput(from.sequenceoutput());
  }
  if (from.hasbiasvectors() != 0) {
    set_hasbiasvectors(from.hasbiasvectors());
  }
  if (from.reverseinput() != 0) {
    set_reverseinput(from.reverseinput());
  }
}

void GRULayerParams::CopyFrom(const GRULayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.GRULayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool GRULayerParams::IsInitialized() const {
  return true;
}

void GRULayerParams::Swap(GRULayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void GRULayerParams::InternalSwap(GRULayerParams* other) {
  activations_.InternalSwap(&other->activations_);
  std::swap(updategateweightmatrix_, other->updategateweightmatrix_);
  std::swap(resetgateweightmatrix_, other->resetgateweightmatrix_);
  std::swap(outputgateweightmatrix_, other->outputgateweightmatrix_);
  std::swap(updategaterecursionmatrix_, other->updategaterecursionmatrix_);
  std::swap(resetgaterecursionmatrix_, other->resetgaterecursionmatrix_);
  std::swap(outputgaterecursionmatrix_, other->outputgaterecursionmatrix_);
  std::swap(updategatebiasvector_, other->updategatebiasvector_);
  std::swap(resetgatebiasvector_, other->resetgatebiasvector_);
  std::swap(outputgatebiasvector_, other->outputgatebiasvector_);
  std::swap(inputvectorsize_, other->inputvectorsize_);
  std::swap(outputvectorsize_, other->outputvectorsize_);
  std::swap(sequenceoutput_, other->sequenceoutput_);
  std::swap(hasbiasvectors_, other->hasbiasvectors_);
  std::swap(reverseinput_, other->reverseinput_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string GRULayerParams::GetTypeName() const {
  return "CoreML.Specification.GRULayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// GRULayerParams

// uint64 inputVectorSize = 1;
void GRULayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 GRULayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.inputVectorSize)
  return inputvectorsize_;
}
void GRULayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {
  
  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.inputVectorSize)
}

// uint64 outputVectorSize = 2;
void GRULayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 GRULayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputVectorSize)
  return outputvectorsize_;
}
void GRULayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {
  
  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.outputVectorSize)
}

// repeated .CoreML.Specification.ActivationParams activations = 10;
int GRULayerParams::activations_size() const {
  return activations_.size();
}
void GRULayerParams::clear_activations() {
  activations_.Clear();
}
const ::CoreML::Specification::ActivationParams& GRULayerParams::activations(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.activations)
  return activations_.Get(index);
}
::CoreML::Specification::ActivationParams* GRULayerParams::mutable_activations(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.activations)
  return activations_.Mutable(index);
}
::CoreML::Specification::ActivationParams* GRULayerParams::add_activations() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.GRULayerParams.activations)
  return activations_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
GRULayerParams::mutable_activations() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.GRULayerParams.activations)
  return &activations_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
GRULayerParams::activations() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.GRULayerParams.activations)
  return activations_;
}

// bool sequenceOutput = 15;
void GRULayerParams::clear_sequenceoutput() {
  sequenceoutput_ = false;
}
bool GRULayerParams::sequenceoutput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.sequenceOutput)
  return sequenceoutput_;
}
void GRULayerParams::set_sequenceoutput(bool value) {
  
  sequenceoutput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.sequenceOutput)
}

// bool hasBiasVectors = 20;
void GRULayerParams::clear_hasbiasvectors() {
  hasbiasvectors_ = false;
}
bool GRULayerParams::hasbiasvectors() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.hasBiasVectors)
  return hasbiasvectors_;
}
void GRULayerParams::set_hasbiasvectors(bool value) {
  
  hasbiasvectors_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.hasBiasVectors)
}

// .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
bool GRULayerParams::has_updategateweightmatrix() const {
  return this != internal_default_instance() && updategateweightmatrix_ != NULL;
}
void GRULayerParams::clear_updategateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && updategateweightmatrix_ != NULL) delete updategateweightmatrix_;
  updategateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::updategateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
  return updategateweightmatrix_ != NULL ? *updategateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_updategateweightmatrix() {
  
  if (updategateweightmatrix_ == NULL) {
    updategateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
  return updategateweightmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_updategateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = updategateweightmatrix_;
  updategateweightmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_updategateweightmatrix(::CoreML::Specification::WeightParams* updategateweightmatrix) {
  delete updategateweightmatrix_;
  updategateweightmatrix_ = updategateweightmatrix;
  if (updategateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
}

// .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
bool GRULayerParams::has_resetgateweightmatrix() const {
  return this != internal_default_instance() && resetgateweightmatrix_ != NULL;
}
void GRULayerParams::clear_resetgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && resetgateweightmatrix_ != NULL) delete resetgateweightmatrix_;
  resetgateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::resetgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
  return resetgateweightmatrix_ != NULL ? *resetgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_resetgateweightmatrix() {
  
  if (resetgateweightmatrix_ == NULL) {
    resetgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
  return resetgateweightmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_resetgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = resetgateweightmatrix_;
  resetgateweightmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_resetgateweightmatrix(::CoreML::Specification::WeightParams* resetgateweightmatrix) {
  delete resetgateweightmatrix_;
  resetgateweightmatrix_ = resetgateweightmatrix;
  if (resetgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
}

// .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
bool GRULayerParams::has_outputgateweightmatrix() const {
  return this != internal_default_instance() && outputgateweightmatrix_ != NULL;
}
void GRULayerParams::clear_outputgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgateweightmatrix_ != NULL) delete outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::outputgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
  return outputgateweightmatrix_ != NULL ? *outputgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_outputgateweightmatrix() {
  
  if (outputgateweightmatrix_ == NULL) {
    outputgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
  return outputgateweightmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_outputgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_outputgateweightmatrix(::CoreML::Specification::WeightParams* outputgateweightmatrix) {
  delete outputgateweightmatrix_;
  outputgateweightmatrix_ = outputgateweightmatrix;
  if (outputgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
}

// .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
bool GRULayerParams::has_updategaterecursionmatrix() const {
  return this != internal_default_instance() && updategaterecursionmatrix_ != NULL;
}
void GRULayerParams::clear_updategaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && updategaterecursionmatrix_ != NULL) delete updategaterecursionmatrix_;
  updategaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::updategaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
  return updategaterecursionmatrix_ != NULL ? *updategaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_updategaterecursionmatrix() {
  
  if (updategaterecursionmatrix_ == NULL) {
    updategaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
  return updategaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_updategaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = updategaterecursionmatrix_;
  updategaterecursionmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_updategaterecursionmatrix(::CoreML::Specification::WeightParams* updategaterecursionmatrix) {
  delete updategaterecursionmatrix_;
  updategaterecursionmatrix_ = updategaterecursionmatrix;
  if (updategaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
bool GRULayerParams::has_resetgaterecursionmatrix() const {
  return this != internal_default_instance() && resetgaterecursionmatrix_ != NULL;
}
void GRULayerParams::clear_resetgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && resetgaterecursionmatrix_ != NULL) delete resetgaterecursionmatrix_;
  resetgaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::resetgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
  return resetgaterecursionmatrix_ != NULL ? *resetgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_resetgaterecursionmatrix() {
  
  if (resetgaterecursionmatrix_ == NULL) {
    resetgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
  return resetgaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_resetgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = resetgaterecursionmatrix_;
  resetgaterecursionmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_resetgaterecursionmatrix(::CoreML::Specification::WeightParams* resetgaterecursionmatrix) {
  delete resetgaterecursionmatrix_;
  resetgaterecursionmatrix_ = resetgaterecursionmatrix;
  if (resetgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
bool GRULayerParams::has_outputgaterecursionmatrix() const {
  return this != internal_default_instance() && outputgaterecursionmatrix_ != NULL;
}
void GRULayerParams::clear_outputgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgaterecursionmatrix_ != NULL) delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::outputgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_ != NULL ? *outputgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_outputgaterecursionmatrix() {
  
  if (outputgaterecursionmatrix_ == NULL) {
    outputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_outputgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_outputgaterecursionmatrix(::CoreML::Specification::WeightParams* outputgaterecursionmatrix) {
  delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = outputgaterecursionmatrix;
  if (outputgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams updateGateBiasVector = 70;
bool GRULayerParams::has_updategatebiasvector() const {
  return this != internal_default_instance() && updategatebiasvector_ != NULL;
}
void GRULayerParams::clear_updategatebiasvector() {
  if (GetArenaNoVirtual() == NULL && updategatebiasvector_ != NULL) delete updategatebiasvector_;
  updategatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::updategatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.updateGateBiasVector)
  return updategatebiasvector_ != NULL ? *updategatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_updategatebiasvector() {
  
  if (updategatebiasvector_ == NULL) {
    updategatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.updateGateBiasVector)
  return updategatebiasvector_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_updategatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.updateGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = updategatebiasvector_;
  updategatebiasvector_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_updategatebiasvector(::CoreML::Specification::WeightParams* updategatebiasvector) {
  delete updategatebiasvector_;
  updategatebiasvector_ = updategatebiasvector;
  if (updategatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.updateGateBiasVector)
}

// .CoreML.Specification.WeightParams resetGateBiasVector = 71;
bool GRULayerParams::has_resetgatebiasvector() const {
  return this != internal_default_instance() && resetgatebiasvector_ != NULL;
}
void GRULayerParams::clear_resetgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && resetgatebiasvector_ != NULL) delete resetgatebiasvector_;
  resetgatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::resetgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.resetGateBiasVector)
  return resetgatebiasvector_ != NULL ? *resetgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_resetgatebiasvector() {
  
  if (resetgatebiasvector_ == NULL) {
    resetgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.resetGateBiasVector)
  return resetgatebiasvector_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_resetgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.resetGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = resetgatebiasvector_;
  resetgatebiasvector_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_resetgatebiasvector(::CoreML::Specification::WeightParams* resetgatebiasvector) {
  delete resetgatebiasvector_;
  resetgatebiasvector_ = resetgatebiasvector;
  if (resetgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.resetGateBiasVector)
}

// .CoreML.Specification.WeightParams outputGateBiasVector = 72;
bool GRULayerParams::has_outputgatebiasvector() const {
  return this != internal_default_instance() && outputgatebiasvector_ != NULL;
}
void GRULayerParams::clear_outputgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && outputgatebiasvector_ != NULL) delete outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::outputgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputGateBiasVector)
  return outputgatebiasvector_ != NULL ? *outputgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_outputgatebiasvector() {
  
  if (outputgatebiasvector_ == NULL) {
    outputgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.outputGateBiasVector)
  return outputgatebiasvector_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_outputgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.outputGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_outputgatebiasvector(::CoreML::Specification::WeightParams* outputgatebiasvector) {
  delete outputgatebiasvector_;
  outputgatebiasvector_ = outputgatebiasvector;
  if (outputgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.outputGateBiasVector)
}

// bool reverseInput = 100;
void GRULayerParams::clear_reverseinput() {
  reverseinput_ = false;
}
bool GRULayerParams::reverseinput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.reverseInput)
  return reverseinput_;
}
void GRULayerParams::set_reverseinput(bool value) {
  
  reverseinput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.reverseInput)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LSTMParams::kSequenceOutputFieldNumber;
const int LSTMParams::kHasBiasVectorsFieldNumber;
const int LSTMParams::kForgetBiasFieldNumber;
const int LSTMParams::kHasPeepholeVectorsFieldNumber;
const int LSTMParams::kCoupledInputAndForgetGateFieldNumber;
const int LSTMParams::kCellClipThresholdFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LSTMParams::LSTMParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LSTMParams)
}
LSTMParams::LSTMParams(const LSTMParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&cellclipthreshold_, &from.cellclipthreshold_,
    reinterpret_cast<char*>(&coupledinputandforgetgate_) -
    reinterpret_cast<char*>(&cellclipthreshold_) + sizeof(coupledinputandforgetgate_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LSTMParams)
}

void LSTMParams::SharedCtor() {
  ::memset(&cellclipthreshold_, 0, reinterpret_cast<char*>(&coupledinputandforgetgate_) -
    reinterpret_cast<char*>(&cellclipthreshold_) + sizeof(coupledinputandforgetgate_));
  _cached_size_ = 0;
}

LSTMParams::~LSTMParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LSTMParams)
  SharedDtor();
}

void LSTMParams::SharedDtor() {
}

void LSTMParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LSTMParams& LSTMParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LSTMParams* LSTMParams::New(::google::protobuf::Arena* arena) const {
  LSTMParams* n = new LSTMParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LSTMParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LSTMParams)
  ::memset(&cellclipthreshold_, 0, reinterpret_cast<char*>(&coupledinputandforgetgate_) -
    reinterpret_cast<char*>(&cellclipthreshold_) + sizeof(coupledinputandforgetgate_));
}

bool LSTMParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LSTMParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // bool sequenceOutput = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(80u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &sequenceoutput_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBiasVectors = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(160u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbiasvectors_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool forgetBias = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(240u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &forgetbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasPeepholeVectors = 40;
      case 40: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(320u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &haspeepholevectors_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool coupledInputAndForgetGate = 50;
      case 50: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(400u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &coupledinputandforgetgate_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float cellClipThreshold = 60;
      case 60: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(485u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &cellclipthreshold_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LSTMParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LSTMParams)
  return false;
#undef DO_
}

void LSTMParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LSTMParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // bool sequenceOutput = 10;
  if (this->sequenceoutput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(10, this->sequenceoutput(), output);
  }

  // bool hasBiasVectors = 20;
  if (this->hasbiasvectors() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(20, this->hasbiasvectors(), output);
  }

  // bool forgetBias = 30;
  if (this->forgetbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(30, this->forgetbias(), output);
  }

  // bool hasPeepholeVectors = 40;
  if (this->haspeepholevectors() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(40, this->haspeepholevectors(), output);
  }

  // bool coupledInputAndForgetGate = 50;
  if (this->coupledinputandforgetgate() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(50, this->coupledinputandforgetgate(), output);
  }

  // float cellClipThreshold = 60;
  if (this->cellclipthreshold() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(60, this->cellclipthreshold(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LSTMParams)
}

size_t LSTMParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LSTMParams)
  size_t total_size = 0;

  // float cellClipThreshold = 60;
  if (this->cellclipthreshold() != 0) {
    total_size += 2 + 4;
  }

  // bool sequenceOutput = 10;
  if (this->sequenceoutput() != 0) {
    total_size += 1 + 1;
  }

  // bool hasBiasVectors = 20;
  if (this->hasbiasvectors() != 0) {
    total_size += 2 + 1;
  }

  // bool forgetBias = 30;
  if (this->forgetbias() != 0) {
    total_size += 2 + 1;
  }

  // bool hasPeepholeVectors = 40;
  if (this->haspeepholevectors() != 0) {
    total_size += 2 + 1;
  }

  // bool coupledInputAndForgetGate = 50;
  if (this->coupledinputandforgetgate() != 0) {
    total_size += 2 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LSTMParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LSTMParams*>(&from));
}

void LSTMParams::MergeFrom(const LSTMParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LSTMParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.cellclipthreshold() != 0) {
    set_cellclipthreshold(from.cellclipthreshold());
  }
  if (from.sequenceoutput() != 0) {
    set_sequenceoutput(from.sequenceoutput());
  }
  if (from.hasbiasvectors() != 0) {
    set_hasbiasvectors(from.hasbiasvectors());
  }
  if (from.forgetbias() != 0) {
    set_forgetbias(from.forgetbias());
  }
  if (from.haspeepholevectors() != 0) {
    set_haspeepholevectors(from.haspeepholevectors());
  }
  if (from.coupledinputandforgetgate() != 0) {
    set_coupledinputandforgetgate(from.coupledinputandforgetgate());
  }
}

void LSTMParams::CopyFrom(const LSTMParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LSTMParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LSTMParams::IsInitialized() const {
  return true;
}

void LSTMParams::Swap(LSTMParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LSTMParams::InternalSwap(LSTMParams* other) {
  std::swap(cellclipthreshold_, other->cellclipthreshold_);
  std::swap(sequenceoutput_, other->sequenceoutput_);
  std::swap(hasbiasvectors_, other->hasbiasvectors_);
  std::swap(forgetbias_, other->forgetbias_);
  std::swap(haspeepholevectors_, other->haspeepholevectors_);
  std::swap(coupledinputandforgetgate_, other->coupledinputandforgetgate_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LSTMParams::GetTypeName() const {
  return "CoreML.Specification.LSTMParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LSTMParams

// bool sequenceOutput = 10;
void LSTMParams::clear_sequenceoutput() {
  sequenceoutput_ = false;
}
bool LSTMParams::sequenceoutput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.sequenceOutput)
  return sequenceoutput_;
}
void LSTMParams::set_sequenceoutput(bool value) {
  
  sequenceoutput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.sequenceOutput)
}

// bool hasBiasVectors = 20;
void LSTMParams::clear_hasbiasvectors() {
  hasbiasvectors_ = false;
}
bool LSTMParams::hasbiasvectors() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.hasBiasVectors)
  return hasbiasvectors_;
}
void LSTMParams::set_hasbiasvectors(bool value) {
  
  hasbiasvectors_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.hasBiasVectors)
}

// bool forgetBias = 30;
void LSTMParams::clear_forgetbias() {
  forgetbias_ = false;
}
bool LSTMParams::forgetbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.forgetBias)
  return forgetbias_;
}
void LSTMParams::set_forgetbias(bool value) {
  
  forgetbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.forgetBias)
}

// bool hasPeepholeVectors = 40;
void LSTMParams::clear_haspeepholevectors() {
  haspeepholevectors_ = false;
}
bool LSTMParams::haspeepholevectors() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.hasPeepholeVectors)
  return haspeepholevectors_;
}
void LSTMParams::set_haspeepholevectors(bool value) {
  
  haspeepholevectors_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.hasPeepholeVectors)
}

// bool coupledInputAndForgetGate = 50;
void LSTMParams::clear_coupledinputandforgetgate() {
  coupledinputandforgetgate_ = false;
}
bool LSTMParams::coupledinputandforgetgate() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.coupledInputAndForgetGate)
  return coupledinputandforgetgate_;
}
void LSTMParams::set_coupledinputandforgetgate(bool value) {
  
  coupledinputandforgetgate_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.coupledInputAndForgetGate)
}

// float cellClipThreshold = 60;
void LSTMParams::clear_cellclipthreshold() {
  cellclipthreshold_ = 0;
}
float LSTMParams::cellclipthreshold() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.cellClipThreshold)
  return cellclipthreshold_;
}
void LSTMParams::set_cellclipthreshold(float value) {
  
  cellclipthreshold_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.cellClipThreshold)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LSTMWeightParams::kInputGateWeightMatrixFieldNumber;
const int LSTMWeightParams::kForgetGateWeightMatrixFieldNumber;
const int LSTMWeightParams::kBlockInputWeightMatrixFieldNumber;
const int LSTMWeightParams::kOutputGateWeightMatrixFieldNumber;
const int LSTMWeightParams::kInputGateRecursionMatrixFieldNumber;
const int LSTMWeightParams::kForgetGateRecursionMatrixFieldNumber;
const int LSTMWeightParams::kBlockInputRecursionMatrixFieldNumber;
const int LSTMWeightParams::kOutputGateRecursionMatrixFieldNumber;
const int LSTMWeightParams::kInputGateBiasVectorFieldNumber;
const int LSTMWeightParams::kForgetGateBiasVectorFieldNumber;
const int LSTMWeightParams::kBlockInputBiasVectorFieldNumber;
const int LSTMWeightParams::kOutputGateBiasVectorFieldNumber;
const int LSTMWeightParams::kInputGatePeepholeVectorFieldNumber;
const int LSTMWeightParams::kForgetGatePeepholeVectorFieldNumber;
const int LSTMWeightParams::kOutputGatePeepholeVectorFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LSTMWeightParams::LSTMWeightParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LSTMWeightParams)
}
LSTMWeightParams::LSTMWeightParams(const LSTMWeightParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_inputgateweightmatrix()) {
    inputgateweightmatrix_ = new ::CoreML::Specification::WeightParams(*from.inputgateweightmatrix_);
  } else {
    inputgateweightmatrix_ = NULL;
  }
  if (from.has_forgetgateweightmatrix()) {
    forgetgateweightmatrix_ = new ::CoreML::Specification::WeightParams(*from.forgetgateweightmatrix_);
  } else {
    forgetgateweightmatrix_ = NULL;
  }
  if (from.has_blockinputweightmatrix()) {
    blockinputweightmatrix_ = new ::CoreML::Specification::WeightParams(*from.blockinputweightmatrix_);
  } else {
    blockinputweightmatrix_ = NULL;
  }
  if (from.has_outputgateweightmatrix()) {
    outputgateweightmatrix_ = new ::CoreML::Specification::WeightParams(*from.outputgateweightmatrix_);
  } else {
    outputgateweightmatrix_ = NULL;
  }
  if (from.has_inputgaterecursionmatrix()) {
    inputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.inputgaterecursionmatrix_);
  } else {
    inputgaterecursionmatrix_ = NULL;
  }
  if (from.has_forgetgaterecursionmatrix()) {
    forgetgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.forgetgaterecursionmatrix_);
  } else {
    forgetgaterecursionmatrix_ = NULL;
  }
  if (from.has_blockinputrecursionmatrix()) {
    blockinputrecursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.blockinputrecursionmatrix_);
  } else {
    blockinputrecursionmatrix_ = NULL;
  }
  if (from.has_outputgaterecursionmatrix()) {
    outputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.outputgaterecursionmatrix_);
  } else {
    outputgaterecursionmatrix_ = NULL;
  }
  if (from.has_inputgatebiasvector()) {
    inputgatebiasvector_ = new ::CoreML::Specification::WeightParams(*from.inputgatebiasvector_);
  } else {
    inputgatebiasvector_ = NULL;
  }
  if (from.has_forgetgatebiasvector()) {
    forgetgatebiasvector_ = new ::CoreML::Specification::WeightParams(*from.forgetgatebiasvector_);
  } else {
    forgetgatebiasvector_ = NULL;
  }
  if (from.has_blockinputbiasvector()) {
    blockinputbiasvector_ = new ::CoreML::Specification::WeightParams(*from.blockinputbiasvector_);
  } else {
    blockinputbiasvector_ = NULL;
  }
  if (from.has_outputgatebiasvector()) {
    outputgatebiasvector_ = new ::CoreML::Specification::WeightParams(*from.outputgatebiasvector_);
  } else {
    outputgatebiasvector_ = NULL;
  }
  if (from.has_inputgatepeepholevector()) {
    inputgatepeepholevector_ = new ::CoreML::Specification::WeightParams(*from.inputgatepeepholevector_);
  } else {
    inputgatepeepholevector_ = NULL;
  }
  if (from.has_forgetgatepeepholevector()) {
    forgetgatepeepholevector_ = new ::CoreML::Specification::WeightParams(*from.forgetgatepeepholevector_);
  } else {
    forgetgatepeepholevector_ = NULL;
  }
  if (from.has_outputgatepeepholevector()) {
    outputgatepeepholevector_ = new ::CoreML::Specification::WeightParams(*from.outputgatepeepholevector_);
  } else {
    outputgatepeepholevector_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LSTMWeightParams)
}

void LSTMWeightParams::SharedCtor() {
  ::memset(&inputgateweightmatrix_, 0, reinterpret_cast<char*>(&outputgatepeepholevector_) -
    reinterpret_cast<char*>(&inputgateweightmatrix_) + sizeof(outputgatepeepholevector_));
  _cached_size_ = 0;
}

LSTMWeightParams::~LSTMWeightParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LSTMWeightParams)
  SharedDtor();
}

void LSTMWeightParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete inputgateweightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete forgetgateweightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete blockinputweightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete outputgateweightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete inputgaterecursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete forgetgaterecursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete blockinputrecursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete outputgaterecursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete inputgatebiasvector_;
  }
  if (this != internal_default_instance()) {
    delete forgetgatebiasvector_;
  }
  if (this != internal_default_instance()) {
    delete blockinputbiasvector_;
  }
  if (this != internal_default_instance()) {
    delete outputgatebiasvector_;
  }
  if (this != internal_default_instance()) {
    delete inputgatepeepholevector_;
  }
  if (this != internal_default_instance()) {
    delete forgetgatepeepholevector_;
  }
  if (this != internal_default_instance()) {
    delete outputgatepeepholevector_;
  }
}

void LSTMWeightParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LSTMWeightParams& LSTMWeightParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LSTMWeightParams* LSTMWeightParams::New(::google::protobuf::Arena* arena) const {
  LSTMWeightParams* n = new LSTMWeightParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LSTMWeightParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LSTMWeightParams)
  if (GetArenaNoVirtual() == NULL && inputgateweightmatrix_ != NULL) {
    delete inputgateweightmatrix_;
  }
  inputgateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && forgetgateweightmatrix_ != NULL) {
    delete forgetgateweightmatrix_;
  }
  forgetgateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && blockinputweightmatrix_ != NULL) {
    delete blockinputweightmatrix_;
  }
  blockinputweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgateweightmatrix_ != NULL) {
    delete outputgateweightmatrix_;
  }
  outputgateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && inputgaterecursionmatrix_ != NULL) {
    delete inputgaterecursionmatrix_;
  }
  inputgaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && forgetgaterecursionmatrix_ != NULL) {
    delete forgetgaterecursionmatrix_;
  }
  forgetgaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && blockinputrecursionmatrix_ != NULL) {
    delete blockinputrecursionmatrix_;
  }
  blockinputrecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgaterecursionmatrix_ != NULL) {
    delete outputgaterecursionmatrix_;
  }
  outputgaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && inputgatebiasvector_ != NULL) {
    delete inputgatebiasvector_;
  }
  inputgatebiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && forgetgatebiasvector_ != NULL) {
    delete forgetgatebiasvector_;
  }
  forgetgatebiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && blockinputbiasvector_ != NULL) {
    delete blockinputbiasvector_;
  }
  blockinputbiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgatebiasvector_ != NULL) {
    delete outputgatebiasvector_;
  }
  outputgatebiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && inputgatepeepholevector_ != NULL) {
    delete inputgatepeepholevector_;
  }
  inputgatepeepholevector_ = NULL;
  if (GetArenaNoVirtual() == NULL && forgetgatepeepholevector_ != NULL) {
    delete forgetgatepeepholevector_;
  }
  forgetgatepeepholevector_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgatepeepholevector_ != NULL) {
    delete outputgatepeepholevector_;
  }
  outputgatepeepholevector_ = NULL;
}

bool LSTMWeightParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LSTMWeightParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_inputgateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_forgetgateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_blockinputweightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_inputgaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
      case 21: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(170u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_forgetgaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
      case 22: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(178u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_blockinputrecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
      case 23: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(186u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams inputGateBiasVector = 40;
      case 40: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_inputgatebiasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
      case 41: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(330u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_forgetgatebiasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams blockInputBiasVector = 42;
      case 42: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(338u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_blockinputbiasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams outputGateBiasVector = 43;
      case 43: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(346u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgatebiasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
      case 60: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(482u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_inputgatepeepholevector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
      case 61: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(490u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_forgetgatepeepholevector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
      case 62: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(498u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgatepeepholevector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LSTMWeightParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LSTMWeightParams)
  return false;
#undef DO_
}

void LSTMWeightParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LSTMWeightParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
  if (this->has_inputgateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->inputgateweightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
  if (this->has_forgetgateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->forgetgateweightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
  if (this->has_blockinputweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      3, *this->blockinputweightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
  if (this->has_outputgateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      4, *this->outputgateweightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
  if (this->has_inputgaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *this->inputgaterecursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
  if (this->has_forgetgaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      21, *this->forgetgaterecursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
  if (this->has_blockinputrecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      22, *this->blockinputrecursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
  if (this->has_outputgaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      23, *this->outputgaterecursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams inputGateBiasVector = 40;
  if (this->has_inputgatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      40, *this->inputgatebiasvector_, output);
  }

  // .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
  if (this->has_forgetgatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      41, *this->forgetgatebiasvector_, output);
  }

  // .CoreML.Specification.WeightParams blockInputBiasVector = 42;
  if (this->has_blockinputbiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      42, *this->blockinputbiasvector_, output);
  }

  // .CoreML.Specification.WeightParams outputGateBiasVector = 43;
  if (this->has_outputgatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      43, *this->outputgatebiasvector_, output);
  }

  // .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
  if (this->has_inputgatepeepholevector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      60, *this->inputgatepeepholevector_, output);
  }

  // .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
  if (this->has_forgetgatepeepholevector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      61, *this->forgetgatepeepholevector_, output);
  }

  // .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
  if (this->has_outputgatepeepholevector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      62, *this->outputgatepeepholevector_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LSTMWeightParams)
}

size_t LSTMWeightParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LSTMWeightParams)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
  if (this->has_inputgateweightmatrix()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->inputgateweightmatrix_);
  }

  // .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
  if (this->has_forgetgateweightmatrix()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->forgetgateweightmatrix_);
  }

  // .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
  if (this->has_blockinputweightmatrix()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->blockinputweightmatrix_);
  }

  // .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
  if (this->has_outputgateweightmatrix()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgateweightmatrix_);
  }

  // .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
  if (this->has_inputgaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->inputgaterecursionmatrix_);
  }

  // .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
  if (this->has_forgetgaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->forgetgaterecursionmatrix_);
  }

  // .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
  if (this->has_blockinputrecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->blockinputrecursionmatrix_);
  }

  // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
  if (this->has_outputgaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgaterecursionmatrix_);
  }

  // .CoreML.Specification.WeightParams inputGateBiasVector = 40;
  if (this->has_inputgatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->inputgatebiasvector_);
  }

  // .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
  if (this->has_forgetgatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->forgetgatebiasvector_);
  }

  // .CoreML.Specification.WeightParams blockInputBiasVector = 42;
  if (this->has_blockinputbiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->blockinputbiasvector_);
  }

  // .CoreML.Specification.WeightParams outputGateBiasVector = 43;
  if (this->has_outputgatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgatebiasvector_);
  }

  // .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
  if (this->has_inputgatepeepholevector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->inputgatepeepholevector_);
  }

  // .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
  if (this->has_forgetgatepeepholevector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->forgetgatepeepholevector_);
  }

  // .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
  if (this->has_outputgatepeepholevector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgatepeepholevector_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LSTMWeightParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LSTMWeightParams*>(&from));
}

void LSTMWeightParams::MergeFrom(const LSTMWeightParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LSTMWeightParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_inputgateweightmatrix()) {
    mutable_inputgateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.inputgateweightmatrix());
  }
  if (from.has_forgetgateweightmatrix()) {
    mutable_forgetgateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.forgetgateweightmatrix());
  }
  if (from.has_blockinputweightmatrix()) {
    mutable_blockinputweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.blockinputweightmatrix());
  }
  if (from.has_outputgateweightmatrix()) {
    mutable_outputgateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgateweightmatrix());
  }
  if (from.has_inputgaterecursionmatrix()) {
    mutable_inputgaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.inputgaterecursionmatrix());
  }
  if (from.has_forgetgaterecursionmatrix()) {
    mutable_forgetgaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.forgetgaterecursionmatrix());
  }
  if (from.has_blockinputrecursionmatrix()) {
    mutable_blockinputrecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.blockinputrecursionmatrix());
  }
  if (from.has_outputgaterecursionmatrix()) {
    mutable_outputgaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgaterecursionmatrix());
  }
  if (from.has_inputgatebiasvector()) {
    mutable_inputgatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.inputgatebiasvector());
  }
  if (from.has_forgetgatebiasvector()) {
    mutable_forgetgatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.forgetgatebiasvector());
  }
  if (from.has_blockinputbiasvector()) {
    mutable_blockinputbiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.blockinputbiasvector());
  }
  if (from.has_outputgatebiasvector()) {
    mutable_outputgatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgatebiasvector());
  }
  if (from.has_inputgatepeepholevector()) {
    mutable_inputgatepeepholevector()->::CoreML::Specification::WeightParams::MergeFrom(from.inputgatepeepholevector());
  }
  if (from.has_forgetgatepeepholevector()) {
    mutable_forgetgatepeepholevector()->::CoreML::Specification::WeightParams::MergeFrom(from.forgetgatepeepholevector());
  }
  if (from.has_outputgatepeepholevector()) {
    mutable_outputgatepeepholevector()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgatepeepholevector());
  }
}

void LSTMWeightParams::CopyFrom(const LSTMWeightParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LSTMWeightParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LSTMWeightParams::IsInitialized() const {
  return true;
}

void LSTMWeightParams::Swap(LSTMWeightParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LSTMWeightParams::InternalSwap(LSTMWeightParams* other) {
  std::swap(inputgateweightmatrix_, other->inputgateweightmatrix_);
  std::swap(forgetgateweightmatrix_, other->forgetgateweightmatrix_);
  std::swap(blockinputweightmatrix_, other->blockinputweightmatrix_);
  std::swap(outputgateweightmatrix_, other->outputgateweightmatrix_);
  std::swap(inputgaterecursionmatrix_, other->inputgaterecursionmatrix_);
  std::swap(forgetgaterecursionmatrix_, other->forgetgaterecursionmatrix_);
  std::swap(blockinputrecursionmatrix_, other->blockinputrecursionmatrix_);
  std::swap(outputgaterecursionmatrix_, other->outputgaterecursionmatrix_);
  std::swap(inputgatebiasvector_, other->inputgatebiasvector_);
  std::swap(forgetgatebiasvector_, other->forgetgatebiasvector_);
  std::swap(blockinputbiasvector_, other->blockinputbiasvector_);
  std::swap(outputgatebiasvector_, other->outputgatebiasvector_);
  std::swap(inputgatepeepholevector_, other->inputgatepeepholevector_);
  std::swap(forgetgatepeepholevector_, other->forgetgatepeepholevector_);
  std::swap(outputgatepeepholevector_, other->outputgatepeepholevector_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LSTMWeightParams::GetTypeName() const {
  return "CoreML.Specification.LSTMWeightParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LSTMWeightParams

// .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
bool LSTMWeightParams::has_inputgateweightmatrix() const {
  return this != internal_default_instance() && inputgateweightmatrix_ != NULL;
}
void LSTMWeightParams::clear_inputgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && inputgateweightmatrix_ != NULL) delete inputgateweightmatrix_;
  inputgateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
  return inputgateweightmatrix_ != NULL ? *inputgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgateweightmatrix() {
  
  if (inputgateweightmatrix_ == NULL) {
    inputgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
  return inputgateweightmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = inputgateweightmatrix_;
  inputgateweightmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_inputgateweightmatrix(::CoreML::Specification::WeightParams* inputgateweightmatrix) {
  delete inputgateweightmatrix_;
  inputgateweightmatrix_ = inputgateweightmatrix;
  if (inputgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
}

// .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
bool LSTMWeightParams::has_forgetgateweightmatrix() const {
  return this != internal_default_instance() && forgetgateweightmatrix_ != NULL;
}
void LSTMWeightParams::clear_forgetgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && forgetgateweightmatrix_ != NULL) delete forgetgateweightmatrix_;
  forgetgateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
  return forgetgateweightmatrix_ != NULL ? *forgetgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgateweightmatrix() {
  
  if (forgetgateweightmatrix_ == NULL) {
    forgetgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
  return forgetgateweightmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = forgetgateweightmatrix_;
  forgetgateweightmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_forgetgateweightmatrix(::CoreML::Specification::WeightParams* forgetgateweightmatrix) {
  delete forgetgateweightmatrix_;
  forgetgateweightmatrix_ = forgetgateweightmatrix;
  if (forgetgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
}

// .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
bool LSTMWeightParams::has_blockinputweightmatrix() const {
  return this != internal_default_instance() && blockinputweightmatrix_ != NULL;
}
void LSTMWeightParams::clear_blockinputweightmatrix() {
  if (GetArenaNoVirtual() == NULL && blockinputweightmatrix_ != NULL) delete blockinputweightmatrix_;
  blockinputweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::blockinputweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
  return blockinputweightmatrix_ != NULL ? *blockinputweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_blockinputweightmatrix() {
  
  if (blockinputweightmatrix_ == NULL) {
    blockinputweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
  return blockinputweightmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_blockinputweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = blockinputweightmatrix_;
  blockinputweightmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_blockinputweightmatrix(::CoreML::Specification::WeightParams* blockinputweightmatrix) {
  delete blockinputweightmatrix_;
  blockinputweightmatrix_ = blockinputweightmatrix;
  if (blockinputweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
}

// .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
bool LSTMWeightParams::has_outputgateweightmatrix() const {
  return this != internal_default_instance() && outputgateweightmatrix_ != NULL;
}
void LSTMWeightParams::clear_outputgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgateweightmatrix_ != NULL) delete outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
  return outputgateweightmatrix_ != NULL ? *outputgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgateweightmatrix() {
  
  if (outputgateweightmatrix_ == NULL) {
    outputgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
  return outputgateweightmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_outputgateweightmatrix(::CoreML::Specification::WeightParams* outputgateweightmatrix) {
  delete outputgateweightmatrix_;
  outputgateweightmatrix_ = outputgateweightmatrix;
  if (outputgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
}

// .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
bool LSTMWeightParams::has_inputgaterecursionmatrix() const {
  return this != internal_default_instance() && inputgaterecursionmatrix_ != NULL;
}
void LSTMWeightParams::clear_inputgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && inputgaterecursionmatrix_ != NULL) delete inputgaterecursionmatrix_;
  inputgaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
  return inputgaterecursionmatrix_ != NULL ? *inputgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgaterecursionmatrix() {
  
  if (inputgaterecursionmatrix_ == NULL) {
    inputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
  return inputgaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = inputgaterecursionmatrix_;
  inputgaterecursionmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_inputgaterecursionmatrix(::CoreML::Specification::WeightParams* inputgaterecursionmatrix) {
  delete inputgaterecursionmatrix_;
  inputgaterecursionmatrix_ = inputgaterecursionmatrix;
  if (inputgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
bool LSTMWeightParams::has_forgetgaterecursionmatrix() const {
  return this != internal_default_instance() && forgetgaterecursionmatrix_ != NULL;
}
void LSTMWeightParams::clear_forgetgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && forgetgaterecursionmatrix_ != NULL) delete forgetgaterecursionmatrix_;
  forgetgaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
  return forgetgaterecursionmatrix_ != NULL ? *forgetgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgaterecursionmatrix() {
  
  if (forgetgaterecursionmatrix_ == NULL) {
    forgetgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
  return forgetgaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = forgetgaterecursionmatrix_;
  forgetgaterecursionmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_forgetgaterecursionmatrix(::CoreML::Specification::WeightParams* forgetgaterecursionmatrix) {
  delete forgetgaterecursionmatrix_;
  forgetgaterecursionmatrix_ = forgetgaterecursionmatrix;
  if (forgetgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
bool LSTMWeightParams::has_blockinputrecursionmatrix() const {
  return this != internal_default_instance() && blockinputrecursionmatrix_ != NULL;
}
void LSTMWeightParams::clear_blockinputrecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && blockinputrecursionmatrix_ != NULL) delete blockinputrecursionmatrix_;
  blockinputrecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::blockinputrecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
  return blockinputrecursionmatrix_ != NULL ? *blockinputrecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_blockinputrecursionmatrix() {
  
  if (blockinputrecursionmatrix_ == NULL) {
    blockinputrecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
  return blockinputrecursionmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_blockinputrecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = blockinputrecursionmatrix_;
  blockinputrecursionmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_blockinputrecursionmatrix(::CoreML::Specification::WeightParams* blockinputrecursionmatrix) {
  delete blockinputrecursionmatrix_;
  blockinputrecursionmatrix_ = blockinputrecursionmatrix;
  if (blockinputrecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
}

// .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
bool LSTMWeightParams::has_outputgaterecursionmatrix() const {
  return this != internal_default_instance() && outputgaterecursionmatrix_ != NULL;
}
void LSTMWeightParams::clear_outputgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgaterecursionmatrix_ != NULL) delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_ != NULL ? *outputgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgaterecursionmatrix() {
  
  if (outputgaterecursionmatrix_ == NULL) {
    outputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_outputgaterecursionmatrix(::CoreML::Specification::WeightParams* outputgaterecursionmatrix) {
  delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = outputgaterecursionmatrix;
  if (outputgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams inputGateBiasVector = 40;
bool LSTMWeightParams::has_inputgatebiasvector() const {
  return this != internal_default_instance() && inputgatebiasvector_ != NULL;
}
void LSTMWeightParams::clear_inputgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && inputgatebiasvector_ != NULL) delete inputgatebiasvector_;
  inputgatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
  return inputgatebiasvector_ != NULL ? *inputgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgatebiasvector() {
  
  if (inputgatebiasvector_ == NULL) {
    inputgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
  return inputgatebiasvector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = inputgatebiasvector_;
  inputgatebiasvector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_inputgatebiasvector(::CoreML::Specification::WeightParams* inputgatebiasvector) {
  delete inputgatebiasvector_;
  inputgatebiasvector_ = inputgatebiasvector;
  if (inputgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
}

// .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
bool LSTMWeightParams::has_forgetgatebiasvector() const {
  return this != internal_default_instance() && forgetgatebiasvector_ != NULL;
}
void LSTMWeightParams::clear_forgetgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && forgetgatebiasvector_ != NULL) delete forgetgatebiasvector_;
  forgetgatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
  return forgetgatebiasvector_ != NULL ? *forgetgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgatebiasvector() {
  
  if (forgetgatebiasvector_ == NULL) {
    forgetgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
  return forgetgatebiasvector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = forgetgatebiasvector_;
  forgetgatebiasvector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_forgetgatebiasvector(::CoreML::Specification::WeightParams* forgetgatebiasvector) {
  delete forgetgatebiasvector_;
  forgetgatebiasvector_ = forgetgatebiasvector;
  if (forgetgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
}

// .CoreML.Specification.WeightParams blockInputBiasVector = 42;
bool LSTMWeightParams::has_blockinputbiasvector() const {
  return this != internal_default_instance() && blockinputbiasvector_ != NULL;
}
void LSTMWeightParams::clear_blockinputbiasvector() {
  if (GetArenaNoVirtual() == NULL && blockinputbiasvector_ != NULL) delete blockinputbiasvector_;
  blockinputbiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::blockinputbiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
  return blockinputbiasvector_ != NULL ? *blockinputbiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_blockinputbiasvector() {
  
  if (blockinputbiasvector_ == NULL) {
    blockinputbiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
  return blockinputbiasvector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_blockinputbiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = blockinputbiasvector_;
  blockinputbiasvector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_blockinputbiasvector(::CoreML::Specification::WeightParams* blockinputbiasvector) {
  delete blockinputbiasvector_;
  blockinputbiasvector_ = blockinputbiasvector;
  if (blockinputbiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
}

// .CoreML.Specification.WeightParams outputGateBiasVector = 43;
bool LSTMWeightParams::has_outputgatebiasvector() const {
  return this != internal_default_instance() && outputgatebiasvector_ != NULL;
}
void LSTMWeightParams::clear_outputgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && outputgatebiasvector_ != NULL) delete outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
  return outputgatebiasvector_ != NULL ? *outputgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgatebiasvector() {
  
  if (outputgatebiasvector_ == NULL) {
    outputgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
  return outputgatebiasvector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_outputgatebiasvector(::CoreML::Specification::WeightParams* outputgatebiasvector) {
  delete outputgatebiasvector_;
  outputgatebiasvector_ = outputgatebiasvector;
  if (outputgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
}

// .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
bool LSTMWeightParams::has_inputgatepeepholevector() const {
  return this != internal_default_instance() && inputgatepeepholevector_ != NULL;
}
void LSTMWeightParams::clear_inputgatepeepholevector() {
  if (GetArenaNoVirtual() == NULL && inputgatepeepholevector_ != NULL) delete inputgatepeepholevector_;
  inputgatepeepholevector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgatepeepholevector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
  return inputgatepeepholevector_ != NULL ? *inputgatepeepholevector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgatepeepholevector() {
  
  if (inputgatepeepholevector_ == NULL) {
    inputgatepeepholevector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
  return inputgatepeepholevector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgatepeepholevector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
  
  ::CoreML::Specification::WeightParams* temp = inputgatepeepholevector_;
  inputgatepeepholevector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_inputgatepeepholevector(::CoreML::Specification::WeightParams* inputgatepeepholevector) {
  delete inputgatepeepholevector_;
  inputgatepeepholevector_ = inputgatepeepholevector;
  if (inputgatepeepholevector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
}

// .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
bool LSTMWeightParams::has_forgetgatepeepholevector() const {
  return this != internal_default_instance() && forgetgatepeepholevector_ != NULL;
}
void LSTMWeightParams::clear_forgetgatepeepholevector() {
  if (GetArenaNoVirtual() == NULL && forgetgatepeepholevector_ != NULL) delete forgetgatepeepholevector_;
  forgetgatepeepholevector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgatepeepholevector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
  return forgetgatepeepholevector_ != NULL ? *forgetgatepeepholevector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgatepeepholevector() {
  
  if (forgetgatepeepholevector_ == NULL) {
    forgetgatepeepholevector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
  return forgetgatepeepholevector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgatepeepholevector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
  
  ::CoreML::Specification::WeightParams* temp = forgetgatepeepholevector_;
  forgetgatepeepholevector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_forgetgatepeepholevector(::CoreML::Specification::WeightParams* forgetgatepeepholevector) {
  delete forgetgatepeepholevector_;
  forgetgatepeepholevector_ = forgetgatepeepholevector;
  if (forgetgatepeepholevector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
}

// .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
bool LSTMWeightParams::has_outputgatepeepholevector() const {
  return this != internal_default_instance() && outputgatepeepholevector_ != NULL;
}
void LSTMWeightParams::clear_outputgatepeepholevector() {
  if (GetArenaNoVirtual() == NULL && outputgatepeepholevector_ != NULL) delete outputgatepeepholevector_;
  outputgatepeepholevector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgatepeepholevector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
  return outputgatepeepholevector_ != NULL ? *outputgatepeepholevector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgatepeepholevector() {
  
  if (outputgatepeepholevector_ == NULL) {
    outputgatepeepholevector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
  return outputgatepeepholevector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgatepeepholevector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
  
  ::CoreML::Specification::WeightParams* temp = outputgatepeepholevector_;
  outputgatepeepholevector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_outputgatepeepholevector(::CoreML::Specification::WeightParams* outputgatepeepholevector) {
  delete outputgatepeepholevector_;
  outputgatepeepholevector_ = outputgatepeepholevector;
  if (outputgatepeepholevector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int UniDirectionalLSTMLayerParams::kInputVectorSizeFieldNumber;
const int UniDirectionalLSTMLayerParams::kOutputVectorSizeFieldNumber;
const int UniDirectionalLSTMLayerParams::kActivationsFieldNumber;
const int UniDirectionalLSTMLayerParams::kParamsFieldNumber;
const int UniDirectionalLSTMLayerParams::kWeightParamsFieldNumber;
const int UniDirectionalLSTMLayerParams::kReverseInputFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

UniDirectionalLSTMLayerParams::UniDirectionalLSTMLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.UniDirectionalLSTMLayerParams)
}
UniDirectionalLSTMLayerParams::UniDirectionalLSTMLayerParams(const UniDirectionalLSTMLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      activations_(from.activations_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_params()) {
    params_ = new ::CoreML::Specification::LSTMParams(*from.params_);
  } else {
    params_ = NULL;
  }
  if (from.has_weightparams()) {
    weightparams_ = new ::CoreML::Specification::LSTMWeightParams(*from.weightparams_);
  } else {
    weightparams_ = NULL;
  }
  ::memcpy(&inputvectorsize_, &from.inputvectorsize_,
    reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(reverseinput_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.UniDirectionalLSTMLayerParams)
}

void UniDirectionalLSTMLayerParams::SharedCtor() {
  ::memset(&params_, 0, reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&params_) + sizeof(reverseinput_));
  _cached_size_ = 0;
}

UniDirectionalLSTMLayerParams::~UniDirectionalLSTMLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.UniDirectionalLSTMLayerParams)
  SharedDtor();
}

void UniDirectionalLSTMLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete params_;
  }
  if (this != internal_default_instance()) {
    delete weightparams_;
  }
}

void UniDirectionalLSTMLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const UniDirectionalLSTMLayerParams& UniDirectionalLSTMLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

UniDirectionalLSTMLayerParams* UniDirectionalLSTMLayerParams::New(::google::protobuf::Arena* arena) const {
  UniDirectionalLSTMLayerParams* n = new UniDirectionalLSTMLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void UniDirectionalLSTMLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  activations_.Clear();
  if (GetArenaNoVirtual() == NULL && params_ != NULL) {
    delete params_;
  }
  params_ = NULL;
  if (GetArenaNoVirtual() == NULL && weightparams_ != NULL) {
    delete weightparams_;
  }
  weightparams_ = NULL;
  ::memset(&inputvectorsize_, 0, reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(reverseinput_));
}

bool UniDirectionalLSTMLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 inputVectorSize = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 outputVectorSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.ActivationParams activations = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_activations()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LSTMParams params = 15;
      case 15: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(122u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_params()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LSTMWeightParams weightParams = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weightparams()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool reverseInput = 100;
      case 100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(800u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reverseinput_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.UniDirectionalLSTMLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.UniDirectionalLSTMLayerParams)
  return false;
#undef DO_
}

void UniDirectionalLSTMLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputvectorsize(), output);
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputvectorsize(), output);
  }

  // repeated .CoreML.Specification.ActivationParams activations = 10;
  for (unsigned int i = 0, n = this->activations_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, this->activations(i), output);
  }

  // .CoreML.Specification.LSTMParams params = 15;
  if (this->has_params()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      15, *this->params_, output);
  }

  // .CoreML.Specification.LSTMWeightParams weightParams = 20;
  if (this->has_weightparams()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *this->weightparams_, output);
  }

  // bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(100, this->reverseinput(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.UniDirectionalLSTMLayerParams)
}

size_t UniDirectionalLSTMLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  size_t total_size = 0;

  // repeated .CoreML.Specification.ActivationParams activations = 10;
  {
    unsigned int count = this->activations_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->activations(i));
    }
  }

  // .CoreML.Specification.LSTMParams params = 15;
  if (this->has_params()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->params_);
  }

  // .CoreML.Specification.LSTMWeightParams weightParams = 20;
  if (this->has_weightparams()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weightparams_);
  }

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputvectorsize());
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputvectorsize());
  }

  // bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    total_size += 2 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void UniDirectionalLSTMLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const UniDirectionalLSTMLayerParams*>(&from));
}

void UniDirectionalLSTMLayerParams::MergeFrom(const UniDirectionalLSTMLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  activations_.MergeFrom(from.activations_);
  if (from.has_params()) {
    mutable_params()->::CoreML::Specification::LSTMParams::MergeFrom(from.params());
  }
  if (from.has_weightparams()) {
    mutable_weightparams()->::CoreML::Specification::LSTMWeightParams::MergeFrom(from.weightparams());
  }
  if (from.inputvectorsize() != 0) {
    set_inputvectorsize(from.inputvectorsize());
  }
  if (from.outputvectorsize() != 0) {
    set_outputvectorsize(from.outputvectorsize());
  }
  if (from.reverseinput() != 0) {
    set_reverseinput(from.reverseinput());
  }
}

void UniDirectionalLSTMLayerParams::CopyFrom(const UniDirectionalLSTMLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool UniDirectionalLSTMLayerParams::IsInitialized() const {
  return true;
}

void UniDirectionalLSTMLayerParams::Swap(UniDirectionalLSTMLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void UniDirectionalLSTMLayerParams::InternalSwap(UniDirectionalLSTMLayerParams* other) {
  activations_.InternalSwap(&other->activations_);
  std::swap(params_, other->params_);
  std::swap(weightparams_, other->weightparams_);
  std::swap(inputvectorsize_, other->inputvectorsize_);
  std::swap(outputvectorsize_, other->outputvectorsize_);
  std::swap(reverseinput_, other->reverseinput_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string UniDirectionalLSTMLayerParams::GetTypeName() const {
  return "CoreML.Specification.UniDirectionalLSTMLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// UniDirectionalLSTMLayerParams

// uint64 inputVectorSize = 1;
void UniDirectionalLSTMLayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 UniDirectionalLSTMLayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.inputVectorSize)
  return inputvectorsize_;
}
void UniDirectionalLSTMLayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {
  
  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UniDirectionalLSTMLayerParams.inputVectorSize)
}

// uint64 outputVectorSize = 2;
void UniDirectionalLSTMLayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 UniDirectionalLSTMLayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.outputVectorSize)
  return outputvectorsize_;
}
void UniDirectionalLSTMLayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {
  
  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UniDirectionalLSTMLayerParams.outputVectorSize)
}

// repeated .CoreML.Specification.ActivationParams activations = 10;
int UniDirectionalLSTMLayerParams::activations_size() const {
  return activations_.size();
}
void UniDirectionalLSTMLayerParams::clear_activations() {
  activations_.Clear();
}
const ::CoreML::Specification::ActivationParams& UniDirectionalLSTMLayerParams::activations(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_.Get(index);
}
::CoreML::Specification::ActivationParams* UniDirectionalLSTMLayerParams::mutable_activations(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_.Mutable(index);
}
::CoreML::Specification::ActivationParams* UniDirectionalLSTMLayerParams::add_activations() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
UniDirectionalLSTMLayerParams::mutable_activations() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return &activations_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
UniDirectionalLSTMLayerParams::activations() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_;
}

// .CoreML.Specification.LSTMParams params = 15;
bool UniDirectionalLSTMLayerParams::has_params() const {
  return this != internal_default_instance() && params_ != NULL;
}
void UniDirectionalLSTMLayerParams::clear_params() {
  if (GetArenaNoVirtual() == NULL && params_ != NULL) delete params_;
  params_ = NULL;
}
const ::CoreML::Specification::LSTMParams& UniDirectionalLSTMLayerParams::params() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
  return params_ != NULL ? *params_
                         : *::CoreML::Specification::LSTMParams::internal_default_instance();
}
::CoreML::Specification::LSTMParams* UniDirectionalLSTMLayerParams::mutable_params() {
  
  if (params_ == NULL) {
    params_ = new ::CoreML::Specification::LSTMParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
  return params_;
}
::CoreML::Specification::LSTMParams* UniDirectionalLSTMLayerParams::release_params() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
  
  ::CoreML::Specification::LSTMParams* temp = params_;
  params_ = NULL;
  return temp;
}
void UniDirectionalLSTMLayerParams::set_allocated_params(::CoreML::Specification::LSTMParams* params) {
  delete params_;
  params_ = params;
  if (params) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
}

// .CoreML.Specification.LSTMWeightParams weightParams = 20;
bool UniDirectionalLSTMLayerParams::has_weightparams() const {
  return this != internal_default_instance() && weightparams_ != NULL;
}
void UniDirectionalLSTMLayerParams::clear_weightparams() {
  if (GetArenaNoVirtual() == NULL && weightparams_ != NULL) delete weightparams_;
  weightparams_ = NULL;
}
const ::CoreML::Specification::LSTMWeightParams& UniDirectionalLSTMLayerParams::weightparams() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
  return weightparams_ != NULL ? *weightparams_
                         : *::CoreML::Specification::LSTMWeightParams::internal_default_instance();
}
::CoreML::Specification::LSTMWeightParams* UniDirectionalLSTMLayerParams::mutable_weightparams() {
  
  if (weightparams_ == NULL) {
    weightparams_ = new ::CoreML::Specification::LSTMWeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
  return weightparams_;
}
::CoreML::Specification::LSTMWeightParams* UniDirectionalLSTMLayerParams::release_weightparams() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
  
  ::CoreML::Specification::LSTMWeightParams* temp = weightparams_;
  weightparams_ = NULL;
  return temp;
}
void UniDirectionalLSTMLayerParams::set_allocated_weightparams(::CoreML::Specification::LSTMWeightParams* weightparams) {
  delete weightparams_;
  weightparams_ = weightparams;
  if (weightparams) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
}

// bool reverseInput = 100;
void UniDirectionalLSTMLayerParams::clear_reverseinput() {
  reverseinput_ = false;
}
bool UniDirectionalLSTMLayerParams::reverseinput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.reverseInput)
  return reverseinput_;
}
void UniDirectionalLSTMLayerParams::set_reverseinput(bool value) {
  
  reverseinput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UniDirectionalLSTMLayerParams.reverseInput)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BiDirectionalLSTMLayerParams::kInputVectorSizeFieldNumber;
const int BiDirectionalLSTMLayerParams::kOutputVectorSizeFieldNumber;
const int BiDirectionalLSTMLayerParams::kActivationsForwardLSTMFieldNumber;
const int BiDirectionalLSTMLayerParams::kActivationsBackwardLSTMFieldNumber;
const int BiDirectionalLSTMLayerParams::kParamsFieldNumber;
const int BiDirectionalLSTMLayerParams::kWeightParamsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BiDirectionalLSTMLayerParams::BiDirectionalLSTMLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BiDirectionalLSTMLayerParams)
}
BiDirectionalLSTMLayerParams::BiDirectionalLSTMLayerParams(const BiDirectionalLSTMLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      activationsforwardlstm_(from.activationsforwardlstm_),
      activationsbackwardlstm_(from.activationsbackwardlstm_),
      weightparams_(from.weightparams_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_params()) {
    params_ = new ::CoreML::Specification::LSTMParams(*from.params_);
  } else {
    params_ = NULL;
  }
  ::memcpy(&inputvectorsize_, &from.inputvectorsize_,
    reinterpret_cast<char*>(&outputvectorsize_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(outputvectorsize_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BiDirectionalLSTMLayerParams)
}

void BiDirectionalLSTMLayerParams::SharedCtor() {
  ::memset(&params_, 0, reinterpret_cast<char*>(&outputvectorsize_) -
    reinterpret_cast<char*>(&params_) + sizeof(outputvectorsize_));
  _cached_size_ = 0;
}

BiDirectionalLSTMLayerParams::~BiDirectionalLSTMLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BiDirectionalLSTMLayerParams)
  SharedDtor();
}

void BiDirectionalLSTMLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete params_;
  }
}

void BiDirectionalLSTMLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BiDirectionalLSTMLayerParams& BiDirectionalLSTMLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BiDirectionalLSTMLayerParams* BiDirectionalLSTMLayerParams::New(::google::protobuf::Arena* arena) const {
  BiDirectionalLSTMLayerParams* n = new BiDirectionalLSTMLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BiDirectionalLSTMLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  activationsforwardlstm_.Clear();
  activationsbackwardlstm_.Clear();
  weightparams_.Clear();
  if (GetArenaNoVirtual() == NULL && params_ != NULL) {
    delete params_;
  }
  params_ = NULL;
  ::memset(&inputvectorsize_, 0, reinterpret_cast<char*>(&outputvectorsize_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(outputvectorsize_));
}

bool BiDirectionalLSTMLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 inputVectorSize = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 outputVectorSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_activationsforwardlstm()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
      case 11: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(90u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_activationsbackwardlstm()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LSTMParams params = 15;
      case 15: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(122u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_params()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_weightparams()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BiDirectionalLSTMLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BiDirectionalLSTMLayerParams)
  return false;
#undef DO_
}

void BiDirectionalLSTMLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputvectorsize(), output);
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputvectorsize(), output);
  }

  // repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
  for (unsigned int i = 0, n = this->activationsforwardlstm_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, this->activationsforwardlstm(i), output);
  }

  // repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
  for (unsigned int i = 0, n = this->activationsbackwardlstm_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      11, this->activationsbackwardlstm(i), output);
  }

  // .CoreML.Specification.LSTMParams params = 15;
  if (this->has_params()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      15, *this->params_, output);
  }

  // repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
  for (unsigned int i = 0, n = this->weightparams_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, this->weightparams(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BiDirectionalLSTMLayerParams)
}

size_t BiDirectionalLSTMLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  size_t total_size = 0;

  // repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
  {
    unsigned int count = this->activationsforwardlstm_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->activationsforwardlstm(i));
    }
  }

  // repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
  {
    unsigned int count = this->activationsbackwardlstm_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->activationsbackwardlstm(i));
    }
  }

  // repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
  {
    unsigned int count = this->weightparams_size();
    total_size += 2UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->weightparams(i));
    }
  }

  // .CoreML.Specification.LSTMParams params = 15;
  if (this->has_params()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->params_);
  }

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputvectorsize());
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputvectorsize());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BiDirectionalLSTMLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BiDirectionalLSTMLayerParams*>(&from));
}

void BiDirectionalLSTMLayerParams::MergeFrom(const BiDirectionalLSTMLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  activationsforwardlstm_.MergeFrom(from.activationsforwardlstm_);
  activationsbackwardlstm_.MergeFrom(from.activationsbackwardlstm_);
  weightparams_.MergeFrom(from.weightparams_);
  if (from.has_params()) {
    mutable_params()->::CoreML::Specification::LSTMParams::MergeFrom(from.params());
  }
  if (from.inputvectorsize() != 0) {
    set_inputvectorsize(from.inputvectorsize());
  }
  if (from.outputvectorsize() != 0) {
    set_outputvectorsize(from.outputvectorsize());
  }
}

void BiDirectionalLSTMLayerParams::CopyFrom(const BiDirectionalLSTMLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BiDirectionalLSTMLayerParams::IsInitialized() const {
  return true;
}

void BiDirectionalLSTMLayerParams::Swap(BiDirectionalLSTMLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BiDirectionalLSTMLayerParams::InternalSwap(BiDirectionalLSTMLayerParams* other) {
  activationsforwardlstm_.InternalSwap(&other->activationsforwardlstm_);
  activationsbackwardlstm_.InternalSwap(&other->activationsbackwardlstm_);
  weightparams_.InternalSwap(&other->weightparams_);
  std::swap(params_, other->params_);
  std::swap(inputvectorsize_, other->inputvectorsize_);
  std::swap(outputvectorsize_, other->outputvectorsize_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BiDirectionalLSTMLayerParams::GetTypeName() const {
  return "CoreML.Specification.BiDirectionalLSTMLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BiDirectionalLSTMLayerParams

// uint64 inputVectorSize = 1;
void BiDirectionalLSTMLayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BiDirectionalLSTMLayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.inputVectorSize)
  return inputvectorsize_;
}
void BiDirectionalLSTMLayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {
  
  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BiDirectionalLSTMLayerParams.inputVectorSize)
}

// uint64 outputVectorSize = 2;
void BiDirectionalLSTMLayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BiDirectionalLSTMLayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.outputVectorSize)
  return outputvectorsize_;
}
void BiDirectionalLSTMLayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {
  
  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BiDirectionalLSTMLayerParams.outputVectorSize)
}

// repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
int BiDirectionalLSTMLayerParams::activationsforwardlstm_size() const {
  return activationsforwardlstm_.size();
}
void BiDirectionalLSTMLayerParams::clear_activationsforwardlstm() {
  activationsforwardlstm_.Clear();
}
const ::CoreML::Specification::ActivationParams& BiDirectionalLSTMLayerParams::activationsforwardlstm(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_.Get(index);
}
::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::mutable_activationsforwardlstm(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_.Mutable(index);
}
::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::add_activationsforwardlstm() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
BiDirectionalLSTMLayerParams::mutable_activationsforwardlstm() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return &activationsforwardlstm_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
BiDirectionalLSTMLayerParams::activationsforwardlstm() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_;
}

// repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
int BiDirectionalLSTMLayerParams::activationsbackwardlstm_size() const {
  return activationsbackwardlstm_.size();
}
void BiDirectionalLSTMLayerParams::clear_activationsbackwardlstm() {
  activationsbackwardlstm_.Clear();
}
const ::CoreML::Specification::ActivationParams& BiDirectionalLSTMLayerParams::activationsbackwardlstm(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_.Get(index);
}
::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::mutable_activationsbackwardlstm(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_.Mutable(index);
}
::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::add_activationsbackwardlstm() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
BiDirectionalLSTMLayerParams::mutable_activationsbackwardlstm() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return &activationsbackwardlstm_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
BiDirectionalLSTMLayerParams::activationsbackwardlstm() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_;
}

// .CoreML.Specification.LSTMParams params = 15;
bool BiDirectionalLSTMLayerParams::has_params() const {
  return this != internal_default_instance() && params_ != NULL;
}
void BiDirectionalLSTMLayerParams::clear_params() {
  if (GetArenaNoVirtual() == NULL && params_ != NULL) delete params_;
  params_ = NULL;
}
const ::CoreML::Specification::LSTMParams& BiDirectionalLSTMLayerParams::params() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
  return params_ != NULL ? *params_
                         : *::CoreML::Specification::LSTMParams::internal_default_instance();
}
::CoreML::Specification::LSTMParams* BiDirectionalLSTMLayerParams::mutable_params() {
  
  if (params_ == NULL) {
    params_ = new ::CoreML::Specification::LSTMParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
  return params_;
}
::CoreML::Specification::LSTMParams* BiDirectionalLSTMLayerParams::release_params() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
  
  ::CoreML::Specification::LSTMParams* temp = params_;
  params_ = NULL;
  return temp;
}
void BiDirectionalLSTMLayerParams::set_allocated_params(::CoreML::Specification::LSTMParams* params) {
  delete params_;
  params_ = params;
  if (params) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
}

// repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
int BiDirectionalLSTMLayerParams::weightparams_size() const {
  return weightparams_.size();
}
void BiDirectionalLSTMLayerParams::clear_weightparams() {
  weightparams_.Clear();
}
const ::CoreML::Specification::LSTMWeightParams& BiDirectionalLSTMLayerParams::weightparams(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_.Get(index);
}
::CoreML::Specification::LSTMWeightParams* BiDirectionalLSTMLayerParams::mutable_weightparams(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_.Mutable(index);
}
::CoreML::Specification::LSTMWeightParams* BiDirectionalLSTMLayerParams::add_weightparams() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams >*
BiDirectionalLSTMLayerParams::mutable_weightparams() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return &weightparams_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams >&
BiDirectionalLSTMLayerParams::weightparams() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int CustomLayerParams_CustomLayerParamValue::kDoubleValueFieldNumber;
const int CustomLayerParams_CustomLayerParamValue::kStringValueFieldNumber;
const int CustomLayerParams_CustomLayerParamValue::kIntValueFieldNumber;
const int CustomLayerParams_CustomLayerParamValue::kLongValueFieldNumber;
const int CustomLayerParams_CustomLayerParamValue::kBoolValueFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CustomLayerParams_CustomLayerParamValue::CustomLayerParams_CustomLayerParamValue()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
}
CustomLayerParams_CustomLayerParamValue::CustomLayerParams_CustomLayerParamValue(const CustomLayerParams_CustomLayerParamValue& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  clear_has_value();
  switch (from.value_case()) {
    case kDoubleValue: {
      set_doublevalue(from.doublevalue());
      break;
    }
    case kStringValue: {
      set_stringvalue(from.stringvalue());
      break;
    }
    case kIntValue: {
      set_intvalue(from.intvalue());
      break;
    }
    case kLongValue: {
      set_longvalue(from.longvalue());
      break;
    }
    case kBoolValue: {
      set_boolvalue(from.boolvalue());
      break;
    }
    case VALUE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
}

void CustomLayerParams_CustomLayerParamValue::SharedCtor() {
  clear_has_value();
  _cached_size_ = 0;
}

CustomLayerParams_CustomLayerParamValue::~CustomLayerParams_CustomLayerParamValue() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  SharedDtor();
}

void CustomLayerParams_CustomLayerParamValue::SharedDtor() {
  if (has_value()) {
    clear_value();
  }
}

void CustomLayerParams_CustomLayerParamValue::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CustomLayerParams_CustomLayerParamValue& CustomLayerParams_CustomLayerParamValue::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

CustomLayerParams_CustomLayerParamValue* CustomLayerParams_CustomLayerParamValue::New(::google::protobuf::Arena* arena) const {
  CustomLayerParams_CustomLayerParamValue* n = new CustomLayerParams_CustomLayerParamValue;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CustomLayerParams_CustomLayerParamValue::clear_value() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  switch (value_case()) {
    case kDoubleValue: {
      // No need to clear
      break;
    }
    case kStringValue: {
      value_.stringvalue_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
      break;
    }
    case kIntValue: {
      // No need to clear
      break;
    }
    case kLongValue: {
      // No need to clear
      break;
    }
    case kBoolValue: {
      // No need to clear
      break;
    }
    case VALUE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = VALUE_NOT_SET;
}


void CustomLayerParams_CustomLayerParamValue::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  clear_value();
}

bool CustomLayerParams_CustomLayerParamValue::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // double doubleValue = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(81u)) {
          clear_value();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   double, ::google::protobuf::internal::WireFormatLite::TYPE_DOUBLE>(
                 input, &value_.doublevalue_)));
          set_has_doublevalue();
        } else {
          goto handle_unusual;
        }
        break;
      }

      // string stringValue = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_stringvalue()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->stringvalue().data(), this->stringvalue().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // int32 intValue = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(240u)) {
          clear_value();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int32, ::google::protobuf::internal::WireFormatLite::TYPE_INT32>(
                 input, &value_.intvalue_)));
          set_has_intvalue();
        } else {
          goto handle_unusual;
        }
        break;
      }

      // int64 longValue = 40;
      case 40: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(320u)) {
          clear_value();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &value_.longvalue_)));
          set_has_longvalue();
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool boolValue = 50;
      case 50: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(400u)) {
          clear_value();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &value_.boolvalue_)));
          set_has_boolvalue();
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  return false;
#undef DO_
}

void CustomLayerParams_CustomLayerParamValue::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // double doubleValue = 10;
  if (has_doublevalue()) {
    ::google::protobuf::internal::WireFormatLite::WriteDouble(10, this->doublevalue(), output);
  }

  // string stringValue = 20;
  if (has_stringvalue()) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->stringvalue().data(), this->stringvalue().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      20, this->stringvalue(), output);
  }

  // int32 intValue = 30;
  if (has_intvalue()) {
    ::google::protobuf::internal::WireFormatLite::WriteInt32(30, this->intvalue(), output);
  }

  // int64 longValue = 40;
  if (has_longvalue()) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(40, this->longvalue(), output);
  }

  // bool boolValue = 50;
  if (has_boolvalue()) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(50, this->boolvalue(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
}

size_t CustomLayerParams_CustomLayerParamValue::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  size_t total_size = 0;

  switch (value_case()) {
    // double doubleValue = 10;
    case kDoubleValue: {
      total_size += 1 + 8;
      break;
    }
    // string stringValue = 20;
    case kStringValue: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::StringSize(
          this->stringvalue());
      break;
    }
    // int32 intValue = 30;
    case kIntValue: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(
          this->intvalue());
      break;
    }
    // int64 longValue = 40;
    case kLongValue: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int64Size(
          this->longvalue());
      break;
    }
    // bool boolValue = 50;
    case kBoolValue: {
      total_size += 2 + 1;
      break;
    }
    case VALUE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CustomLayerParams_CustomLayerParamValue::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CustomLayerParams_CustomLayerParamValue*>(&from));
}

void CustomLayerParams_CustomLayerParamValue::MergeFrom(const CustomLayerParams_CustomLayerParamValue& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  switch (from.value_case()) {
    case kDoubleValue: {
      set_doublevalue(from.doublevalue());
      break;
    }
    case kStringValue: {
      set_stringvalue(from.stringvalue());
      break;
    }
    case kIntValue: {
      set_intvalue(from.intvalue());
      break;
    }
    case kLongValue: {
      set_longvalue(from.longvalue());
      break;
    }
    case kBoolValue: {
      set_boolvalue(from.boolvalue());
      break;
    }
    case VALUE_NOT_SET: {
      break;
    }
  }
}

void CustomLayerParams_CustomLayerParamValue::CopyFrom(const CustomLayerParams_CustomLayerParamValue& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CustomLayerParams_CustomLayerParamValue::IsInitialized() const {
  return true;
}

void CustomLayerParams_CustomLayerParamValue::Swap(CustomLayerParams_CustomLayerParamValue* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CustomLayerParams_CustomLayerParamValue::InternalSwap(CustomLayerParams_CustomLayerParamValue* other) {
  std::swap(value_, other->value_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CustomLayerParams_CustomLayerParamValue::GetTypeName() const {
  return "CoreML.Specification.CustomLayerParams.CustomLayerParamValue";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CustomLayerParams_CustomLayerParamValue

// double doubleValue = 10;
bool CustomLayerParams_CustomLayerParamValue::has_doublevalue() const {
  return value_case() == kDoubleValue;
}
void CustomLayerParams_CustomLayerParamValue::set_has_doublevalue() {
  _oneof_case_[0] = kDoubleValue;
}
void CustomLayerParams_CustomLayerParamValue::clear_doublevalue() {
  if (has_doublevalue()) {
    value_.doublevalue_ = 0;
    clear_has_value();
  }
}
double CustomLayerParams_CustomLayerParamValue::doublevalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.doubleValue)
  if (has_doublevalue()) {
    return value_.doublevalue_;
  }
  return 0;
}
void CustomLayerParams_CustomLayerParamValue::set_doublevalue(double value) {
  if (!has_doublevalue()) {
    clear_value();
    set_has_doublevalue();
  }
  value_.doublevalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.doubleValue)
}

// string stringValue = 20;
bool CustomLayerParams_CustomLayerParamValue::has_stringvalue() const {
  return value_case() == kStringValue;
}
void CustomLayerParams_CustomLayerParamValue::set_has_stringvalue() {
  _oneof_case_[0] = kStringValue;
}
void CustomLayerParams_CustomLayerParamValue::clear_stringvalue() {
  if (has_stringvalue()) {
    value_.stringvalue_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
    clear_has_value();
  }
}
const ::std::string& CustomLayerParams_CustomLayerParamValue::stringvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  if (has_stringvalue()) {
    return value_.stringvalue_.GetNoArena();
  }
  return *&::google::protobuf::internal::GetEmptyStringAlreadyInited();
}
void CustomLayerParams_CustomLayerParamValue::set_stringvalue(const ::std::string& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  if (!has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  value_.stringvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}
#if LANG_CXX11
void CustomLayerParams_CustomLayerParamValue::set_stringvalue(::std::string&& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  if (!has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  value_.stringvalue_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}
#endif
void CustomLayerParams_CustomLayerParamValue::set_stringvalue(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  if (!has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  value_.stringvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}
void CustomLayerParams_CustomLayerParamValue::set_stringvalue(const char* value, size_t size) {
  if (!has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  value_.stringvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(
      reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}
::std::string* CustomLayerParams_CustomLayerParamValue::mutable_stringvalue() {
  if (!has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  return value_.stringvalue_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* CustomLayerParams_CustomLayerParamValue::release_stringvalue() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  if (has_stringvalue()) {
    clear_has_value();
    return value_.stringvalue_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  } else {
    return NULL;
  }
}
void CustomLayerParams_CustomLayerParamValue::set_allocated_stringvalue(::std::string* stringvalue) {
  if (!has_stringvalue()) {
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  clear_value();
  if (stringvalue != NULL) {
    set_has_stringvalue();
    value_.stringvalue_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
        stringvalue);
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}

// int32 intValue = 30;
bool CustomLayerParams_CustomLayerParamValue::has_intvalue() const {
  return value_case() == kIntValue;
}
void CustomLayerParams_CustomLayerParamValue::set_has_intvalue() {
  _oneof_case_[0] = kIntValue;
}
void CustomLayerParams_CustomLayerParamValue::clear_intvalue() {
  if (has_intvalue()) {
    value_.intvalue_ = 0;
    clear_has_value();
  }
}
::google::protobuf::int32 CustomLayerParams_CustomLayerParamValue::intvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.intValue)
  if (has_intvalue()) {
    return value_.intvalue_;
  }
  return 0;
}
void CustomLayerParams_CustomLayerParamValue::set_intvalue(::google::protobuf::int32 value) {
  if (!has_intvalue()) {
    clear_value();
    set_has_intvalue();
  }
  value_.intvalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.intValue)
}

// int64 longValue = 40;
bool CustomLayerParams_CustomLayerParamValue::has_longvalue() const {
  return value_case() == kLongValue;
}
void CustomLayerParams_CustomLayerParamValue::set_has_longvalue() {
  _oneof_case_[0] = kLongValue;
}
void CustomLayerParams_CustomLayerParamValue::clear_longvalue() {
  if (has_longvalue()) {
    value_.longvalue_ = GOOGLE_LONGLONG(0);
    clear_has_value();
  }
}
::google::protobuf::int64 CustomLayerParams_CustomLayerParamValue::longvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.longValue)
  if (has_longvalue()) {
    return value_.longvalue_;
  }
  return GOOGLE_LONGLONG(0);
}
void CustomLayerParams_CustomLayerParamValue::set_longvalue(::google::protobuf::int64 value) {
  if (!has_longvalue()) {
    clear_value();
    set_has_longvalue();
  }
  value_.longvalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.longValue)
}

// bool boolValue = 50;
bool CustomLayerParams_CustomLayerParamValue::has_boolvalue() const {
  return value_case() == kBoolValue;
}
void CustomLayerParams_CustomLayerParamValue::set_has_boolvalue() {
  _oneof_case_[0] = kBoolValue;
}
void CustomLayerParams_CustomLayerParamValue::clear_boolvalue() {
  if (has_boolvalue()) {
    value_.boolvalue_ = false;
    clear_has_value();
  }
}
bool CustomLayerParams_CustomLayerParamValue::boolvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.boolValue)
  if (has_boolvalue()) {
    return value_.boolvalue_;
  }
  return false;
}
void CustomLayerParams_CustomLayerParamValue::set_boolvalue(bool value) {
  if (!has_boolvalue()) {
    clear_value();
    set_has_boolvalue();
  }
  value_.boolvalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.boolValue)
}

bool CustomLayerParams_CustomLayerParamValue::has_value() const {
  return value_case() != VALUE_NOT_SET;
}
void CustomLayerParams_CustomLayerParamValue::clear_has_value() {
  _oneof_case_[0] = VALUE_NOT_SET;
}
CustomLayerParams_CustomLayerParamValue::ValueCase CustomLayerParams_CustomLayerParamValue::value_case() const {
  return CustomLayerParams_CustomLayerParamValue::ValueCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if PROTOBUF_INLINE_NOT_IN_HEADERS
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int CustomLayerParams::kClassNameFieldNumber;
const int CustomLayerParams::kWeightsFieldNumber;
const int CustomLayerParams::kParametersFieldNumber;
const int CustomLayerParams::kDescriptionFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CustomLayerParams::CustomLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CustomLayerParams)
}
CustomLayerParams::CustomLayerParams(const CustomLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      weights_(from.weights_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  parameters_.MergeFrom(from.parameters_);
  classname_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.classname().size() > 0) {
    classname_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.classname_);
  }
  description_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.description().size() > 0) {
    description_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.description_);
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CustomLayerParams)
}

void CustomLayerParams::SharedCtor() {
  classname_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  description_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  _cached_size_ = 0;
}

CustomLayerParams::~CustomLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CustomLayerParams)
  SharedDtor();
}

void CustomLayerParams::SharedDtor() {
  classname_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  description_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}

void CustomLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CustomLayerParams& CustomLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

CustomLayerParams* CustomLayerParams::New(::google::protobuf::Arena* arena) const {
  CustomLayerParams* n = new CustomLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CustomLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CustomLayerParams)
  weights_.Clear();
  parameters_.Clear();
  classname_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  description_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}

bool CustomLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CustomLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // string className = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_classname()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->classname().data(), this->classname().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.CustomLayerParams.className"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.WeightParams weights = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_weights()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // map<string, .CoreML.Specification.CustomLayerParams.CustomLayerParamValue> parameters = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(242u)) {
          CustomLayerParams_ParametersEntry::Parser< ::google::protobuf::internal::MapFieldLite<
              CustomLayerParams_ParametersEntry,
              ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue,
              ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
              ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
              0 >,
            ::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue > > parser(&parameters_);
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
              input, &parser));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            parser.key().data(), parser.key().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.CustomLayerParams.ParametersEntry.key"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // string description = 40;
      case 40: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_description()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->description().data(), this->description().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.CustomLayerParams.description"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CustomLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CustomLayerParams)
  return false;
#undef DO_
}

void CustomLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CustomLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // string className = 10;
  if (this->classname().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->classname().data(), this->classname().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.CustomLayerParams.className");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      10, this->classname(), output);
  }

  // repeated .CoreML.Specification.WeightParams weights = 20;
  for (unsigned int i = 0, n = this->weights_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, this->weights(i), output);
  }

  // map<string, .CoreML.Specification.CustomLayerParams.CustomLayerParamValue> parameters = 30;
  if (!this->parameters().empty()) {
    typedef ::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >::const_pointer
        ConstPtr;
    typedef ConstPtr SortItem;
    typedef ::google::protobuf::internal::CompareByDerefFirst<SortItem> Less;
    struct Utf8Check {
      static void Check(ConstPtr p) {
        ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
          p->first.data(), p->first.length(),
          ::google::protobuf::internal::WireFormatLite::SERIALIZE,
          "CoreML.Specification.CustomLayerParams.ParametersEntry.key");
      }
    };

    if (output->IsSerializationDeterministic() &&
        this->parameters().size() > 1) {
      ::google::protobuf::scoped_array<SortItem> items(
          new SortItem[this->parameters().size()]);
      typedef ::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >::size_type size_type;
      size_type n = 0;
      for (::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >::const_iterator
          it = this->parameters().begin();
          it != this->parameters().end(); ++it, ++n) {
        items[n] = SortItem(&*it);
      }
      ::std::sort(&items[0], &items[n], Less());
      ::google::protobuf::scoped_ptr<CustomLayerParams_ParametersEntry> entry;
      for (size_type i = 0; i < n; i++) {
        entry.reset(parameters_.NewEntryWrapper(
            items[i]->first, items[i]->second));
        ::google::protobuf::internal::WireFormatLite::WriteMessage(
            30, *entry, output);
        Utf8Check::Check(items[i]);
      }
    } else {
      ::google::protobuf::scoped_ptr<CustomLayerParams_ParametersEntry> entry;
      for (::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >::const_iterator
          it = this->parameters().begin();
          it != this->parameters().end(); ++it) {
        entry.reset(parameters_.NewEntryWrapper(
            it->first, it->second));
        ::google::protobuf::internal::WireFormatLite::WriteMessage(
            30, *entry, output);
        Utf8Check::Check(&*it);
      }
    }
  }

  // string description = 40;
  if (this->description().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->description().data(), this->description().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.CustomLayerParams.description");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      40, this->description(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CustomLayerParams)
}

size_t CustomLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CustomLayerParams)
  size_t total_size = 0;

  // repeated .CoreML.Specification.WeightParams weights = 20;
  {
    unsigned int count = this->weights_size();
    total_size += 2UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->weights(i));
    }
  }

  // map<string, .CoreML.Specification.CustomLayerParams.CustomLayerParamValue> parameters = 30;
  total_size += 2 *
      ::google::protobuf::internal::FromIntSize(this->parameters_size());
  {
    ::google::protobuf::scoped_ptr<CustomLayerParams_ParametersEntry> entry;
    for (::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >::const_iterator
        it = this->parameters().begin();
        it != this->parameters().end(); ++it) {
      entry.reset(parameters_.NewEntryWrapper(it->first, it->second));
      total_size += ::google::protobuf::internal::WireFormatLite::
          MessageSizeNoVirtual(*entry);
    }
  }

  // string className = 10;
  if (this->classname().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->classname());
  }

  // string description = 40;
  if (this->description().size() > 0) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->description());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CustomLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CustomLayerParams*>(&from));
}

void CustomLayerParams::MergeFrom(const CustomLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CustomLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  weights_.MergeFrom(from.weights_);
  parameters_.MergeFrom(from.parameters_);
  if (from.classname().size() > 0) {

    classname_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.classname_);
  }
  if (from.description().size() > 0) {

    description_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.description_);
  }
}

void CustomLayerParams::CopyFrom(const CustomLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CustomLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CustomLayerParams::IsInitialized() const {
  return true;
}

void CustomLayerParams::Swap(CustomLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CustomLayerParams::InternalSwap(CustomLayerParams* other) {
  weights_.InternalSwap(&other->weights_);
  parameters_.Swap(&other->parameters_);
  classname_.Swap(&other->classname_);
  description_.Swap(&other->description_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CustomLayerParams::GetTypeName() const {
  return "CoreML.Specification.CustomLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CustomLayerParams

// string className = 10;
void CustomLayerParams::clear_classname() {
  classname_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& CustomLayerParams::classname() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.className)
  return classname_.GetNoArena();
}
void CustomLayerParams::set_classname(const ::std::string& value) {
  
  classname_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.className)
}
#if LANG_CXX11
void CustomLayerParams::set_classname(::std::string&& value) {
  
  classname_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.CustomLayerParams.className)
}
#endif
void CustomLayerParams::set_classname(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  classname_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.CustomLayerParams.className)
}
void CustomLayerParams::set_classname(const char* value, size_t size) {
  
  classname_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.CustomLayerParams.className)
}
::std::string* CustomLayerParams::mutable_classname() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CustomLayerParams.className)
  return classname_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* CustomLayerParams::release_classname() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CustomLayerParams.className)
  
  return classname_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void CustomLayerParams::set_allocated_classname(::std::string* classname) {
  if (classname != NULL) {
    
  } else {
    
  }
  classname_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), classname);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CustomLayerParams.className)
}

// repeated .CoreML.Specification.WeightParams weights = 20;
int CustomLayerParams::weights_size() const {
  return weights_.size();
}
void CustomLayerParams::clear_weights() {
  weights_.Clear();
}
const ::CoreML::Specification::WeightParams& CustomLayerParams::weights(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.weights)
  return weights_.Get(index);
}
::CoreML::Specification::WeightParams* CustomLayerParams::mutable_weights(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CustomLayerParams.weights)
  return weights_.Mutable(index);
}
::CoreML::Specification::WeightParams* CustomLayerParams::add_weights() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.CustomLayerParams.weights)
  return weights_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::WeightParams >*
CustomLayerParams::mutable_weights() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.CustomLayerParams.weights)
  return &weights_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::WeightParams >&
CustomLayerParams::weights() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.CustomLayerParams.weights)
  return weights_;
}

// map<string, .CoreML.Specification.CustomLayerParams.CustomLayerParamValue> parameters = 30;
int CustomLayerParams::parameters_size() const {
  return parameters_.size();
}
void CustomLayerParams::clear_parameters() {
  parameters_.Clear();
}
 const ::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >&
CustomLayerParams::parameters() const {
  // @@protoc_insertion_point(field_map:CoreML.Specification.CustomLayerParams.parameters)
  return parameters_.GetMap();
}
 ::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >*
CustomLayerParams::mutable_parameters() {
  // @@protoc_insertion_point(field_mutable_map:CoreML.Specification.CustomLayerParams.parameters)
  return parameters_.MutableMap();
}

// string description = 40;
void CustomLayerParams::clear_description() {
  description_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& CustomLayerParams::description() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.description)
  return description_.GetNoArena();
}
void CustomLayerParams::set_description(const ::std::string& value) {
  
  description_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.description)
}
#if LANG_CXX11
void CustomLayerParams::set_description(::std::string&& value) {
  
  description_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.CustomLayerParams.description)
}
#endif
void CustomLayerParams::set_description(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  description_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.CustomLayerParams.description)
}
void CustomLayerParams::set_description(const char* value, size_t size) {
  
  description_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.CustomLayerParams.description)
}
::std::string* CustomLayerParams::mutable_description() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CustomLayerParams.description)
  return description_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* CustomLayerParams::release_description() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CustomLayerParams.description)
  
  return description_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void CustomLayerParams::set_allocated_description(::std::string* description) {
  if (description != NULL) {
    
  } else {
    
  }
  description_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), description);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CustomLayerParams.description)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkClassifier::kLayersFieldNumber;
const int NeuralNetworkClassifier::kPreprocessingFieldNumber;
const int NeuralNetworkClassifier::kStringClassLabelsFieldNumber;
const int NeuralNetworkClassifier::kInt64ClassLabelsFieldNumber;
const int NeuralNetworkClassifier::kLabelProbabilityLayerNameFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkClassifier::NeuralNetworkClassifier()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkClassifier)
}
NeuralNetworkClassifier::NeuralNetworkClassifier(const NeuralNetworkClassifier& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      layers_(from.layers_),
      preprocessing_(from.preprocessing_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  labelprobabilitylayername_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.labelprobabilitylayername().size() > 0) {
    labelprobabilitylayername_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.labelprobabilitylayername_);
  }
  clear_has_ClassLabels();
  switch (from.ClassLabels_case()) {
    case kStringClassLabels: {
      mutable_stringclasslabels()->::CoreML::Specification::StringVector::MergeFrom(from.stringclasslabels());
      break;
    }
    case kInt64ClassLabels: {
      mutable_int64classlabels()->::CoreML::Specification::Int64Vector::MergeFrom(from.int64classlabels());
      break;
    }
    case CLASSLABELS_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkClassifier)
}

void NeuralNetworkClassifier::SharedCtor() {
  labelprobabilitylayername_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_ClassLabels();
  _cached_size_ = 0;
}

NeuralNetworkClassifier::~NeuralNetworkClassifier() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkClassifier)
  SharedDtor();
}

void NeuralNetworkClassifier::SharedDtor() {
  labelprobabilitylayername_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (has_ClassLabels()) {
    clear_ClassLabels();
  }
}

void NeuralNetworkClassifier::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkClassifier& NeuralNetworkClassifier::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NeuralNetworkClassifier* NeuralNetworkClassifier::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkClassifier* n = new NeuralNetworkClassifier;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkClassifier::clear_ClassLabels() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.NeuralNetworkClassifier)
  switch (ClassLabels_case()) {
    case kStringClassLabels: {
      delete ClassLabels_.stringclasslabels_;
      break;
    }
    case kInt64ClassLabels: {
      delete ClassLabels_.int64classlabels_;
      break;
    }
    case CLASSLABELS_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = CLASSLABELS_NOT_SET;
}


void NeuralNetworkClassifier::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkClassifier)
  layers_.Clear();
  preprocessing_.Clear();
  labelprobabilitylayername_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_ClassLabels();
}

bool NeuralNetworkClassifier::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkClassifier)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_layers()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_preprocessing()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.StringVector stringClassLabels = 100;
      case 100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(802u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_stringclasslabels()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.Int64Vector int64ClassLabels = 101;
      case 101: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(810u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_int64classlabels()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // string labelProbabilityLayerName = 200;
      case 200: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1602u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_labelprobabilitylayername()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->labelprobabilitylayername().data(), this->labelprobabilitylayername().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkClassifier)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkClassifier)
  return false;
#undef DO_
}

void NeuralNetworkClassifier::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkClassifier)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  for (unsigned int i = 0, n = this->layers_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, this->layers(i), output);
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  for (unsigned int i = 0, n = this->preprocessing_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, this->preprocessing(i), output);
  }

  // .CoreML.Specification.StringVector stringClassLabels = 100;
  if (has_stringclasslabels()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      100, *ClassLabels_.stringclasslabels_, output);
  }

  // .CoreML.Specification.Int64Vector int64ClassLabels = 101;
  if (has_int64classlabels()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      101, *ClassLabels_.int64classlabels_, output);
  }

  // string labelProbabilityLayerName = 200;
  if (this->labelprobabilitylayername().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->labelprobabilitylayername().data(), this->labelprobabilitylayername().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      200, this->labelprobabilitylayername(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkClassifier)
}

size_t NeuralNetworkClassifier::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkClassifier)
  size_t total_size = 0;

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  {
    unsigned int count = this->layers_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->layers(i));
    }
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  {
    unsigned int count = this->preprocessing_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->preprocessing(i));
    }
  }

  // string labelProbabilityLayerName = 200;
  if (this->labelprobabilitylayername().size() > 0) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->labelprobabilitylayername());
  }

  switch (ClassLabels_case()) {
    // .CoreML.Specification.StringVector stringClassLabels = 100;
    case kStringClassLabels: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *ClassLabels_.stringclasslabels_);
      break;
    }
    // .CoreML.Specification.Int64Vector int64ClassLabels = 101;
    case kInt64ClassLabels: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *ClassLabels_.int64classlabels_);
      break;
    }
    case CLASSLABELS_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkClassifier::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkClassifier*>(&from));
}

void NeuralNetworkClassifier::MergeFrom(const NeuralNetworkClassifier& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkClassifier)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  layers_.MergeFrom(from.layers_);
  preprocessing_.MergeFrom(from.preprocessing_);
  if (from.labelprobabilitylayername().size() > 0) {

    labelprobabilitylayername_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.labelprobabilitylayername_);
  }
  switch (from.ClassLabels_case()) {
    case kStringClassLabels: {
      mutable_stringclasslabels()->::CoreML::Specification::StringVector::MergeFrom(from.stringclasslabels());
      break;
    }
    case kInt64ClassLabels: {
      mutable_int64classlabels()->::CoreML::Specification::Int64Vector::MergeFrom(from.int64classlabels());
      break;
    }
    case CLASSLABELS_NOT_SET: {
      break;
    }
  }
}

void NeuralNetworkClassifier::CopyFrom(const NeuralNetworkClassifier& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkClassifier)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NeuralNetworkClassifier::IsInitialized() const {
  return true;
}

void NeuralNetworkClassifier::Swap(NeuralNetworkClassifier* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkClassifier::InternalSwap(NeuralNetworkClassifier* other) {
  layers_.InternalSwap(&other->layers_);
  preprocessing_.InternalSwap(&other->preprocessing_);
  labelprobabilitylayername_.Swap(&other->labelprobabilitylayername_);
  std::swap(ClassLabels_, other->ClassLabels_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkClassifier::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkClassifier";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkClassifier

// repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
int NeuralNetworkClassifier::layers_size() const {
  return layers_.size();
}
void NeuralNetworkClassifier::clear_layers() {
  layers_.Clear();
}
const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetworkClassifier::layers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_.Get(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkClassifier::mutable_layers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkClassifier::add_layers() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
NeuralNetworkClassifier::mutable_layers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkClassifier.layers)
  return &layers_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
NeuralNetworkClassifier::layers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_;
}

// repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
int NeuralNetworkClassifier::preprocessing_size() const {
  return preprocessing_.size();
}
void NeuralNetworkClassifier::clear_preprocessing() {
  preprocessing_.Clear();
}
const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetworkClassifier::preprocessing(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_.Get(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkClassifier::mutable_preprocessing(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkClassifier::add_preprocessing() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
NeuralNetworkClassifier::mutable_preprocessing() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return &preprocessing_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
NeuralNetworkClassifier::preprocessing() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_;
}

// .CoreML.Specification.StringVector stringClassLabels = 100;
bool NeuralNetworkClassifier::has_stringclasslabels() const {
  return ClassLabels_case() == kStringClassLabels;
}
void NeuralNetworkClassifier::set_has_stringclasslabels() {
  _oneof_case_[0] = kStringClassLabels;
}
void NeuralNetworkClassifier::clear_stringclasslabels() {
  if (has_stringclasslabels()) {
    delete ClassLabels_.stringclasslabels_;
    clear_has_ClassLabels();
  }
}
 const ::CoreML::Specification::StringVector& NeuralNetworkClassifier::stringclasslabels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  return has_stringclasslabels()
      ? *ClassLabels_.stringclasslabels_
      : ::CoreML::Specification::StringVector::default_instance();
}
::CoreML::Specification::StringVector* NeuralNetworkClassifier::mutable_stringclasslabels() {
  if (!has_stringclasslabels()) {
    clear_ClassLabels();
    set_has_stringclasslabels();
    ClassLabels_.stringclasslabels_ = new ::CoreML::Specification::StringVector;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  return ClassLabels_.stringclasslabels_;
}
::CoreML::Specification::StringVector* NeuralNetworkClassifier::release_stringclasslabels() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  if (has_stringclasslabels()) {
    clear_has_ClassLabels();
    ::CoreML::Specification::StringVector* temp = ClassLabels_.stringclasslabels_;
    ClassLabels_.stringclasslabels_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkClassifier::set_allocated_stringclasslabels(::CoreML::Specification::StringVector* stringclasslabels) {
  clear_ClassLabels();
  if (stringclasslabels) {
    set_has_stringclasslabels();
    ClassLabels_.stringclasslabels_ = stringclasslabels;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
}

// .CoreML.Specification.Int64Vector int64ClassLabels = 101;
bool NeuralNetworkClassifier::has_int64classlabels() const {
  return ClassLabels_case() == kInt64ClassLabels;
}
void NeuralNetworkClassifier::set_has_int64classlabels() {
  _oneof_case_[0] = kInt64ClassLabels;
}
void NeuralNetworkClassifier::clear_int64classlabels() {
  if (has_int64classlabels()) {
    delete ClassLabels_.int64classlabels_;
    clear_has_ClassLabels();
  }
}
 const ::CoreML::Specification::Int64Vector& NeuralNetworkClassifier::int64classlabels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  return has_int64classlabels()
      ? *ClassLabels_.int64classlabels_
      : ::CoreML::Specification::Int64Vector::default_instance();
}
::CoreML::Specification::Int64Vector* NeuralNetworkClassifier::mutable_int64classlabels() {
  if (!has_int64classlabels()) {
    clear_ClassLabels();
    set_has_int64classlabels();
    ClassLabels_.int64classlabels_ = new ::CoreML::Specification::Int64Vector;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  return ClassLabels_.int64classlabels_;
}
::CoreML::Specification::Int64Vector* NeuralNetworkClassifier::release_int64classlabels() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  if (has_int64classlabels()) {
    clear_has_ClassLabels();
    ::CoreML::Specification::Int64Vector* temp = ClassLabels_.int64classlabels_;
    ClassLabels_.int64classlabels_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkClassifier::set_allocated_int64classlabels(::CoreML::Specification::Int64Vector* int64classlabels) {
  clear_ClassLabels();
  if (int64classlabels) {
    set_has_int64classlabels();
    ClassLabels_.int64classlabels_ = int64classlabels;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
}

// string labelProbabilityLayerName = 200;
void NeuralNetworkClassifier::clear_labelprobabilitylayername() {
  labelprobabilitylayername_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& NeuralNetworkClassifier::labelprobabilitylayername() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
  return labelprobabilitylayername_.GetNoArena();
}
void NeuralNetworkClassifier::set_labelprobabilitylayername(const ::std::string& value) {
  
  labelprobabilitylayername_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
#if LANG_CXX11
void NeuralNetworkClassifier::set_labelprobabilitylayername(::std::string&& value) {
  
  labelprobabilitylayername_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
#endif
void NeuralNetworkClassifier::set_labelprobabilitylayername(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  labelprobabilitylayername_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
void NeuralNetworkClassifier::set_labelprobabilitylayername(const char* value, size_t size) {
  
  labelprobabilitylayername_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
::std::string* NeuralNetworkClassifier::mutable_labelprobabilitylayername() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
  return labelprobabilitylayername_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* NeuralNetworkClassifier::release_labelprobabilitylayername() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
  
  return labelprobabilitylayername_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void NeuralNetworkClassifier::set_allocated_labelprobabilitylayername(::std::string* labelprobabilitylayername) {
  if (labelprobabilitylayername != NULL) {
    
  } else {
    
  }
  labelprobabilitylayername_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), labelprobabilitylayername);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}

bool NeuralNetworkClassifier::has_ClassLabels() const {
  return ClassLabels_case() != CLASSLABELS_NOT_SET;
}
void NeuralNetworkClassifier::clear_has_ClassLabels() {
  _oneof_case_[0] = CLASSLABELS_NOT_SET;
}
NeuralNetworkClassifier::ClassLabelsCase NeuralNetworkClassifier::ClassLabels_case() const {
  return NeuralNetworkClassifier::ClassLabelsCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkRegressor::kLayersFieldNumber;
const int NeuralNetworkRegressor::kPreprocessingFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkRegressor::NeuralNetworkRegressor()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkRegressor)
}
NeuralNetworkRegressor::NeuralNetworkRegressor(const NeuralNetworkRegressor& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      layers_(from.layers_),
      preprocessing_(from.preprocessing_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkRegressor)
}

void NeuralNetworkRegressor::SharedCtor() {
  _cached_size_ = 0;
}

NeuralNetworkRegressor::~NeuralNetworkRegressor() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkRegressor)
  SharedDtor();
}

void NeuralNetworkRegressor::SharedDtor() {
}

void NeuralNetworkRegressor::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkRegressor& NeuralNetworkRegressor::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NeuralNetworkRegressor* NeuralNetworkRegressor::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkRegressor* n = new NeuralNetworkRegressor;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkRegressor::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkRegressor)
  layers_.Clear();
  preprocessing_.Clear();
}

bool NeuralNetworkRegressor::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkRegressor)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_layers()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_preprocessing()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkRegressor)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkRegressor)
  return false;
#undef DO_
}

void NeuralNetworkRegressor::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkRegressor)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  for (unsigned int i = 0, n = this->layers_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, this->layers(i), output);
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  for (unsigned int i = 0, n = this->preprocessing_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, this->preprocessing(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkRegressor)
}

size_t NeuralNetworkRegressor::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkRegressor)
  size_t total_size = 0;

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  {
    unsigned int count = this->layers_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->layers(i));
    }
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  {
    unsigned int count = this->preprocessing_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->preprocessing(i));
    }
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkRegressor::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkRegressor*>(&from));
}

void NeuralNetworkRegressor::MergeFrom(const NeuralNetworkRegressor& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkRegressor)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  layers_.MergeFrom(from.layers_);
  preprocessing_.MergeFrom(from.preprocessing_);
}

void NeuralNetworkRegressor::CopyFrom(const NeuralNetworkRegressor& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkRegressor)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NeuralNetworkRegressor::IsInitialized() const {
  return true;
}

void NeuralNetworkRegressor::Swap(NeuralNetworkRegressor* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkRegressor::InternalSwap(NeuralNetworkRegressor* other) {
  layers_.InternalSwap(&other->layers_);
  preprocessing_.InternalSwap(&other->preprocessing_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkRegressor::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkRegressor";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkRegressor

// repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
int NeuralNetworkRegressor::layers_size() const {
  return layers_.size();
}
void NeuralNetworkRegressor::clear_layers() {
  layers_.Clear();
}
const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetworkRegressor::layers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_.Get(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkRegressor::mutable_layers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkRegressor::add_layers() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
NeuralNetworkRegressor::mutable_layers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkRegressor.layers)
  return &layers_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
NeuralNetworkRegressor::layers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_;
}

// repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
int NeuralNetworkRegressor::preprocessing_size() const {
  return preprocessing_.size();
}
void NeuralNetworkRegressor::clear_preprocessing() {
  preprocessing_.Clear();
}
const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetworkRegressor::preprocessing(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_.Get(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkRegressor::mutable_preprocessing(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkRegressor::add_preprocessing() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
NeuralNetworkRegressor::mutable_preprocessing() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return &preprocessing_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
NeuralNetworkRegressor::preprocessing() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// @@protoc_insertion_point(namespace_scope)

}  // namespace Specification
}  // namespace CoreML

// @@protoc_insertion_point(global_scope)
