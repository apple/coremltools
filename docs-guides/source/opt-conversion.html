
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Conversion &#8212; Guide to Core ML Tools</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/imgstyle.css?v=27a1495e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=c1ce5b23"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'source/opt-conversion';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Compressing Neural Network Weights" href="quantization-neural-network.html" />
    <link rel="prev" title="Combining Compression Types" href="opt-joint-compression.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Guide to Core ML Tools</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://apple.github.io/coremltools/index.html">coremltools API Reference</a></li>
<li class="toctree-l1"><a class="reference external" href="https://apple.github.io/coremltools/mlmodel/index.html">Core ML Model Format</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="overview-coremltools.html">What Is Core ML Tools?</a></li>
<li class="toctree-l1"><a class="reference internal" href="installing-coremltools.html">Installing Core ML Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="introductory-quickstart.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="new-features.html">New Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="faqs.html">Core ML Tools FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="coremltools-examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="how-to-contribute.html">Contributing</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unified Conversion</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="unified-conversion-api.html">Core ML Tools API Overview</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="convert-learning-models.html">Converting Deep Learning Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="target-conversion-formats.html">Source and Conversion Formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="load-and-convert-model.html">Load and Convert Model Workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-to-ml-program.html">Convert Models to ML Programs</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-to-neural-network.html">Convert Models to Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="comparing-ml-programs-and-neural-networks.html">Comparing ML Programs and Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="typed-execution.html">Typed Execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="typed-execution-example.html">Typed Execution Workflow Example</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="convert-tensorflow.html">Converting from TensorFlow</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="tensorflow-1-workflow.html">TensorFlow 1 Workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-a-tensorflow-1-image-classifier.html">Converting a TensorFlow 1 Image Classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-a-tensorflow-1-deepspeech-model.html">Converting a TensorFlow 1 DeepSpeech Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensorflow-2.html">TensorFlow 2 Workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-tensorflow-2-bert-transformer-models.html">Converting TensorFlow 2 BERT Transformer Models</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="convert-pytorch.html">Converting from PyTorch</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="convert-pytorch-workflow.html">PyTorch Conversion Workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="model-tracing.html">Model Tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="model-scripting.html">Model Scripting</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-nlp-model.html">Converting a Natural Language Processing Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="convert-a-torchvision-model-from-pytorch.html">Converting a torchvision Model from PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytorch-conversion-examples.html">Converting a PyTorch Segmentation Model</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="conversion-options.html">Conversion Options</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="new-conversion-options.html">New Conversion Options</a></li>
<li class="toctree-l2"><a class="reference internal" href="model-input-and-output-types.html">Model Input and Output Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-inputs.html">Image Input and Output</a></li>
<li class="toctree-l2"><a class="reference internal" href="stateful-models.html">Stateful Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="classifiers.html">Classifiers</a></li>
<li class="toctree-l2"><a class="reference internal" href="flexible-inputs.html">Flexible Input Shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="composite-operators.html">Composite Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="custom-operators.html">Custom Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="graph-passes-intro.html">Graph Passes</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="model-intermediate-language.html">Model Intermediate Language</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Optimization</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="opt-overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="opt-whats-new.html">What’s New</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="opt-overview-examples.html">Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="opt-resnet.html">Optimizing ResNet50 Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-opt1_3.html">Optimizing OPT Model</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="opt-workflow.html">Optimization Workflow</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="opt-palettization.html">Palettization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="opt-palettization-overview.html">Palettization Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-palettization-perf.html">Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-palettization-algos.html">Palettization Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-palettization-api.html">API Overview</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="opt-quantization.html">Linear Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="opt-quantization-overview.html">Quantization Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-quantization-perf.html">Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-quantization-algos.html">Quantization Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-quantization-api.html">API Overview</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="opt-pruning.html">Pruning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="opt-pruning-overview.html">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-pruning-perf.html">Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-pruning-algos.html">Pruning Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="opt-pruning-api.html">API Overview</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="opt-joint-compression.html">Combining Compression Types</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Conversion</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization-neural-network.html">Compressing Neural Network Weights</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Other Converters</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="libsvm-conversion.html">LibSVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="sci-kit-learn-conversion.html">Scikit-learn</a></li>
<li class="toctree-l1"><a class="reference internal" href="xgboost-conversion.html">XGBoost</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">MLModel</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mlmodel.html">MLModel Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="multifunction-models.html">Multifunction Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="xcode-model-preview-types.html">Xcode Model Preview Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlmodel-utilities.html">MLModel Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="model-prediction.html">Model Prediction</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="updatable-model-examples.html">Updatable Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="updatable-neural-network-classifier-on-mnist-dataset.html">Neural Network Classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="updatable-tiny-drawing-classifier-pipeline-model.html">Pipeline Classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="updatable-nearest-neighbor-classifier.html">Nearest Neighbor Classifier</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/apple/coremltools" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/source/opt-conversion.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Conversion</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#specify-pass-pipeline">Specify pass pipeline</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convert-models-with-sparse-weights">Convert models with sparse weights</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convert-models-with-palettized-weights">Convert models with palettized weights</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#use-compression-info-embedded-in-torch-models">Use compression info embedded in torch models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Convert models with sparse weights</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Convert models with palettized weights</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convert-pytorch-models-with-quantized-weights-and-activations">Convert PyTorch models with quantized weights and activations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convert-models-with-jointly-compressed-weights">Convert models with jointly compressed weights</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compression-info-protocol">Compression info protocol</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#version-1">Version 1</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="conversion">
<h1>Conversion<a class="headerlink" href="#conversion" title="Link to this heading">#</a></h1>
<p>Given a compressed torch model whose weights are compressed but still
represented in dense <code class="docutils literal notranslate"><span class="pre">float</span></code> format,
we want the converted Core ML model to have the corresponding
<a class="reference external" href="https://apple.github.io/coremltools/source/coremltools.converters.mil.mil.ops.defs.html#module-coremltools.converters.mil.mil.ops.defs.iOS16.constexpr_ops">compressed MIL</a>
ops. This will ensure that the compression benefits at runtime are realized.</p>
<p>The converted model with the compressed MIL ops can be generated in two ways:</p>
<ul class="simple">
<li><p>By using the compression info stored explicitly in the torch model: if you are using
coremltools 8 and <code class="docutils literal notranslate"><span class="pre">ct.optimize.torch.*</span></code> APIs, the APIs
will automatically insert compression info in the torch model, which will be picked
by <code class="docutils literal notranslate"><span class="pre">ct.convert</span></code>. To learn more details on how this is implemented,
see the section <a class="reference internal" href="#use-compression-info-embedded-in-torch-models">Use compression info embedded in torch models</a>.</p></li>
<li><p>By specifying a graph pass during conversion: this is required for some compression modes if
you are using Core ML Tools 7. To learn more about this, see the section <a class="reference internal" href="#specify-pass-pipeline">Specify pass pipeline</a>.
Note that this does not cover all the available compression modes. The first option above
is the recommended workflow.</p></li>
</ul>
<section id="specify-pass-pipeline">
<h2>Specify pass pipeline<a class="headerlink" href="#specify-pass-pipeline" title="Link to this heading">#</a></h2>
<p>If you are using Core ML Tools 7, you need to specify the graph pass for converting palettized or
sparse torch models.</p>
<section id="convert-models-with-sparse-weights">
<h3>Convert models with sparse weights<a class="headerlink" href="#convert-models-with-sparse-weights" title="Link to this heading">#</a></h3>
<p>If your source model weights have lots of zeros, then specify the <code class="docutils literal notranslate"><span class="pre">pass_pipeline</span></code> argument as
follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">coremltools</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">model_with_sparse_weights</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span>
    <span class="n">mlmodel</span><span class="p">,</span>
    <span class="n">pass_pipeline</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">PassPipeline</span><span class="o">.</span><span class="n">DEFAULT_PRUNING</span><span class="p">,</span>
    <span class="n">minimum_deployment_target</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">iOS17</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>During the conversion process, an additional <a class="reference internal" href="graph-passes-intro.html"><span class="doc std std-doc">graph pass</span></a> will be run, which
will convert the weight values below a certain low threshold (<code class="docutils literal notranslate"><span class="pre">default=1e-3</span></code>) to exact zeros, and
then use a sparse representation to store the weights, thereby saving space.</p>
</section>
<section id="convert-models-with-palettized-weights">
<h3>Convert models with palettized weights<a class="headerlink" href="#convert-models-with-palettized-weights" title="Link to this heading">#</a></h3>
<p>If your source model weights are palettized; that is, clustered and can only take on a few discrete
values, then you can save space by invoking the following pass during conversion:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">coremltools</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">model_with_lut_weights</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span>
    <span class="n">mlmodel</span><span class="p">,</span>
    <span class="n">pass_pipeline</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">PassPipeline</span><span class="o">.</span><span class="n">DEFAULT_PALETTIZATION</span><span class="p">,</span>
    <span class="n">minimum_deployment_target</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">macOS13</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>This will invoke an additional <a class="reference internal" href="graph-passes-intro.html"><span class="doc std std-doc">graph pass</span></a> that will automatically detect
whether the weights have repeated values. If they do, and if the overall weight tensor has at most
256 or less unique values (which means it can be represented with an 8-bit or
less <a class="reference external" href="https://en.wikipedia.org/wiki/Lookup_table">lookup table</a>), it will use a more compact
representation using lookup tables (LUTs) to store them.</p>
</section>
</section>
<section id="use-compression-info-embedded-in-torch-models">
<h2>Use compression info embedded in torch models<a class="headerlink" href="#use-compression-info-embedded-in-torch-models" title="Link to this heading">#</a></h2>
<p>The previous method with pass pipeline has the following limitations:</p>
<ul class="simple">
<li><p>With more compression granularity supported in Core ML Tools 8, it’s hard to infer the
compression info accurately. For example, if the model is compressed by grouped channel-wise
palettization, we need group size and channel axis to correctly divide weights into groups. In addition,
the fp16 numerical instability could lead to incorrect inferred n-bit values.</p></li>
<li><p>You always need to specify the <code class="docutils literal notranslate"><span class="pre">pass_pipeline</span></code> parameter. If you forget, the conversion
will still succeed, but the converted Core ML model will be a float model without compressed MIL ops.</p></li>
</ul>
<p>In Core ML tools 8, the <code class="docutils literal notranslate"><span class="pre">ct.optimize.torch.*</span></code> APIs have been updated so that they
now embed the info about the compression that was performed on the torch model.
The info is then read by the <code class="docutils literal notranslate"><span class="pre">ct.convert</span></code> API automatically, so as a user you do not have to do
anything special: just use the APIs to compress your torch models and convert them. There is no need
to specify any pass pipelines.</p>
<p>This approach is more general and allows for handling compression modes, such as per-grouped-channel
palettization, per-block 4-bit quantization, etc., which are not natively expressible in torch models.</p>
<p>Here is the protocol of the compression information embedded into torch models:</p>
<ul class="simple">
<li><p>The compression information is stored by torch’s registered buffers.</p></li>
<li><p>The registered buffer names should be in the format of <code class="docutils literal notranslate"><span class="pre">_COREML_/&lt;parameter_name&gt;/&lt;field_name&gt;</span></code>.
For example, <code class="docutils literal notranslate"><span class="pre">'dense1._COREML_/weight/n_bits'</span></code>, <code class="docutils literal notranslate"><span class="pre">'dense1._COREML_/weight/quantization_scale'</span></code>, etc.</p></li>
<li><p>Available <code class="docutils literal notranslate"><span class="pre">field_name</span></code> options are <code class="docutils literal notranslate"><span class="pre">compression_type</span></code>, <code class="docutils literal notranslate"><span class="pre">quantization_n_bits</span></code>, <code class="docutils literal notranslate"><span class="pre">quantization_scale</span></code>,
<code class="docutils literal notranslate"><span class="pre">zero_point</span></code>, <code class="docutils literal notranslate"><span class="pre">lut</span></code>, and <code class="docutils literal notranslate"><span class="pre">palettization_scale</span></code>.
Details about the meaning of each field can be found
in <a class="reference internal" href="#compression-info-protocol">Compression info protocol</a>. The following subsections
also go over a few examples.</p></li>
</ul>
<section id="id1">
<h3>Convert models with sparse weights<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>If the model is produced by <code class="docutils literal notranslate"><span class="pre">ct.optimize.torch.pruning.*</span></code> APIs, you can convert it directly.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">coremltools</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">model_with_sparse_weights</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span>
    <span class="n">pruned_torch_model</span><span class="p">,</span>
    <span class="n">minimum_deployment_target</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">iOS18</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>If the model is produced by other tools, you can update the torch model, prior to tracing and
conversion, by embedding the compression information via <code class="docutils literal notranslate"><span class="pre">register_buffer</span></code> for each module that has been
compressed, as shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Assume the model has two conv layers and both of them got pruned.</span>
<span class="n">pruned_torch_model</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/metadata_version&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">pruned_torch_model</span><span class="o">.</span><span class="n">conv_1</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/weight/compression_type&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">pruned_torch_model</span><span class="o">.</span><span class="n">conv_2</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/weight/compression_type&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">model_with_sparse_weights</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span>
    <span class="n">pruned_torch_model</span><span class="p">,</span>
    <span class="n">minimum_deployment_target</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">iOS18</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id2">
<h3>Convert models with palettized weights<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>If the model is produced by <code class="docutils literal notranslate"><span class="pre">ct.optimize.torch</span></code>, you can convert it directly.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">coremltools</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">model_with_lut_weights</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span>
    <span class="n">palettized_torch_model</span><span class="p">,</span>
    <span class="n">minimum_deployment_target</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">iOS18</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>If the model is produced by other tools, you can embed compression information by registering buffers
for each layer, as shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Assume the model has two conv layers, and both of them got palettized, while the first layer has per-channel-scale.</span>
<span class="n">palettized_torch_model</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/metadata_version&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">palettized_torch_model</span><span class="o">.</span><span class="n">conv_1</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/weight/compression_type&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">]))</span>
<span class="n">palettized_torch_model</span><span class="o">.</span><span class="n">conv_1</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/weight/lut&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">lut1</span><span class="p">))</span>
<span class="n">palettized_torch_model</span><span class="o">.</span><span class="n">conv_1</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
    <span class="s2">&quot;_COREML_/weight/palettization_scale&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">scale_1</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">palettized_torch_model</span><span class="o">.</span><span class="n">conv_2</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/weight/compression_type&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">]))</span>
<span class="n">palettized_torch_model</span><span class="o">.</span><span class="n">conv_2</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/weight/lut&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">lut2</span><span class="p">))</span>

<span class="n">model_with_lut_weights</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span>
    <span class="n">palettized_torch_model</span><span class="p">,</span>
    <span class="n">minimum_deployment_target</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">iOS18</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="convert-pytorch-models-with-quantized-weights-and-activations">
<h3>Convert PyTorch models with quantized weights and activations<a class="headerlink" href="#convert-pytorch-models-with-quantized-weights-and-activations" title="Link to this heading">#</a></h3>
<p>If you use PyTorch’s built-in quantization tool, the produced compressed models store quantized
weights or activations using <code class="docutils literal notranslate"><span class="pre">qint</span></code> or <code class="docutils literal notranslate"><span class="pre">quint</span></code> data types, and additional quantization ops are used.
This is picked up automatically by the conversion process, which then automatically uses linear
quantized storage format to store weights.</p>
<p>However, for now, the PyTorch built-in quantization tool only supports per-tensor and per-channel
quantization. For more compression schemas / granularity, you need to use <code class="docutils literal notranslate"><span class="pre">ct.optimize.torch.*</span></code> APIs
or other third-party tools.</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">ct.optimize.torch.*</span></code> APIs to quantize your torch model, the compression information will be
automatically stored in the compressed torch model, which can be converted seamlessly.</p>
<p>If you use other tools, you can inject compression information by registering buffers for each layer, as
shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Assume the model has two linear layers and only the second linear layer&#39;s weight and bias got quantized.</span>
<span class="n">quantized_torch_model</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/metadata_version&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">quantized_torch_model</span><span class="o">.</span><span class="n">linear_2</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/weight/compression_type&quot;</span><span class="p">,</span>
                                               <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">]))</span>
<span class="n">quantized_torch_model</span><span class="o">.</span><span class="n">linear_2</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/weight/quantization_n_bits&quot;</span><span class="p">,</span>
                                               <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">n_bits</span><span class="p">))</span>
<span class="n">quantized_torch_model</span><span class="o">.</span><span class="n">linear_2</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
    <span class="s2">&quot;_COREML_/weight/quantization_scale&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">weight_scale</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">quantized_torch_model</span><span class="o">.</span><span class="n">linear_2</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/bias/compression_type&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">]))</span>
<span class="n">quantized_torch_model</span><span class="o">.</span><span class="n">linear_2</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/bias/quantization_n_bits&quot;</span><span class="p">,</span>
                                               <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">n_bits</span><span class="p">))</span>
<span class="n">quantized_torch_model</span><span class="o">.</span><span class="n">linear_2</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
    <span class="s2">&quot;_COREML_/bias/quantization_scale&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">bias_scale</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">model_with_quantized_weights</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span>
    <span class="n">quantized_torch_model</span><span class="p">,</span>
    <span class="n">minimum_deployment_target</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">iOS18</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="convert-models-with-jointly-compressed-weights">
<h3>Convert models with jointly compressed weights<a class="headerlink" href="#convert-models-with-jointly-compressed-weights" title="Link to this heading">#</a></h3>
<p>Starting from Core ML Tools 8, the <code class="docutils literal notranslate"><span class="pre">ct.optimize.torch.*</span></code> APIs allows for joint compression schemes,
such as <code class="docutils literal notranslate"><span class="pre">pruning</span> <span class="pre">+</span> <span class="pre">quantization</span></code> and <code class="docutils literal notranslate"><span class="pre">pruning</span> <span class="pre">+</span> <span class="pre">palettization</span></code>.
The produced models can also be converted seamlessly.</p>
<p>Similarly, if you use other tools which quantized the non-zero entries of the sparse weight, you can
inject compression information by registering buffers for each layer, as shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Assume the model has two convs layers and both got jointly compressed by pruning + quantization.</span>
<span class="n">joint_compressed_model</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/metadata_version&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">joint_compressed_model</span><span class="o">.</span><span class="n">conv_1</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/weight/compression_type&quot;</span><span class="p">,</span>
                                              <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]))</span>
<span class="n">joint_compressed_model</span><span class="o">.</span><span class="n">conv_1</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/weight/quantization_n_bits&quot;</span><span class="p">,</span>
                                              <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">n_bits</span><span class="p">))</span>
<span class="n">joint_compressed_model</span><span class="o">.</span><span class="n">conv_1</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
    <span class="s2">&quot;_COREML_/weight/quantization_scale&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">scale_1</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">joint_compressed_model</span><span class="o">.</span><span class="n">conv_1</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/weight/zero_point&quot;</span><span class="p">,</span>
                                              <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">zero_point_1</span><span class="p">))</span>
<span class="n">joint_compressed_model</span><span class="o">.</span><span class="n">conv_2</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/weight/compression_type&quot;</span><span class="p">,</span>
                                              <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]))</span>
<span class="n">joint_compressed_model</span><span class="o">.</span><span class="n">conv_2</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/weight/quantization_n_bits&quot;</span><span class="p">,</span>
                                              <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">n_bits</span><span class="p">))</span>
<span class="n">joint_compressed_model</span><span class="o">.</span><span class="n">conv_2</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
    <span class="s2">&quot;_COREML_/weight/quantization_scale&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">scale_2</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">joint_compressed_model</span><span class="o">.</span><span class="n">conv_2</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/weight/zero_point&quot;</span><span class="p">,</span>
                                              <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">zero_point_2</span><span class="p">))</span>

<span class="n">joint_compressed_mlmodel</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span>
    <span class="n">joint_compressed_model</span><span class="p">,</span>
    <span class="n">minimum_deployment_target</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">iOS18</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>For joint pruning + palettization, it’s similar:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Assume the model has two convs layers and both got jointly compressed by pruning + palettization.</span>
<span class="n">joint_compressed_model</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/metadata_version&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">joint_compressed_model</span><span class="o">.</span><span class="n">conv_1</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/weight/compression_type&quot;</span><span class="p">,</span>
                                              <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span>
<span class="n">joint_compressed_model</span><span class="o">.</span><span class="n">conv_1</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/weight/lut&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">lut_1_params</span><span class="o">.</span><span class="n">lut</span><span class="p">))</span>
<span class="n">joint_compressed_model</span><span class="o">.</span><span class="n">conv_2</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/weight/compression_type&quot;</span><span class="p">,</span>
                                              <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span>
<span class="n">joint_compressed_model</span><span class="o">.</span><span class="n">conv_2</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_COREML_/weight/lut&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">lut_2_params</span><span class="o">.</span><span class="n">lut</span><span class="p">))</span>

<span class="n">joint_compressed_mlmodel</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span>
    <span class="n">joint_compressed_model</span><span class="p">,</span>
    <span class="n">minimum_deployment_target</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">iOS18</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="compression-info-protocol">
<h3>Compression info protocol<a class="headerlink" href="#compression-info-protocol" title="Link to this heading">#</a></h3>
<p>As described in <a class="reference internal" href="#use-compression-info-embedded-in-torch-models">Use compression info embedded in torch models</a>,
you can register buffers in the compressed torch model to specify compression related information.
All the information stored in this newly introduced protocol will be fully honored by Core ML Tools
when constructing the compressed MIL ops.</p>
<p>This section records the detailed meaning of each field that you could specify, as well as the
versioning change. The version is an integer that can be specified by <code class="docutils literal notranslate"><span class="pre">_COREML_/metadata_version</span></code>
at the model level.</p>
<section id="version-1">
<h4>Version 1<a class="headerlink" href="#version-1" title="Link to this heading">#</a></h4>
<p><strong>Quantization Related Fields</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Key</p></th>
<th class="head"><p>Original Type</p></th>
<th class="head"><p>Optional</p></th>
<th class="head"><p>Sample Value / Shape</p></th>
<th class="head"><p>Additional Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>quantization_n_bits</p></td>
<td><p>int</p></td>
<td><p>No</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.tensor(4)</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>quantization_scale</p></td>
<td><p>tensor</p></td>
<td><p>No</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.Size([3072,</span> <span class="pre">1])</span></code></p></td>
<td><p>Rank of the scale need to match weight’s rank. The block_size on each dim will be inferred by the shape.</p></td>
</tr>
<tr class="row-even"><td><p>zero_point</p></td>
<td><p>tensor</p></td>
<td><p>Yes</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.Size([3072,</span> <span class="pre">1])</span></code></p></td>
<td><p>Same shape as <code class="docutils literal notranslate"><span class="pre">quantization_scale</span></code>.</p></td>
</tr>
</tbody>
</table>
</div>
<p>Details about the shape of the scale and zero-point can be found in the <code class="docutils literal notranslate"><span class="pre">iOS18</span></code> <code class="docutils literal notranslate"><span class="pre">constexpr_blockwise_shift_scale</span></code> op.</p>
<p><strong>Palettization Related Fields</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Key</p></th>
<th class="head"><p>Original Type</p></th>
<th class="head"><p>Optional</p></th>
<th class="head"><p>Sample Value / Shape</p></th>
<th class="head"><p>Additional Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>lut</p></td>
<td><p>tensor</p></td>
<td><p>No</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.Size([2,</span> <span class="pre">2,</span> <span class="pre">16,</span> <span class="pre">1])</span></code></p></td>
<td><p>Rank of the lut should be weight’s rank + 2. The shape can be used to infer palettization configurations like group size, axis, cluster dim and n-bit.</p></td>
</tr>
<tr class="row-odd"><td><p>palettization_scale</p></td>
<td><p>tensor</p></td>
<td><p>Yes</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.Size([3072,</span> <span class="pre">1])</span></code></p></td>
<td><p>Rank of the scale need to match weight’s rank.</p></td>
</tr>
</tbody>
</table>
</div>
<p>Details about the shape of the lookup table can be found in the <code class="docutils literal notranslate"><span class="pre">iOS18</span></code> <code class="docutils literal notranslate"><span class="pre">constexpr_lut_to_dense</span></code> op.</p>
<p><strong>Compression Type Field</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Key</p></th>
<th class="head"><p>Original Type</p></th>
<th class="head"><p>Optional</p></th>
<th class="head"><p>Sample Value / Shape</p></th>
<th class="head"><p>Additional Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>compression_type</p></td>
<td><p>list of int</p></td>
<td><p>No</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.tensor([1,</span> <span class="pre">3])</span></code></p></td>
<td><p>List of compression types applied to the parameter in the order in which they were applied. <code class="docutils literal notranslate"><span class="pre">1</span></code> means pruning, <code class="docutils literal notranslate"><span class="pre">2</span></code> means palettization, <code class="docutils literal notranslate"><span class="pre">3</span></code> means quantization</p></td>
</tr>
</tbody>
</table>
</div>
<p>All these fields can be combined to cover all Core ML Tools 8 new compression schemas, as described
below.</p>
<p><strong>Quantization</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Required Fields</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>blockwise quantization</p></td>
<td><p>n_bits<br>quantization_scale<br>zero_point (for affine &amp; unsigned symmetric)<br>compression_type = [3]</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>4-bit quantization</p></td>
<td><p>n_bits<br>quantization_scale<br>zero_point (for affine &amp; unsigned symmetric)<br>compression_type = [3]</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Palettization</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Required Fields</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>LUT with per-channel scales</p></td>
<td><p>palettization_scale<br>compression_type = [2]</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>vector palettization</p></td>
<td><p>cluster_dim<br>compression_type = [2]</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>grouped-channel palettization</p></td>
<td><p>group_size<br>group_axis<br>compression_type = [2]</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>LUT with 8-bit values</p></td>
<td><p>n_bits<br>quantization_scale<br>zero_point (for affine &amp; unsigned symmetric)<br>compression_type = [2, 3]</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Joint compression</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Required Fields</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>pruning + quantization (sparse weight where non-zero values are 8-bit quantized)</p></td>
<td><p>compression_type = [1, 3]<br>n_bits<br>quantization_scale<br>zero_point (for affine and unsigned symmetric)</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>pruning + palettization (sparse weight where non-zero values are n-bit palettized)</p></td>
<td><p>compression_type = [1, 2]<br>Same as required keys for palettization</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>pruning + palettization + quantization (sparse weight where non-zero values are n-bit palettized with int8 LUT)</p></td>
<td><p>compression_type = [1, 2, 3]<br>Same as required keys for LUT with 8-bit values</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="opt-joint-compression.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Combining Compression Types</p>
      </div>
    </a>
    <a class="right-next"
       href="quantization-neural-network.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Compressing Neural Network Weights</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#specify-pass-pipeline">Specify pass pipeline</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convert-models-with-sparse-weights">Convert models with sparse weights</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convert-models-with-palettized-weights">Convert models with palettized weights</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#use-compression-info-embedded-in-torch-models">Use compression info embedded in torch models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Convert models with sparse weights</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Convert models with palettized weights</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convert-pytorch-models-with-quantized-weights-and-activations">Convert PyTorch models with quantized weights and activations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convert-models-with-jointly-compressed-weights">Convert models with jointly compressed weights</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compression-info-protocol">Compression info protocol</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#version-1">Version 1</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Apple
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023, Apple Inc.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>