<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Palettization &mdash; coremltools API Reference 8.3 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=fd3f3429" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/norightmargin.css?v=eea1f72d" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=8cc1add8"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Pruning" href="coremltools.optimize.torch.pruning.html" />
    <link rel="prev" title="Optimizers" href="coremltools.optimize.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            coremltools API Reference
          </a>
              <div class="version">
                8.3
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">API Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="coremltools.converters.html">Converters</a></li>
<li class="toctree-l1"><a class="reference internal" href="coremltools.models.html">Model APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="coremltools.converters.mil.html">MIL Builder</a></li>
<li class="toctree-l1"><a class="reference internal" href="coremltools.converters.mil.input_types.html">MIL Input Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="coremltools.converters.mil.mil.ops.defs.html">MIL Ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="coremltools.converters.mil.mil.passes.defs.html">MIL Graph Passes</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="coremltools.optimize.html">Optimizers</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="coremltools.optimize.html#pytorch">PyTorch</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Palettization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#coremltools.optimize.torch.palettization.ModuleDKMPalettizerConfig"><code class="docutils literal notranslate"><span class="pre">ModuleDKMPalettizerConfig</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#coremltools.optimize.torch.palettization.DKMPalettizerConfig"><code class="docutils literal notranslate"><span class="pre">DKMPalettizerConfig</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#coremltools.optimize.torch.palettization.DKMPalettizer"><code class="docutils literal notranslate"><span class="pre">DKMPalettizer</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#coremltools.optimize.torch.palettization.ModulePostTrainingPalettizerConfig"><code class="docutils literal notranslate"><span class="pre">ModulePostTrainingPalettizerConfig</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#coremltools.optimize.torch.palettization.PostTrainingPalettizer"><code class="docutils literal notranslate"><span class="pre">PostTrainingPalettizer</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#coremltools.optimize.torch.palettization.PostTrainingPalettizerConfig"><code class="docutils literal notranslate"><span class="pre">PostTrainingPalettizerConfig</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#coremltools.optimize.torch.palettization.ModuleSKMPalettizerConfig"><code class="docutils literal notranslate"><span class="pre">ModuleSKMPalettizerConfig</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#coremltools.optimize.torch.palettization.SKMPalettizer"><code class="docutils literal notranslate"><span class="pre">SKMPalettizer</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#coremltools.optimize.torch.palettization.SKMPalettizerConfig"><code class="docutils literal notranslate"><span class="pre">SKMPalettizerConfig</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="coremltools.optimize.torch.pruning.html">Pruning</a></li>
<li class="toctree-l3"><a class="reference internal" href="coremltools.optimize.torch.quantization.html">Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="coremltools.optimize.torch.examples.html">Examples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="coremltools.optimize.html#core-ml">Core ML</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://apple.github.io/coremltools/docs-guides/index.html">Guide and Examples</a></li>
<li class="toctree-l1"><a class="reference external" href="https://apple.github.io/coremltools/mlmodel/index.html">Core ML Format Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="api-versions.html">Previous Versions</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/apple/coremltools">GitHub</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">coremltools API Reference</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="coremltools.optimize.html">Optimizers</a></li>
      <li class="breadcrumb-item active">Palettization</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/source/coremltools.optimize.torch.palettization.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="palettization">
<h1>Palettization<a class="headerlink" href="#palettization" title="Link to this heading"></a></h1>
<p>Palettization is a mechanism for compressing a model by clustering the model’s float
weights into a lookup table (LUT) of centroids and indices.</p>
<p>Palettization is implemented as an extension of <a class="reference external" href="https://pytorch.org/docs/stable/quantization.html">PyTorch’s QAT</a>
APIs. It works by inserting palettization layers in appropriate places inside a model.
The model can then be fine-tuned to learn the new palettized layers’ weights in the form
of a LUT and indices.</p>
<dl class="py class">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.ModuleDKMPalettizerConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">coremltools.optimize.torch.palettization.</span></span><span class="sig-name descname"><span class="pre">ModuleDKMPalettizerConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_bits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_threshold</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">granularity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'per_tensor'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_per_channel_scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">milestone</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cluster_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quant_min</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quant_max</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">127</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">dtype</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">torch.qint8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lut_dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'f32'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize_activations</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cluster_permute</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">palett_max_mem</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kmeans_max_iter</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prune_threshold</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-07</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kmeans_init</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kmeans_opt1d_threshold</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1024</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enforce_zero</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">palett_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'dkm'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">palett_tau</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">palett_epsilon</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">palett_lambda</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_extra_centroid</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">palett_cluster_tol</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">palett_min_tsize</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">65536</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">palett_unique</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">palett_shard</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">palett_batch_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">palett_dist</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_channel_scaling_factor_scheme</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'min_max'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">percentage_palett_enable</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kmeans_batch_threshold</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kmeans_n_init</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zero_threshold</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-07</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kmeans_error_bnd</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">partition_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cluster_dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/coremltools/optimize/torch/palettization/palettization_config.html#ModuleDKMPalettizerConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#coremltools.optimize.torch.palettization.ModuleDKMPalettizerConfig" title="Link to this definition"></a></dt>
<dd><p>Configuration class for specifying global and module-level options for the palettization
algorithm implemented in <a class="reference internal" href="#coremltools.optimize.torch.palettization.DKMPalettizer" title="coremltools.optimize.torch.palettization.DKMPalettizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DKMPalettizer</span></code></a>.</p>
<p>The parameters specified in this config control the DKM algorithm, described in
<a class="reference external" href="https://arxiv.org/abs/2108.12659">DKM: Differentiable K-Means Clustering Layer for Neural Network Compression</a>.</p>
<p>For most use cases, the only parameters you need to specify are <code class="docutils literal notranslate"><span class="pre">n_bits</span></code>,  <code class="docutils literal notranslate"><span class="pre">weight_threshold</span></code>,
and <code class="docutils literal notranslate"><span class="pre">milestone</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Most of the parameters in this class are meant for advanced use cases and for further fine-tuning the
DKM algorithm. The default values usually work for a majority of tasks.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Change the following parameters only when you use activation quantization in conjunction with
DKM weight palettization: <code class="docutils literal notranslate"><span class="pre">quant_min</span></code>, <code class="docutils literal notranslate"><span class="pre">quant_max</span></code>, <code class="docutils literal notranslate"><span class="pre">dtype</span></code>, and <code class="docutils literal notranslate"><span class="pre">quantize_activations</span></code>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_bits</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) – Number of clusters. The number of clusters used is <span class="math notranslate nohighlight">\(2^{n\_bits}\)</span>.
Defaults to <code class="docutils literal notranslate"><span class="pre">4</span></code> for linear layers and <code class="docutils literal notranslate"><span class="pre">2</span></code> for all other layers.</p></li>
<li><p><strong>weight_threshold</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) – A module is only palettized if the number of elements in
its weight matrix exceeds <code class="docutils literal notranslate"><span class="pre">weight_threshold</span></code>. If there are multiple weights in a
module, such as <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.MultiheadAttention</span></code>, all of them must have
more elements than the <code class="docutils literal notranslate"><span class="pre">weight_threshold</span></code> for the module to be palettized.
Defaults to <code class="docutils literal notranslate"><span class="pre">2048</span></code>.</p></li>
<li><p><strong>granularity</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">PalettizationGranularity</span></code>) – Granularity for palettization.
One of <code class="docutils literal notranslate"><span class="pre">per_tensor</span></code> or <code class="docutils literal notranslate"><span class="pre">per_grouped_channel</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">per_tensor</span></code>.</p></li>
<li><p><strong>group_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) – Specify the number of channels in a group.
Only effective when granularity is <code class="docutils literal notranslate"><span class="pre">per_grouped_channel</span></code>.</p></li>
<li><p><strong>channel_axis</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) – Specify the channel axis to form a group of channels.
Only effective when granularity is <code class="docutils literal notranslate"><span class="pre">per_grouped_channel</span></code>. Defaults to output channel axis. For now, only
output channel axis is supported by DKM.</p></li>
<li><p><strong>enable_per_channel_scale</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, per-channel scaling is used along the channel
dimension.</p></li>
<li><p><strong>milestone</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) – Step or epoch at which palettization begins. Defaults to <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p><strong>cluster_dim</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – The dimension of each cluster.</p></li>
<li><p><strong>quant_min</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – The minimum value for each element in the weight clusters if they are
quantized. Defaults to <code class="docutils literal notranslate"><span class="pre">-128</span></code>.</p></li>
<li><p><strong>quant_max</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – The maximum value for each element in the weight clusters if they are
quantized. Defaults to <code class="docutils literal notranslate"><span class="pre">127</span></code></p></li>
<li><p><strong>dtype</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – The <code class="docutils literal notranslate"><span class="pre">dtype</span></code> to use for quantizing the activations. Only applies
when <code class="docutils literal notranslate"><span class="pre">quantize_activations</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>. Defaults to <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.qint8</span></code>.</p></li>
<li><p><strong>lut_dtype</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – <code class="docutils literal notranslate"><span class="pre">dtype</span></code> to use for quantizing the clusters. Allowed options are
<code class="docutils literal notranslate"><span class="pre">'i8'</span></code>, <code class="docutils literal notranslate"><span class="pre">'u8'</span></code>, <code class="docutils literal notranslate"><span class="pre">'f16'</span></code>, <code class="docutils literal notranslate"><span class="pre">'bf16'</span></code>, <code class="docutils literal notranslate"><span class="pre">'f32'</span></code>.  Defaults to <code class="docutils literal notranslate"><span class="pre">'f32'</span></code>, so
by default, the clusters aren’t quantized.</p></li>
<li><p><strong>quantize_activations</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – When <code class="docutils literal notranslate"><span class="pre">True</span></code>, the activations are quantized.
Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>cluster_permute</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – Permutation order to apply to weight partitions.
Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>palett_max_mem</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – Proportion of available GPU memory that should be used for
palettization. Defaults to <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p></li>
<li><p><strong>kmeans_max_iter</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – Maximum number of differentiable <code class="docutils literal notranslate"><span class="pre">k-means</span></code> iterations.
Defaults to <code class="docutils literal notranslate"><span class="pre">3</span></code>.</p></li>
<li><p><strong>prune_threshold</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – Hardshrinks weights between [<code class="docutils literal notranslate"><span class="pre">-prune_threshold</span></code>,
<code class="docutils literal notranslate"><span class="pre">prune_threshold</span></code>] to zero. Useful for joint pruning and palettization. Defaults to <code class="docutils literal notranslate"><span class="pre">1e-7</span></code>.</p></li>
<li><p><strong>kmeans_init</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – <code class="docutils literal notranslate"><span class="pre">k-means</span></code> algorithm to use. Allowed options are
<code class="docutils literal notranslate"><span class="pre">opt1d</span></code>, <code class="docutils literal notranslate"><span class="pre">cpu.kmeans++</span></code> and <code class="docutils literal notranslate"><span class="pre">kmeans++</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">auto</span></code>.</p></li>
<li><p><strong>kmeans_opt1d_threshold</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – Channel threshold to decide if <code class="docutils literal notranslate"><span class="pre">opt1d</span> <span class="pre">kmeans</span></code>
should be used. Defaults to <code class="docutils literal notranslate"><span class="pre">1024</span></code>.</p></li>
<li><p><strong>enforce_zero</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, enforces the LUT centroid which is closest to
the origin to be fixed to zero. Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>palett_mode</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – Criteria to calculate attention during <code class="docutils literal notranslate"><span class="pre">k-means</span></code>. Allowed options are
<code class="docutils literal notranslate"><span class="pre">gsm</span></code>, <code class="docutils literal notranslate"><span class="pre">dkm</span></code> and <code class="docutils literal notranslate"><span class="pre">hard</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">dkm</span></code>.</p></li>
<li><p><strong>palett_tau</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – Temperature factor for softmax used in DKM algorithm.
Defaults to <code class="docutils literal notranslate"><span class="pre">0.0001</span></code>.</p></li>
<li><p><strong>palett_epsilon</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – Distance threshold for clusters between <code class="docutils literal notranslate"><span class="pre">k-means</span></code> iterations.
Defaults to <code class="docutils literal notranslate"><span class="pre">0.0001</span></code>.</p></li>
<li><p><strong>palett_lambda</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – Reduces effects of outliers during centroid calculation.
Defaults to <code class="docutils literal notranslate"><span class="pre">0.0</span></code>.</p></li>
<li><p><strong>add_extra_centroid</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds an extra centroid to the LUT.
Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>palett_cluster_tol</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – Tolerance for non-unique centroids in the LUT.
The higher the number, the more tolerance for non-unique centroids. Defaults to <code class="docutils literal notranslate"><span class="pre">0.0</span></code>.</p></li>
<li><p><strong>palett_min_tsize</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – Weight threshold beyond which to use custom packing and unpacking
hook for autograd. Defaults to <code class="docutils literal notranslate"><span class="pre">64*1024</span></code>.</p></li>
<li><p><strong>palett_unique</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, reduces the attention map by leveraging the fact that
FP16 only has <code class="docutils literal notranslate"><span class="pre">2^16</span></code> unique values. Useful for Large Models like LLMs where attention maps can be huge.
Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>. For more details, read <a class="reference external" href="https://arxiv.org/pdf/2309.00964.pdf">eDKM: An Efficient and Accurate Train-time Weight
Clustering for Large Language Models</a> .</p></li>
<li><p><strong>palett_shard</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the index list is sharded across GPUs.
Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>. For more details, read <a class="reference external" href="https://arxiv.org/pdf/2309.00964.pdf">eDKM: An Efficient and Accurate Train-time Weight
Clustering for Large Language Models</a> .</p></li>
<li><p><strong>palett_batch_mode</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, performs batch DKM across different partitions
created for different blocks. Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>. More details can be found <a class="reference external" href="https://arxiv.org/pdf/2309.00964.pdf">eDKM: An Efficient and Accurate Train-time Weight
Clustering for Large Language Models</a> .</p></li>
<li><p><strong>palett_dist</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, performs distributed distance calculation in batch_mode if
distributed torch is available. Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>per_channel_scaling_factor_scheme</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – Criteria to calculate the
<code class="docutils literal notranslate"><span class="pre">per_channel_scaling_factor</span></code>. Allowed options are <code class="docutils literal notranslate"><span class="pre">min_max</span></code> and <code class="docutils literal notranslate"><span class="pre">abs</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">min_max</span></code>.</p></li>
<li><p><strong>percentage_palett_enable</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – Percentage partitions to enable for DKM. Defaults to <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p></li>
<li><p><strong>kmeans_batch_threshold</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – Threshold to decide what the <code class="docutils literal notranslate"><span class="pre">num_partitions</span></code> value should be to go through with
the sharded centroids list. <code class="docutils literal notranslate"><span class="pre">num_partitions</span></code> is calculated by dividing the channel size by the <code class="docutils literal notranslate"><span class="pre">group_size</span></code>
provided. If <code class="docutils literal notranslate"><span class="pre">num_partitions`</span></code> matches <code class="docutils literal notranslate"><span class="pre">kmeans_batch_threshold</span></code>, the algorithm resorts to performing distributed k-means for lower
partition numbers, given that <code class="docutils literal notranslate"><span class="pre">num_partition</span></code> number of GPUs are available. Defaults to <code class="docutils literal notranslate"><span class="pre">4</span></code>.</p></li>
<li><p><strong>kmeans_n_init</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – Number of time the k-means algorithm will be run with different
centroid seeds. The final results will be the best output of <code class="docutils literal notranslate"><span class="pre">kmeans_n_init</span></code> consecutive runs in terms of inertia.</p></li>
<li><p><strong>zero_threshold</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – Zero threshold to be used to decide the minimum value of clamp for softmax. Defaults to <code class="docutils literal notranslate"><span class="pre">1e-7</span></code>.</p></li>
<li><p><strong>kmeans_error_bnd</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">optional</span></code>) – Distance threshold to decide at what distance between parameters
and clusters to stop the <code class="docutils literal notranslate"><span class="pre">k-means</span></code> operation. Defaults to <code class="docutils literal notranslate"><span class="pre">0.0</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>This class supports two different configurations to structure the palettization:</p>
<p>1. <strong>Per-tensor palettization</strong>: This is the default configuration where the whole tensor shares a single lookup
table. The <code class="docutils literal notranslate"><span class="pre">granularity</span></code> is set to <code class="docutils literal notranslate"><span class="pre">per_tensor</span></code> and <code class="docutils literal notranslate"><span class="pre">group_size</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<p>2. <strong>Per-grouped-channel palettization</strong>: In this configuration, <code class="docutils literal notranslate"><span class="pre">group_size</span></code> number of channels along
<code class="docutils literal notranslate"><span class="pre">channel_axis</span></code> share the same lookup table. For example, for a weight matrix of shape <code class="docutils literal notranslate"><span class="pre">(16,</span> <span class="pre">25)</span></code>, if we provide
<code class="docutils literal notranslate"><span class="pre">group_size</span> <span class="pre">=</span> <span class="pre">8</span></code>, the shape of the lookup table would be <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">2^n_bits)</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Grouping is currently only supported along the output channel axis.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.ModuleDKMPalettizerConfig.as_dict">
<span class="sig-name descname"><span class="pre">as_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#coremltools.optimize.torch.palettization.ModuleDKMPalettizerConfig.as_dict" title="Link to this definition"></a></dt>
<dd><p>Returns the config as a dictionary.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.ModuleDKMPalettizerConfig.from_dict">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">DictableDataClass</span></span></span><a class="headerlink" href="#coremltools.optimize.torch.palettization.ModuleDKMPalettizerConfig.from_dict" title="Link to this definition"></a></dt>
<dd><p>Create class from a dictionary of string keys and values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>data_dict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code> of <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> and values) – A nested dictionary of strings
and values.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.ModuleDKMPalettizerConfig.from_yaml">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_yaml</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">yml</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">IO</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">DictableDataClass</span></span></span><a class="headerlink" href="#coremltools.optimize.torch.palettization.ModuleDKMPalettizerConfig.from_yaml" title="Link to this definition"></a></dt>
<dd><p>Create class from a yaml stream.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>yml</strong> – An <code class="xref py py-class docutils literal notranslate"><span class="pre">IO</span></code> stream containing yaml or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>
path to the yaml file.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.DKMPalettizerConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">coremltools.optimize.torch.palettization.</span></span><span class="sig-name descname"><span class="pre">DKMPalettizerConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">global_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">GlobalConfigType</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module_type_configs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ModuleTypeConfigType</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">NOTHING</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module_name_configs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ModuleNameConfigType</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">NOTHING</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/coremltools/optimize/torch/palettization/palettization_config.html#DKMPalettizerConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#coremltools.optimize.torch.palettization.DKMPalettizerConfig" title="Link to this definition"></a></dt>
<dd><p>Configuration for specifying how different submodules of a model are palettized by
<a class="reference internal" href="#coremltools.optimize.torch.palettization.DKMPalettizer" title="coremltools.optimize.torch.palettization.DKMPalettizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DKMPalettizer</span></code></a>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">module_type_configs</span></code> parameter can accept a list of <a class="reference internal" href="#coremltools.optimize.torch.palettization.ModuleDKMPalettizerConfig" title="coremltools.optimize.torch.palettization.ModuleDKMPalettizerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModuleDKMPalettizerConfig</span></code></a>
as values for a given module type. The list can specify
different parameters for different <code class="docutils literal notranslate"><span class="pre">weight_threshold</span></code> values. This is useful if
you want to apply different configs to layers of the same type with weights of different sizes.</p>
<p>For example, to use <code class="docutils literal notranslate"><span class="pre">4</span></code> -bit palettization for weights with more than <code class="docutils literal notranslate"><span class="pre">1000</span></code> elements and
<code class="docutils literal notranslate"><span class="pre">2</span></code> -bit palettization for weights with more than <code class="docutils literal notranslate"><span class="pre">300</span></code> but less than <code class="docutils literal notranslate"><span class="pre">1000</span></code> elements,
create a config as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">custom_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;n_bits&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;cluster_dim&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;weight_threshold&quot;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;n_bits&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;cluster_dim&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;weight_threshold&quot;</span><span class="p">:</span> <span class="mi">300</span><span class="p">},</span>
    <span class="p">]</span>
<span class="p">}</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">DKMPalettizerConfig</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span><span class="s2">&quot;module_type_configs&quot;</span><span class="p">:</span> <span class="n">custom_config</span><span class="p">})</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>global_config</strong> (<a class="reference internal" href="#coremltools.optimize.torch.palettization.ModuleDKMPalettizerConfig" title="coremltools.optimize.torch.palettization.ModuleDKMPalettizerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModuleDKMPalettizerConfig</span></code></a>) – Config to be applied globally
to all supported modules. Missing values are chosen from the default config.</p></li>
<li><p><strong>module_type_configs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code> of <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> to <a class="reference internal" href="#coremltools.optimize.torch.palettization.ModuleDKMPalettizerConfig" title="coremltools.optimize.torch.palettization.ModuleDKMPalettizerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModuleDKMPalettizerConfig</span></code></a>) – Module type level configs applied to a specific module class, such as <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code>.
The keys can be either strings or module classes. When <code class="docutils literal notranslate"><span class="pre">module_type_config</span></code> is set to <code class="docutils literal notranslate"><span class="pre">None</span></code>
for a module type, it is not palettized.</p></li>
<li><p><strong>module_name_configs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code> of <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> to <a class="reference internal" href="#coremltools.optimize.torch.palettization.ModuleDKMPalettizerConfig" title="coremltools.optimize.torch.palettization.ModuleDKMPalettizerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModuleDKMPalettizerConfig</span></code></a>) – Module-level configs applied to specific modules.
The name of the module must be a fully qualified name that can be used to fetch it
from the top-level module using the <code class="docutils literal notranslate"><span class="pre">module.get_submodule(target)</span></code> method. When
<code class="docutils literal notranslate"><span class="pre">module_name_config</span></code> is set to <code class="docutils literal notranslate"><span class="pre">None</span></code> for a module, it is not palettized.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.DKMPalettizerConfig.as_dict">
<span class="sig-name descname"><span class="pre">as_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#coremltools.optimize.torch.palettization.DKMPalettizerConfig.as_dict" title="Link to this definition"></a></dt>
<dd><p>Returns the config as a dictionary.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.DKMPalettizerConfig.from_dict">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#coremltools.optimize.torch.palettization.DKMPalettizerConfig" title="coremltools.optimize.torch.palettization.palettization_config.DKMPalettizerConfig"><span class="pre">DKMPalettizerConfig</span></a></span></span><a class="reference internal" href="../_modules/coremltools/optimize/torch/palettization/palettization_config.html#DKMPalettizerConfig.from_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#coremltools.optimize.torch.palettization.DKMPalettizerConfig.from_dict" title="Link to this definition"></a></dt>
<dd><p>Create class from a dictionary of string keys and values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>config_dict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code> of <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> and values) – A nested dictionary of strings
and values.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.DKMPalettizerConfig.from_yaml">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_yaml</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">yml</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">IO</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">DictableDataClass</span></span></span><a class="headerlink" href="#coremltools.optimize.torch.palettization.DKMPalettizerConfig.from_yaml" title="Link to this definition"></a></dt>
<dd><p>Create class from a yaml stream.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>yml</strong> – An <code class="xref py py-class docutils literal notranslate"><span class="pre">IO</span></code> stream containing yaml or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>
path to the yaml file.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.DKMPalettizerConfig.set_global">
<span class="sig-name descname"><span class="pre">set_global</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">global_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ModuleOptimizationConfig</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">OptimizationConfig</span></span></span><a class="headerlink" href="#coremltools.optimize.torch.palettization.DKMPalettizerConfig.set_global" title="Link to this definition"></a></dt>
<dd><p>Set the global config.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.DKMPalettizerConfig.set_module_name">
<span class="sig-name descname"><span class="pre">set_module_name</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ModuleOptimizationConfig</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">OptimizationConfig</span></span></span><a class="headerlink" href="#coremltools.optimize.torch.palettization.DKMPalettizerConfig.set_module_name" title="Link to this definition"></a></dt>
<dd><p>Set the module level optimization config for a given module instance. If the module level optimization config
for an existing module was already set, the new config will override the old one.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.DKMPalettizerConfig.set_module_type">
<span class="sig-name descname"><span class="pre">set_module_type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">object_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ModuleOptimizationConfig</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">OptimizationConfig</span></span></span><a class="headerlink" href="#coremltools.optimize.torch.palettization.DKMPalettizerConfig.set_module_type" title="Link to this definition"></a></dt>
<dd><p>Set the module level optimization config for a given module type. If the module level optimization config
for an existing module type was already set, the new config will override the old one.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.DKMPalettizer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">coremltools.optimize.torch.palettization.</span></span><span class="sig-name descname"><span class="pre">DKMPalettizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#coremltools.optimize.torch.palettization.DKMPalettizerConfig" title="coremltools.optimize.torch.palettization.palettization_config.DKMPalettizerConfig"><span class="pre">DKMPalettizerConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/coremltools/optimize/torch/palettization/palettizer.html#DKMPalettizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#coremltools.optimize.torch.palettization.DKMPalettizer" title="Link to this definition"></a></dt>
<dd><p>A palettization algorithm based on <a class="reference external" href="https://arxiv.org/pdf/2108.12659.pdf">“DKM: Differentiable K-Means Clustering Layer for Neural Network
Compression”</a>. It clusters the weights
using a differentiable version of <code class="docutils literal notranslate"><span class="pre">k-means</span></code>, allowing the lookup table (LUT)
and indices of palettized weights to be learnt using a gradient-based optimization algorithm such as SGD.</p>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">coremltools.optimize.torch.palettization</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">DKMPalettizer</span><span class="p">,</span>
    <span class="n">DKMPalettizerConfig</span><span class="p">,</span>
    <span class="n">ModuleDKMPalettizerConfig</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># code that defines the pytorch model, loss and optimizer.</span>
<span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">create_model_loss_and_optimizer</span><span class="p">()</span>

<span class="c1"># initialize the palettizer</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">DKMPalettizerConfig</span><span class="p">(</span><span class="n">global_config</span><span class="o">=</span><span class="n">ModuleDKMPalettizerConfig</span><span class="p">(</span><span class="n">n_bits</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>

<span class="n">palettizer</span> <span class="o">=</span> <span class="n">DKMPalettizer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

<span class="c1"># prepare the model to insert FakePalettize layers for palettization</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">palettizer</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># use palettizer in your PyTorch training loop</span>
<span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">palettizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># fold LUT and indices into weights</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">palettizer</span><span class="o">.</span><span class="n">finalize</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>) – Model on which the palettizer will act.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="#coremltools.optimize.torch.palettization.DKMPalettizerConfig" title="coremltools.optimize.torch.palettization.DKMPalettizerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DKMPalettizerConfig</span></code></a>) – Config which specifies how
different submodules in the model will be configured for palettization.
Default config is used when passed as <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.DKMPalettizer.finalize">
<span class="sig-name descname"><span class="pre">finalize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Module</span></span></span><a class="reference internal" href="../_modules/coremltools/optimize/torch/palettization/palettizer.html#DKMPalettizer.finalize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#coremltools.optimize.torch.palettization.DKMPalettizer.finalize" title="Link to this definition"></a></dt>
<dd><p>Removes <code class="xref py py-class docutils literal notranslate"><span class="pre">FakePalettize</span></code> layers from a model and creates new model weights from the <code class="docutils literal notranslate"><span class="pre">LUT</span></code> and
<code class="docutils literal notranslate"><span class="pre">indices</span></code> buffers.</p>
<p>This function is called to prepare a palettized model for export using
<a class="reference external" href="https://coremltools.readme.io/docs">coremltools</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>) – model to finalize.</p></li>
<li><p><strong>inplace</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, model transformations are carried out in-place and
the original module is mutated; otherwise, a copy of the model is mutated and returned.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.DKMPalettizer.prepare">
<span class="sig-name descname"><span class="pre">prepare</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Module</span></span></span><a class="reference internal" href="../_modules/coremltools/optimize/torch/palettization/palettizer.html#DKMPalettizer.prepare"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#coremltools.optimize.torch.palettization.DKMPalettizer.prepare" title="Link to this definition"></a></dt>
<dd><p>Prepares a model for palettization aware training by inserting <code class="xref py py-class docutils literal notranslate"><span class="pre">FakePalettize</span></code> layers in appropriate
places as specified by the config.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inplace</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, model transformations are carried out in-place and
the original module is mutated, otherwise a copy of the model is mutated and returned.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.DKMPalettizer.report">
<span class="sig-name descname"><span class="pre">report</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_Report</span></span></span><a class="reference internal" href="../_modules/coremltools/optimize/torch/palettization/palettizer.html#DKMPalettizer.report"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#coremltools.optimize.torch.palettization.DKMPalettizer.report" title="Link to this definition"></a></dt>
<dd><p>Returns a dictionary with important statistics related to current state of palettization.
Each key in the dictionary corresponds to a module name, and the
value is a dictionary containing the statistics, such as number of clusters and
cluster dimension, number of parameters, and so on.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.DKMPalettizer.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/coremltools/optimize/torch/palettization/palettizer.html#DKMPalettizer.step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#coremltools.optimize.torch.palettization.DKMPalettizer.step" title="Link to this definition"></a></dt>
<dd><p>Step through the palettizer. When the number of times <code class="docutils literal notranslate"><span class="pre">step</span></code>
is called is equal to <code class="docutils literal notranslate"><span class="pre">milestone</span></code>, palettization is enabled.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.ModulePostTrainingPalettizerConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">coremltools.optimize.torch.palettization.</span></span><span class="sig-name descname"><span class="pre">ModulePostTrainingPalettizerConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_bits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lut_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">granularity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'per_tensor'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cluster_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_per_channel_scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_fast_kmeans_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rounding_precision</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/coremltools/optimize/torch/palettization/post_training_palettization.html#ModulePostTrainingPalettizerConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#coremltools.optimize.torch.palettization.ModulePostTrainingPalettizerConfig" title="Link to this definition"></a></dt>
<dd><p>Configuration class for specifying global and module-level palettization options for
<a class="reference internal" href="#coremltools.optimize.torch.palettization.PostTrainingPalettizerConfig" title="coremltools.optimize.torch.palettization.PostTrainingPalettizerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PostTrainingPalettizerConfig</span></code></a> algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_bits</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) – Number of bits to use for palettizing the weights. Defaults to <code class="docutils literal notranslate"><span class="pre">4</span></code>.</p></li>
<li><p><strong>lut_dtype</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>) – The dtype to use for representing each element in lookup tables.
When value is <code class="docutils literal notranslate"><span class="pre">None</span></code>, no quantization is performed. Supported values are <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.int8</span></code> and
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.uint8</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>granularity</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">PalettizationGranularity</span></code>) – One of <code class="docutils literal notranslate"><span class="pre">per_tensor</span></code> or <code class="docutils literal notranslate"><span class="pre">per_grouped_channel</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">per_tensor</span></code>.</p></li>
<li><p><strong>group_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) – Specify the number of channels in a group.
Only effective when granularity is <code class="docutils literal notranslate"><span class="pre">per_grouped_channel</span></code>.</p></li>
<li><p><strong>channel_axis</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) – Specify the channel axis to form a group of channels.
Only effective when granularity is <code class="docutils literal notranslate"><span class="pre">per_grouped_channel</span></code>. Defaults to output channel axis.</p></li>
<li><p><strong>cluster_dim</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) – The dimension of centroids for each lookup table.
The centroid is a scalar by default. When <code class="docutils literal notranslate"><span class="pre">cluster_dim</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>, it indicates 2-D clustering,
and each <code class="docutils literal notranslate"><span class="pre">cluster_dim</span></code> length of weight vectors along the output channel are palettized
using the same 2-D centroid. The length of each entry in the lookup tables is equal to <code class="docutils literal notranslate"><span class="pre">cluster_dim</span></code>.</p></li>
<li><p><strong>enable_per_channel_scale</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, weights are normalized along the output channels
using per-channel scales before being palettized. This is not supported with <code class="docutils literal notranslate"><span class="pre">cluster_dim</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>.</p></li>
<li><p><strong>enable_fast_kmeans_mode</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>) – When turned on, will round the weights before clustering if data is in fp16 range.
If weight dtype is fp32, weights are cast to fp16 and then rounded. This is not supported with <code class="docutils literal notranslate"><span class="pre">cluster_dim</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>.
Defaults to True.</p></li>
<li><p><strong>rounding_precision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) – The number of decimal places to set for rounding, when <cite>enable_fast_kmeans_mode</cite> is enabled.
Choose a lower precision for faster processing, at the cost of coarser approximation. Defaults to 4.</p></li>
</ul>
</dd>
</dl>
<p>This class supports two different configurations to structure the palettization:</p>
<p>1. <strong>Per-tensor palettization</strong>:  This is the default configuration where the whole tensor shares a single lookup
table. The <code class="docutils literal notranslate"><span class="pre">granularity</span></code> is set to <code class="docutils literal notranslate"><span class="pre">per_tensor</span></code>, and <code class="docutils literal notranslate"><span class="pre">group_size</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<p>2. <strong>Per-grouped-channel palettization</strong>: In this configuration, the number of channels <code class="docutils literal notranslate"><span class="pre">group_size</span></code> along
<code class="docutils literal notranslate"><span class="pre">channel_axis</span></code> share the same lookup table. For example, for a weight matrix of shape <code class="docutils literal notranslate"><span class="pre">(16,</span> <span class="pre">25)</span></code>, if we provide
<code class="docutils literal notranslate"><span class="pre">group_size</span> <span class="pre">=</span> <span class="pre">8</span></code>, the shape of the lookup table would be <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">2^n_bits)</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Grouping is currently only supported along either the input or output channel axis.</p>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.PostTrainingPalettizer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">coremltools.optimize.torch.palettization.</span></span><span class="sig-name descname"><span class="pre">PostTrainingPalettizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#coremltools.optimize.torch.palettization.PostTrainingPalettizerConfig" title="coremltools.optimize.torch.palettization.post_training_palettization.PostTrainingPalettizerConfig"><span class="pre">PostTrainingPalettizerConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/coremltools/optimize/torch/palettization/post_training_palettization.html#PostTrainingPalettizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#coremltools.optimize.torch.palettization.PostTrainingPalettizer" title="Link to this definition"></a></dt>
<dd><p>Perform post-training palettization on a torch model. Post palettization, all the weights in supported
layers point to elements in a lookup table after performing a k-means operation.</p>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">coremltools.optimize.torch.palettization</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">PostTrainingPalettizerConfig</span><span class="p">,</span>
    <span class="n">PostTrainingPalettizer</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">OrderedDict</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;conv&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span>
            <span class="s2">&quot;relu1&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="s2">&quot;conv2&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span>
            <span class="s2">&quot;relu2&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="p">}</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="c1"># initialize the palettizer</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">PostTrainingPalettizerConfig</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;global_config&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;n_bits&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="n">ptpalettizer</span> <span class="o">=</span> <span class="n">PostTrainingPalettizer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
<span class="n">palettized_model</span> <span class="o">=</span> <span class="n">ptpalettizer</span><span class="o">.</span><span class="n">compress</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>) – Module to be compressed.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="#coremltools.optimize.torch.palettization.PostTrainingPalettizerConfig" title="coremltools.optimize.torch.palettization.PostTrainingPalettizerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PostTrainingPalettizerConfig</span></code></a>) – Config that specifies how
different submodules in the model will be palettized.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.PostTrainingPalettizerConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">coremltools.optimize.torch.palettization.</span></span><span class="sig-name descname"><span class="pre">PostTrainingPalettizerConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">global_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#coremltools.optimize.torch.palettization.ModulePostTrainingPalettizerConfig" title="coremltools.optimize.torch.palettization.post_training_palettization.ModulePostTrainingPalettizerConfig"><span class="pre">ModulePostTrainingPalettizerConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module_type_configs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ModuleTypeConfigType</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">NOTHING</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module_name_configs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#coremltools.optimize.torch.palettization.ModulePostTrainingPalettizerConfig" title="coremltools.optimize.torch.palettization.post_training_palettization.ModulePostTrainingPalettizerConfig"><span class="pre">ModulePostTrainingPalettizerConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">NOTHING</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/coremltools/optimize/torch/palettization/post_training_palettization.html#PostTrainingPalettizerConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#coremltools.optimize.torch.palettization.PostTrainingPalettizerConfig" title="Link to this definition"></a></dt>
<dd><p>Configuration class for specifying how different submodules of a model
should be post-training palettized by <a class="reference internal" href="#coremltools.optimize.torch.palettization.PostTrainingPalettizer" title="coremltools.optimize.torch.palettization.PostTrainingPalettizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">PostTrainingPalettizer</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>global_config</strong> (<a class="reference internal" href="#coremltools.optimize.torch.palettization.ModulePostTrainingPalettizerConfig" title="coremltools.optimize.torch.palettization.ModulePostTrainingPalettizerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModulePostTrainingPalettizerConfig</span></code></a>) – Config to be applied globally
to all supported modules.</p></li>
<li><p><strong>module_type_configs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code> of <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> to <a class="reference internal" href="#coremltools.optimize.torch.palettization.ModulePostTrainingPalettizerConfig" title="coremltools.optimize.torch.palettization.ModulePostTrainingPalettizerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModulePostTrainingPalettizerConfig</span></code></a>) – Module type configs applied to a specific module class, such as <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code>.
The keys can be either strings or module classes.</p></li>
<li><p><strong>module_name_configs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code> of <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> to <a class="reference internal" href="#coremltools.optimize.torch.palettization.ModulePostTrainingPalettizerConfig" title="coremltools.optimize.torch.palettization.ModulePostTrainingPalettizerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModulePostTrainingPalettizerConfig</span></code></a>) – Module name configs applied to specific modules. This can be a dictionary with module names pointing to their
corresponding <a class="reference internal" href="#coremltools.optimize.torch.palettization.ModulePostTrainingPalettizerConfig" title="coremltools.optimize.torch.palettization.ModulePostTrainingPalettizerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModulePostTrainingPalettizerConfig</span></code></a>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.ModuleSKMPalettizerConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">coremltools.optimize.torch.palettization.</span></span><span class="sig-name descname"><span class="pre">ModuleSKMPalettizerConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_bits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lut_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">granularity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'per_tensor'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cluster_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_per_channel_scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/coremltools/optimize/torch/palettization/sensitive_k_means.html#ModuleSKMPalettizerConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#coremltools.optimize.torch.palettization.ModuleSKMPalettizerConfig" title="Link to this definition"></a></dt>
<dd><p>Configuration class for specifying global and module-level palettization options for
<a class="reference internal" href="#coremltools.optimize.torch.palettization.SKMPalettizer" title="coremltools.optimize.torch.palettization.SKMPalettizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">SKMPalettizer</span></code></a> algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_bits</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) – Number of bits to use for palettizing the weights. Defaults to <code class="docutils literal notranslate"><span class="pre">4</span></code>.</p></li>
<li><p><strong>lut_dtype</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>) – The dtype to use for representing each element in lookup tables.
When value is <code class="docutils literal notranslate"><span class="pre">None</span></code>, no quantization is performed. Supported values are <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.int8</span></code> and
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.uint8</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>granularity</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">PalettizationGranularity</span></code>) – One of <code class="docutils literal notranslate"><span class="pre">per_tensor</span></code> or <code class="docutils literal notranslate"><span class="pre">per_grouped_channel</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">per_tensor</span></code>.</p></li>
<li><p><strong>group_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) – Specify the number of channels in a group.
Only effective when granularity is <code class="docutils literal notranslate"><span class="pre">per_grouped_channel</span></code>.</p></li>
<li><p><strong>channel_axis</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) – Specify the channel axis to form a group of channels.
Only effective when granularity is <code class="docutils literal notranslate"><span class="pre">per_grouped_channel</span></code>. Defaults to output channel axis.</p></li>
<li><p><strong>cluster_dim</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) – The dimension of centroids for each lookup table.
The centroid is a scalar by default. When <code class="docutils literal notranslate"><span class="pre">cluster_dim</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>, it indicates 2-D clustering,
and each <code class="docutils literal notranslate"><span class="pre">cluster_dim</span></code> length of weight vectors along the output channel are palettized
using the same 2-D centroid. The length of each entry in the lookup tables is equal to <code class="docutils literal notranslate"><span class="pre">cluster_dim</span></code>.</p></li>
<li><p><strong>enable_per_channel_scale</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, weights are normalized along the output channels
using per-channel scales before being palettized. This is not supported with <code class="docutils literal notranslate"><span class="pre">cluster_dim</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>This class supports two different configurations to structure the palettization:</p>
<p>1. <strong>Per-tensor palettization</strong>:  This is the default configuration where the whole tensor shares a single lookup
table. The <code class="docutils literal notranslate"><span class="pre">granularity</span></code> is set to <code class="docutils literal notranslate"><span class="pre">per_tensor</span></code>, and <code class="docutils literal notranslate"><span class="pre">group_size</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<p>2. <strong>Per-grouped-channel palettization</strong>: In this configuration, the number of channels <code class="docutils literal notranslate"><span class="pre">group_size</span></code> along
<code class="docutils literal notranslate"><span class="pre">channel_axis</span></code> share the same lookup table. For example, for a weight matrix of shape <code class="docutils literal notranslate"><span class="pre">(16,</span> <span class="pre">25)</span></code>, if we provide
<code class="docutils literal notranslate"><span class="pre">group_size</span> <span class="pre">=</span> <span class="pre">8</span></code>, the shape of the lookup table would be <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">2^n_bits)</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Grouping is currently only supported along either the input or output channel axis.</p>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.SKMPalettizer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">coremltools.optimize.torch.palettization.</span></span><span class="sig-name descname"><span class="pre">SKMPalettizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#coremltools.optimize.torch.palettization.SKMPalettizerConfig" title="coremltools.optimize.torch.palettization.sensitive_k_means.SKMPalettizerConfig"><span class="pre">SKMPalettizerConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/coremltools/optimize/torch/palettization/sensitive_k_means.html#SKMPalettizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#coremltools.optimize.torch.palettization.SKMPalettizer" title="Link to this definition"></a></dt>
<dd><p>Perform post-training palettization of weights by running a weighted k-means
on the model weights. The weight values used for weighing different elements of
a model’s weight matrix are computed using the Fisher information matrix, which
is an approximation of the Hessian. These weight values indicate how sensitive
a given weight element is: the more sensitive an element, the larger the impact perturbing
or palettizing it has on the model’s loss function. This means that weighted k-means
moves the clusters closer to the sensitive weight values, allowing them to be
represented more exactly. This leads to a lower degradation in model performance
after palettization. The Fisher information matrix is computed using a few
samples of calibration data.</p>
<p>This algorithm implements <a class="reference external" href="https://arxiv.org/pdf/2306.07629.pdf">SqueezeLLM: Dense-and-Sparse Quantization</a>.</p>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">coremltools.optimize.torch.palettization</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">SKMPalettizer</span><span class="p">,</span>
    <span class="n">SKMPalettizerConfig</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">OrderedDict</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;conv&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span>
            <span class="s2">&quot;relu1&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="s2">&quot;conv2&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span>
            <span class="s2">&quot;relu2&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="p">}</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="n">dataloader</span> <span class="o">=</span> <span class="n">load_calibration_data</span><span class="p">()</span>

<span class="c1"># define callable for loss function</span>
<span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="n">inp</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="c1"># initialize the palettizer</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">SKMPalettizerConfig</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;global_config&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;n_bits&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="s2">&quot;calibration_nsamples&quot;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="n">compressor</span> <span class="o">=</span> <span class="n">SKMPalettizer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
<span class="n">compressed_model</span> <span class="o">=</span> <span class="n">compressor</span><span class="o">.</span><span class="n">compress</span><span class="p">(</span><span class="n">dataloader</span><span class="o">=</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>) – Module to be compressed.</p></li>
<li><p><strong>config</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">LayerwiseCompressorConfig</span></code>) – Config that specifies how
different submodules in the model will be compressed.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="coremltools.optimize.torch.palettization.SKMPalettizerConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">coremltools.optimize.torch.palettization.</span></span><span class="sig-name descname"><span class="pre">SKMPalettizerConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">global_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#coremltools.optimize.torch.palettization.ModuleSKMPalettizerConfig" title="coremltools.optimize.torch.palettization.sensitive_k_means.ModuleSKMPalettizerConfig"><span class="pre">ModuleSKMPalettizerConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module_type_configs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ModuleTypeConfigType</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">NOTHING</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module_name_configs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#coremltools.optimize.torch.palettization.ModuleSKMPalettizerConfig" title="coremltools.optimize.torch.palettization.sensitive_k_means.ModuleSKMPalettizerConfig"><span class="pre">ModuleSKMPalettizerConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">NOTHING</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">calibration_nsamples</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">128</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/coremltools/optimize/torch/palettization/sensitive_k_means.html#SKMPalettizerConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#coremltools.optimize.torch.palettization.SKMPalettizerConfig" title="Link to this definition"></a></dt>
<dd><p>Configuration class for specifying how different submodules of a model are
palettized by <a class="reference internal" href="#coremltools.optimize.torch.palettization.SKMPalettizer" title="coremltools.optimize.torch.palettization.SKMPalettizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">SKMPalettizer</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>global_config</strong> (<a class="reference internal" href="#coremltools.optimize.torch.palettization.ModuleSKMPalettizerConfig" title="coremltools.optimize.torch.palettization.ModuleSKMPalettizerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModuleSKMPalettizerConfig</span></code></a>) – Config to be applied globally
to all supported modules. Missing values are chosen from the default config.</p></li>
<li><p><strong>module_type_configs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code> of <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> to <a class="reference internal" href="#coremltools.optimize.torch.palettization.ModuleSKMPalettizerConfig" title="coremltools.optimize.torch.palettization.ModuleSKMPalettizerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModuleSKMPalettizerConfig</span></code></a>) – Module type configs applied to a specific module class, such as <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code>.
The keys can be either strings or module classes.</p></li>
<li><p><strong>module_name_configs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code> of <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> to <a class="reference internal" href="#coremltools.optimize.torch.palettization.ModuleSKMPalettizerConfig" title="coremltools.optimize.torch.palettization.ModuleSKMPalettizerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModuleSKMPalettizerConfig</span></code></a>) – Module-level configs applied to specific modules. The name of the module must either be
a regex or a fully qualified name that can be used to fetch it from the top level module
using the <code class="docutils literal notranslate"><span class="pre">module.get_submodule(target)</span></code> method.</p></li>
<li><p><strong>calibration_nsamples</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) – Number of samples to be used for calibration.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="coremltools.optimize.html" class="btn btn-neutral float-left" title="Optimizers" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="coremltools.optimize.torch.pruning.html" class="btn btn-neutral float-right" title="Pruning" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Apple Inc.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>