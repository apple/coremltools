{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Palettization Using Differentiable K-Means\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, you learn how to palettize a\nnetwork trained on [MNIST](http://yann.lecun.com/exdb/mnist/) using\n:py:class:`~.palettizer.DKMPalettizer`.\n\nLearn more about other palettization in the coremltools \n[Training-Time Palettization Documentation](https://coremltools.readme.io/v7.0/docs/training-time-palettization).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining the Network and Dataset\n\nFirst, define your network:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef mnist_net(num_classes=10):\n    return nn.Sequential(OrderedDict([\n        ('conv1', nn.Conv2d(1, 32, 5, padding='same')),\n        ('relu1', nn.ReLU()),\n        ('pool1', nn.MaxPool2d(2, stride=2, padding=0)),\n        ('bn1', nn.BatchNorm2d(32, eps=0.001, momentum=0.01)),\n        ('conv2', nn.Conv2d(32, 64, 5, padding='same')),\n        ('relu2', nn.ReLU()),\n        ('pool2', nn.MaxPool2d(2, stride=2, padding=0)),\n        ('flatten', nn.Flatten()),\n        ('dense1', nn.Linear(3136, 1024)),\n        ('relu3', nn.ReLU()),\n        ('dropout', nn.Dropout(p=0.4)),\n        ('dense2', nn.Linear(1024, num_classes)),\n        ('softmax', nn.LogSoftmax())]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For training, use the MNIST dataset provided by\n[PyTorch](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#mnist).\nApply a very simple transformation to the input images to normalize them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nfrom torchvision import datasets, transforms\n\n\ndef mnist_dataset(data_dir=\"~/.mnist_palettization_data\"):\n    transform = transforms.Compose(\n        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n    )\n    data_path = os.path.expanduser(f\"{data_dir}/mnist\")\n    if not os.path.exists(data_path):\n        os.makedirs(data_path)\n    train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n    test = datasets.MNIST(data_path, train=False, transform=transform)\n    return train, test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize the model and the dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = mnist_net()\n\nbatch_size = 128\ntrain_dataset, test_dataset = mnist_dataset(\"~/.mnist_data/mnist_palettization\")\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the Model Without Palettization\n\nTrain the model without applying any palettization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.008)\naccuracy_unpalettized = 0.0\nnum_epochs = 2\n\n\ndef train_step(model, optimizer, train_loader, data, target, batch_idx, epoch, palettizer = None):\n    optimizer.zero_grad()\n    if palettizer is not None:\n        palettizer.step()\n    output = model(data)\n    loss = F.nll_loss(output, target)\n    loss.backward()\n    optimizer.step()\n    if batch_idx % 100 == 0:\n        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n            epoch, batch_idx * len(data), len(train_loader.dataset),\n                   100. * batch_idx / len(train_loader), loss.item()))\n\n\ndef eval_model(model, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n        test_loss /= len(test_loader.dataset)\n        accuracy = 100. * correct / len(test_loader.dataset)\n\n        print(\n            \"\\nTest set: Average loss: {:.4f}, Accuracy: {:.1f}%\\n\".format(\n                test_loss, accuracy\n            )\n        )\n    return accuracy\n\n\nfor epoch in range(num_epochs):\n    # train one epoch\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        train_step(model, optimizer, train_loader, data, target, batch_idx, epoch)\n\n    # evaluate\n    accuracy_unpalettized = eval_model(model, test_loader)\n\nprint(\"Accuracy of unpalettized network: {:.1f}%\\n\".format(accuracy_unpalettized))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuring Palettization\n\nInsert palettization layers into the trained model.\nFor this example, apply a ``4-bit`` palettization to the ``conv2`` layer. This\nwould mean that for all the weights that exist in this layer, you try to map\neach weight element to one of $2^4$,\nthat is, ``16`` clusters.\n\nNote that calling :py:meth:`~.palettization.DKMPalettizer.prepare` simply inserts palettization\nlayers into the model. It doesn't actually palettize the weights. You do that in the next step when\nyou fine-tune the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from coremltools.optimize.torch.palettization import DKMPalettizer, DKMPalettizerConfig\n\nconfig = DKMPalettizerConfig.from_dict(\n    {\"module_name_configs\": {\"conv2\": {\"n_bits\": 4}}}\n)\npalettizer = DKMPalettizer(model, config)\n\nprepared_model = palettizer.prepare()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-Tuning the Palettized Model\n\nFine-tune the model with palettization applied. This helps the model learn the new palettized\nlayers' weights in the form of a LUT and indices.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(prepared_model.parameters(), lr=0.008)\naccuracy_palettized = 0.0\nnum_epochs = 2\n\nfor epoch in range(num_epochs):\n    prepared_model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        train_step(prepared_model, optimizer, train_loader, data, target, batch_idx, epoch, palettizer)\n\n    # evaluate\n    accuracy_palettized = eval_model(prepared_model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The evaluation shows that you can train a palettized network without losing much accuracy\nwith the final model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy of unpalettized network: {:.1f}%\\n\".format(accuracy_unpalettized))\nprint(\"Accuracy of palettized network: {:.1f}%\\n\".format(accuracy_palettized))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Restoring LUT and Indices as Weights\n\nUse :py:meth:`~.palettization.Palettizer.finalize` to\nrestore the LUT and indices of the palettized modules as weights in the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "finalized_model = palettizer.finalize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exporting the Model for On-Device Execution\n\nTo deploy the model on device, convert it to a Core ML model.\n\nTo export the model with Core ML Tools, first trace the model with an input, and then\nuse the Core ML Tools converter, as described in\n[Converting from PyTorch](https://coremltools.readme.io/docs/pytorch-conversion).\nThe parameter ``ct.PassPipeline.DEFAULT_PALETTIZATION`` signals to the\nconverter a palettized model is being converted, and allows its weights to be\nrepresented using a look-up table (LUT) and indices, which have a much smaller\nfootprint on disk as compared to the dense weights.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import coremltools as ct\n\nfinalized_model.eval()\nexample_input = torch.rand(1, 1, 28, 28)\ntraced_model = torch.jit.trace(finalized_model, example_input)\n\n\ncoreml_model = ct.convert(\n    traced_model,\n    inputs=[ct.TensorType(shape=example_input.shape)],\n    pass_pipeline=ct.PassPipeline.DEFAULT_PALETTIZATION,\n    minimum_deployment_target=ct.target.iOS16,\n)\n\ncoreml_model.save(\"~/.mnist_palettization_data/palettized_model.mlpackage\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}