{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Linear Quantization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, you learn how to train a simple convolutional neural network on\n[MNIST](http://yann.lecun.com/exdb/mnist/) using :py:class:`~.quantization.LinearQuantizer`.\n\nLearn more about other quantization in the coremltools \n[Training-Time Quantization Documentation](https://coremltools.readme.io/v7.0/docs/data-dependent-quantization).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Network and Dataset Definition\nFirst define your network, which consists of a single convolution layer\nfollowed by a dense (linear) layer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef mnist_net(num_classes=10):\n    return nn.Sequential(\n        OrderedDict(\n            [\n                (\"conv\", nn.Conv2d(1, 12, 3, padding=1)),\n                (\"relu\", nn.ReLU()),\n                (\"pool\", nn.MaxPool2d(2, stride=2, padding=0)),\n                (\"flatten\", nn.Flatten()),\n                (\"dense\", nn.Linear(2352, num_classes)),\n                (\"softmax\", nn.LogSoftmax()),\n            ]\n        )\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the [MNIST dataset provided by PyTorch](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#mnist)\nfor training. Apply a very simple transformation to the input\nimages to normalize them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nfrom torchvision import datasets, transforms\n\n\ndef mnist_dataset(data_dir=\"~/.mnist_qat_data\"):\n    transform = transforms.Compose(\n        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n    )\n    data_path = os.path.expanduser(f\"{data_dir}/mnist\")\n    if not os.path.exists(data_path):\n        os.makedirs(data_path)\n    train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n    test = datasets.MNIST(data_path, train=False, transform=transform)\n    return train, test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, initialize the model and the dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = mnist_net()\n\nbatch_size = 128\ntrain_dataset, test_dataset = mnist_dataset()\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True\n)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the Model Without Quantization\nTrain the model without any quantization applied.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), eps=1e-07)\naccuracy_unquantized = 0.0\nnum_epochs = 4\n\n\ndef train_step(model, optimizer, train_loader, data, target, batch_idx, epoch):\n    optimizer.zero_grad()\n    output = model(data)\n    loss = F.nll_loss(output, target)\n    loss.backward()\n    optimizer.step()\n    if batch_idx % 100 == 0:\n        print(\n            \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n                epoch,\n                batch_idx * len(data),\n                len(train_loader.dataset),\n                100.0 * batch_idx / len(train_loader),\n                loss.item(),\n            )\n        )\n\n\ndef eval_model(model, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n        test_loss /= len(test_loader.dataset)\n        accuracy = 100.0 * correct / len(test_loader.dataset)\n\n        print(\n            \"\\nTest set: Average loss: {:.4f}, Accuracy: {:.1f}%\\n\".format(\n                test_loss, accuracy\n            )\n        )\n    return accuracy\n\n\nfor epoch in range(num_epochs):\n    # train one epoch\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        train_step(model, optimizer, train_loader, data, target, batch_idx, epoch)\n\n    # evaluate\n    accuracy_unquantized = eval_model(model, test_loader)\n\n\nprint(\"Accuracy of unquantized network: {:.1f}%\\n\".format(accuracy_unquantized))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Insert Quantization Layers in the Model\nInstall :py:class:`~.quantization.LinearQuantizer` in the trained model.\n\nCreate an instance of the :py:class:`~.quantization.LinearQuantizerConfig` class\nto specify quantization parameters. ``milestones=[0, 1, 2, 1]`` refers to the following:\n\n* *Index 0*: At 0th epoch, observers will start collecting statistics of values of tensors being quantized\n* *Index 1*: At 1st epoch, quantization simulation will begin\n* *Index 2*: At 2nd epoch, observers will stop collecting and quantization parameters will be frozen\n* *Index 3*: At 1st epoch, batch normalization layers will stop collecting mean and variance, and will start running in inference mode\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from coremltools.optimize.torch.quantization import (\n    LinearQuantizer,\n    LinearQuantizerConfig,\n    ModuleLinearQuantizerConfig,\n)\n\nglobal_config = ModuleLinearQuantizerConfig(milestones=[0, 1, 2, 1])\nconfig = LinearQuantizerConfig(global_config=global_config)\n\nquantizer = LinearQuantizer(model, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, call :py:meth:`~.quantization.LinearQuantizer.prepare` to insert fake quantization\nlayers in the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "qmodel = quantizer.prepare(example_inputs=torch.randn(1, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-Tuning the Model\nThe next step is to fine tune the model with quantization applied.\nCall :py:meth:`~.quantization.LinearQuantizer.step` to step through the\nquantization milestones.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(qmodel.parameters(), eps=1e-07)\naccuracy_quantized = 0.0\nnum_epochs = 4\n\nfor epoch in range(num_epochs):\n    # train one epoch\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        quantizer.step()\n        train_step(qmodel, optimizer, train_loader, data, target, batch_idx, epoch)\n\n    # evaluate\n    accuracy_quantized = eval_model(qmodel, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The evaluation shows that you can train a quantized network without a significant loss\nin model accuracy. In practice, for more complex models,\nquantization can be lossy and lead to degradation in validation accuracy.\nIn such cases, you can choose to not quantize certain layers which are\nless amenable to quantization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy of quantized network: {:.1f}%\\n\".format(accuracy_quantized))\nprint(\"Accuracy of unquantized network: {:.1f}%\\n\".format(accuracy_unquantized))\n\nnp.testing.assert_allclose(accuracy_quantized, accuracy_unquantized, atol=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Finalizing the Model for Export\n\nThe example shows that you can quantize the model with a few code changes to your\nexisting PyTorch training code. Now you can deploy this model on a device.\n\nTo finalize the model for export, call :py:meth:`~.pruning.LinearQuantizer.finalize`\non the quantizer. This folds the quantization parameters like scale and zero point\ninto the weights.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "qmodel.eval()\nquantized_model = quantizer.finalize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exporting the Model for On-Device Execution\n\nIn order to deploy the model, convert it to a Core ML model.\n\nFollow the same steps in Core ML Tools for exporting a regular PyTorch model\n(for details, see [Converting from PyTorch](https://coremltools.readme.io/docs/pytorch-conversion)).\nThe parameter ``ct.target.iOS17`` is necessary here because activation quantization\nops are only supported on iOS versions >= 17.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import coremltools as ct\n\nexample_input = torch.rand(1, 1, 28, 28)\ntraced_model = torch.jit.trace(quantized_model, example_input)\n\ncoreml_model = ct.convert(\n    traced_model,\n    inputs=[ct.TensorType(shape=example_input.shape)],\n    minimum_deployment_target=ct.target.iOS17,\n)\n\ncoreml_model.save(\"~/.mnist_qat_data/quantized_model.mlpackage\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}