<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>coremltools.converters.mil.mil.ops.defs.recurrent &mdash; coremltools API Reference  documentation</title>
      <link rel="stylesheet" href="../../../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../../_static/graphviz.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../../../../" id="documentation_options" src="../../../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../../../_static/jquery.js"></script>
        <script src="../../../../../../../_static/underscore.js"></script>
        <script src="../../../../../../../_static/doctools.js"></script>
    <script src="../../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../../../../index.html" class="icon icon-home"> coremltools API Reference
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">API Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../source/coremltools.converters.html">Converters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../source/coremltools.models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../source/coremltools.converters.mil.html">MIL Builder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../source/coremltools.converters.mil.input_types.html">MIL Input Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../source/coremltools.converters.mil.mil.ops.defs.html">MIL Ops</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://coremltools.readme.io/docs">Guides and examples</a></li>
<li class="toctree-l1"><a class="reference external" href="https://apple.github.io/coremltools/mlmodel/index.html">Core ML Format Specification</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/apple/coremltools">GitHub</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../index.html">coremltools API Reference</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../../../../index.html">Module code</a> &raquo;</li>
      <li>coremltools.converters.mil.mil.ops.defs.recurrent</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for coremltools.converters.mil.mil.ops.defs.recurrent</h1><div class="highlight"><pre>
<span></span><span class="c1">#  Copyright (c) 2020, Apple Inc. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1">#  Use of this source code is governed by a BSD-3-clause license that can be</span>
<span class="c1">#  found in the LICENSE.txt file or at https://opensource.org/licenses/BSD-3-Clause</span>

<span class="kn">from</span> <span class="nn">._op_reqs</span> <span class="kn">import</span> <span class="n">register_op</span>
<span class="kn">from</span> <span class="nn">coremltools.converters.mil.mil</span> <span class="kn">import</span> <span class="n">Operation</span><span class="p">,</span> <span class="n">types</span>
<span class="kn">from</span> <span class="nn">coremltools.converters.mil.mil.input_type</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">BoolInputType</span><span class="p">,</span>
    <span class="n">DefaultInputs</span><span class="p">,</span>
    <span class="n">FloatInputType</span><span class="p">,</span>
    <span class="n">InputSpec</span><span class="p">,</span>
    <span class="n">TensorInputType</span><span class="p">,</span>
    <span class="n">StringInputType</span>
<span class="p">)</span>


<div class="viewcode-block" id="gru"><a class="viewcode-back" href="../../../../../../../source/coremltools.converters.mil.mil.ops.defs.html#coremltools.converters.mil.mil.ops.defs.recurrent.gru">[docs]</a><span class="nd">@register_op</span><span class="p">(</span><span class="n">doc_str</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">gru</span><span class="p">(</span><span class="n">Operation</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gated recurrent unit (GRU).</span>

<span class="sd">    .. math::</span>
<span class="sd">       r_t = \rm{recurrent\_activation}(W_{ir} x_t + b_{ir} + W_{hr} h_{t-1} + b_{hr})</span>

<span class="sd">    .. math::</span>
<span class="sd">       z_t = \rm{recurrent\_activation}(W_{iz} x_t + b_{iz} + W_{hz} h_(t−1) + b_{hz})</span>

<span class="sd">    .. math::</span>
<span class="sd">       o_t = activation(W_{io} x_t + b_{io} + r_t * W_{ho} h_(t−1) + b_{ho})</span>

<span class="sd">    .. math::</span>
<span class="sd">       h_t = (1 − z_t) * o_t + z_t * h_{(t−1)}</span>

<span class="sd">    Where:</span>

<span class="sd">    * ``W_{ir}``, ``W_{io}``, and ``W_{iz}`` state input-hidden weight for reset, output</span>
<span class="sd">      and update gate, respectively.</span>
<span class="sd">    * ``W_{h[r|o|z]}`` are recurrent weights on hidden state to reset, output, update gate.</span>
<span class="sd">    * ``h_t``  is the hidden state at time ``t``.</span>
<span class="sd">    * ``x_t`` is the input at time ``t``.</span>
<span class="sd">    * ``h_(t-1)`` is the hidden state of the layer at time ``t-1`` or the initial</span>
<span class="sd">      hidden state at time ``0``.</span>
<span class="sd">    * ``r_t``, ``o_t``, and ``z_t`` are the reset, new, and update gates, respectively.</span>
<span class="sd">    * ``*`` is elementwise product.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x: &lt;s, b, I, T&gt; (Required)</span>
<span class="sd">        * ``s`` is the sequence length, ``b`` is the batch size, and ``I`` is the</span>
<span class="sd">          input dimension.</span>

<span class="sd">    initial_h: &lt;b, H, T&gt; (Required)</span>
<span class="sd">        * ``H`` denotes hidden size.</span>

<span class="sd">    weight_ih: const&lt;3*H, I, T&gt; (Required) - Weight matrix</span>
<span class="sd">        * ``weigh_ih = [W_{ir} | W_{io} | W_{iz}]`` where ``[a|b]`` denotes column</span>
<span class="sd">          concatenation and ``[a, b]`` denotes row concatenation. ``W_{ir}``,</span>
<span class="sd">          ``W_{io}``, and ``W_{iz}`` have shape ``(H, I)``.</span>
<span class="sd">        * This is used when direction=&quot;forward&quot; or &quot;reverse&quot;.</span>

<span class="sd">    weight_hh: const&lt;3*H, H, T&gt; (Required) - Weight matrix</span>
<span class="sd">        * ``weight_hh =  [W_{hr} | W_{ho} | W_{hz}]``: ``W_{hr}``, ``W_{ho}``, and</span>
<span class="sd">          ``W_{hz}`` have shape ``(H, H)``.</span>
<span class="sd">        * This is used when direction=&quot;forward&quot; or &quot;reverse&quot;.</span>

<span class="sd">    bias: const&lt;3*H, T&gt; (Optional) [Default all 0s]</span>
<span class="sd">        * ``bias[0]`` are input-hidden and hidden-hidden bias.</span>
<span class="sd">        * ``3*H`` are biases for ``[b_{ir} | b_{io} | b_{hz}]``.</span>
<span class="sd">        * This is used when direction=&quot;forward&quot; or &quot;reverse&quot;.</span>

<span class="sd">    direction: const&lt;str&gt; (Optional) [Default=forward]</span>
<span class="sd">        * Either ``forward`` or ``reverse``.</span>

<span class="sd">    output_sequence: const&lt;bool&gt; (Optional) [Default=False]</span>
<span class="sd">        * Outputs every step if ``True``.</span>

<span class="sd">    recurrent_activation: const&lt;str&gt; (Optional) [Default=sigmoid]</span>
<span class="sd">        * Activation applied on update and reset gate.</span>

<span class="sd">    activation: const&lt;str&gt; (Optional) [Default=tanh]</span>
<span class="sd">        * Activation applied on output gate.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    &lt;s, b, H, T&gt; or &lt;1, b, H, T&gt;</span>
<span class="sd">        * If ``output_sequence == True`` (hidden states from every step):</span>
<span class="sd">          ``&lt;s, b, H, T&gt;``.</span>
<span class="sd">        * Else ``&lt;1, b, H, T&gt;`` (hidden states of the final step).</span>
<span class="sd">    &lt;b, H, T&gt;</span>
<span class="sd">        * Hidden states of the final step.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    T: fp32</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">input_spec</span> <span class="o">=</span> <span class="n">InputSpec</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">TensorInputType</span><span class="p">(),</span>
        <span class="n">initial_h</span><span class="o">=</span><span class="n">TensorInputType</span><span class="p">(),</span>
        <span class="n">weight_ih</span><span class="o">=</span><span class="n">TensorInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">weight_hh</span><span class="o">=</span><span class="n">TensorInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">TensorInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">direction</span><span class="o">=</span><span class="n">StringInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">output_sequence</span><span class="o">=</span><span class="n">BoolInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">recurrent_activation</span><span class="o">=</span><span class="n">StringInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">StringInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">default_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DefaultInputs</span><span class="p">(</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">direction</span><span class="o">=</span><span class="s2">&quot;forward&quot;</span><span class="p">,</span>
            <span class="n">output_sequence</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">recurrent_activation</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">gru</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">type_inference</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">rank</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid input shape. Expecting Rank 3 input, got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">rank</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">input_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_ih</span><span class="o">.</span><span class="n">rank</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid weight shape. Expecting Rank 2 input, got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_ih</span><span class="o">.</span><span class="n">rank</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_hh</span><span class="o">.</span><span class="n">rank</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid weight shape. Expecting Rank 2 input, got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_hh</span><span class="o">.</span><span class="n">rank</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_hh</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">direction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">direction</span><span class="o">.</span><span class="n">val</span>
        <span class="n">valid_directions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;forward&quot;</span><span class="p">,</span> <span class="s2">&quot;reverse&quot;</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">direction</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_directions</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Direction </span><span class="si">{}</span><span class="s2"> not supported. Supported directions: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">direction</span><span class="p">,</span> <span class="n">valid_directions</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="n">dim_factor</span> <span class="o">=</span> <span class="mi">3</span>
        <span class="k">if</span> <span class="n">hidden_size</span> <span class="o">!=</span> <span class="p">(</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">dim_factor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Incorrect weight matrix: hidden dim size mismatch. </span><span class="se">\</span>
<span class="s2">                Provided weight_ih </span><span class="si">{}</span><span class="s2">, weight_hh </span><span class="si">{}</span><span class="s2">. Expecting &lt;b, 3*H&gt;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">weight_ih</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_hh</span><span class="o">.</span><span class="n">shape</span>
            <span class="p">)</span>

        <span class="n">out_seq_len</span> <span class="o">=</span> <span class="n">sequence_length</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_sequence</span><span class="o">.</span><span class="n">val</span> <span class="k">else</span> <span class="mi">1</span>
        <span class="n">output_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">out_seq_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">]</span>
        <span class="n">output_h_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">types</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)),</span>
            <span class="n">types</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">output_h_shape</span><span class="p">)),</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="lstm"><a class="viewcode-back" href="../../../../../../../source/coremltools.converters.mil.mil.ops.defs.html#coremltools.converters.mil.mil.ops.defs.recurrent.lstm">[docs]</a><span class="nd">@register_op</span><span class="p">(</span><span class="n">doc_str</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">lstm</span><span class="p">(</span><span class="n">Operation</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Single long short-term memory (LSTM) sequence.</span>

<span class="sd">    .. math::</span>
<span class="sd">       i_t = \rm{recurrent\_activation}(W_{ii} x_t + B_{ii} + W_{hi} h_(t-1) + B_{hi})</span>

<span class="sd">    .. math::</span>
<span class="sd">       f_t = \rm{recurrent\_activation}(W_{if} x_t + B_{if} + W_{hf} h_(t-1) + B_{hf})</span>

<span class="sd">    .. math::</span>
<span class="sd">       z_t = cell_activation(W_{iz} x_t + B_{iz} + W_{hz} h_(t-1) + B_{hz})</span>

<span class="sd">    .. math::</span>
<span class="sd">       o_t = \rm{recurrent\_activation}(W_{io} x_t + B_{io} + W_{ho} h_(t-1) + B_{ho})</span>

<span class="sd">    .. math::</span>
<span class="sd">       c_t = f_t * c_(t-1) + i_t * z_t</span>

<span class="sd">    .. math::</span>
<span class="sd">       h_t = o_t * activation(c_t)</span>

<span class="sd">    Where:</span>

<span class="sd">    * ``i_t``, ``f_t``, ``o_t``, and ``z_t`` are input, forget, output, and cell gates,</span>
<span class="sd">      respectively, at time ``t``.</span>
<span class="sd">    * ``c_t`` is cell state at time ``t``.</span>
<span class="sd">    * ``h_t``  is the hidden state at time ``t``.</span>
<span class="sd">    * ``W_{ii}``, ``W_{if}``, ``W_{io}``, and ``W_{iz}`` are input weights for input,</span>
<span class="sd">      forget, output and cell gate, respectively.</span>
<span class="sd">    * ``W_{hi}``, ``W_{hf}``, ``W_{ho}``, and ``W_{hz}`` are recurrent weights for input,</span>
<span class="sd">      forget, output and cell gate, respectively.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x: &lt;s, b, I, T&gt; (Required)</span>
<span class="sd">        * ``s`` is the sequence length, ``b`` is the batch size, and ``I`` is the</span>
<span class="sd">          input dimension.</span>

<span class="sd">    initial_h: &lt;b, DIRECTION*H, T&gt; (Required)</span>
<span class="sd">        * Initial hidden state. ``DIRECTION = 1`` for uni-directional, ``2`` for</span>
<span class="sd">          bi-directional LSTM.</span>
<span class="sd">        * ``H`` denotes hidden size.</span>
<span class="sd">        * ``[b, :H]`` and ``[b, H:]`` represents forward and reverse direction</span>
<span class="sd">          values, respectively.</span>

<span class="sd">    initial_c: &lt;b, DIRECTION*H, T&gt; (Required)</span>
<span class="sd">        * Initial cell state.</span>
<span class="sd">        * Format is same as ``initial_h``.</span>

<span class="sd">    weight_ih: const&lt;4*H, I, T&gt; (Required)</span>
<span class="sd">        * Input-hidden weight matrix</span>
<span class="sd">        * Weight tensor should be in order of</span>
<span class="sd">          ``[input_gate, forget_gate, output_gate, cell_gate]``.</span>
<span class="sd">        * If direction==&quot;bidirectional&quot;, this is applied in forward direction.</span>
<span class="sd">        * If direction==&quot;forward&quot; or &quot;backward&quot; these weights are used.</span>

<span class="sd">    weight_hh: const&lt;4*H, H, T&gt; (Required)</span>
<span class="sd">        * Hidden-hidden weight matrix.</span>
<span class="sd">        * Weight tensor should be in order of</span>
<span class="sd">          ``[input_gate, forget_gate, output_gate, cell_gate]``.</span>
<span class="sd">        * If direction==&quot;bidirectional&quot;, this is applied in forward direction.</span>
<span class="sd">        * If direction==&quot;forward&quot; or &quot;backward&quot; these weights are used.</span>

<span class="sd">    bias: const&lt;4*H, T&gt; (Optional) [Default all 0s]</span>
<span class="sd">        * bias = input-hidden bias + hidden-hidden bias</span>
<span class="sd">        * If direction==&quot;bidirectional&quot;, this is applied in forward direction.</span>
<span class="sd">        * If direction==&quot;forward&quot; or &quot;backward&quot; this bias are used.</span>

<span class="sd">    peephole: const&lt;3*H, T&gt; (Optional, default to 0)</span>
<span class="sd">        * Weight tensor for peephole.</span>
<span class="sd">        * Order is ``[input_gate, forget_gate, output_gate]``.</span>
<span class="sd">        * Shape of each peephole vector is ``(H,)`` (``H`` is hidden size).</span>
<span class="sd">        * If direction==&quot;bidirectional&quot;, this is applied in forward direction.</span>
<span class="sd">        * If direction==&quot;forward&quot; or &quot;backward&quot; these weights are used.</span>

<span class="sd">    weight_ih_back: const&lt;4*H, I, T&gt; (Optional) -</span>
<span class="sd">        * Input-hidden weight matrix for backward direction for `bidirectinal LSTM`.</span>
<span class="sd">        * Weight tensor should be in order of</span>
<span class="sd">          ``[input_gate, forget_gate, output_gate, cell_gate]``.</span>
<span class="sd">        * Must be provided for `bidirectional LSTM`.</span>
<span class="sd">        * This is only used when `direction` is &quot;bidirectional&quot;.</span>
<span class="sd">        * For direction=&quot;reverse&quot; use `weight_ih` instead.</span>

<span class="sd">    weight_hh_back: const&lt;4*H, H, T&gt; (Optional) - Hidden-hidden weight matrix</span>
<span class="sd">        * Hidden-hidden weight matrix for backward direction for `bidirectinal LSTM`.</span>
<span class="sd">        * Weight tensor should be in order of</span>
<span class="sd">          ``[input_gate, forget_gate, output_gate, cell_gate]``.</span>
<span class="sd">        * Must be provided for `bidirectional LSTM`.</span>
<span class="sd">        * This is only used when `direction` is &quot;bidirectional&quot;.</span>
<span class="sd">        * For direction=&quot;reverse&quot; use `weight_hh` instead.</span>

<span class="sd">    bias_back: const&lt;4*H, T&gt; (Optional) [Default all 0s]</span>
<span class="sd">        * bias = input-hidden bias + hidden-hidden bias.</span>
<span class="sd">        * Bias of backward direction for `bidirectional lstm`</span>
<span class="sd">        * This is only used when `direction` is &quot;bidirectional&quot;.</span>
<span class="sd">        * For direction=&quot;reverse&quot; use `bias` instead.</span>

<span class="sd">    peephole_back: const&lt;3*H, T&gt; (Optional, default to 0)</span>
<span class="sd">        * Weight tensor for peephole in backward direction for `bidirectional LSTM`.</span>
<span class="sd">        * Order is ``[input_gate, forget_gate, output_gate]``.</span>
<span class="sd">        * Shape of each peephole vector is ``(H,)`` (``H`` is hidden size).</span>
<span class="sd">        * Peephole of backward direction for `bidirectional lstm`</span>
<span class="sd">        * Bias of backward direction for `bidirectional lstm`</span>
<span class="sd">        * This is only used when `direction` is &quot;bidirectional&quot;.</span>
<span class="sd">        * For direction=&quot;reverse&quot; use `peephole` instead.</span>

<span class="sd">    direction: const&lt;str&gt; (Optional) [Default=forward]</span>
<span class="sd">        * One of the following: ``forward``, ``reverse``, or ``bidirectional``.</span>
<span class="sd">        * Must match ``DIRECTIONAL`` in initial states and weight parameters.</span>

<span class="sd">    output_sequence: const&lt;bool&gt; (Optional) [Default=False]</span>
<span class="sd">        * Outputs every step if ``True``.</span>

<span class="sd">    recurrent_activation: const&lt;str&gt; (Optional) [Default=sigmoid]</span>
<span class="sd">        * Activation applied on input, forget, and output gates.</span>

<span class="sd">    cell_activation: const&lt;str&gt; (Optional) [Default=tang]</span>
<span class="sd">        * Activation applied on cell gate.</span>

<span class="sd">    activation: const&lt;str&gt; (Optional) [Default=tanh]</span>
<span class="sd">        * Activation applied on output gate.</span>

<span class="sd">    clip: const&lt;fp32&gt; (optional) [Default=None]</span>
<span class="sd">        * Cell gate is clipped to ``[-clip, +clip]``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    &lt;s, b, DIRECTION*H, T&gt; or &lt;1, b, DIRECTION*H, T&gt;</span>
<span class="sd">        * If ``output_sequence == True`` (hidden states from every step):</span>
<span class="sd">          ``&lt;s, b, DIRECTION*H, T&gt;``.</span>
<span class="sd">        * Else ``&lt;1, b, DIRECTION*H, T&gt;`` (hidden states of the final step).</span>
<span class="sd">    &lt;b, DIRECTION*H, T&gt;</span>
<span class="sd">        * Hidden states of the final step.</span>
<span class="sd">    &lt;b, DIRECTION*H, T&gt;</span>
<span class="sd">        * Memory state of the final step.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    T: fp32</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">input_spec</span> <span class="o">=</span> <span class="n">InputSpec</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">TensorInputType</span><span class="p">(),</span>
        <span class="n">initial_h</span><span class="o">=</span><span class="n">TensorInputType</span><span class="p">(),</span>
        <span class="n">initial_c</span><span class="o">=</span><span class="n">TensorInputType</span><span class="p">(),</span>
        <span class="n">weight_ih</span><span class="o">=</span><span class="n">TensorInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>  <span class="c1"># ifoz layout,</span>
        <span class="n">weight_hh</span><span class="o">=</span><span class="n">TensorInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>  <span class="c1"># ifoz layout</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">TensorInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>  <span class="c1"># ifoz layout</span>
        <span class="n">peephole</span><span class="o">=</span><span class="n">TensorInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>  <span class="c1"># ifo layout</span>
        <span class="n">weight_ih_back</span><span class="o">=</span><span class="n">TensorInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>  <span class="c1"># ifoz layout,</span>
        <span class="n">weight_hh_back</span><span class="o">=</span><span class="n">TensorInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>  <span class="c1"># ifoz layout</span>
        <span class="n">bias_back</span><span class="o">=</span><span class="n">TensorInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>  <span class="c1"># ifoz layout</span>
        <span class="n">peephole_back</span><span class="o">=</span><span class="n">TensorInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>  <span class="c1"># ifo layout</span>
        <span class="n">direction</span><span class="o">=</span><span class="n">StringInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">output_sequence</span><span class="o">=</span><span class="n">BoolInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">recurrent_activation</span><span class="o">=</span><span class="n">StringInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">cell_activation</span><span class="o">=</span><span class="n">StringInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">StringInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">clip</span><span class="o">=</span><span class="n">FloatInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">default_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DefaultInputs</span><span class="p">(</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">direction</span><span class="o">=</span><span class="s2">&quot;forward&quot;</span><span class="p">,</span>
            <span class="n">output_sequence</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">recurrent_activation</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">,</span>
            <span class="n">cell_activation</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">,</span>
            <span class="n">peephole</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">clip</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">lstm</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">type_inference</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">rank</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid input shape. Expecting Rank 3 input, got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">rank</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">input_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">def</span> <span class="nf">weight_shape_check</span><span class="p">(</span><span class="n">wt_ih</span><span class="p">,</span> <span class="n">wt_hh</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">wt_ih</span><span class="o">.</span><span class="n">rank</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">wt_hh</span><span class="o">.</span><span class="n">rank</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Expecting Rank 2 input, got weight_ih rank: </span><span class="si">{}</span><span class="s2">, weight_hh rank: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">wt_ih</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span> <span class="n">wt_hh</span><span class="o">.</span><span class="n">rank</span>
                    <span class="p">)</span>

            <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">wt_hh</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">wt_hh</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">hidden_size</span> <span class="o">!=</span> <span class="mi">4</span> <span class="ow">or</span> <span class="n">wt_ih</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">hidden_size</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Incorrect weight matrix: hidden dim size mismatch. </span><span class="se">\</span>
<span class="s2">                                Provided weight_ih </span><span class="si">{}</span><span class="s2">, weight_hh </span><span class="si">{}</span><span class="s2">. Expecting &lt;4*H, H&gt;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                                    <span class="n">wt_ih</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">wt_hh</span><span class="o">.</span><span class="n">shape</span>
                <span class="p">)</span>

        <span class="n">direction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">direction</span><span class="o">.</span><span class="n">val</span>
        <span class="n">valid_directions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;forward&quot;</span><span class="p">,</span> <span class="s2">&quot;reverse&quot;</span><span class="p">,</span> <span class="s2">&quot;bidirectional&quot;</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">direction</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_directions</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Direction </span><span class="si">{}</span><span class="s2"> not supported. Supported directions: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">direction</span><span class="p">,</span> <span class="n">valid_directions</span>
            <span class="p">)</span>

        <span class="n">weight_shape_check</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_ih</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_hh</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">direction</span> <span class="o">==</span> <span class="s2">&quot;bidirectional&quot;</span><span class="p">:</span>
            <span class="n">weight_shape_check</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_ih_back</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_hh_back</span><span class="p">)</span>

        <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_hh</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">dim_factor</span> <span class="o">=</span> <span class="mi">8</span> <span class="k">if</span> <span class="n">direction</span> <span class="o">==</span> <span class="s2">&quot;bidirectional&quot;</span> <span class="k">else</span> <span class="mi">4</span>
        <span class="n">out_seq_len</span> <span class="o">=</span> <span class="n">sequence_length</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_sequence</span><span class="o">.</span><span class="n">val</span> <span class="k">else</span> <span class="mi">1</span>
        <span class="n">num_directions</span> <span class="o">=</span> <span class="n">dim_factor</span> <span class="o">//</span> <span class="mi">4</span>
        <span class="n">output_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">out_seq_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_directions</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">]</span>
        <span class="n">output_h_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_directions</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">]</span>
        <span class="n">output_c_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_directions</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">types</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)),</span>
            <span class="n">types</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">output_h_shape</span><span class="p">)),</span>
            <span class="n">types</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">output_c_shape</span><span class="p">)),</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="rnn"><a class="viewcode-back" href="../../../../../../../source/coremltools.converters.mil.mil.ops.defs.html#coremltools.converters.mil.mil.ops.defs.recurrent.rnn">[docs]</a><span class="nd">@register_op</span><span class="p">(</span><span class="n">doc_str</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">rnn</span><span class="p">(</span><span class="n">Operation</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Recurrent neural network (RNN).</span>

<span class="sd">    .. math::</span>
<span class="sd">       h_t = activation(W_{ih} x_t + b_{ih} + W_{hh} h_(t−1) + b_{hh})</span>

<span class="sd">    Where:</span>

<span class="sd">    * ``W_{ih}`` is input weight.</span>
<span class="sd">    * ``W_{hh}`` is hidden/recurrent weight.</span>
<span class="sd">    * ``h_t``  is the hidden state at time ``t``.</span>
<span class="sd">    * ``x_t`` is the input at time ``t``.</span>
<span class="sd">    * ``h_(t-1)`` is the hidden state of the layer at time ``t-1`` or the initial</span>
<span class="sd">      hidden state at time ``0``.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x: &lt;s, b, I, T&gt; (Required)</span>
<span class="sd">        * ``s`` is the sequence length, ``b`` is the batch size, and ``I`` is the</span>
<span class="sd">          input dimension.</span>

<span class="sd">    initial_h: &lt;b, H, T&gt; (Required)</span>
<span class="sd">        * ``H`` denotes hidden size.</span>

<span class="sd">    weight_ih: const&lt;H, I, T&gt; (Required) - Input-hidden weight matrix</span>

<span class="sd">    weight_hh: const&lt;H, H, T&gt; (Required) - Hidden-hidden weight matrix</span>

<span class="sd">    bias: const&lt;H, T&gt; (Optional) [Default all 0s]</span>
<span class="sd">        * bias for input-hidden and hidden-hidden</span>

<span class="sd">    direction: const&lt;str&gt; (Optional) [Default=forward]</span>
<span class="sd">        * Either ``forward`` or ``reverse``.</span>

<span class="sd">    output_sequence: const&lt;bool&gt; (Optional) [Default=False]</span>
<span class="sd">        * Outputs every step if ``True``.</span>

<span class="sd">    activation: const&lt;str&gt; (Optional) [Default=tanh]</span>
<span class="sd">        * Supported activation functions: ``relu``, ``tanh``, ``sigmoid``,</span>
<span class="sd">          ``sigmoid_hard``, ``scaled_tanh``, and ``linear``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    &lt;s, b, H, T&gt; or &lt;1, b, H, T&gt;</span>
<span class="sd">        * If ``output_sequence == True`` (hidden states from every step):</span>
<span class="sd">          ``&lt;s, b, H, T&gt;``.</span>
<span class="sd">        * Else ``&lt;1, b, H, T&gt;`` (hidden states of the final step).</span>
<span class="sd">    &lt;b, H, T&gt;</span>
<span class="sd">        * Hidden states of the final step.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    T: fp32</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">input_spec</span> <span class="o">=</span> <span class="n">InputSpec</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">TensorInputType</span><span class="p">(),</span>
        <span class="n">initial_h</span><span class="o">=</span><span class="n">TensorInputType</span><span class="p">(),</span>
        <span class="n">weight_ih</span><span class="o">=</span><span class="n">TensorInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">weight_hh</span><span class="o">=</span><span class="n">TensorInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">TensorInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">direction</span><span class="o">=</span><span class="n">StringInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">output_sequence</span><span class="o">=</span><span class="n">BoolInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">StringInputType</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">default_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DefaultInputs</span><span class="p">(</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">direction</span><span class="o">=</span><span class="s2">&quot;forward&quot;</span><span class="p">,</span>
            <span class="n">output_sequence</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">rnn</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">type_inference</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">rank</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid input shape. Expecting Rank 3 input, got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">rank</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">input_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_ih</span><span class="o">.</span><span class="n">rank</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_hh</span><span class="o">.</span><span class="n">rank</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid weight shape. Expecting Rank 2 input, got weight_ih </span><span class="si">{}</span><span class="s2">, weight_hh </span><span class="si">{}</span><span class="s2">&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">weight_ih</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_hh</span><span class="o">.</span><span class="n">rank</span>
                <span class="p">)</span>

        <span class="n">hidden_size</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_ih</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">direction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">direction</span><span class="o">.</span><span class="n">val</span>
        <span class="n">valid_directions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;forward&quot;</span><span class="p">,</span> <span class="s2">&quot;reverse&quot;</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">direction</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_directions</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Direction </span><span class="si">{}</span><span class="s2"> not supported. Supported directions: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">direction</span><span class="p">,</span> <span class="n">valid_directions</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="n">out_seq_len</span> <span class="o">=</span> <span class="n">sequence_length</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_sequence</span><span class="o">.</span><span class="n">val</span> <span class="k">else</span> <span class="mi">1</span>
        <span class="n">output_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">out_seq_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">]</span>
        <span class="n">output_h_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">types</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)),</span>
            <span class="n">types</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">output_h_shape</span><span class="p">)),</span>
        <span class="p">)</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Apple Inc.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>